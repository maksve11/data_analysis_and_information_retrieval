{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ea9fb4",
   "metadata": {},
   "source": [
    "## Задание:\n",
    "1. Выбрать тематику для поиска (например, “machine learning”) и использовать arXiv API для извлечения статей. Выгрузить названия, авторов и аннотации статей.\n",
    "2. Разбиение на чанки. Разделить текст аннотаций на чанки по 500 слов с перекрытием в 50 слов для последующей обработки.\n",
    "3. Векторизация текста. Использовать модель эмбеддингов (например, ‘all-MiniLM-L6-v2’) для преобразования текстов в векторное представление. Построить векторное хранилище с помощью FAISS.\n",
    "4. Интеграция с LLM. Настроить взаимодействие с LLM (например, FLAN-T5 или OpenAI GPT). Использовать промпт для получения ключевых идей статьи или ответа на конкретный вопрос пользователя.\n",
    "5. Поиск по корпусу. Настроить промпт-инжиниринг для извлечения ключевых идей и результатов из текста. Добавить обработку опечаток в пользовательских запросах с использованием библиотек типа SymSpell или rapidfuzz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb19be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (6.0.11)\n",
      "Requirement already satisfied: pandas in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: fastparquet in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (2024.11.0)\n",
      "Requirement already satisfied: bs4 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (0.0.2)\n",
      "Requirement already satisfied: pypdf2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (3.0.1)\n",
      "Requirement already satisfied: pdfplumber in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (0.11.5)\n",
      "Requirement already satisfied: sgmllib3k in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from fastparquet) (2.9.1)\n",
      "Requirement already satisfied: fsspec in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from fastparquet) (2025.2.0)\n",
      "Requirement already satisfied: packaging in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from beautifulsoup4->bs4) (4.12.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser pandas fastparquet bs4 pypdf2 pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e996cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import urllib.request as libreq\n",
    "import feedparser\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import pdfplumber\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d88a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = \"math.ST\"\n",
    "RESULTS = 260  \n",
    "BASE_URL = \"http://export.arxiv.org/api/query?\"\n",
    "BATCH_SIZE = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20621d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    \"\"\"Ищем ключевые слова в тексте.\"\"\"\n",
    "    keywords_match = re.search(r'(?i)(?:keywords?|KEYWORDS?):\\s*([^\\n]+)', text, re.IGNORECASE | re.DOTALL)\n",
    "    if keywords_match:\n",
    "        return [kw.strip() for kw in keywords_match.group(1).split(',')]\n",
    "    return None\n",
    "\n",
    "def fetch_keywords_from_pdf(pdf_url):\n",
    "    \"\"\"Скачиваем PDF, извлекаем текст, ищем Keywords.\"\"\"\n",
    "    try:\n",
    "        with libreq.urlopen(pdf_url) as response:\n",
    "            pdf_data = response.read()\n",
    "\n",
    "        with pdfplumber.open(io.BytesIO(pdf_data)) as pdf:\n",
    "            full_text = \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "\n",
    "        return extract_keywords(full_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка скачивания/парсинга PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_arxiv_papers(category, max_results=RESULTS, batch_size=BATCH_SIZE):\n",
    "    papers = []\n",
    "\n",
    "    def process_paper(entry):\n",
    "        abs_url = entry.id\n",
    "        pdf_url = abs_url.replace(\"abs\", \"pdf\")  \n",
    "        keywords = fetch_keywords_from_pdf(pdf_url) \n",
    "\n",
    "        return {\n",
    "            \"title\": entry.title,\n",
    "            \"authors\": [author.name for author in entry.authors],\n",
    "            \"summary\": entry.summary,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for start in range(0, max_results, batch_size):\n",
    "            query = f\"search_query=cat:{category}&start={start}&max_results={batch_size}&sortBy=submittedDate&sortOrder=descending\"\n",
    "            url = BASE_URL + query\n",
    "\n",
    "            with libreq.urlopen(url) as response:\n",
    "                data = response.read()\n",
    "\n",
    "            feed = feedparser.parse(data)\n",
    "\n",
    "            results = list(executor.map(process_paper, feed.entries))\n",
    "\n",
    "            papers.extend(results)\n",
    "\n",
    "            time.sleep(1)  \n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad45e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка скачивания/парсинга PDF: HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "papers = fetch_arxiv_papers(CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ccce1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Nonparametric local polynomial regression for functional covariates',\n",
       "  'authors': ['Moritz Jirak',\n",
       "   'Alois Kneip',\n",
       "   'Alexander Meister',\n",
       "   'Mario Pahl'],\n",
       "  'summary': 'We consider nonparametric regression with functional covariates, that is,\\nthey are elements of an infinite-dimensional Hilbert space. A locally\\npolynomial estimator is constructed, where an orthonormal basis and various\\ntuning parameters remain to be selected. We provide a general asymptotic upper\\nbound on the estimation error and show that this procedure achieves polynomial\\nconvergence rates under appropriate tuning and supersmoothness of the\\nregression function. Such polynomial convergence rates have usually been\\nconsidered to be non-attainable in nonparametric functional regression without\\nany additional strong structural constraints such as linearity of the\\nregression function.',\n",
       "  'keywords': ['functional data analysis; kernel estimation; nonparametric regression; polyno-']},\n",
       " {'title': 'Revisiting poverty measures using quantile functions',\n",
       "  'authors': ['N. Unnikrishnan Nair', 'S. M. Sunoj'],\n",
       "  'summary': 'In this article we redefine various poverty measures in literature in terms\\nof quantile functions instead of distribution functions in the prevailing\\napproach. This enables provision for alternative methodology for poverty\\nmeasurement and analysis along with some new results that are difficult to\\nobtain in the existing framework. Several flexible quantile function models\\nthat can enrich the existing ones are proposed and their utility is\\ndemonstrated for real data.',\n",
       "  'keywords': ['Poverty measures',\n",
       "   'Sen index',\n",
       "   'quantile function',\n",
       "   'income models.']},\n",
       " {'title': 'Online Bernstein-von Mises theorem',\n",
       "  'authors': ['Jeyong Lee', 'Junhyeok Choi', 'Minwoo Chae'],\n",
       "  'summary': 'Online learning is an inferential paradigm in which parameters are updated\\nincrementally from sequentially available data, in contrast to batch learning,\\nwhere the entire dataset is processed at once. In this paper, we assume that\\nmini-batches from the full dataset become available sequentially. The Bayesian\\nframework, which updates beliefs about unknown parameters after observing each\\nmini-batch, is naturally suited for online learning. At each step, we update\\nthe posterior distribution using the current prior and new observations, with\\nthe updated posterior serving as the prior for the next step. However, this\\nrecursive Bayesian updating is rarely computationally tractable unless the\\nmodel and prior are conjugate. When the model is regular, the updated posterior\\ncan be approximated by a normal distribution, as justified by the Bernstein-von\\nMises theorem. We adopt a variational approximation at each step and\\ninvestigate the frequentist properties of the final posterior obtained through\\nthis sequential procedure. Under mild assumptions, we show that the accumulated\\napproximation error becomes negligible once the mini-batch size exceeds a\\nthreshold depending on the parameter dimension. As a result, the sequentially\\nupdated posterior is asymptotically indistinguishable from the full posterior.',\n",
       "  'keywords': None},\n",
       " {'title': 'Debiasing Continuous-time Nonlinear Autoregressions',\n",
       "  'authors': ['Simon Kuang', 'Xinfan Lin'],\n",
       "  'summary': 'We study how to identify a class of continuous-time nonlinear systems defined\\nby an ordinary differential equation affine in the unknown parameter. We define\\na notion of asymptotic consistency as $(n, h) \\\\to (\\\\infty, 0)$, and we achieve\\nit using a family of direct methods where the first step is differentiating a\\nnoisy time series and the second step is a plug-in linear estimator. The first\\nstep, differentiation, is a signal processing adaptation of the nonparametric\\nstatistical technique of local polynomial regression. The second step,\\ngeneralized linear regression, can be consistent using a least squares\\nestimator, but we demonstrate two novel bias corrections that improve the\\naccuracy for finite $h$. These methods significantly broaden the class of\\ncontinuous-time systems that can be consistently estimated by direct methods.',\n",
       "  'keywords': ['systemidentification']},\n",
       " {'title': 'A Generalized Tangent Approximation Framework for Strongly\\n  Super-Gaussian Likelihoods',\n",
       "  'authors': ['Somjit Roy', 'Pritam Dey', 'Debdeep Pati', 'Bani K. Mallick'],\n",
       "  'summary': 'Tangent approximation form a popular class of variational inference (VI)\\ntechniques for Bayesian analysis in intractable non-conjugate models. It is\\nbased on the principle of convex duality to construct a minorant of the\\nmarginal likelihood, making the problem tractable. Despite its extensive\\napplications, a general methodology for tangent approximation encompassing a\\nlarge class of likelihoods beyond logit models with provable optimality\\nguarantees is still elusive. In this article, we propose a general Tangent\\nApproximation based Variational InferencE (TAVIE) framework for strongly\\nsuper-Gaussian (SSG) likelihood functions which includes a broad class of\\nflexible probability models. Specifically, TAVIE obtains a quadratic lower\\nbound of the corresponding log-likelihood, thus inducing conjugacy with\\nGaussian priors over the model parameters. Under mild assumptions on the\\ndata-generating process, we demonstrate the optimality of our proposed\\nmethodology in the fractional likelihood setup. Furthermore, we illustrate the\\nempirical performance of TAVIE through extensive simulations and an application\\non the U.S. 2000 Census real data.',\n",
       "  'keywords': None},\n",
       " {'title': 'Dimension-Free Convergence of Diffusion Models for Approximate Gaussian\\n  Mixtures',\n",
       "  'authors': ['Gen Li', 'Changxiao Cai', 'Yuting Wei'],\n",
       "  'summary': 'Diffusion models are distinguished by their exceptional generative\\nperformance, particularly in producing high-quality samples through iterative\\ndenoising. While current theory suggests that the number of denoising steps\\nrequired for accurate sample generation should scale linearly with data\\ndimension, this does not reflect the practical efficiency of widely used\\nalgorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper\\ninvestigates the effectiveness of diffusion models in sampling from complex\\nhigh-dimensional distributions that can be well-approximated by Gaussian\\nMixture Models (GMMs). For these distributions, our main result shows that DDPM\\ntakes at most $\\\\widetilde{O}(1/\\\\varepsilon)$ iterations to attain an\\n$\\\\varepsilon$-accurate distribution in total variation (TV) distance,\\nindependent of both the ambient dimension $d$ and the number of components $K$,\\nup to logarithmic factors. Furthermore, this result remains robust to score\\nestimation errors. These findings highlight the remarkable effectiveness of\\ndiffusion models in high-dimensional settings given the universal approximation\\ncapability of GMMs, and provide theoretical insights into their practical\\nsuccess.',\n",
       "  'keywords': ['diffusion models',\n",
       "   'Gaussian mixture models',\n",
       "   'DDPM',\n",
       "   'generative modeling',\n",
       "   'dimension-free con-']},\n",
       " {'title': 'DDPM Score Matching and Distribution Learning',\n",
       "  'authors': ['Sinho Chewi',\n",
       "   'Alkis Kalavasis',\n",
       "   'Anay Mehrotra',\n",
       "   'Omar Montasser'],\n",
       "  'summary': 'Score estimation is the backbone of score-based generative models (SGMs),\\nespecially denoising diffusion probabilistic models (DDPMs). A key result in\\nthis area shows that with accurate score estimates, SGMs can efficiently\\ngenerate samples from any realistic data distribution (Chen et al., ICLR\\'23;\\nLee et al., ALT\\'23). This distribution learning result, where the learned\\ndistribution is implicitly that of the sampler\\'s output, does not explain how\\nscore estimation relates to classical tasks of parameter and density\\nestimation.\\n  This paper introduces a framework that reduces score estimation to these two\\ntasks, with various implications for statistical and computational learning\\ntheory:\\n  Parameter Estimation: Koehler et al. (ICLR\\'23) demonstrate that a\\nscore-matching variant is statistically inefficient for the parametric\\nestimation of multimodal densities common in practice. In contrast, we show\\nthat under mild conditions, denoising score-matching in DDPMs is asymptotically\\nefficient.\\n  Density Estimation: By linking generation to score estimation, we lift\\nexisting score estimation guarantees to $(\\\\epsilon,\\\\delta)$-PAC density\\nestimation, i.e., a function approximating the target log-density within\\n$\\\\epsilon$ on all but a $\\\\delta$-fraction of the space. We provide (i) minimax\\nrates for density estimation over H\\\\\"older classes and (ii) a quasi-polynomial\\nPAC density estimation algorithm for the classical Gaussian location mixture\\nmodel, building on and addressing an open problem from Gatmiry et al.\\n(arXiv\\'24).\\n  Lower Bounds for Score Estimation: Our framework offers the first principled\\nmethod to prove computational lower bounds for score estimation across general\\ndistributions. As an application, we establish cryptographic lower bounds for\\nscore estimation in general Gaussian mixture models, conceptually recovering\\nSong\\'s (NeurIPS\\'24) result and advancing his key open problem.',\n",
       "  'keywords': None},\n",
       " {'title': 'Detecting relevant dependencies under measurement error with\\n  applications to the analysis of planetary system evolution',\n",
       "  'authors': ['Patrick Bastian', 'Nicolai Bissantz'],\n",
       "  'summary': 'Exoplanets play an important role in understanding the mechanics of planetary\\nsystem formation and orbital evolution. In this context the correlations of\\ndifferent parameters of the planets and their host star are useful guides in\\nthe search for explanatory mechanisms. Based on a reanalysis of the data set\\nfrom \\\\cite{figueria14} we study the as of now still poorly understood\\ncorrelation between planetary surface gravity and stellar activity of Hot\\nJupiters. Unfortunately, data collection often suffers from measurement errors\\ndue to complicated and indirect measurement setups, rendering standard\\ninference techniques unreliable.\\n  We present new methods to estimate and test for correlations in a\\ndeconvolution framework and thereby improve the state of the art analysis of\\nthe data in two directions. First, we are now able to account for additive\\nmeasurement errors which facilitates reliable inference. Second we test for\\nrelevant changes, i.e. we are testing for correlations exceeding a certain\\nthreshold $\\\\Delta$. This reflects the fact that small nonzero correlations are\\nto be expected for real life data almost always and that standard statistical\\ntests will therefore always reject the null of no correlation given sufficient\\ndata. Our theory focuses on quantities that can be estimated by U-Statistics\\nwhich contain a variety of correlation measures. We propose a bootstrap test\\nand establish its theoretical validity. As a by product we also obtain\\nconfidence intervals. Applying our methods to the Hot Jupiter data set from\\n\\\\cite{figueria14}, we observe that taking into account the measurement errors\\nyields smaller point estimates and the null of no relevant correlation is\\nrejected only for very small $\\\\Delta$. This demonstrates the importance of\\nconsidering the impact of measurement errors to avoid misleading conclusions\\nfrom the resulting statistical analysis.',\n",
       "  'keywords': None},\n",
       " {'title': 'SurvSurf: a partially monotonic neural network for first-hitting time\\n  prediction of intermittently observed discrete and continuous sequential\\n  events',\n",
       "  'authors': ['Yichen Kelly Chen',\n",
       "   'Sören Dittmer',\n",
       "   'Kinga Bernatowicz',\n",
       "   'Josep Arús-Pous',\n",
       "   'Kamen Bliznashki',\n",
       "   'John Aston',\n",
       "   'James H. F. Rudd',\n",
       "   'Carola-Bibiane Schönlieb',\n",
       "   'James Jones',\n",
       "   'Michael Roberts'],\n",
       "  'summary': 'We propose a neural-network based survival model (SurvSurf) specifically\\ndesigned for direct and simultaneous probabilistic prediction of the first\\nhitting time of sequential events from baseline. Unlike existing models,\\nSurvSurf is theoretically guaranteed to never violate the monotonic\\nrelationship between the cumulative incidence functions of sequential events,\\nwhile allowing nonlinear influence from predictors. It also incorporates\\nimplicit truths for unobserved intermediate events in model fitting, and\\nsupports both discrete and continuous time and events. We also identified a\\nvariant of the Integrated Brier Score (IBS) that showed robust correlation with\\nthe mean squared error (MSE) between the true and predicted probabilities by\\naccounting for implied truths about the missing intermediate events. We\\ndemonstrated the superiority of SurvSurf compared to modern and traditional\\npredictive survival models in two simulated datasets and two real-world\\ndatasets, using MSE, the more robust IBS and by measuring the extent of\\nmonotonicity violation.',\n",
       "  'keywords': None},\n",
       " {'title': 'Truncated sequential guaranteed estimation for the Cox-Ingersoll-Ross\\n  models',\n",
       "  'authors': ['Mohamed Ben Alaya',\n",
       "   'Thi-Bao Trâm Ngô',\n",
       "   'Serguei Pergamenchtchikov'],\n",
       "  'summary': 'The drift sequential parameter estimation problems for the Cox-Ingersoll-Ross\\n(CIR) processes under the limited duration of observation are studied.\\nTruncated sequential estimation methods for both scalar and {two}-dimensional\\nparameter cases are proposed. In the non-asymptotic setting, for the proposed\\ntruncated estimators, the properties of guaranteed mean-square estimation\\naccuracy are established. In the asymptotic formulation, when the observation\\ntime tends to infinity, it is shown that the proposed sequential procedures are\\nasymptotically optimal among all possible sequential and non-sequential\\nestimates with an average estimation time less than the fixed observation\\nduration. It also turned out that asymptotically, without degrading the\\nestimation quality, they significantly reduce the observation duration compared\\nto classical non-sequential maximum likelihood estimations based on a fixed\\nobservation duration.',\n",
       "  'keywords': None},\n",
       " {'title': \"Extension of Yager's negation of probability distribution based on\\n  uncertainty measures\",\n",
       "  'authors': ['Santosh Kumar Chaudhary', 'Pradeep Kumar Sahu', 'Nitin Gupta'],\n",
       "  'summary': 'Existing research on negations primarily focuses on entropy and extropy.\\nRecently, new functions such as varentropy and varextropy have been developed,\\nwhich can be considered as\\n  extensions of entropy and extropy. However, the impact of negation on these\\nextended measures, particularly varentropy and varextropy, has not been\\nextensively explored. To address\\n  this gap, this paper investigates the effect of negation on Shannon entropy,\\nvarentropy, and varextropy. We explore how the negation of a probability\\ndistribution influences these\\n  measures, showing that the negated distribution consistently leads to higher\\nvalues of Shannon entropy, varentropy, and varextropy compared to the original\\ndistribution.\\n  Additionally, we prove that the negation of a probability distribution\\nmaximizes these measures during the process. The paper provides theoretical\\nproofs and a detailed analysis of\\n  the behaviour of these measures, contributing to a better understanding of\\nthe interplay between probability distributions, negation, and\\ninformation-theoretic quantities.',\n",
       "  'keywords': ['Extropy', 'Entropy', 'Varextropy', 'Varextropy', 'Negation.']},\n",
       " {'title': 'Gaussian Mean Testing under Truncation',\n",
       "  'authors': ['Clément L. Canonne',\n",
       "   'Themis Gouleakis',\n",
       "   'Yuhao Wang',\n",
       "   'Joy Qiping Yang'],\n",
       "  'summary': 'We consider the task of Gaussian mean testing, that is, of testing whether a\\nhigh-dimensional vector perturbed by white noise has large magnitude, or is the\\nzero vector. This question, originating from the signal processing community,\\nhas recently seen a surge of interest from the machine learning and theoretical\\ncomputer science community, and is by now fairly well understood. What is much\\nless understood, and the focus of our work, is how to perform this task under\\ntruncation: that is, when the observations (i.i.d.\\\\ samples from the underlying\\nhigh-dimensional Gaussian) are only observed when they fall in an given subset\\nof the domain $\\\\R^d$. This truncation model, previously studied in the context\\nof learning (instead of testing) the mean vector, has a range of applications,\\nin particular in Economics and Social Sciences. As our work shows, sample\\ntruncations affect the complexity of the testing task in a rather subtle and\\nsurprising way.',\n",
       "  'keywords': None},\n",
       " {'title': 'On the bias of the Gini coefficient estimator for zero-truncated Poisson\\n  distributions',\n",
       "  'authors': ['Roberto Vila', 'Helton Saulo'],\n",
       "  'summary': 'This paper analyzes the Gini coefficient estimator for zero-truncated Poisson\\npopulations, revealing the presence of bias, and provides a mathematical\\nexpression for the bias, along with a bias-corrected estimator, which is\\nevaluated using Monte Carlo simulation methods.',\n",
       "  'keywords': None},\n",
       " {'title': 'Balancing Complexity and Informativeness in LLM-Based Clustering:\\n  Finding the Goldilocks Zone',\n",
       "  'authors': ['Justin Miller', 'Tristram Alexander'],\n",
       "  'summary': 'The challenge of clustering short text data lies in balancing informativeness\\nwith interpretability. Traditional evaluation metrics often overlook this\\ntrade-off. Inspired by linguistic principles of communicative efficiency, this\\npaper investigates the optimal number of clusters by quantifying the trade-off\\nbetween informativeness and cognitive simplicity. We use large language models\\n(LLMs) to generate cluster names and evaluate their effectiveness through\\nsemantic density, information theory, and clustering accuracy. Our results show\\nthat Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM,\\nincreases semantic density compared to random assignment, effectively grouping\\nsimilar bios. However, as clusters increase, interpretability declines, as\\nmeasured by a generative LLM\\'s ability to correctly assign bios based on\\ncluster names. A logistic regression analysis confirms that classification\\naccuracy depends on the semantic similarity between bios and their assigned\\ncluster names, as well as their distinction from alternatives.\\n  These findings reveal a \"Goldilocks zone\" where clusters remain distinct yet\\ninterpretable. We identify an optimal range of 16-22 clusters, paralleling\\nlinguistic efficiency in lexical categorization. These insights inform both\\ntheoretical models and practical applications, guiding future research toward\\noptimising cluster interpretability and usefulness.',\n",
       "  'keywords': None},\n",
       " {'title': 'Regression Discontinuity Design with Distribution-Valued Outcomes',\n",
       "  'authors': ['David Van Dijcke'],\n",
       "  'summary': 'This article introduces Regression Discontinuity Design (RDD) with\\nDistribution-Valued Outcomes (R3D), extending the standard RDD framework to\\nsettings where the outcome is a distribution rather than a scalar. Such\\nsettings arise when treatment is assigned at a higher level of aggregation than\\nthe outcome-for example, when a subsidy is allocated based on a firm-level\\nrevenue cutoff while the outcome of interest is the distribution of employee\\nwages within the firm. Since standard RDD methods cannot accommodate such\\ntwo-level randomness, I propose a novel approach based on random distributions.\\nThe target estimand is a \"local average quantile treatment effect\", which\\naverages across random quantiles. To estimate this target, I introduce two\\nrelated approaches: one that extends local polynomial regression to random\\nquantiles and another based on local Fr\\\\\\'echet regression, a form of functional\\nregression. For both estimators, I establish asymptotic normality and develop\\nuniform, debiased confidence bands together with a data-driven bandwidth\\nselection procedure. Simulations validate these theoretical properties and show\\nexisting methods to be biased and inconsistent in this setting. I then apply\\nthe proposed methods to study the effects of gubernatorial party control on\\nwithin-state income distributions in the US, using a close-election design. The\\nresults suggest a classic equality-efficiency tradeoff under Democratic\\ngovernorship, driven by reductions in income at the top of the distribution.',\n",
       "  'keywords': ['causal inference',\n",
       "   'random distributions',\n",
       "   'quantile treatment effects',\n",
       "   'Fr´echet']},\n",
       " {'title': 'Common Drivers in Sparsely Interacting Hawkes Processes',\n",
       "  'authors': ['Alexander Kreiss', 'Enno Mammen', 'Wolfgang Polonik'],\n",
       "  'summary': 'We study a multivariate Hawkes process as a model for time-continuous\\nrelational event networks. The model does not assume the network to be known,\\nit includes covariates, and it allows for both common drivers, parameters\\ncommon to all the actors in the network, and also local parameters specific for\\neach actor. We derive rates of convergence for all of the model parameters when\\nboth the number of actors and the time horizon tends to infinity. To prevent an\\nexploding network, sparseness is assumed. We also discuss numerical aspects.',\n",
       "  'keywords': None},\n",
       " {'title': 'Stochastic Optimization with Optimal Importance Sampling',\n",
       "  'authors': ['Liviu Aolaritei',\n",
       "   'Bart P. G. Van Parys',\n",
       "   'Henry Lam',\n",
       "   'Michael I. Jordan'],\n",
       "  'summary': \"Importance Sampling (IS) is a widely used variance reduction technique for\\nenhancing the efficiency of Monte Carlo methods, particularly in rare-event\\nsimulation and related applications. Despite its power, the performance of IS\\nis often highly sensitive to the choice of the proposal distribution and\\nfrequently requires stochastic calibration techniques. While the design and\\nanalysis of IS have been extensively studied in estimation settings, applying\\nIS within stochastic optimization introduces a unique challenge: the decision\\nand the IS distribution are mutually dependent, creating a circular\\noptimization structure. This interdependence complicates both the analysis of\\nconvergence for decision iterates and the efficiency of the IS scheme. In this\\npaper, we propose an iterative gradient-based algorithm that jointly updates\\nthe decision variable and the IS distribution without requiring time-scale\\nseparation between the two. Our method achieves the lowest possible asymptotic\\nvariance and guarantees global convergence under convexity of the objective and\\nmild assumptions on the IS distribution family. Furthermore, we show that these\\nproperties are preserved under linear constraints by incorporating a recent\\nvariant of Nesterov's dual averaging method.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Identifiability of VAR(1) model in a stationary setting',\n",
       "  'authors': ['Bixuan Liu'],\n",
       "  'summary': 'We consider a classical First-order Vector AutoRegressive (VAR(1)) model,\\nwhere we interpret the autoregressive interaction matrix as influence\\nrelationships among the components of the VAR(1) process that can be encoded by\\na weighted directed graph. A majority of previous work studies the structural\\nidentifiability of the graph based on time series observations and therefore\\nrelies on dynamical information. In this work we assume that an equilibrium\\nexists, and study instead the identifiability of the graph from the stationary\\ndistribution, meaning that we seek a way to reconstruct the influence graph\\nunderlying the dynamic network using only static information. We use an\\napproach from algebraic statistics that characterizes models using the Jacobian\\nmatroids associated with the parametrization of the models, and we introduce\\nsufficient graphical conditions under which different graphs yield distinct\\nsteady-state distributions. Additionally, we illustrate how our results could\\nbe applied to characterize networks inspired by ecological research.',\n",
       "  'keywords': ['model identifiability',\n",
       "   'vector autoregressive models (VAR)',\n",
       "   'algebraic matroids',\n",
       "   'directed graph']},\n",
       " {'title': 'On empirical Hodge Laplacians under the manifold hypothesis',\n",
       "  'authors': ['Jan-Paul Lerch', 'Martin Wahl'],\n",
       "  'summary': 'Given i.i.d. observations uniformly distributed on a closed submanifold of\\nthe Euclidean space, we study higher-order generalizations of graph Laplacians,\\nso-called Hodge Laplacians on graphs, as approximations of the Laplace-Beltrami\\noperator on differential forms. Our main result is a high-probability error\\nbound for the associated Dirichlet forms. This bound improves existing\\nDirichlet form error bounds for graph Laplacians in the context of Laplacian\\nEigenmaps, and it provides insights into the Betti numbers studied in\\ntopological data analysis and the complementing positive part of the spectrum.',\n",
       "  'keywords': None},\n",
       " {'title': 'On the rate of convergence of an over-parametrized deep neural network\\n  regression estimate learned by gradient descent',\n",
       "  'authors': ['Michael Kohler'],\n",
       "  'summary': 'Nonparametric regression with random design is considered.\\n  The $L_2$ error with integration with respect to the design\\n  measure is used as the error criterion.\\n  An over-parametrized deep neural network\\n  regression estimate\\n  with logistic activation function\\n  is defined, where all weights are learned\\n  by gradient descent. It is shown that the estimate\\n  achieves a nearly optimal rate of convergence in case\\n  that the regression function is $(p,C)$--smooth.',\n",
       "  'keywords': None},\n",
       " {'title': 'Eigen-inference by Marchenko-Pastur inversion',\n",
       "  'authors': ['Ben Deitmar'],\n",
       "  'summary': 'A new formula for Marchenko-Pastur inversion is derived and used for\\ninference of population linear spectral statistics. The formula allows for\\nestimation of the Stieltjes transform of the population spectral distribution\\n$s_H(z)$, when $z$ is sufficiently far from the support of the population\\nspectral distribution $H$. If the dimension $d$ and the sample size $n$ go to\\ninfinity simultaneously such that $\\\\frac{d}{n} \\\\rightarrow c>0$, the estimation\\nerror is shown to be asymptotically less than $\\\\frac{n^{\\\\varepsilon}}{n}$ for\\narbitrary $\\\\varepsilon > 0$. By integrating along a curve around the support of\\n$H$, estimators for population linear spectral statistics are constructed,\\nwhich benefit from this convergence speed of $\\\\frac{n^{\\\\varepsilon}}{n}$.',\n",
       "  'keywords': None},\n",
       " {'title': 'Existence and non-existence of consistent estimators in supercritical\\n  controlled branching processes',\n",
       "  'authors': ['Peter Braunsteins', 'Sophie Hautphenne', 'James Kerlidis'],\n",
       "  'summary': 'We consider the problem of estimating the parameters of a supercritical\\ncontrolled branching process consistently from a single observed trajectory of\\npopulation size counts. Our goal is to establish which parameters can and\\ncannot be consistently estimated. When a parameter can be consistently\\nestimated, we derive an explicit expression for the estimator. We address these\\nquestions in three scenarios: when the distribution of the control function\\ndistribution is known, when it is unknown, and when progenitor numbers are\\nobserved alongside population size counts. Our results offer a theoretical\\njustification for the common practice in population ecology of estimating\\ndemographic and environmental stochasticity using separate observation schemes.',\n",
       "  'keywords': ['branching process',\n",
       "   'consistent estimation',\n",
       "   'control function.']},\n",
       " {'title': 'Adaptive sparse variational approximations for Gaussian process\\n  regression',\n",
       "  'authors': ['Dennis Nieman', 'Botond Szabó'],\n",
       "  'summary': 'Accurate tuning of hyperparameters is crucial to ensure that models can\\ngeneralise effectively across different settings. In this paper, we present\\ntheoretical guarantees for hyperparameter selection using variational Bayes in\\nthe nonparametric regression model. We construct a variational approximation to\\na hierarchical Bayes procedure, and derive upper bounds for the contraction\\nrate of the variational posterior in an abstract setting. The theory is applied\\nto various Gaussian process priors and variational classes, resulting in\\nminimax optimal rates. Our theoretical results are accompanied with numerical\\nanalysis both on synthetic and real world data sets.',\n",
       "  'keywords': ['variationalinference',\n",
       "   'Bayesianmodelselection',\n",
       "   'Gaussianprocesses',\n",
       "   'nonpara-']},\n",
       " {'title': 'A computational transition for detecting multivariate shuffled linear\\n  regression by low-degree polynomials',\n",
       "  'authors': ['Zhangsong Li'],\n",
       "  'summary': 'In this paper, we study the problem of multivariate shuffled linear\\nregression, where the correspondence between predictors and responses in a\\nlinear model is obfuscated by a latent permutation. Specifically, we\\ninvestigate the model $Y=\\\\tfrac{1}{\\\\sqrt{1+\\\\sigma^2}}(\\\\Pi_* X Q_* + \\\\sigma Z)$,\\nwhere $X$ is an $n*d$ standard Gaussian design matrix, $Z$ is an $n*m$ Gaussian\\nnoise matrix, $\\\\Pi_*$ is an unknown $n*n$ permutation matrix, and $Q_*$ is an\\nunknown $d*m$ on the Grassmanian manifold satisfying $Q_*^{\\\\top} Q_* = \\\\mathbb\\nI_m$.\\n  Consider the hypothesis testing problem of distinguishing this model from the\\ncase where $X$ and $Y$ are independent Gaussian random matrices of sizes $n*d$\\nand $n*m$, respectively. Our results reveal a phase transition phenomenon in\\nthe performance of low-degree polynomial algorithms for this task. (1) When\\n$m=o(d)$, we show that all degree-$D$ polynomials fail to distinguish these two\\nmodels even when $\\\\sigma=0$, provided with $D^4=o\\\\big( \\\\tfrac{d}{m} \\\\big)$. (2)\\nWhen $m=d$ and $\\\\sigma=\\\\omega(1)$, we show that all degree-$D$ polynomials fail\\nto distinguish these two models provided with $D=o(\\\\sigma)$. (3) When $m=d$ and\\n$\\\\sigma=o(1)$, we show that there exists a constant-degree polynomial that\\nstrongly distinguish these two models. These results establish a smooth\\ntransition in the effectiveness of low-degree polynomial algorithms for this\\nproblem, highlighting the interplay between the dimensions $m$ and $d$, the\\nnoise level $\\\\sigma$, and the computational complexity of the testing task.',\n",
       "  'keywords': None},\n",
       " {'title': 'A Lanczos-Based Algorithmic Approach for Spike Detection in Large Sample\\n  Covariance Matrices',\n",
       "  'authors': ['Charbel Abi Younes', 'Xiucai Ding', 'Thomas Trogdon'],\n",
       "  'summary': 'We introduce a new approach for estimating the number of spikes in a general\\nclass of spiked covariance models without directly computing the eigenvalues of\\nthe sample covariance matrix. This approach is based on the Lanczos algorithm\\nand the asymptotic properties of the associated Jacobi matrix and its Cholesky\\nfactorization. A key aspect of the analysis is interpreting the eigenvector\\nspectral distribution as a perturbation of its asymptotic counterpart. The\\nspecific exponential-type asymptotics of the Jacobi matrix enables an efficient\\napproximation of the Stieltjes transform of the asymptotic spectral\\ndistribution via a finite continued fraction. As a consequence, we also obtain\\nestimates for the density of the asymptotic distribution and the location of\\noutliers. We provide consistency guarantees for our proposed estimators,\\nproving their convergence in the high-dimensional regime. We demonstrate that,\\nwhen applied to standard spiked covariance models, our approach outperforms\\nexisting methods in computational efficiency and runtime, while still\\nmaintaining robustness to exotic population covariances.',\n",
       "  'keywords': None},\n",
       " {'title': 'High-dimensional ridge regression with random features for\\n  non-identically distributed data with a variance profile',\n",
       "  'authors': ['Issa-Mbenard Dabo', 'Jérémie Bigot'],\n",
       "  'summary': \"The behavior of the random feature model in the high-dimensional regression\\nframework has become a popular issue of interest in the machine learning\\nliterature}. This model is generally considered for feature vectors $x_i =\\n\\\\Sigma^{1/2} x_i'$, where $x_i'$ is a random vector made of independent and\\nidentically distributed (iid) entries, and $\\\\Sigma$ is a positive definite\\nmatrix representing the covariance of the features.\\n  In this paper, we move beyond {\\\\CB this standard assumption by studying the\\nperformances of the random features model in the setting of non-iid feature\\nvectors}. Our approach is related to the analysis of the spectrum of large\\nrandom matrices through random matrix theory (RMT) {\\\\CB and free probability}\\nresults. We turn to the analysis of non-iid data by using the notion of\\nvariance profile {\\\\CB which} is {\\\\CB well studied in RMT.} Our main\\ncontribution is then the study of the limits of the training and {\\\\CB\\nprediction} risks associated to the ridge estimator in the random features\\nmodel when its dimensions grow. We provide asymptotic equivalents of these\\nrisks that capture the behavior of ridge regression with random features in a\\n{\\\\CB high-dimensional} framework. These asymptotic equivalents, {\\\\CB which\\nprove to be sharp in numerical experiments}, are retrieved by adapting, to our\\nsetting, established results from operator-valued free probability theory.\\nMoreover, {\\\\CB for various classes of random feature vectors that have not been\\nconsidered so far in the literature}, our approach allows to show the\\nappearance of the double descent phenomenon when the ridge regularization\\nparameter is small enough.\",\n",
       "  'keywords': ['Two-layersneuralnetworks;Randomfeatures;Ridgeregression;Non-identicallydistributeddata;']},\n",
       " {'title': 'E-variables for hypotheses generated by constraints',\n",
       "  'authors': ['Martin Larsson', 'Aaditya Ramdas', 'Johannes Ruf'],\n",
       "  'summary': 'An e-variable for a family of distributions $\\\\mathcal{P}$ is a nonnegative\\nrandom variable whose expected value under every distribution in $\\\\mathcal{P}$\\nis at most one. E-variables have recently been recognized as fundamental\\nobjects in hypothesis testing, and a rapidly growing body of work has attempted\\nto derive admissible or optimal e-variables for various families $\\\\mathcal{P}$.\\nIn this paper, we study classes $\\\\mathcal{P}$ that are specified by\\nconstraints. Simple examples include bounds on the moments, but our general\\ntheory covers arbitrary sets of measurable constraints. Our main results\\ncharacterize the set of all e-variables for such classes, as well as maximal\\nones. Three case studies illustrate the scope of our theory: finite constraint\\nsets, one-sided sub-$\\\\psi$ distributions, and distributions invariant under a\\ngroup of symmetries. In particular, we generalize recent results of Clerico\\n(2024a) by dropping all assumptions on the constraints.',\n",
       "  'keywords': None},\n",
       " {'title': 'Kullback-Leibler Consistency of $p$-dimensional Pólya Tree Posteriors\\n  and Differential Entropy Estimation',\n",
       "  'authors': ['Fernando Corrêa', 'Rafael Bassi Stern', 'Julio Michael Stern'],\n",
       "  'summary': \"We exploit the multiplicative structure of P\\\\'olya Tree priors for density\\nand differential entropy estimation in $p$-dimensions. We establish: (i) a\\nrepresentation theorem of entropy functionals and (ii) conditions on the\\nparameters of P\\\\'olya Trees to obtain Kullback-Leibler and Total Variation\\nconsistency for vectors with compact support. Those results motivate a novel\\ndifferential entropy estimator that is consistent in probability for compact\\nsupported vectors under mild conditions. In order to enable applications of\\nboth results, we also provide a theoretical motivation for the truncation of\\nUnivariate P\\\\'olya Trees at level $3 \\\\log_2 n $.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Universal Log-Optimality for General Classes of e-processes and\\n  Sequential Hypothesis Tests',\n",
       "  'authors': ['Ian Waudby-Smith', 'Ricardo Sandoval', 'Michael I. Jordan'],\n",
       "  'summary': 'We consider the problem of sequential hypothesis testing by betting. For a\\ngeneral class of composite testing problems -- which include bounded mean\\ntesting, equal mean testing for bounded random tuples, and some key ingredients\\nof two-sample and independence testing as special cases -- we show that any\\n$e$-process satisfying a certain sublinear regret bound is adaptively,\\nasymptotically, and almost surely log-optimal for a composite alternative. This\\nis a strong notion of optimality that has not previously been established for\\nthe aforementioned problems and we provide explicit test supermartingales and\\n$e$-processes satisfying this notion in the more general case. Furthermore, we\\nderive matching lower and upper bounds on the expected rejection time for the\\nresulting sequential tests in all of these cases. The proofs of these results\\nmake weak, algorithm-agnostic moment assumptions and rely on a general-purpose\\nproof technique involving the aforementioned regret and a family of numeraire\\nportfolios. Finally, we discuss how all of these theorems hold in a\\ndistribution-uniform sense, a notion of log-optimality that is stronger still\\nand seems to be new to the literature.',\n",
       "  'keywords': None},\n",
       " {'title': 'Computing High-dimensional Confidence Sets for Arbitrary Distributions',\n",
       "  'authors': ['Chao Gao',\n",
       "   'Liren Shan',\n",
       "   'Vaidehi Srinivas',\n",
       "   'Aravindan Vijayaraghavan'],\n",
       "  'summary': 'We study the problem of learning a high-density region of an arbitrary\\ndistribution over $\\\\mathbb{R}^d$. Given a target coverage parameter $\\\\delta$,\\nand sample access to an arbitrary distribution $D$, we want to output a\\nconfidence set $S \\\\subset \\\\mathbb{R}^d$ such that $S$ achieves $\\\\delta$\\ncoverage of $D$, i.e., $\\\\mathbb{P}_{y \\\\sim D} \\\\left[ y \\\\in S \\\\right] \\\\ge\\n\\\\delta$, and the volume of $S$ is as small as possible. This is a central\\nproblem in high-dimensional statistics with applications in finding confidence\\nsets, uncertainty quantification, and support estimation.\\n  In the most general setting, this problem is statistically intractable, so we\\nrestrict our attention to competing with sets from a concept class $C$ with\\nbounded VC-dimension. An algorithm is competitive with class $C$ if, given\\nsamples from an arbitrary distribution $D$, it outputs in polynomial time a set\\nthat achieves $\\\\delta$ coverage of $D$, and whose volume is competitive with\\nthe smallest set in $C$ with the required coverage $\\\\delta$. This problem is\\ncomputationally challenging even in the basic setting when $C$ is the set of\\nall Euclidean balls. Existing algorithms based on coresets find in polynomial\\ntime a ball whose volume is $\\\\exp(\\\\tilde{O}( d/ \\\\log d))$-factor competitive\\nwith the volume of the best ball.\\n  Our main result is an algorithm that finds a confidence set whose volume is\\n$\\\\exp(\\\\tilde{O}(d^{2/3}))$ factor competitive with the optimal ball having the\\ndesired coverage. The algorithm is improper (it outputs an ellipsoid). Combined\\nwith our computational intractability result for proper learning balls within\\nan $\\\\exp(\\\\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results\\nprovide an interesting separation between proper and (improper) learning of\\nconfidence sets.',\n",
       "  'keywords': None},\n",
       " {'title': 'The Markov approximation of the periodic multivariate Poisson\\n  autoregression',\n",
       "  'authors': ['Mahmoud Khabou', 'Edward A. K. Cohen', 'Almut E. D. Veraart'],\n",
       "  'summary': 'This paper introduces a periodic multivariate Poisson autoregression with\\npotentially infinite memory, with a special focus on the network setting. Using\\ncontraction techniques, we study the stability of such a process and provide\\nupper bounds on how fast it reaches the periodically stationary regime. We then\\npropose a computationally efficient Markov approximation using the properties\\nof the exponential function and a density result. Furthermore, we prove the\\nstrong consistency of the maximum likelihood estimator for the Markov\\napproximation and empirically test its robustness in the case of\\nmisspecification. Our model is applied to the prediction of weekly Rotavirus\\ncases in Berlin, demonstrating superior performance compared to the existing\\nPNAR model.',\n",
       "  'keywords': ['Multi-variate count time series',\n",
       "   'periodicity',\n",
       "   'Markov approximation',\n",
       "   'strong consistency',\n",
       "   '']},\n",
       " {'title': 'Fermat Distance-to-Measure: a robust Fermat-like metric',\n",
       "  'authors': ['Jérôme Taupin', 'Frédéric Chazal'],\n",
       "  'summary': 'Given a probability measure with density, Fermat distances and density-driven\\nmetrics are conformal transformation of the Euclidean metric that shrink\\ndistances in high density areas and enlarge distances in low density areas.\\nAlthough they have been widely studied and have shown to be useful in various\\nmachine learning tasks, they are limited to measures with density (with respect\\nto Lebesgue measure, or volume form on manifold). In this paper, by replacing\\nthe density with the Distance-to-Measure, we introduce a new metric, the Fermat\\nDistance-to-Measure, defined for any probability measure in R^d. We derive\\nstrong stability properties for the Fermat Distance-to-Measure with respect to\\nthe measure and propose an estimator from random sampling of the measure,\\nfeaturing an explicit bound on its convergence speed.',\n",
       "  'keywords': None},\n",
       " {'title': 'Unifying Different Theories of Conformal Prediction',\n",
       "  'authors': ['Rina Foygel Barber', 'Ryan J. Tibshirani'],\n",
       "  'summary': 'This paper presents a unified framework for understanding the methodology and\\ntheory behind several different methods in the conformal prediction literature,\\nwhich includes standard conformal prediction (CP), weighted conformal\\nprediction (WCP), nonexchangeable conformal prediction (NexCP), and\\nrandomly-localized conformal prediction (RLCP), among others. At the crux of\\nour framework is the idea that conformal methods are based on revealing partial\\ninformation about the data at hand, and positing a conditional distribution for\\nthe data given the partial information. Different methods arise from different\\nchoices of partial information, and of the corresponding (approximate)\\nconditional distribution. In addition to recovering and unifying existing\\nresults, our framework leads to both new theoretical guarantees for existing\\nmethods, and new extensions of the conformal methodology.',\n",
       "  'keywords': None},\n",
       " {'title': 'Testing independence and conditional independence in high dimensions via\\n  coordinatewise Gaussianization',\n",
       "  'authors': ['Jinyuan Chang', 'Yue Du', 'Jing He', 'Qiwei Yao'],\n",
       "  'summary': 'We propose new statistical tests, in high-dimensional settings, for testing\\nthe independence of two random vectors and their conditional independence given\\na third random vector. The key idea is simple, i.e., we first transform each\\ncomponent variable to standard normal via its marginal empirical distribution,\\nand we then test for independence and conditional independence of the\\ntransformed random vectors using appropriate $L_\\\\infty$-type test statistics.\\nWhile we are testing some necessary conditions of the independence or the\\nconditional independence, the new tests outperform the 13 frequently used\\ntesting methods in a large scale simulation comparison. The advantage of the\\nnew tests can be summarized as follows: (i) they do not require any moment\\nconditions, (ii) they allow arbitrary dependence structures of the components\\namong the random vectors, and (iii) they allow the dimensions of random vectors\\ndiverge at the exponential rates of the sample size. The critical values of the\\nproposed tests are determined by a computationally efficient multiplier\\nbootstrap procedure. Theoretical analysis shows that the sizes of the proposed\\ntests can be well controlled by the nominal significance level, and the\\nproposed tests are also consistent under certain local alternatives. The finite\\nsample performance of the new tests is illustrated via extensive simulation\\nstudies and a real data application.',\n",
       "  'keywords': ['Conditional independence test',\n",
       "   'coordinatewise Gaussianization',\n",
       "   'Gaussian approxi-']},\n",
       " {'title': 'On the Geometry of Receiver Operating Characteristic and\\n  Precision-Recall Curves',\n",
       "  'authors': ['Reza Sameni'],\n",
       "  'summary': 'We study the geometry of Receiver Operating Characteristic (ROC) and\\nPrecision-Recall (PR) curves in binary classification problems. The key finding\\nis that many of the most commonly used binary classification metrics are merely\\nfunctions of the composition function $G := F_p \\\\circ F_n^{-1}$, where\\n$F_p(\\\\cdot)$ and $F_n(\\\\cdot)$ are the class-conditional cumulative distribution\\nfunctions of the classifier scores in the positive and negative classes,\\nrespectively. This geometric perspective facilitates the selection of operating\\npoints, understanding the effect of decision thresholds, and comparison between\\nclassifiers. It also helps explain how the shapes and geometry of ROC/PR curves\\nreflect classifier behavior, providing objective tools for building classifiers\\noptimized for specific applications with context-specific constraints. We\\nfurther explore the conditions for classifier dominance, present analytical and\\nnumerical examples demonstrating the effects of class separability and variance\\non ROC and PR geometries, and derive a link between the positive-to-negative\\nclass leakage function $G(\\\\cdot)$ and the Kullback--Leibler divergence. The\\nframework highlights practical considerations, such as model calibration,\\ncost-sensitive optimization, and operating point selection under real-world\\ncapacity constraints, enabling more informed approaches to classifier\\ndeployment and decision-making.',\n",
       "  'keywords': None},\n",
       " {'title': 'Estimation of the complier causal hazard ratio under dependent censoring',\n",
       "  'authors': ['Gilles Crommen', 'Jad Beyhum', 'Ingrid Van Keilegom'],\n",
       "  'summary': 'In this work, we are interested in studying the causal effect of an\\nendogenous binary treatment on a dependently censored duration outcome. By\\ndependent censoring, it is meant that the duration time ($T$) and right\\ncensoring time ($C$) are not statistically independent of each other, even\\nafter conditioning on the measured covariates. The endogeneity issue is handled\\nby making use of a binary instrumental variable for the treatment. To deal with\\nthe dependent censoring problem, it is assumed that on the stratum of\\ncompliers: (i) $T$ follows a semiparametric proportional hazards model; (ii)\\n$C$ follows a fully parametric model; and (iii) the relation between $T$ and\\n$C$ is modeled by a parametric copula, such that the association parameter can\\nbe left unspecified. In this framework, the treatment effect of interest is the\\ncomplier causal hazard ratio (CCHR). We devise an estimation procedure that is\\nbased on a weighted maximum likelihood approach, where the weights are the\\nprobabilities of an observation coming from a complier. The weights are\\nestimated non-parametrically in a first stage, followed by the estimation of\\nthe CCHR. Novel conditions under which the model is identifiable are given, a\\ntwo-step estimation procedure is proposed and some important asymptotic\\nproperties are established. Simulations are used to assess the validity and\\nfinite-sample performance of the estimation procedure. Finally, we apply the\\napproach to estimate the CCHR of both job training programs on unemployment\\nduration and periodic screening examinations on time until death from breast\\ncancer. The data come from the National Job Training Partnership Act study and\\nthe Health Insurance Plan of Greater New York experiment respectively.',\n",
       "  'keywords': None},\n",
       " {'title': 'Cramér--Rao Inequalities for Several Generalized Fisher Information',\n",
       "  'authors': ['Hao Wu', 'Lei Yu'],\n",
       "  'summary': \"The de Bruijn identity states that Fisher information is the half of the\\nderivative of Shannon differential entropy along heat flow. In the same spirit,\\nin this paper we introduce a generalized version of Fisher information, named\\nas the R\\\\'enyi--Fisher information, which is the half of the derivative of\\nR\\\\'enyi information along heat flow. Based on this R\\\\'enyi--Fisher information,\\nwe establish sharp R\\\\'enyi-entropic isoperimetric inequalities, which\\ngeneralize the classic entropic isoperimetric inequality to the R\\\\'enyi\\nsetting. Utilizing these isoperimetric inequalities, we extend the classical\\nCram\\\\'er--Rao inequality from Fisher information to R\\\\'enyi--Fisher\\ninformation. Lastly, we use these generalized Cram\\\\'er--Rao inequalities to\\ndetermine the signs of derivatives of entropy along heat flow, strengthening\\nexisting results on the complete monotonicity of entropy.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Estimating hazard rates from $δ$-records in discrete distributions',\n",
       "  'authors': ['Martín Alcalde',\n",
       "   'Miguel Lafuente',\n",
       "   'F. Javier López',\n",
       "   'Lina Maldonado',\n",
       "   'Gerardo Sanz'],\n",
       "  'summary': 'This paper focuses on nonparametric statistical inference of the hazard rate\\nfunction of discrete distributions based on $\\\\delta$-record data. We derive the\\nexplicit expression of the maximum likelihood estimator and determine its exact\\ndistribution, as well as some important characteristics such as its bias and\\nmean squared error. We then discuss the construction of confidence intervals\\nand goodness-of-fit tests. The performance of our proposals is evaluated using\\nsimulation methods. Applications to real data are given, as well. The\\nestimation of the hazard rate function based on usual records has been studied\\nin the literature, although many procedures require several samples of records.\\nIn contrast, our approach relies on a single sequence of $\\\\delta$-records,\\nsimplifying the experimental design and increasing the applicability of the\\nmethods.',\n",
       "  'keywords': ['Discretehazardrate',\n",
       "   'records',\n",
       "   'δ-records',\n",
       "   'near-records',\n",
       "   'nonparametricmaximumlikelihoodestimation',\n",
       "   '']},\n",
       " {'title': 'Proper scoring rules for estimation and forecast evaluation',\n",
       "  'authors': ['Kartik Waghmare', 'Johanna Ziegel'],\n",
       "  'summary': 'Proper scoring rules have been a subject of growing interest in recent years,\\nnot only as tools for evaluation of probabilistic forecasts but also as methods\\nfor estimating probability distributions. In this article, we review the\\nmathematical foundations of proper scoring rules including general\\ncharacterization results and important families of scoring rules. We discuss\\ntheir role in statistics and machine learning for estimation and forecast\\nevaluation. Furthermore, we comment on interesting developments of their usage\\nin applications.',\n",
       "  'keywords': None},\n",
       " {'title': 'Asymptotic analysis of the finite predictor for the fractional Gaussian\\n  noise',\n",
       "  'authors': ['P. Chigansky', 'M. Kleptsyna'],\n",
       "  'summary': 'The goal of this paper is to propose a new approach to asymptotic analysis of\\nthe finite predictor for stationary sequences. It produces the exact\\nasymptotics of the relative prediction error and the partial correlation\\ncoefficients. The assumptions are analytic in nature and applicable to\\nprocesses with long range dependence. The ARIMA type process driven by the\\nfractional Gaussian noise (fGn), which previously remained elusive, serves as\\nour study case.',\n",
       "  'keywords': None},\n",
       " {'title': 'On Robust Empirical Likelihood for Nonparametric Regression with\\n  Application to Regression Discontinuity Designs',\n",
       "  'authors': ['Qin Fang', 'Shaojun Guo', 'Yang Hong', 'Xinghao Qiao'],\n",
       "  'summary': \"Empirical likelihood serves as a powerful tool for constructing confidence\\nintervals in nonparametric regression and regression discontinuity designs\\n(RDD). The original empirical likelihood framework can be naturally extended to\\nthese settings using local linear smoothers, with Wilks' theorem holding only\\nwhen an undersmoothed bandwidth is selected. However, the generalization of\\nbias-corrected versions of empirical likelihood under more realistic conditions\\nis non-trivial and has remained an open challenge in the literature. This paper\\nprovides a satisfactory solution by proposing a novel approach, referred to as\\nrobust empirical likelihood, designed for nonparametric regression and RDD. The\\ncore idea is to construct robust weights which simultaneously achieve bias\\ncorrection and account for the additional variability introduced by the\\nestimated bias, thereby enabling valid confidence interval construction without\\nextra estimation steps involved. We demonstrate that the Wilks' phenomenon\\nstill holds under weaker conditions in nonparametric regression, sharp and\\nfuzzy RDD settings. Extensive simulation studies confirm the effectiveness of\\nour proposed approach, showing superior performance over existing methods in\\nterms of coverage probabilities and interval lengths. Moreover, the proposed\\nprocedure exhibits robustness to bandwidth selection, making it a flexible and\\nreliable tool for empirical analyses. The practical usefulness is further\\nillustrated through applications to two real datasets.\",\n",
       "  'keywords': ['Empirical likelihood; Local polynomials; Wilks’ phenomenon; Regression']},\n",
       " {'title': 'Tail Bounds for Canonical $U$-Statistics and $U$-Processes with\\n  Unbounded Kernels',\n",
       "  'authors': ['Abhishek Chakrabortty', 'Arun K. Kuchibhotla'],\n",
       "  'summary': 'In this paper, we prove exponential tail bounds for canonical (or degenerate)\\n$U$-statistics and $U$-processes under exponential-type tail assumptions on the\\nkernels. Most of the existing results in the relevant literature often assume\\nbounded kernels or obtain sub-optimal tail behavior under unbounded kernels. We\\nobtain sharp rates and optimal tail behavior under sub-Weibull kernel\\nfunctions. Some examples from nonparametric and semiparametric statistics\\nliterature are considered.',\n",
       "  'keywords': None},\n",
       " {'title': 'On spectral gap decomposition for Markov chains',\n",
       "  'authors': ['Qian Qin'],\n",
       "  'summary': 'Multiple works regarding convergence analysis of Markov chains have led to\\nspectral gap decomposition formulas of the form \\\\[ \\\\mathrm{Gap}(S) \\\\geq c_0\\n\\\\left[\\\\inf_z \\\\mathrm{Gap}(Q_z)\\\\right] \\\\mathrm{Gap}(\\\\bar{S}), \\\\] where $c_0$ is\\na constant, $\\\\mathrm{Gap}$ denotes the right spectral gap of a reversible\\nMarkov operator, $S$ is the Markov transition kernel (Mtk) of interest,\\n$\\\\bar{S}$ is an idealized or simplified version of $S$, and $\\\\{Q_z\\\\}$ is a\\ncollection of Mtks characterizing the differences between $S$ and $\\\\bar{S}$.\\n  This type of relationship has been established in various contexts,\\nincluding: 1. decomposition of Markov chains based on a finite cover of the\\nstate space, 2. hybrid Gibbs samplers, and 3. spectral independence and\\nlocalization schemes.\\n  We show that multiple key decomposition results across these domains can be\\nconnected within a unified framework, rooted in a simple sandwich structure of\\n$S$. Within the general framework, we establish new instances of spectral gap\\ndecomposition for hybrid hit-and-run samplers and hybrid data augmentation\\nalgorithms with two intractable conditional distributions. Additionally, we\\nexplore several other properties of the sandwich structure, and derive\\nextensions of the spectral gap decomposition formula.',\n",
       "  'keywords': None},\n",
       " {'title': 'Confidence Bands for Multiparameter Persistence Landscapes',\n",
       "  'authors': ['Inés García-Redondo', 'Anthea Monod', 'Qiquan Wang'],\n",
       "  'summary': 'Multiparameter persistent homology is a generalization of classical\\npersistent homology, a central and widely-used methodology from topological\\ndata analysis, which takes into account density estimation and is an effective\\ntool for data analysis in the presence of noise. Similar to its classical\\nsingle-parameter counterpart, however, it is challenging to compute and use in\\npractice due to its complex algebraic construction. In this paper, we study a\\npopular and tractable invariant for multiparameter persistent homology in a\\nstatistical setting: the multiparameter persistence landscape. We derive a\\nfunctional central limit theorem for multiparameter persistence landscapes,\\nfrom which we compute confidence bands, giving rise to one of the first\\nstatistical inference methodologies for multiparameter persistence landscapes.\\nWe provide an implementation of confidence bands and demonstrate their\\napplication in a machine learning task on synthetic data.',\n",
       "  'keywords': None},\n",
       " {'title': 'Causal Models for Growing Networks',\n",
       "  'authors': ['Gecia Bravo-Hermsdorff', 'Lee M. Gunderson', 'Kayvan Sadeghi'],\n",
       "  'summary': 'Real-world networks grow over time; statistical models based on node\\nexchangeability are not appropriate. Instead of constraining the structure of\\nthe \\\\textit{distribution} of edges, we propose that the relevant symmetries\\nrefer to the \\\\textit{causal structure} between them. We first enumerate the 96\\ncausal directed acyclic graph (DAG) models over pairs of nodes (dyad variables)\\nin a growing network with finite ancestral sets that are invariant to node\\ndeletion. We then partition them into 21 classes with ancestral sets that are\\nclosed under node marginalization. Several of these classes are remarkably\\namenable to distributed and asynchronous evaluation. As an example, we\\nhighlight a simple model that exhibits flexible power-law degree distributions\\nand emergent phase transitions in sparsity, which we characterize analytically.\\nWith few parameters and much conditional independence, our proposed framework\\nprovides natural baseline models for causal inference in relational data.',\n",
       "  'keywords': None},\n",
       " {'title': 'Nonparametric spectral density estimation using interactive mechanisms\\n  under local differential privacy',\n",
       "  'authors': ['Cristina Butucea', 'Karolina Klockmann', 'Tatyana Krivobokova'],\n",
       "  'summary': 'We address the problem of nonparametric estimation of the spectral density\\nfor a centered stationary Gaussian time series under local differential privacy\\nconstraints. Specifically, we propose new interactive privacy mechanisms for\\nthree tasks: estimating a single covariance coefficient, estimating the\\nspectral density at a fixed frequency, and estimating the entire spectral\\ndensity function. Our approach achieves faster rates through a two-stage\\nprocess: we apply first the Laplace mechanism to the truncated value and then\\nuse the former privatized sample to gain knowledge on the dependence mechanism\\nin the time series. For spectral densities belonging to H\\\\\"older and Sobolev\\nsmoothness classes, we demonstrate that our estimators improve upon the\\nnon-interactive mechanism of Kroll (2024) for small privacy parameter $\\\\alpha$,\\nsince the pointwise rates depend on $n\\\\alpha^2$ instead of $n\\\\alpha^4$.\\nMoreover, we show that the rate $(n\\\\alpha^4)^{-1}$ is optimal for estimating a\\ncovariance coefficient with non-interactive mechanisms. However, the $L_2$ rate\\nof our interactive estimator is slower than the pointwise rate. We show how to\\nuse these estimators to provide a bona-fide locally differentially private\\ncovariance matrix estimator.',\n",
       "  'keywords': ['Autocovariance function',\n",
       "   'Covariance matrix',\n",
       "   'Local Differential Privacy',\n",
       "   'Non-']},\n",
       " {'title': 'Power comparison of sequential testing by betting procedures',\n",
       "  'authors': ['Amaury Durand', 'Olivier Wintenberger'],\n",
       "  'summary': 'In this paper, we derive power guarantees of some sequential tests for\\nbounded mean under general alternatives. We focus on testing procedures using\\nnonnegative supermartingales which are anytime valid and consider alternatives\\nwhich coincide asymptotically with the null (e.g. vanishing mean) while still\\nallowing to reject in finite time. Introducing variance constraints, we show\\nthat the alternative can be broaden while keeping power guarantees for certain\\nsecond-order testing procedures. We also compare different test procedures in\\nmultidimensional setting using characteristics of the rejection times. Finally,\\nwe extend our analysis to other functionals as well as testing and comparing\\nforecasters. Our results are illustrated with numerical simulations including\\nbounded mean testing and comparison of forecasters.',\n",
       "  'keywords': None},\n",
       " {'title': 'Graphical Models and Efficient Inference Methods for Multivariate Phase\\n  Probability Distributions',\n",
       "  'authors': ['Andrew S. Perley', 'Todd P. Coleman'],\n",
       "  'summary': 'Multivariate phase relationships are important to characterize and understand\\nnumerous physical, biological, and chemical systems, from electromagnetic waves\\nto neural oscillations. These systems exhibit complex spatiotemporal dynamics\\nand intricate interdependencies among their constituent elements. While\\nclassical models of multivariate phase relationships, such as the wave equation\\nand Kuramoto model, give theoretical models to describe phenomena, the\\ndevelopment of statistical tools for hypothesis testing and inference for\\nmultivariate phase relationships in complex systems remains limited. This paper\\nintroduces a novel probabilistic modeling framework to characterize\\nmultivariate phase relationships, with wave-like phenomena serving as a key\\nexample. This approach describes spatial patterns and interactions between\\noscillators through a pairwise exponential family distribution. Building upon\\nthe literature of graphical model inference, including methods like Ising\\nmodels, graphical lasso, and interaction screening, this work bridges the gap\\nbetween classical wave dynamics and modern statistical approaches. Efficient\\ninference methods are introduced, leveraging the Chow-Liu algorithm for\\ndirected tree approximations and interaction screening for general graphical\\nmodels. Simulated experiments demonstrate the utility of these methods for\\nuncovering wave properties and sparse interaction structures, highlighting\\ntheir applicability to diverse scientific domains. This framework establishes a\\nnew paradigm for statistical modeling of multivariate phase relationships,\\nproviding a powerful toolset for exploring the complexity of these systems.',\n",
       "  'keywords': None},\n",
       " {'title': 'Non-parametric cure models through extreme-value tail estimation',\n",
       "  'authors': ['Jan Beirlant', 'Martin Bladt', 'Ingrid Van Keilegom'],\n",
       "  'summary': \"In survival analysis, the estimation of the proportion of subjects who will\\nnever experience the event of interest, termed the cure rate, has received\\nconsiderable attention recently. Its estimation can be a particularly difficult\\ntask when follow-up is not sufficient, that is when the censoring mechanism has\\na smaller support than the distribution of the target data. In the latter case,\\nnon-parametric estimators were recently proposed using extreme value\\nmethodology, assuming that the distribution of the susceptible population is in\\nthe Fr\\\\'echet or Gumbel max-domains of attraction. In this paper, we take the\\nextreme value techniques one step further, to jointly estimate the cure rate\\nand the extreme value index, using probability plotting methodology, and in\\nparticular using the full information contained in the top order statistics. In\\nother words, under sufficient or insufficient follow-up, we reconstruct the\\nimmune proportion. To this end, a Peaks-over-Threshold approach is proposed\\nunder the Gumbel max-domain assumption. Next, the approach is also transferred\\nto more specific models such as Pareto, log-normal and Weibull tail models,\\nallowing to recognize the most important tail characteristics of the\\nsusceptible population. We establish the asymptotic behavior of our estimators\\nunder regularization. Though simulation studies, our estimators are show to\\nrival and often outperform established models, even when purely considering\\ncure rate estimation. Finally, we provide an application of our method to\\nNorwegian birth registry data.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Non-Asymptotic Analysis of Classical Spectrum Estimators for $L$-mixing\\n  Time-series Data with Unknown Means',\n",
       "  'authors': ['Yuping Zheng', 'Andrew Lamperski'],\n",
       "  'summary': 'Spectral estimation is an important tool in time series analysis, with\\napplications including economics, astronomy, and climatology. The asymptotic\\ntheory for non-parametric estimation is well-known but the development of\\nnon-asymptotic theory is still ongoing. Our recent work obtained the first\\nnon-asymptotic error bounds on the Bartlett and Welch methods for $L$-mixing\\nstochastic processes. The class of $L$-mixing processes contains common models\\nin time series analysis, including autoregressive processes and measurements of\\ngeometrically ergodic Markov chains. Our prior analysis assumes that the\\nprocess has zero mean. While zero-mean assumptions are common, real-world\\ntime-series data often has unknown, non-zero mean. In this work, we derive\\nnon-asymptotic error bounds for both Bartlett and Welch estimators for\\n$L$-mixing time-series data with unknown means. The obtained error bounds are\\nof $O(\\\\frac{1}{\\\\sqrt{k}})$, where $k$ is the number of data segments used in\\nthe algorithm, which are tighter than our previous results under the zero-mean\\nassumption.',\n",
       "  'keywords': None},\n",
       " {'title': \"Estimating a graph's spectrum via random Kirchhoff forests\",\n",
       "  'authors': ['Simon Barthelmé',\n",
       "   'Fabienne Castell',\n",
       "   'Alexandre Gaudillière',\n",
       "   'Clothilde Melot',\n",
       "   'Matteo Quattropani',\n",
       "   'Nicolas Tremblay'],\n",
       "  'summary': \"Exact eigendecomposition of large matrices is very expensive, and it is\\npractically impossible to compute exact eigenvalues. Instead, one may set a\\nmore modest goal of approaching the empirical distribution of the eigenvalues,\\nrecovering the overall shape of the eigenspectrum. Current approaches to\\nspectral estimation typically work with \\\\emph{moments} of the spectral\\ndistribution. These moments are first estimated using Monte Carlo trace\\nestimators, then the estimates are combined to approximate the spectral\\ndensity. In this article we show how \\\\emph{Kirchhoff forests}, which are random\\nforests on graphs, can be used to estimate certain non-linear moments of very\\nlarge graph Laplacians. We show how to combine these moments into an estimate\\nof the spectral density. If the estimate's desired precision isn't too high,\\nour approach paves the way to the estimation of a graph's spectrum in time\\nsublinear in the number of links.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Optimal low-rank approximations for linear Gaussian inverse problems on\\n  Hilbert spaces, Part II: posterior mean approximation',\n",
       "  'authors': ['Giuseppe Carere', 'Han Cheng Lie'],\n",
       "  'summary': \"In this work, we construct optimal low-rank approximations for the Gaussian\\nposterior distribution in linear Gaussian inverse problems. The parameter space\\nis a separable Hilbert space of possibly infinite dimension, and the data space\\nis assumed to be finite-dimensional. We consider various types of approximation\\nfamilies for the posterior. We first consider approximate posteriors in which\\nthe means vary among a class of either structure-preserving or\\nstructure-ignoring low-rank transformations of the data, and in which the\\nposterior covariance is kept fixed. We give necessary and sufficient conditions\\nfor these approximating posteriors to be equivalent to the exact posterior, for\\nall possible realisations of the data simultaneously. For such approximations,\\nwe measure approximation error with the Kullback-Leibler, R\\\\'enyi and Amari\\n$\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, and with the Hellinger distance, all\\naveraged over the data distribution. With these losses, we find the optimal\\napproximations and formulate an equivalent condition for their uniqueness,\\nextending the work in finite dimensions of Spantini et al. (SIAM J. Sci.\\nComput. 2015). We then consider joint approximation of the mean and covariance,\\nby also varying the posterior covariance over the low-rank updates considered\\nin Part I of this work. For the reverse Kullback-Leibler divergence, we show\\nthat the separate optimal approximations of the mean and of the covariance can\\nbe combined to yield an optimal joint approximation of the mean and covariance.\\nIn addition, we interpret the joint approximation with the optimal\\nstructure-ignoring approximate mean in terms of an optimal projector in\\nparameter space.\",\n",
       "  'keywords': ['nonparametric linear Bayesianinverse problems',\n",
       "   'Gaussianmeasures',\n",
       "   'low-rankoperator']},\n",
       " {'title': 'Asymptotically distribution-free goodness-of-fit testing for point\\n  processes',\n",
       "  'authors': ['Justin Baars', 'Sami Umut Can', 'Roger J. A. Laeven'],\n",
       "  'summary': 'Consider an observation of a multivariate temporal point process $N$ with law\\n$\\\\mathcal P$ on the time interval $[0,T]$. To test the null hypothesis that\\n$\\\\mathcal P$ belongs to a given parametric family, we construct a convergent\\ncompensated counting process to which we apply an innovation martingale\\ntransformation. We prove that the resulting process converges weakly to a\\nstandard Wiener process. Consequently, taking a suitable functional of this\\nprocess yields an asymptotically distribution-free goodness-of-fit test for\\npoint processes. For several standard tests based on the increments of this\\ntransformed process, we establish consistency under alternative hypotheses.\\nFinally, we assess the performance of the proposed testing procedure through a\\nMonte Carlo simulation study and illustrate its practical utility with two\\nreal-data examples.',\n",
       "  'keywords': None},\n",
       " {'title': 'Smooth and rough paths in mean derivative estimation for functional data',\n",
       "  'authors': ['Max Berger', 'Hajo Holzmann'],\n",
       "  'summary': 'In this paper, in a multivariate setting we derive near optimal rates of\\nconvergence in the minimax sense for estimating partial derivatives of the mean\\nfunction for functional data observed under a fixed synchronous design over\\nH\\\\\"older smoothness classes. We focus on the supremum norm since it corresponds\\nto the visualisation of the estimation error, and is closely related to the\\nconstruction of uniform confidence bands. In contrast to mean function\\nestimation, for derivative estimation the smoothness of the paths of the\\nprocesses is crucial for the rates of convergence. On the one hand, if the\\npaths have higher-order smoothness than the order of the partial derivative to\\nbe estimated, the parametric $\\\\sqrt n$ rate can be achieved under sufficiently\\ndense design. On the other hand, for processes with rough paths of lower-order\\nsmoothness, we show that the rates of convergence are necessarily slower than\\nthe parametric rate, and determine a near-optimal rate at which estimation is\\nstill possible. We implement a multivariate local polynomial derivative\\nestimator and illustrate its finite-sample performance in a simulation as well\\nas for two real-data sets. To assess the smoothness of the sample paths in the\\napplications we further discuss a method based on comparing restricted\\nestimates of the partial derivatives of the covariance kernel.',\n",
       "  'keywords': None},\n",
       " {'title': 'Wasserstein KL-divergence for Gaussian distributions',\n",
       "  'authors': ['Adwait Datar', 'Nihat Ay'],\n",
       "  'summary': 'We introduce a new version of the KL-divergence for Gaussian distributions\\nwhich is based on Wasserstein geometry and referred to as WKL-divergence. We\\nshow that this version is consistent with the geometry of the sample space\\n${\\\\Bbb R}^n$. In particular, we can evaluate the WKL-divergence of the Dirac\\nmeasures concentrated in two points which turns out to be proportional to the\\nsquared distance between these points.',\n",
       "  'keywords': ['Wassersteingeometry·Kullback-Leiblerdivergence·Gaus-']},\n",
       " {'title': 'Optimal low-rank approximations for linear Gaussian inverse problems on\\n  Hilbert spaces, Part I: posterior covariance approximation',\n",
       "  'authors': ['Giuseppe Carere', 'Han Cheng Lie'],\n",
       "  'summary': \"For linear inverse problems with Gaussian priors and Gaussian observation\\nnoise, the posterior is Gaussian, with mean and covariance determined by the\\nconditioning formula. Using the Feldman-Hajek theorem, we analyse the\\nprior-to-posterior update and its low-rank approximation for\\ninfinite-dimensional Hilbert parameter spaces and finite-dimensional\\nobservations. We show that the posterior distribution differs from the prior on\\na finite-dimensional subspace, and construct low-rank approximations to the\\nposterior covariance, while keeping the mean fixed. Since in infinite\\ndimensions, not all low-rank covariance approximations yield approximate\\nposterior distributions which are equivalent to the posterior and prior\\ndistribution, we characterise the low-rank covariance approximations which do\\nyield this equivalence, and their respective inverses, or `precisions'. For\\nsuch approximations, a family of measure approximation problems is solved by\\nidentifying the low-rank approximations which are optimal for various losses\\nsimultaneously. These loss functions include the family of R\\\\'enyi divergences,\\nthe Amari $\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, the Hellinger metric and\\nthe Kullback-Leibler divergence. Our results extend those of Spantini et al.\\n(SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide\\ntheoretical underpinning for the construction of low-rank approximations of\\ndiscretised versions of the infinite-dimensional inverse problem, by\\nformulating discretization independent results.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Multivariate Species Sampling Models',\n",
       "  'authors': ['Beatrice Franzolini',\n",
       "   'Antonio Lijoi',\n",
       "   'Igor Prünster',\n",
       "   'Giovanni Rebaudo'],\n",
       "  'summary': 'Species sampling processes have long served as the framework for studying\\nrandom discrete distributions. However, their statistical applicability is\\nlimited when partial exchangeability is assumed as probabilistic invariance for\\nthe observables. Despite numerous discrete models for partially exchangeable\\nobservations, a unifying framework is currently missing, leaving many questions\\nabout the induced learning mechanisms unanswered in this setting. To fill this\\ngap, we consider the natural extension of species sampling models to a\\nmultivariate framework, obtaining a general class of models characterized by\\ntheir partially exchangeable partition probability function. A notable\\nsubclass, named regular multivariate species sampling models, exists among\\nthese models. In the subclass, dependence across processes is accurately\\ncaptured by the correlation among them: a correlation of one equals full\\nexchangeability and a null correlation corresponds to independence. Regular\\nmultivariate species sampling models encompass discrete processes for partial\\nexchangeable data used in Bayesian models, thereby highlighting their core\\ndistributional properties and providing a means for developing new models.',\n",
       "  'keywords': ['Bayesiannonparametrics',\n",
       "   'Dependentnonparametricprior',\n",
       "   'Hierarchicalprocess',\n",
       "   '']},\n",
       " {'title': 'Operator limit of Wigner matrices I',\n",
       "  'authors': ['Debapratim Banerjee'],\n",
       "  'summary': 'We consider the Wigner matrix $W_{n}$ of dimension $n \\\\times n$ as $n \\\\to\\n\\\\infty$. The objective of this paper is two folds: first we construct an\\noperator $\\\\mathcal{W}$ on a suitable Hilbert space $\\\\mathcal{H}$ and then\\ndefine a suitable notion of convergence such that the matrices $W_{n}$ converge\\nin that notion of convergence to $\\\\mathcal{W}$. We further investigate some\\nproperties of $\\\\mathcal{W}$ and $\\\\mathcal{H}$. We show that $\\\\mathcal{H}$ is a\\nnontrivial extension of $L^{2}[0,1]$ with respect to the Lebesgue measure and\\nthe spectral measure of $\\\\mathcal{W}$ at any function $f \\\\in L^{2}[0,1]$ is\\nalmost surely the semicircular law.',\n",
       "  'keywords': None},\n",
       " {'title': 'Distributional regression with reject option',\n",
       "  'authors': ['Ahmed Zaoui', 'Clément Dombry'],\n",
       "  'summary': 'Selective prediction, where a model has the option to abstain from making a\\ndecision, is crucial for machine learning applications in which mistakes are\\ncostly. In this work, we focus on distributional regression and introduce a\\nframework that enables the model to abstain from estimation in situations of\\nhigh uncertainty. We refer to this approach as distributional regression with\\nreject option, inspired by similar concepts in classification and regression\\nwith reject option. We study the scenario where the rejection rate is fixed. We\\nderive a closed-form expression for the optimal rule, which relies on\\nthresholding the entropy function of the Continuous Ranked Probability Score\\n(CRPS). We propose a semi-supervised estimation procedure for the optimal rule,\\nusing two datasets: the first, labeled, is used to estimate both the\\nconditional distribution function and the entropy function of the CRPS, while\\nthe second, unlabeled, is employed to calibrate the desired rejection rate.\\nNotably, the control of the rejection rate is distribution-free. Under mild\\nconditions, we show that our procedure is asymptotically as effective as the\\noptimal rule, both in terms of error rate and rejection rate. Additionally, we\\nestablish rates of convergence for our approach based on distributional\\nk-nearest neighbor. A numerical analysis on real-world datasets demonstrates\\nthe strong performance of our procedure',\n",
       "  'keywords': ['Distributionalregression;Rejectoption;ContinuousRankedProbabil-']},\n",
       " {'title': 'Finite sample valid confidence sets of mode',\n",
       "  'authors': ['Manit Paul', 'Arun Kumar Kuchibhotla'],\n",
       "  'summary': 'Estimating the mode of a unimodal distribution is a classical problem in\\nstatistics. Although there are several approaches for point-estimation of mode\\nin the literature, very little has been explored about the interval-estimation\\nof mode. Our work proposes a collection of novel methods of obtaining finite\\nsample valid confidence set of the mode of a unimodal distribution. We analyze\\nthe behaviour of the width of the proposed confidence sets under some\\nregularity assumptions of the density about the mode and show that the width of\\nthese confidence sets shrink to zero near optimally. Simply put, we show that\\nit is possible to build finite sample valid confidence sets for the mode that\\nshrink to a singleton as sample size increases. We support the theoretical\\nresults by showing the performance of the proposed methods on some synthetic\\ndata-sets. We believe that our confidence sets can be improved both in\\nconstruction and in terms of rate.',\n",
       "  'keywords': None},\n",
       " {'title': 'On Finite Time Span Estimators of Parameters for Ornstein-Uhlenbeck\\n  Processes',\n",
       "  'authors': ['Jun S. Han', 'Nino Kordzakhia'],\n",
       "  'summary': \"We study the bias and the mean-squared error of the maximum likelihood\\nestimators (MLE) of parameters associated with a two-parameter mean-reverting\\nprocess for a finite time $T$. Using the likelihood ratio process, we derive\\nthe expressions for MLEs, then compute the bias and the MSE via the change of\\nmeasure and Ito's formula. We apply the derived expressions to the general\\nOrnstein-Uhlenbeck process, where the bias and the MSE are numerically computed\\nthrough a joint moment-generating function of key functionals of the O-U\\nprocess. A numerical study is provided to illustrate the behaviour of bias and\\nthe MSE for the MLE of the mean-reverting speed parameter.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Bayesian Inference for High-dimensional Time Series with a Directed\\n  Acyclic Graphical Structure',\n",
       "  'authors': ['Arkaprava Roy', 'Anindya Roy', 'Subhashis Ghosal'],\n",
       "  'summary': \"In multivariate time series analysis, understanding the underlying causal\\nrelationships among variables is often of interest for various applications.\\nDirected acyclic graphs (DAGs) provide a powerful framework for representing\\ncausal dependencies. This paper proposes a novel Bayesian approach for modeling\\nmultivariate time series where conditional independencies and causal structure\\nare encoded by a DAG. The proposed model allows structural properties such as\\nstationarity to be easily accommodated. Given the application, we further\\nextend the model for matrix-variate time series. We take a Bayesian approach to\\ninference, and a ``projection-posterior'' based efficient computational\\nalgorithm is developed. The posterior convergence properties of the proposed\\nmethod are established along with two identifiability results for the\\nunrestricted structural equation models. The utility of the proposed method is\\ndemonstrated through simulation studies and real data analysis.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Modeling Maximum drawdown Records with Piecewise Deterministic Markov\\n  Processe in Capital Markets',\n",
       "  'authors': ['Rolando Rubilar-Torrealba',\n",
       "   'Lisandro Fermin',\n",
       "   'Soledad Torres'],\n",
       "  'summary': 'We propose to model the records of the maximum Drawdown in capital markets by\\nmeans a Piecewise Deterministic Markov Process (PDMP). We derive statistical\\nresults such as the mean and variance that describes the sequence of maximum\\nDrawdown records. In addition, we developed a simulation study and techniques\\nfor estimating the parameters governing the stochastic process, using a\\npractical example in the capital market to illustrate the procedure.',\n",
       "  'keywords': None},\n",
       " {'title': 'Optimal Change Point Detection and Inference in the Spectral Density of\\n  General Time Series Models',\n",
       "  'authors': ['Sepideh Mosaferi', 'Abolfazl Safikhani', 'Peiliang Bai'],\n",
       "  'summary': 'This paper addresses the problem of detecting change points in the spectral\\ndensity of time series, motivated by EEG analysis of seizure patients. Seizures\\ndisrupt coherence and functional connectivity, necessitating precise detection.\\nDeparting from traditional parametric approaches, we utilize the Wold\\ndecomposition, representing general time series as autoregressive processes\\nwith infinite lags, which are truncated and estimated around the change point.\\nOur detection procedure employs an initial estimator that systematically\\nsearches across time points. We examine the localization error and its\\ndependence on time series properties and sample size. To enhance accuracy, we\\nintroduce an optimal rate method with an asymptotic distribution, facilitating\\nthe construction of confidence intervals. The proposed method effectively\\nidentifies seizure onset in EEG data and extends to event detection in video\\ndata. Comprehensive numerical experiments demonstrate its superior performance\\ncompared to existing techniques.',\n",
       "  'keywords': ['Confidence interval; spectral density functions; optimal detection; Wold repre-']},\n",
       " {'title': 'Tracy-Widom, Gaussian, and Bootstrap: Approximations for Leading\\n  Eigenvalues in High-Dimensional PCA',\n",
       "  'authors': ['Nina Dörnemann', 'Miles E. Lopes'],\n",
       "  'summary': 'Under certain conditions, the largest eigenvalue of a sample covariance\\nmatrix undergoes a well-known phase transition when the sample size $n$ and\\ndata dimension $p$ diverge proportionally. In the subcritical regime, this\\neigenvalue has fluctuations of order $n^{-2/3}$ that can be approximated by a\\nTracy-Widom distribution, while in the supercritical regime, it has\\nfluctuations of order $n^{-1/2}$ that can be approximated with a Gaussian\\ndistribution. However, the statistical problem of determining which regime\\nunderlies a given dataset is far from resolved. We develop a new testing\\nframework and procedure to address this problem. In particular, we demonstrate\\nthat the procedure has an asymptotically controlled level, and that it is power\\nconsistent for certain alternatives. Also, this testing procedure enables the\\ndesign a new bootstrap method for approximating the distributions of\\nfunctionals of the leading sample eigenvalues within the subcritical regime --\\nwhich is the first such method that is supported by theoretical guarantees.',\n",
       "  'keywords': ['High-dimensional statistics',\n",
       "   'hypothesis testing',\n",
       "   'bootstrap',\n",
       "   'covariance matri-']},\n",
       " {'title': 'Optimal treatment regimes for the net benefit of a treatment',\n",
       "  'authors': ['François Petit', 'Gérard Biau', 'Raphaël Porcher'],\n",
       "  'summary': \"We developed a mathematical setup inspired by Buyse's generalized pairwise\\ncomparisons to define a notion of optimal individualized treatment rule (ITR)\\nin the presence of prioritized outcomes in a randomized controlled trial,\\nterming such an ITR pairwise optimal. We present two approaches to estimate\\npairwise optimal ITRs. The first is a variant of the k-nearest neighbors\\nalgorithm. The second is a meta-learner based on a randomized bagging scheme,\\nallowing the use of any classification algorithm for constructing an ITR. We\\nstudy the behavior of these estimation schemes from a theoretical standpoint\\nand through Monte Carlo simulations and illustrate their use on trial data.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Inference on effect size after multiple hypothesis testing',\n",
       "  'authors': ['Andreas Dzemski', 'Ryo Okui', 'Wenjie Wang'],\n",
       "  'summary': 'Significant treatment effects are often emphasized when interpreting and\\nsummarizing empirical findings in studies that estimate multiple, possibly\\nmany, treatment effects. Under this kind of selective reporting, conventional\\ntreatment effect estimates may be biased and their corresponding confidence\\nintervals may undercover the true effect sizes. We propose new estimators and\\nconfidence intervals that provide valid inferences on the effect sizes of the\\nsignificant effects after multiple hypothesis testing. Our methods are based on\\nthe principle of selective conditional inference and complement a wide range of\\ntests, including step-up tests and bootstrap-based step-down tests. Our\\napproach is scalable, allowing us to study an application with over 370\\nestimated effects. We justify our procedure for asymptotically normal treatment\\neffect estimators. We provide two empirical examples that demonstrate bias\\ncorrection and confidence interval adjustments for significant effects. The\\nmagnitude and direction of the bias correction depend on the correlation\\nstructure of the estimated effects and whether the interpretation of the\\nsignificant effects depends on the (in)significance of other effects.',\n",
       "  'keywords': ['Multiple hypothesis testing',\n",
       "   'post-selection inference',\n",
       "   'conditional']},\n",
       " {'title': 'Conditional Extreme Value Estimation for Dependent Time Series',\n",
       "  'authors': ['Martin Bladt', 'Laurits Glargaard', 'Theodor Henningsen'],\n",
       "  'summary': 'We study the consistency and weak convergence of the conditional tail\\nfunction and conditional Hill estimators under broad dependence assumptions for\\na heavy-tailed response sequence and a covariate sequence. Consistency is\\nestablished under $\\\\alpha$-mixing, while asymptotic normality follows from\\n$\\\\beta$-mixing and second-order conditions. A key aspect of our approach is its\\nversatile functional formulation in terms of the conditional tail process.\\nSimulations demonstrate its performance across dependence scenarios. We apply\\nour method to extreme event modeling in the oil industry, revealing distinct\\ntail behaviors under varying conditioning values.',\n",
       "  'keywords': ['Extremevalues;Timeseries;Conditionalregularvariation;Weakconvergence']},\n",
       " {'title': 'Asymptotic Behavior of Principal Component Projections for Multivariate\\n  Extremes',\n",
       "  'authors': ['Holger Drees'],\n",
       "  'summary': 'The extremal dependence structure of a regularly varying $d$-dimensional\\nrandom vector can be described by its angular measure. The standard\\nnonparametric estimator of this measure is the empirical measure of the\\nobserved angles of the $k$ random vectors with largest norm, for a suitably\\nchosen number $k$. Due to the curse of dimensionality, for moderate or large\\n$d$, this estimator is often inaccurate. If the angular measure is concentrated\\non a vicinity of a lower dimensional subspace, then first projecting the data\\non a lower dimensional subspace obtained by a principal component analysis of\\nthe angles of extreme observations can substantially improve the performance of\\nthe estimator.\\n  We derive the asymptotic behavior of such PCA projections and the resulting\\nexcess risk. In particular, it is shown that, under mild conditions, the excess\\nrisk (as a function of $k$) decreases much faster than it was suggested by\\nempirical risk bounds obtained in \\\\cite{DS21}. Moreover, functional limit\\ntheorems for local empirical processes of the (empirical) reconstruction error\\nof projections uniformly over neighborhoods of the true optimal projection are\\nestablished. Based on these asymptotic results, we propose a data-driven method\\nto select the dimension of the projection space. Finally, the finite sample\\nperformance of resulting estimators is examined in a simulation study.',\n",
       "  'keywords': None},\n",
       " {'title': 'An Improved Satterthwaite Effective Degrees of Freedom Correction for\\n  Weighted Syntheses of Variance',\n",
       "  'authors': ['Matthias von Davier'],\n",
       "  'summary': 'This article presents an improved approximation for the effective degrees of\\nfreedom in the Satterthwaite (1941, 1946) method which estimates the\\ndistribution of a weighted combination of variance components The standard\\nSatterthwaite approximation assumes a scaled chisquare distribution for the\\ncomposite variance estimator but is known to be biased downward when component\\ndegrees of freedom are small. Building on recent work by von Davier (2025) we\\npropose an adjusted estimator that corrects this bias by modifying both the\\nnumerator and denominator of the traditional formula. The new approximation\\nincorporates a weighted average of component degrees of freedom and a scaling\\nfactor that ensures consistency as the number of components or their degrees of\\nfreedom increases. We demonstrate the utility of this adjustment in practical\\nsettings including Rubins (1987) total variance estimation in multiple\\nimputations where weighted variance combinations are common. The proposed\\nestimator generalizes von Daviers (2025) unweighted case and more accurately\\napproximates synthetic variance estimators with arbitrary weights.',\n",
       "  'keywords': ['Satterthwaite approximation', 'effective degrees of free-']},\n",
       " {'title': 'Rolled Gaussian process models for curves on manifolds',\n",
       "  'authors': ['Simon Preston',\n",
       "   'Karthik Bharath',\n",
       "   'Pablo Lopez-Custodio',\n",
       "   'Alfred Kume'],\n",
       "  'summary': 'Given a planar curve, imagine rolling a sphere along that curve without\\nslipping or twisting, and by this means tracing out a curve on the sphere. It\\nis well known that such a rolling operation induces a local isometry between\\nthe sphere and the plane so that the two curves uniquely determine each other,\\nand moreover, the operation extends to a general class of manifolds in any\\ndimension. We use rolling to construct an analogue of a Gaussian process on a\\nmanifold starting from a Euclidean Gaussian process. The resulting model is\\ngenerative, and is amenable to statistical inference given data as curves on a\\nmanifold. We illustrate with examples on the unit sphere, symmetric\\npositive-definite matrices, and with a robotics application involving 3D\\norientations.',\n",
       "  'keywords': None},\n",
       " {'title': 'Wasserstein bounds for non-linear Gaussian filters',\n",
       "  'authors': ['Toni Karvonen', 'Simo Särkkä'],\n",
       "  'summary': \"Most Kalman filters for non-linear systems, such as the unscented Kalman\\nfilter, are based on Gaussian approximations. We use Poincar\\\\'e inequalities to\\nbound the Wasserstein distance between the true joint distribution of the\\nprediction and measurement and its Gaussian approximation. The bounds can be\\nused to assess the performance of non-linear Gaussian filters and determine\\nthose filtering approximations that are most likely to induce error.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Locally minimax optimal and dimension-agnostic discrete argmin inference',\n",
       "  'authors': ['Ilmun Kim', 'Aaditya Ramdas'],\n",
       "  'summary': 'We revisit the discrete argmin inference problem in high-dimensional\\nsettings. Given $n$ observations from a $d$ dimensional vector, the goal is to\\ntest whether the $r$th component of the mean vector is the smallest among all\\ncomponents. We propose dimension-agnostic tests that maintain validity\\nregardless of how $d$ scales with $n$, and regardless of arbitrary ties in the\\nmean vector. Notably, our validity holds under mild moment conditions,\\nrequiring little more than finiteness of a second moment, and permitting\\npossibly strong dependence between coordinates. In addition, we establish the\\nlocal minimax separation rate for this problem, which adapts to the cardinality\\nof a confusion set, and show that the proposed tests attain this rate. Our\\nmethod uses the sample splitting and self-normalization approach of Kim and\\nRamdas (2024). Our tests can be easily inverted to yield confidence sets for\\nthe argmin index. Empirical results illustrate the strong performance of our\\napproach in terms of type I error control and power compared to existing\\nmethods.',\n",
       "  'keywords': None},\n",
       " {'title': 'Empirical Measures and Strong Laws of Large Numbers in Categorical\\n  Probability',\n",
       "  'authors': ['Tobias Fritz',\n",
       "   'Tomáš Gonda',\n",
       "   'Antonio Lorenzin',\n",
       "   'Paolo Perrone',\n",
       "   'Areeb Shah Mohammed'],\n",
       "  'summary': \"The Glivenko-Cantelli theorem is a uniform version of the strong law of large\\nnumbers. It states that for every IID sequence of random variables, the\\nempirical measure converges to the underlying distribution (in the sense of\\nuniform convergence of the CDF). In this work, we provide tools to study such\\nlimits of empirical measures in categorical probability.\\n  We propose two axioms, permutation invariance and empirical adequacy, that a\\nmorphism of type $X^\\\\mathbb{N} \\\\to X$ should satisfy to be interpretable as\\ntaking an infinite sequence as input and producing a sample from its empirical\\nmeasure as output. Since not all sequences have a well-defined empirical\\nmeasure, ``such empirical sampling morphisms'' live in quasi-Markov categories,\\nwhich, unlike Markov categories, allow partial morphisms. Given an empirical\\nsampling morphism and a few other properties, we prove representability as well\\nas abstract versions of the de Finetti theorem, the Glivenko-Cantelli theorem\\nand the strong law of large numbers.\\n  We provide several concrete constructions of empirical sampling morphisms as\\npartially defined Markov kernels on standard Borel spaces. Instantiating our\\nabstract results then recovers the standard Glivenko-Cantelli theorem and the\\nstrong law of large numbers for random variables with finite first moment. Our\\nwork thus provides a joint proof of these two theorems in conjunction with the\\nde Finetti theorem from first principles.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Constraint-based causal discovery with tiered background knowledge and\\n  latent variables in single or overlapping datasets',\n",
       "  'authors': ['Christine W. Bang', 'Vanessa Didelez'],\n",
       "  'summary': \"In this paper we consider the use of tiered background knowledge within\\nconstraint based causal discovery. Our focus is on settings relaxing causal\\nsufficiency, i.e. allowing for latent variables which may arise because\\nrelevant information could not be measured at all, or not jointly, as in the\\ncase of multiple overlapping datasets. We first present novel insights into the\\nproperties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce\\na new extension of the IOD (integrating overlapping datasets) algorithm\\nincorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm.\\nWe show that under full usage of the tiered background knowledge tFCI and tIOD\\nare sound, while simple versions of the tIOD and tFCI are sound and complete.\\nWe further show that the tIOD algorithm can often be expected to be\\nconsiderably more efficient and informative than the IOD algorithm even beyond\\nthe obvious restriction of the Markov equivalence classes. We provide a formal\\nresult on the conditions for this gain in efficiency and informativeness. Our\\nresults are accompanied by a series of examples illustrating the exact role and\\nusefulness of tiered background knowledge.\",\n",
       "  'keywords': ['Causalinference',\n",
       "   'graphicalmodels',\n",
       "   'multi-cohortstudies',\n",
       "   'temporalstructure']},\n",
       " {'title': 'Sparse Bayesian Learning for Label Efficiency in Cardiac Real-Time MRI',\n",
       "  'authors': ['Felix Terhag',\n",
       "   'Philipp Knechtges',\n",
       "   'Achim Basermann',\n",
       "   'Anja Bach',\n",
       "   'Darius Gerlach',\n",
       "   'Jens Tank',\n",
       "   'Raúl Tempone'],\n",
       "  'summary': 'Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology\\nthat images the heart at up to 50 frames per second, offering insight into the\\nrespiratory effects on the heartbeat. However, this method significantly\\nincreases the number of images that must be segmented to derive critical health\\nindicators. Although neural networks perform well on inner slices, predictions\\non outer slices are often unreliable.\\n  This work proposes sparse Bayesian learning (SBL) to predict the ventricular\\nvolume on outer slices with minimal manual labeling to address this challenge.\\nThe ventricular volume over time is assumed to be dominated by sparse\\nfrequencies corresponding to the heart and respiratory rates. Moreover, SBL\\nidentifies these sparse frequencies on well-segmented inner slices by\\noptimizing hyperparameters via type -II likelihood, automatically pruning\\nirrelevant components. The identified sparse frequencies guide the selection of\\nouter slice images for labeling, minimizing posterior variance.\\n  This work provides performance guarantees for the greedy algorithm. Testing\\non patient data demonstrates that only a few labeled images are necessary for\\naccurate volume prediction. The labeling procedure effectively avoids selecting\\ninefficient images. Furthermore, the Bayesian approach provides uncertainty\\nestimates, highlighting unreliable predictions (e.g., when choosing suboptimal\\nlabels).',\n",
       "  'keywords': ['SparseBayesianlearning',\n",
       "   'labelefficiency',\n",
       "   'expectationmaximization(EM)']},\n",
       " {'title': 'Robust Mean Estimation for Optimization: The Impact of Heavy Tails',\n",
       "  'authors': ['Bart P. G. van Parys', 'Bert Zwart'],\n",
       "  'summary': 'We consider the problem of constructing a least conservative estimator of the\\nexpected value $\\\\mu$ of a non-negative heavy-tailed random variable. We require\\nthat the probability of overestimating the expected value $\\\\mu$ is kept\\nappropriately small; a natural requirement if its subsequent use in a decision\\nprocess is anticipated. In this setting, we show it is optimal to estimate\\n$\\\\mu$ by solving a distributionally robust optimization (DRO) problem using the\\nKullback-Leibler (KL) divergence. We further show that the statistical\\nproperties of KL-DRO compare favorably with other estimators based on\\ntruncation, variance regularization, or Wasserstein DRO.',\n",
       "  'keywords': None},\n",
       " {'title': 'Safety of particle filters: Some results on the time evolution of\\n  particle filter estimates',\n",
       "  'authors': ['Mathieu Gerber'],\n",
       "  'summary': 'Particle filters (PFs) is a class of Monte Carlo algorithms that propagate\\nover time a set of $N\\\\in\\\\mathbb{N}$ particles which can be used to estimate, in\\nan online fashion, the sequence of filtering distributions\\n$(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ defined by a state-space model. Despite the\\npopularity of PFs, the study of the time evolution of their estimates has only\\nreceived very little attention in the literature. Denoting by\\n$(\\\\hat{\\\\eta}_t^N)_{t\\\\geq 1}$ the PF estimate of $(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ and\\nletting $\\\\kappa\\\\in (0,1)$, in this work we first show that for any number of\\nparticles $N$ it holds that, with probability one, we have $\\\\|\\\\hat{\\\\eta}_t^N-\\n\\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa$ for infinitely many $t\\\\geq 1$, with $\\\\|\\\\cdot\\\\|$ a\\nmeasure of distance between probability distributions. Considering a simple\\nfiltering problem we then provide reassuring results concerning the ability of\\nPFs to estimate jointly a finite set $\\\\{\\\\hat{\\\\eta}_t\\\\}_{t=1}^T$ of filtering\\ndistributions by studying\\n$\\\\P(\\\\sup_{t\\\\in\\\\{1,\\\\dots,T\\\\}}\\\\|\\\\hat{\\\\eta}_t^{N}-\\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa)$.\\nFinally, on the same toy filtering problem, we prove that sequential\\nquasi-Monte Carlo, a randomized quasi-Monte Carlo version of PF algorithms,\\noffers greater safety guarantees than PFs in the sense that, for this\\nalgorithm, it holds that $\\\\lim_{N\\\\rightarrow\\\\infty}\\\\sup_{t\\\\geq\\n1}\\\\|\\\\hat{\\\\eta}_t^N-\\\\hat{\\\\eta}_t\\\\|=0$ with probability one.',\n",
       "  'keywords': None},\n",
       " {'title': 'G{é}n{é}ration de Matrices de Corr{é}lation avec des Structures de\\n  Graphe par Optimisation Convexe',\n",
       "  'authors': ['Ali Fahkar', 'Kévin Polisano', 'Irène Gannaz', 'Sophie Achard'],\n",
       "  'summary': 'This work deals with the generation of theoretical correlation matrices with\\nspecific sparsity patterns, associated to graph structures. We present a novel\\napproach based on convex optimization, offering greater flexibility compared to\\nexisting techniques, notably by controlling the mean of the entry distribution\\nin the generated correlation matrices. This allows for the generation of\\ncorrelation matrices that better represent realistic data and can be used to\\nbenchmark statistical methods for graph inference.',\n",
       "  'keywords': None},\n",
       " {'title': 'Use of copula functions in error assessment due to deviation from\\n  dependence assumption',\n",
       "  'authors': ['Subarna Bhattacharjee',\n",
       "   'Aninda Kumar Nanda',\n",
       "   'Subhashree Patra'],\n",
       "  'summary': 'In this paper, we analyze the relative errors in various reliability measures\\ndue to the tacit assumption that the components associated with a $n$-component\\nseries system or a parallel system are independently working where the\\ncomponents are dependent. We use Copula functions in said error analysis. This\\ntechnique generalizes the existing work on error assessment for many wide class\\nof distributions.',\n",
       "  'keywords': None},\n",
       " {'title': 'Use of stochastic orders and statistical dependence in error analysis\\n  for multi-component system',\n",
       "  'authors': ['Subarna Bhattacharjee',\n",
       "   'Aninda Kumar Nanda',\n",
       "   'Subhashree Patra'],\n",
       "  'summary': 'In this paper, we analyze the relative errors that crop up in the various\\nreliability measures due to the tacit assumption that the components are\\nindependently working associated with a $n$-component series system or a\\nparallel system where the components are dependent and follow a well-defined\\nmultivariate Weibull or exponential distribution. We also list some important\\nobservations which the previous authors have not noted in their earlier works.\\nIn this paper, we focus on the incurred error in multi-component series and\\nparallel systems having multivariate Weibull distributions. In the upcoming\\nsections, we establish that the present study has relevance with stochastic\\norders and statistical dependence which were not previously pointed out by\\nprevious authors.',\n",
       "  'keywords': None},\n",
       " {'title': 'A computational theory of evaluation for parameterisable subject',\n",
       "  'authors': ['Hedong Yan'],\n",
       "  'summary': 'Evaluation is critical to advance decision making across domains, yet\\nexisting methodologies often struggle to balance theoretical rigor and\\npractical scalability. In order to reduce the cost of experimental evaluation,\\nwe introduce a computational theory of evaluation for parameterisable subjects.\\nWe prove upper bounds of generalized evaluation error and generalized causal\\neffect error of evaluation metric on subject. We also prove efficiency, and\\nconsistency to estimated causal effect of subject on metric by prediction. To\\noptimize evaluation models, we propose a meta-learner to handle heterogeneous\\nevaluation subjects space. Comparing with other computational approaches, our\\n(conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12\\nscenes, including individual medicine, scientific simulation, business\\nactivities, and quantum trade. The evaluation time is reduced 3-7 order of\\nmagnitude comparing with experiments or simulations.',\n",
       "  'keywords': ['evaluatology',\n",
       "   'computationalevaluation',\n",
       "   'errorupperbound',\n",
       "   '']},\n",
       " {'title': 'Variable selection via thresholding',\n",
       "  'authors': ['Ka Long Keith Ho', 'Hien Duy Nguyen'],\n",
       "  'summary': 'Variable selection comprises an important step in many modern statistical\\ninference procedures. In the regression setting, when estimators cannot shrink\\nirrelevant signals to zero, covariates without relationships to the response\\noften manifest small but non-zero regression coefficients. The ad hoc procedure\\nof discarding variables whose coefficients are smaller than some threshold is\\noften employed in practice. We formally analyze a version of such thresholding\\nprocedures and develop a simple thresholding method that consistently estimates\\nthe set of relevant variables under mild regularity assumptions. Using this\\nthresholding procedure, we propose a sparse, $\\\\sqrt{n}$-consistent and\\nasymptotically normal estimator whose non-zero elements do not exhibit\\nshrinkage. The performance and applicability of our approach are examined via\\nnumerical studies of simulated and real data.',\n",
       "  'keywords': ['Hard thresholding; Variable selection consistency; Sparse estimation; Asymptotic']},\n",
       " {'title': 'Teachable normal approximations to binomial and related probabilities or\\n  confidence bounds',\n",
       "  'authors': ['Lutz Mattner'],\n",
       "  'summary': \"This document is an extended version of an abstract for a talk, with\\napproximately the same title, to be held at the 7th Joint Statistical Meeting\\nof the Deutsche Arbeitsgemeinschaft Statistik, from 24 to 28 March 2025 in\\nBerlin.\\n  Here ``teachable'' is meant to apply to people ranging from sufficiently\\nadvanced high school pupils to university students in mathematics or\\nstatistics: For understanding most of the proposed approximation results, it\\nshould suffice to know binomial laws, their means and variances, and the\\nstandard normal distribution function (but not necessarily the concept of a\\ncorresponding normal random variable).\\n  Of the proposed approximations, some are well-known (at least to experts),\\nand some are based on teaching experience and research at Trier University.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Parameter estimation for fractional autoregressive process with periodic\\n  structure',\n",
       "  'authors': ['Chunhao Cai', 'Yiwu Shang'],\n",
       "  'summary': 'This paper introduces a new kind of periodic fractional autoregressive\\nprocess (PFAR) driven by fractional Gaussian noise (fGn). The new model is a\\nspecialized varying coefficient fractional autoregressive model, where the\\ncoefficients adhere to a periodic structure. In this working, Generalized least\\nsquares estimation and GPH method are employed to construct an initial\\nestimator to estimate the joint estimation of the parameters of these models.\\nThen one-step procedure is used to obtain a more asymptotically-efficient\\nestimator. The paper proves that both estimators are consistent and\\nasymptotically normal, and their performance is demonstrated through a\\nsimulation study using finite-size samples via Monte Carlo simulations.\\nSimulation studies suggests that, while both estimation methods can accurately\\nestimate the model, the one-step estimator outperforms the initial estimator.',\n",
       "  'keywords': None},\n",
       " {'title': 'Stochastic Transport Maps in Diffusion Models and Sampling',\n",
       "  'authors': ['Xicheng Zhang'],\n",
       "  'summary': \"In this work, we present a theoretical and computational framework for\\nconstructing stochastic transport maps between probability distributions using\\ndiffusion processes. We begin by proving that the time-marginal distribution of\\nthe sum of two independent diffusion processes satisfies a Fokker-Planck\\nequation. Building on this result and applying Ambrosio-Figalli-Trevisan's\\nsuperposition principle, we establish the existence and uniqueness of solutions\\nto the associated stochastic differential equation (SDE). Leveraging these\\ntheoretical foundations, we develop a method to construct (stochastic)\\ntransport maps between arbitrary probability distributions using dynamical\\nordinary differential equations (ODEs) and SDEs. Furthermore, we introduce a\\nunified framework that generalizes and extends a broad class of diffusion-based\\ngenerative models and sampling techniques. Finally, we analyze the convergence\\nproperties of particle approximations for the SDEs underlying our framework,\\nproviding theoretical guarantees for their practical implementation. This work\\nbridges theoretical insights with practical applications, offering new tools\\nfor generative modeling and sampling in high-dimensional spaces.\",\n",
       "  'keywords': ['Transportmaps',\n",
       "   'Superpositionprinciple',\n",
       "   'Sampling',\n",
       "   'Diffusionmodels',\n",
       "   'Fokker-Planckequations.']},\n",
       " {'title': 'Revisiting general source condition in learning over a Hilbert space',\n",
       "  'authors': ['Naveen Gupta', 'S. Sivananthan'],\n",
       "  'summary': 'In Learning Theory, the smoothness assumption on the target function (known\\nas source condition) is a key factor in establishing theoretical convergence\\nrates for an estimator. The existing general form of the source condition, as\\ndiscussed in learning theory literature, has traditionally been restricted to a\\nclass of functions that can be expressed as a product of an operator monotone\\nfunction and a Lipschitz continuous function. In this note, we remove these\\nrestrictions on the index function and establish optimal convergence rates for\\nleast-square regression over a Hilbert space with general regularization under\\na general source condition, thereby significantly broadening the scope of\\nexisting theoretical results.',\n",
       "  'keywords': None},\n",
       " {'title': 'Concentration inequalities for the sum in sampling without replacement:\\n  an approach via majorization',\n",
       "  'authors': ['Jianhang Ai', 'Ondřej Kuželka', 'Christos Pelekis'],\n",
       "  'summary': 'Let $P=(x_1,\\\\ldots,x_n)$ be a population consisting of $n\\\\ge 2$ real numbers\\nwhose sum is zero, and let $k <n$ be a positive integer. We sample $k$ elements\\nfrom $P$ without replacement and denote by $X_P$ the sum of the elements in our\\nsample. In this article, using ideas from the theory of majorization, we deduce\\nnon-asymptotic lower and upper bounds on the probability that $X_P$ exceeds its\\nexpected value.',\n",
       "  'keywords': ['probability inequalities; sampling without replacement; majorization; non-negative k-']},\n",
       " {'title': 'Estimation and variable selection in nonlinear mixed-effects models',\n",
       "  'authors': ['Antoine Caillebotte', 'Estelle Kuhn', 'Sarah Lemler'],\n",
       "  'summary': 'We consider nonlinear mixed effects models including high-dimensional\\ncovariates to model individual parameters. The objective is to identify\\nrelevant covariates and estimate model parameters. We combine a penalized\\nLASSO-type estimator with an eBIC model choice criterion to select the\\ncovariates of interest. Then we estimate the parameters by maximum likelihood\\nin the reduced model. We calculate the LASSO-type penalized estimator by a\\nweighted proximal gradient descent algorithm with an adaptive learning rate.\\nThis choice allows us in particular to consider models that do not necessarily\\nbelong to the curved exponential family. We compare first the performance of\\nthe proposed methodology with those of the glmmLasso procedure in a linear\\nmixed effects model in a simulation study. We then illustrate its performance\\nin a nonlinear mixed-effects logistic growth model through simulation.',\n",
       "  'keywords': None},\n",
       " {'title': 'Nonparametric MLE for Gaussian Location Mixtures: Certified Computation\\n  and Generic Behavior',\n",
       "  'authors': ['Yury Polyanskiy', 'Mark Sellke'],\n",
       "  'summary': 'We study the nonparametric maximum likelihood estimator $\\\\widehat{\\\\pi}$ for\\nGaussian location mixtures in one dimension. It has been known since (Lindsay,\\n1983) that given an $n$-point dataset, this estimator always returns a mixture\\nwith at most $n$ components, and more recently (Wu-Polyanskiy, 2020) gave a\\nsharp $O(\\\\log n)$ bound for subgaussian data. In this work we study\\ncomputational aspects of $\\\\widehat{\\\\pi}$. We provide an algorithm which for\\nsmall enough $\\\\varepsilon>0$ computes an $\\\\varepsilon$-approximation of\\n$\\\\widehat\\\\pi$ in Wasserstein distance in time $K+Cnk^2\\\\log\\\\log(1/\\\\varepsilon)$.\\nHere $K$ is data-dependent but independent of $\\\\varepsilon$, while $C$ is an\\nabsolute constant and $k=|supp(\\\\widehat{\\\\pi})|\\\\leq n$ is the number of atoms in\\n$\\\\widehat\\\\pi$. We also certifiably compute the exact value of\\n$|supp(\\\\widehat\\\\pi)|$ in finite time. These guarantees hold almost surely\\nwhenever the dataset $(x_1,\\\\dots,x_n)\\\\in [-cn^{1/4},cn^{1/4}]$ consists of\\nindependent points from a probability distribution with a density (relative to\\nLebesgue measure). We also show the distribution of $\\\\widehat\\\\pi$ conditioned\\nto be $k$-atomic admits a density on the associated $2k-1$ dimensional\\nparameter space for all $k\\\\leq \\\\sqrt{n}/3$, and almost sure locally linear\\nconvergence of the EM algorithm. One key tool is a classical Fourier analytic\\nestimate for non-degenerate curves.',\n",
       "  'keywords': None},\n",
       " {'title': 'Functional structural equation models with out-of-sample guarantees',\n",
       "  'authors': ['Philip Kennerberg', 'Ernst C. Wit'],\n",
       "  'summary': 'Statistical learning methods typically assume that the training and test data\\noriginate from the same distribution, enabling effective risk minimization.\\nHowever, real-world applications frequently involve distributional shifts,\\nleading to poor model generalization. To address this, recent advances in\\ncausal inference and robust learning have introduced strategies such as\\ninvariant causal prediction and anchor regression. While these approaches have\\nbeen explored for traditional structural equation models (SEMs), their\\nextension to functional systems remains limited. This paper develops a risk\\nminimization framework for functional SEMs using linear, potentially unbounded\\noperators. We introduce a functional worst-risk minimization approach, ensuring\\nrobust predictive performance across shifted environments. Our key contribution\\nis a novel worst-risk decomposition theorem, which expresses the maximum\\nout-of-sample risk in terms of observed environments. We establish conditions\\nfor the existence and uniqueness of the worst-risk minimizer and provide\\nconsistent estimation procedures. Empirical results on functional systems\\nillustrate the advantages of our method in mitigating distributional shifts.\\nThese findings contribute to the growing literature on robust functional\\nregression and causal learning, offering practical guarantees for out-of-sample\\ngeneralization in dynamic environments.',\n",
       "  'keywords': None},\n",
       " {'title': 'A Martingale Approach to Large-$θ$ Ewens-Pitman Model',\n",
       "  'authors': ['Rodrigo Ribeiro'],\n",
       "  'summary': 'We investigate the asymptotic behavior of the number of parts $K_n$ in the\\nEwens--Pitman partition model under the regime where the diversity parameter is\\nscaled linearly with the sample size, that is, $\\\\theta = \\\\lambda n$ for\\nsome~$\\\\lambda > 0$. While recent work has established a law of large numbers\\n(LLN) and a central limit theorem (CLT) for $K_n$ in this regime, we revisit\\nthese results through a martingale-based approach. Our method yields\\nsignificantly shorter proofs, and leads to sharper convergence rates in the\\nCLT, including improved Berry--Esseen bounds in the case $\\\\alpha = 0$, and a\\nnew result for the regime $\\\\alpha \\\\in (0,1)$, filling a gap in the literature.',\n",
       "  'keywords': None},\n",
       " {'title': 'Estimation of accuracy and reliability of models of\\n  $\\\\varphi$-sub-Gaussian stochastic processes in $C(T)$ spaces',\n",
       "  'authors': ['Oleksandr Mokliachuk'],\n",
       "  'summary': \"At present, in the theory of stochastic process modeling a problem of\\nassessment of reliability and accuracy of stochastic process model in $C(T)$\\nspace wasn't studied for the case of implicit decomposition of process in the\\nform of a series with independent terms. The goal is to study reliability and\\naccuracy in $C(T)$ of models of processes from $Sub_\\\\varphi(\\\\Omega)$ that\\ncannot be decomposed in a series with independent elements explicitly. Using\\nprevious research in the field of modeling of stochastic processes, assumption\\nis considered about possibility of decomposition of a stochastic process in the\\nseries with independent elements that can be found using approximations. Impact\\nof approximation error of process decomposition in series with independent\\nelements on reliability and accuracy of modeling of stochastic process in\\n$C(T)$ is studied. Theorems are proved that allow estimation of reliability and\\naccuracy of a model in $C(T)$ of a stochastic process from\\n$Sub_\\\\varphi(\\\\Omega)$ in the case when decomposition of this process in a\\nseries with independent elements can be found only with some error, for\\nexample, using numerical approximations.\",\n",
       "  'keywords': ['stochastic processes; \\uf06a-sub-Gaussian processes; models of stochastic processes; reliability and accuracy of']},\n",
       " {'title': 'Interpretable Deep Regression Models with Interval-Censored Failure Time\\n  Data',\n",
       "  'authors': ['Changhui Yuan',\n",
       "   'Shishun Zhao',\n",
       "   'Shuwei Li',\n",
       "   'Xinyuan Song',\n",
       "   'Zhao Chen'],\n",
       "  'summary': \"Deep neural networks (DNNs) have become powerful tools for modeling complex\\ndata structures through sequentially integrating simple functions in each\\nhidden layer. In survival analysis, recent advances of DNNs primarily focus on\\nenhancing model capabilities, especially in exploring nonlinear covariate\\neffects under right censoring. However, deep learning methods for\\ninterval-censored data, where the unobservable failure time is only known to\\nlie in an interval, remain underexplored and limited to specific data type or\\nmodel. This work proposes a general regression framework for interval-censored\\ndata with a broad class of partially linear transformation models, where key\\ncovariate effects are modeled parametrically while nonlinear effects of\\nnuisance multi-modal covariates are approximated via DNNs, balancing\\ninterpretability and flexibility. We employ sieve maximum likelihood estimation\\nby leveraging monotone splines to approximate the cumulative baseline hazard\\nfunction. To ensure reliable and tractable estimation, we develop an EM\\nalgorithm incorporating stochastic gradient descent. We establish the\\nasymptotic properties of parameter estimators and show that the DNN estimator\\nachieves minimax-optimal convergence. Extensive simulations demonstrate\\nsuperior estimation and prediction accuracy over state-of-the-art methods.\\nApplying our method to the Alzheimer's Disease Neuroimaging Initiative dataset\\nyields novel insights and improved predictive performance compared to\\ntraditional approaches.\",\n",
       "  'keywords': ['EM algorithm',\n",
       "   'Interval censoring',\n",
       "   'Partially linear model',\n",
       "   'Splines',\n",
       "   'Neural net-']},\n",
       " {'title': 'No-prior Bayesian inference reIMagined: probabilistic approximations of\\n  inferential models',\n",
       "  'authors': ['Ryan Martin'],\n",
       "  'summary': 'When prior information is lacking, the go-to strategy for probabilistic\\ninference is to combine a \"default prior\" and the likelihood via Bayes\\'s\\ntheorem. Objective Bayes, (generalized) fiducial inference, etc. fall under\\nthis umbrella. This construction is natural, but the corresponding posterior\\ndistributions generally only offer limited, approximately valid uncertainty\\nquantification. The present paper takes a reimagined approach offering\\nposterior distributions with stronger reliability properties. The proposed\\nconstruction starts with an inferential model (IM), one that takes the\\nmathematical form of a data-driven possibility measure and features exactly\\nvalid uncertainty quantification, and then returns a so-called inner\\nprobabilistic approximation thereof. This inner probabilistic approximation\\ninherits many of the original IM\\'s desirable properties, including credible\\nsets with exact coverage and asymptotic efficiency. The approximation also\\nagrees with the familiar Bayes/fiducial solution obtained in applications where\\nthe model has a group transformation structure. A Monte Carlo method for\\nevaluating the probabilistic approximation is presented, along with numerical\\nillustrations.',\n",
       "  'keywords': None},\n",
       " {'title': 'Lean Formalization of Generalization Error Bound by Rademacher\\n  Complexity',\n",
       "  'authors': ['Sho Sonoda',\n",
       "   'Kazumi Kasaura',\n",
       "   'Yuma Mizuno',\n",
       "   'Kei Tsukamoto',\n",
       "   'Naoto Onda'],\n",
       "  'summary': \"We formalize the generalization error bound using Rademacher complexity in\\nthe Lean 4 theorem prover. Generalization error quantifies the gap between a\\nlearning machine's performance on given training data versus unseen test data,\\nand Rademacher complexity serves as an estimate of this error based on the\\ncomplexity of learning machines, or hypothesis class. Unlike traditional\\nmethods such as PAC learning and VC dimension, Rademacher complexity is\\napplicable across diverse machine learning scenarios including deep learning\\nand kernel methods. We formalize key concepts and theorems, including the\\nempirical and population Rademacher complexities, and establish generalization\\nerror bounds through formal proofs of McDiarmid's inequality, Hoeffding's\\nlemma, and symmetrization arguments.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Detecting Arbitrary Planted Subgraphs in Random Graphs',\n",
       "  'authors': ['Dor Elimelech', 'Wasim Huleihel'],\n",
       "  'summary': \"The problems of detecting and recovering planted structures/subgraphs in\\nErd\\\\H{o}s-R\\\\'{e}nyi random graphs, have received significant attention over the\\npast three decades, leading to many exciting results and mathematical\\ntechniques. However, prior work has largely focused on specific ad hoc planted\\nstructures and inferential settings, while a general theory has remained\\nelusive. In this paper, we bridge this gap by investigating the detection of an\\n\\\\emph{arbitrary} planted subgraph $\\\\Gamma = \\\\Gamma_n$ in an Erd\\\\H{o}s-R\\\\'{e}nyi\\nrandom graph $\\\\mathcal{G}(n, q_n)$, where the edge probability within $\\\\Gamma$\\nis $p_n$. We examine both the statistical and computational aspects of this\\nproblem and establish the following results. In the dense regime, where the\\nedge probabilities $p_n$ and $q_n$ are fixed, we tightly characterize the\\ninformation-theoretic and computational thresholds for detecting $\\\\Gamma$, and\\nprovide conditions under which a computational-statistical gap arises. Most\\nnotably, these thresholds depend on $\\\\Gamma$ only through its number of edges,\\nmaximum degree, and maximum subgraph density. Our lower and upper bounds are\\ngeneral and apply to any value of $p_n$ and $q_n$ as functions of $n$.\\nAccordingly, we also analyze the sparse regime where $q_n =\\n\\\\Theta(n^{-\\\\alpha})$ and $p_n-q_n =\\\\Theta(q_n)$, with $\\\\alpha\\\\in[0,2]$, as well\\nas the critical regime where $p_n=1-o(1)$ and $q_n = \\\\Theta(n^{-\\\\alpha})$, both\\nof which have been widely studied, for specific choices of $\\\\Gamma$. For these\\nregimes, we show that our bounds are tight for all planted subgraphs\\ninvestigated in the literature thus far\\\\textemdash{}and many more. Finally, we\\nidentify conditions under which detection undergoes sharp phase transition,\\nwhere the boundaries at which algorithms succeed or fail shift abruptly as a\\nfunction of $q_n$.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Calibration Bands for Mean Estimates within the Exponential Dispersion\\n  Family',\n",
       "  'authors': ['Łukasz Delong', 'Selim Gatti', 'Mario V. Wüthrich'],\n",
       "  'summary': 'A statistical model is said to be calibrated if the resulting mean estimates\\nperfectly match the true means of the underlying responses. Aiming for\\ncalibration is often not achievable in practice as one has to deal with finite\\nsamples of noisy observations. A weaker notion of calibration is\\nauto-calibration. An auto-calibrated model satisfies that the expected value of\\nthe responses being given the same mean estimate matches this estimate. Testing\\nfor auto-calibration has only been considered recently in the literature and we\\npropose a new approach based on calibration bands. Calibration bands denote a\\nset of lower and upper bounds such that the probability that the true means lie\\nsimultaneously inside those bounds exceeds some given confidence level. Such\\nbands were constructed by Yang-Barber (2019) for sub-Gaussian distributions.\\nDimitriadis et al. (2023) then introduced narrower bands for the Bernoulli\\ndistribution and we use the same idea in order to extend the construction to\\nthe entire exponential dispersion family that contains for example the\\nbinomial, Poisson, negative binomial, gamma and normal distributions. Moreover,\\nwe show that the obtained calibration bands allow us to construct various tests\\nfor calibration and auto-calibration, respectively.',\n",
       "  'keywords': None},\n",
       " {'title': 'An improved central limit theorem for the empirical sliced Wasserstein\\n  distance',\n",
       "  'authors': ['David Rodríguez-Vítores',\n",
       "   'Eustasio del Barrio',\n",
       "   'Jean-Michel Loubes'],\n",
       "  'summary': 'Optimal transport theory has become a fundamental tool for handling diverse\\ntypes of data, with growing applications across various fields. However, the\\nWasserstein distance presents significant computational and statistical\\nchallenges in high-dimensional settings. To address these issues, alternative\\ndistances such as the sliced Wasserstein distance, which leverages\\none-dimensional projections, have been introduced. In this work, we establish a\\nnovel central limit theorem for the p-sliced Wasserstein distance, for p>1,\\nusing the Efron-Stein inequality-a technique that has proven effective in\\nrelated problems. This approach yields a central limit theorem centered at the\\nexpected value of the empirical cost, under mild regularity conditions.\\nNotably, unlike the general Wasserstein distance in arbitrary dimensions, we\\ndemonstrate that, under specific assumptions, the centering constants can be\\nreplaced by the population cost, which is essential for statistical inference.\\nThis generalizes and significantly refines existing results for the\\none-dimensional case. Consequently, we present the first asymptotically valid\\ninference framework for the sliced Wasserstein distance applicable to measures\\nthat are not necessarily compactly supported, for p>1. Finally, we address key\\npractical aspects for inference, including Monte Carlo estimation of the\\nintegral and estimation of the asymptotic variance, ensuring applicability in\\nreal-world scenarios.',\n",
       "  'keywords': None},\n",
       " {'title': 'Benign landscapes for synchronization on spheres via normalized\\n  Laplacian matrices',\n",
       "  'authors': ['Andrew D. McRae'],\n",
       "  'summary': \"We study the nonconvex optimization landscapes of synchronization problems on\\nspheres. First, we present new results for the statistical problem of\\nsynchronization over the two-element group $\\\\mathbf{Z}_2$. We consider the\\nnonconvex least-squares problem with $\\\\mathbf{Z}_2 = \\\\{\\\\pm 1\\\\}$ relaxed to the\\nunit sphere in $\\\\mathbf{R}^r$ for $r \\\\geq 2$; for several popular models,\\nincluding graph clustering under the binary stochastic block model, we show\\nthat, for any $r \\\\geq 2$, every second-order critical point recovers the ground\\ntruth in the asymptotic regimes where exact recovery is\\ninformation-theoretically possible. Such statistical optimality via spherical\\nrelaxations had previously only been shown for (potentially arbitrarily) larger\\nrelaxation dimension $r$. Second, we consider the global synchronization of\\nnetworks of coupled oscillators under the (homogeneous) Kuramoto model. We\\nprove new and optimal asymptotic results for random signed networks on an\\nErd\\\\H{o}s--R\\\\'enyi graph, and we give new and simple proofs for several\\nexisting state-of-the-art results. Our key tool is a deterministic landscape\\ncondition that extends a recent result of Rakoto Endor and Waldspurger. This\\nresult says that, if a certain problem-dependent Laplacian matrix has small\\nenough condition number, the nonconvex landscape is benign. Our extension\\nallows the condition number to include an arbitrary diagonal preconditioner,\\nwhich gives tighter results for many problems. We show that, for the\\nsynchronization of Kuramoto oscillator networks on nearest-neighbor circulant\\ngraphs as studied by Wiley, Strogatz, and Girvan, this condition is optimal. We\\nalso prove a natural complex extension that may be of interest for\\nsynchronization on the special orthogonal group $\\\\operatorname{SO}(2)$.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Confidence set for mixture order selection',\n",
       "  'authors': ['Alessandro Casa', 'Davide Ferrari'],\n",
       "  'summary': 'A fundamental challenge in the application of finite mixture models is\\nselecting the number of mixture components, also known as order. Traditional\\napproaches rely on selecting a single best model using information criteria.\\nHowever, in the presence of noisy data, and when models with different orders\\nyield similar fits, model selection uncertainty can be substantial, making it\\nchallenging to confidently identify the true number of components. In this\\npaper, we introduce the Model Selection Confidence Set (MSCS) for order\\nselection - a set-valued estimator that, with a predefined confidence level,\\nincludes the true mixture order across repeated samples. Rather than selecting\\na single model, our MSCS identifies all plausible orders by determining whether\\neach candidate model is at least as plausible as the best-selected one, using a\\nscreening test based on a penalized likelihood ratio statistic. We provide\\ntheoretical guarantees for the asymptotic coverage of our confidence set and\\ndemonstrate its practical advantages through simulations and real data\\nanalysis.',\n",
       "  'keywords': ['Finite mixture models',\n",
       "   'order selection',\n",
       "   'penalized likelihood ratio test',\n",
       "   'model']},\n",
       " {'title': 'Differentially Private Joint Independence Test',\n",
       "  'authors': ['Xingwei Liu', 'Yuexin Chen', 'Wangli Xu'],\n",
       "  'summary': 'Identification of joint dependence among more than two random vectors plays\\nan important role in many statistical applications, where the data may contain\\nsensitive or confidential information. In this paper, we consider the the\\nd-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of\\ndifferential privacy. Given the limiting distribution of the empirical estimate\\nof dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy\\nregime is typically based on permutation and bootstrap. To detect joint\\ndependence in privacy, we propose a dHSIC-based testing procedure by employing\\na differentially private permutation methodology. Our method enjoys privacy\\nguarantee, valid level and pointwise consistency, while the bootstrap\\ncounterpart suffers inconsistent power. We further investigate the uniform\\npower of the proposed test in dHSIC metric and $L_2$ metric, indicating that\\nthe proposed test attains the minimax optimal power across different privacy\\nregimes. As a byproduct, our results also contain the pointwise and uniform\\npower of the non-private permutation dHSIC, addressing an unsolved question\\nremained in Pfister et al. (2018).',\n",
       "  'keywords': ['DifferentialPrivacy;IndependenceTest;KernelMethods;ResamplingMethod']},\n",
       " {'title': 'AutoBayes: A Compositional Framework for Generalized Variational\\n  Inference',\n",
       "  'authors': ['Toby St Clere Smithe', 'Marco Perin'],\n",
       "  'summary': 'We introduce a new compositional framework for generalized variational\\ninference, clarifying the different parts of a model, how they interact, and\\nhow they compose. We explain that both exact Bayesian inference and the loss\\nfunctions typical of variational inference (such as variational free energy and\\nits generalizations) satisfy chain rules akin to that of reverse-mode automatic\\ndifferentiation, and we advocate for exploiting this to build and optimize\\nmodels accordingly. To this end, we construct a series of compositional tools:\\nfor building models; for constructing their inversions; for attaching local\\nloss functions; and for exposing parameters. Finally, we explain how the\\nresulting parameterized statistical games may be optimized locally, too. We\\nillustrate our framework with a number of classic examples, pointing to new\\nareas of extensibility that are revealed.',\n",
       "  'keywords': None},\n",
       " {'title': 'Minimax Rate-Optimal Inference for Individualized Quantile Treatment\\n  Effects in High-dimensional Models',\n",
       "  'authors': ['Jiachen Sun', 'Yin Xia'],\n",
       "  'summary': 'The quantification of treatment effects plays an important role in a wide\\nrange of applications, including policy making and bio-pharmaceutical research.\\nIn this article, we study the quantile treatment effect (QTE) while addressing\\ntwo specific types of heterogeneities: (a) personalized heterogeneity, which\\ncaptures the varying treatment effects for different individuals, and (b)\\nquantile heterogeneity, which accounts for how the impact of covariates varies\\nacross different quantile levels. A well-designed debiased estimator for the\\nindividualized quantile treatment effect (IQTE) is proposed to capture such\\nheterogeneities effectively. We show that this estimator converges weakly to a\\nGaussian process as a function of the quantile levels and propose valid\\nstatistical inference methods, including the construction of confidence\\nintervals and the development of hypothesis testing decision rules. In\\naddition, the minimax optimality frameworks for these inference procedures are\\nestablished. Specifically, we derive the minimax optimal rates for the expected\\nlength of confidence intervals and the magnitude of the detection boundary for\\nhypothesis testing procedures, illustrating the superiority of the proposed\\nestimator. The effectiveness of our methods is demonstrated through extensive\\nsimulations and an analysis of the National Health and Nutrition Examination\\nSurvey (NHANES) datasets.',\n",
       "  'keywords': ['Debiasedestimator; Minimaxdetectionboundary; Minimaxexpectedlength;']},\n",
       " {'title': 'Recovering a (1+1)-dimensional wave equation from a single white noise\\n  boundary measurement',\n",
       "  'authors': ['Emilia L. K. Blåsten',\n",
       "   'Tapio Helin',\n",
       "   'Antti Kujanpää',\n",
       "   'Lauri Oksanen',\n",
       "   'Jesse Railo'],\n",
       "  'summary': 'We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave\\nequation on $\\\\mathbb R_+$ with zero initial conditions is excited with a\\nNeumann boundary data modelled as a white noise process. Given also the\\nDirichlet data at the same point, determine the unknown first order coefficient\\nfunction of the system.\\n  We first establish that direct problem is well-posed. The inverse problem is\\nthen solved by showing that correlations of the boundary data determine the\\nNeumann-to-Dirichlet operator in the sense of distributions, which is known to\\nuniquely identify the coefficient. This approach has applications in acoustic\\nmeasurements of internal cross-sections of fluid pipes such as pressurised\\nwater supply pipes and vocal tract shape determination.',\n",
       "  'keywords': ['correlationimaging',\n",
       "   'inverse problem',\n",
       "   'wave equation',\n",
       "   'point measurement',\n",
       "   'blockage detection']},\n",
       " {'title': 'Asymptotically uniformly most powerful tests for diffusion processes\\n  with nonsynchronous observations',\n",
       "  'authors': ['Teppei Ogihara', 'Futo Ueno'],\n",
       "  'summary': 'This paper introduces a quasi-likelihood ratio testing procedure for\\ndiffusion processes observed under nonsynchronous sampling schemes.\\nHigh-frequency data, particularly in financial econometrics, are often recorded\\nat irregular time points, challenging conventional synchronous methods for\\nparameter estimation and hypothesis testing. To address these challenges, we\\ndevelop a quasi-likelihood framework that accommodates irregular sampling while\\nintegrating adaptive estimation techniques for both drift and diffusion\\ncoefficients, thereby enhancing optimization stability and reducing\\ncomputational burden. We rigorously derive the asymptotic properties of the\\nproposed test statistic, showing that it converges to a chi-squared\\ndistribution under the null hypothesis and exhibits consistency under\\nalternatives. Moreover, we establish that the resulting tests are\\nasymptotically uniformly most powerful. Extensive numerical experiments\\ncorroborate the theoretical findings and demonstrate that our method\\noutperforms existing nonparametric approaches.',\n",
       "  'keywords': None},\n",
       " {'title': 'Non-Bayesian Learning in Misspecified Models',\n",
       "  'authors': ['Sebastian Bervoets', 'Mathieu Faure', 'Ludovic Renou'],\n",
       "  'summary': \"Deviations from Bayesian updating are traditionally categorized as biases,\\nerrors, or fallacies, thus implying their inherent ``sub-optimality.'' We offer\\na more nuanced view. We demonstrate that, in learning problems with\\nmisspecified models, non-Bayesian updating can outperform Bayesian updating.\",\n",
       "  'keywords': ['learning', 'Bayesian', 'consistency.']},\n",
       " {'title': 'Two-Sample Tests for Optimal Lifts, Manifold Stability and Reverse\\n  Labeling Reflection Shap',\n",
       "  'authors': ['Do Tran Van',\n",
       "   'Susovan Pal',\n",
       "   'Benjamin Eltzner',\n",
       "   'Stephan F. Huckemann'],\n",
       "  'summary': \"We consider a quotient of a complete Riemannian manifold modulo an\\nisometrically and properly acting Lie group and lifts of the quotient to the\\nmanifolds in optimal position to a reference point on the manifold. With\\nrespect to the pushed forward Riemannian volume onto the quotient we derive\\ncontinuity and uniqueness a.e. and smoothness to large extents also with\\nrespect to the reference point. In consequence we derive a general manifold\\nstability theorem: the Fr\\\\'echet mean lies in the highest dimensional stratum\\nassumed with positive probability, and a strong law for optimal lifts. This\\nallows to define new two-sample tests utilizing individual optimal lifts which\\noutperform existing two-sample tests on simulated data. They also outperform\\nexisting tests on a newly derived reverse labeling reflection shape space, that\\nis used to model filament data of microtubules within cells in a biological\\napplication.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Poisson-Process Topic Model for Integrating Knowledge from Pre-trained\\n  Language Models',\n",
       "  'authors': ['Morgane Austern',\n",
       "   'Yuanchuan Guo',\n",
       "   'Zheng Tracy Ke',\n",
       "   'Tianle Liu'],\n",
       "  'summary': 'Topic modeling is traditionally applied to word counts without accounting for\\nthe context in which words appear. Recent advancements in large language models\\n(LLMs) offer contextualized word embeddings, which capture deeper meaning and\\nrelationships between words. We aim to leverage such embeddings to improve\\ntopic modeling.\\n  We use a pre-trained LLM to convert each document into a sequence of word\\nembeddings. This sequence is then modeled as a Poisson point process, with its\\nintensity measure expressed as a convex combination of $K$ base measures, each\\ncorresponding to a topic. To estimate these topics, we propose a flexible\\nalgorithm that integrates traditional topic modeling methods, enhanced by\\nnet-rounding applied before and kernel smoothing applied after. One advantage\\nof this framework is that it treats the LLM as a black box, requiring no\\nfine-tuning of its parameters. Another advantage is its ability to seamlessly\\nintegrate any traditional topic modeling approach as a plug-in module, without\\nthe need for modifications\\n  Assuming each topic is a $\\\\beta$-H\\\\\"{o}lder smooth intensity measure on the\\nembedded space, we establish the rate of convergence of our method. We also\\nprovide a minimax lower bound and show that the rate of our method matches with\\nthe lower bound when $\\\\beta\\\\leq 1$. Additionally, we apply our method to\\nseveral datasets, providing evidence that it offers an advantage over\\ntraditional topic modeling approaches.',\n",
       "  'keywords': ['kerneldensityestimation',\n",
       "   'minimaxanalysis',\n",
       "   'Topic-SCORE',\n",
       "   'transformer',\n",
       "   'unmixing']},\n",
       " {'title': 'A new tail bound for the sum of bounded independent random variables',\n",
       "  'authors': ['Jackson Loper', 'Jeffrey Regier'],\n",
       "  'summary': \"We construct a new tail bound for the sum of independent random variables for\\nsituations in which the expected value of the sum is known and each random\\nvariable lies within a specified interval, which may be different for each\\nvariable. This new bound can be computed by solving a two-dimensional convex\\noptimization problem. Simulations demonstrate that the new bound is often\\nsubstantially tighter than Hoeffding's inequality for cases in which both\\nbounds are applicable.\",\n",
       "  'keywords': ['Hoeffding’s inequality; tail bounds; convex optimization']},\n",
       " {'title': 'A Statistical Theory of Contrastive Learning via Approximate Sufficient\\n  Statistics',\n",
       "  'authors': ['Licong Lin', 'Song Mei'],\n",
       "  'summary': 'Contrastive learning -- a modern approach to extract useful representations\\nfrom unlabeled data by training models to distinguish similar samples from\\ndissimilar ones -- has driven significant progress in foundation models. In\\nthis work, we develop a new theoretical framework for analyzing data\\naugmentation-based contrastive learning, with a focus on SimCLR as a\\nrepresentative example. Our approach is based on the concept of\\n\\\\emph{approximate sufficient statistics}, which we extend beyond its original\\ndefinition in \\\\cite{oko2025statistical} for contrastive language-image\\npretraining (CLIP) using KL-divergence. We generalize it to equivalent forms\\nand general f-divergences, and show that minimizing SimCLR and other\\ncontrastive losses yields encoders that are approximately sufficient.\\nFurthermore, we demonstrate that these near-sufficient encoders can be\\neffectively adapted to downstream regression and classification tasks, with\\nperformance depending on their sufficiency and the error induced by data\\naugmentation in contrastive learning. Concrete examples in linear regression\\nand topic classification are provided to illustrate the broad applicability of\\nour results.',\n",
       "  'keywords': None},\n",
       " {'title': 'Modeling of stochastic processes in $L_p(T)$ using orthogonal\\n  polynomials',\n",
       "  'authors': ['Oleksandr Mokliachuk'],\n",
       "  'summary': 'In this paper, models that approximate stochastic processes from the space\\n$Sub_\\\\varphi(\\\\Omega)$ with given reliability and accuracy in $L_p(T)$ are\\nconsidered for some specific functions $\\\\varphi(t)$. For processes that are\\ndecomposited in series using orthonormal bases, such models are constructed in\\nthe case where elements of such decomposition cannot be found explicitly.',\n",
       "  'keywords': None},\n",
       " {'title': 'The Entropy and Crossentropy of Generalized Mallows Models',\n",
       "  'authors': ['Marina Meilă'],\n",
       "  'summary': 'The Generalized Mallows Model (GMM) is a well known family of models for\\nranking data. A GMM is a distribution over $\\\\mathbb{S}_n$, the set of\\npermutations of n objects, characterized by a location parameter $\\\\sigma \\\\in\\n\\\\mathbb{S}_n$, known as central permutation and a set of dispersion parameters\\n$\\\\theta_{1:n-1}\\\\in(0,1]$. The GMM shares many properties, such as having\\nsufficient statistics, with exponential models, thus it can be seen as an\\nexponential family with a discrete parameter $\\\\sigma$. This paper shows that\\ncomputing entropy, crossentropy and Kullback-Leibler divergence in the the\\nclass of GMM is tractable, paving the way for a better understanding of this\\nexponential family.',\n",
       "  'keywords': None},\n",
       " {'title': 'Efficient Knowledge Distillation via Curriculum Extraction',\n",
       "  'authors': ['Shivam Gupta', 'Sushrut Karmalkar'],\n",
       "  'summary': \"Knowledge distillation is a technique used to train a small student network\\nusing the output generated by a large teacher network, and has many empirical\\nadvantages~\\\\citep{Hinton2015DistillingTK}. While the standard one-shot approach\\nto distillation only uses the output of the final teacher network, recent\\nwork~\\\\citep{panigrahi2024progressive} has shown that using intermediate\\ncheckpoints from the teacher's training process as an implicit ``curriculum''\\nfor progressive distillation can significantly speed up training. However, such\\nschemes require storing these checkpoints, and often require careful selection\\nof the intermediate checkpoints to train on, which can be impractical for\\nlarge-scale training.\\n  In this paper, we show that a curriculum can be \\\\emph{extracted} from just\\nthe fully trained teacher network, and that this extracted curriculum can give\\nsimilar efficiency benefits to those of progressive distillation. Our\\nextraction scheme is natural; we use a random projection of the hidden\\nrepresentations of the teacher network to progressively train the student\\nnetwork, before training using the output of the full network. We show that our\\nscheme significantly outperforms one-shot distillation and achieves a\\nperformance similar to that of progressive distillation for learning sparse\\nparities with two-layer networks, and provide theoretical guarantees for this\\nsetting. Additionally, we show that our method outperforms one-shot\\ndistillation even when using transformer-based architectures, both for\\nsparse-parity learning, and language modeling tasks.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Glivenko-Cantelli for $f$-divergence',\n",
       "  'authors': ['Haoming Wang', 'Lek-Heng Lim'],\n",
       "  'summary': 'We extend the celebrated Glivenko-Cantelli theorem, sometimes called the\\nfundamental theorem of statistics, from its standard setting of total variation\\ndistance to all $f$-divergences. A key obstacle in this endeavor is to define\\n$f$-divergence on a subcollection of a $\\\\sigma$-algebra that forms a\\n$\\\\pi$-system but not a $\\\\sigma$-subalgebra. This is a side contribution of our\\nwork. We will show that this notion of $f$-divergence on the $\\\\pi$-system of\\nrays preserves nearly all known properties of standard $f$-divergence, yields a\\nnovel integral representation of the Kolmogorov-Smirnov distance, and has a\\nGlivenko-Cantelli theorem. We will also discuss the prospects of a\\nVapnik-Chervonenkis theory for $f$-divergence.',\n",
       "  'keywords': None},\n",
       " {'title': 'On Privately Estimating a Single Parameter',\n",
       "  'authors': ['Hilal Asi', 'John C. Duchi', 'Kunal Talwar'],\n",
       "  'summary': 'We investigate differentially private estimators for individual parameters\\nwithin larger parametric models. While generic private estimators exist, the\\nestimators we provide repose on new local notions of estimand stability, and\\nthese notions allow procedures that provide private certificates of their own\\nstability. By leveraging these private certificates, we provide computationally\\nand statistical efficient mechanisms that release private statistics that are,\\nat least asymptotically in the sample size, essentially unimprovable: they\\nachieve instance optimal bounds. Additionally, we investigate the practicality\\nof the algorithms both in simulated data and in real-world data from the\\nAmerican Community Survey and US Census, highlighting scenarios in which the\\nnew procedures are successful and identifying areas for future work.',\n",
       "  'keywords': None},\n",
       " {'title': 'An improved nonparametric test and sample size procedures for the\\n  randomized complete block designs',\n",
       "  'authors': ['Show-Li Jan', 'Gwowen Shieh'],\n",
       "  'summary': 'The Friedman test has been extensively applied as a nonparametric alternative\\nto the conventional F procedure for comparing treatment effects in randomized\\ncomplete block designs. A chi-square distribution provides a convenient\\napproximation to determining the critical values for the Friedman procedure in\\nhypothesis testing. However, the chi-square approximation is generally\\nconservative and the accuracy declines with increasing number of treatments.\\nThis paper describes an alternative transformation of the Friedman statistic\\nalong with an approximate F distribution that has the same numerator degrees of\\nfreedom as the ANOVA F test. Moreover, two approximate noncentral F\\ndistributions are presented for the proposed F-transformation under the\\nalternative hypothesis of heterogeneous location shifts. Explicit power\\nfunctions are derived when the underlying populations have the uniform, normal,\\nLaplace, and exponential distributions. Theoretical examination and empirical\\nassessment are presented to validate the advantages of the proposed approaches\\nover the existing methods of the Friedman test. The developed test and power\\nprocedures are recommended due to their consistently acceptable Type I error\\nrates and accurate power calculations for the location shift structures and\\npopulation distributions considered here.',\n",
       "  'keywords': ['blockdesign',\n",
       "   'chi-squaredistribution',\n",
       "   'F distribution',\n",
       "   'power',\n",
       "   'TypeI error']},\n",
       " {'title': 'Nonparametric Factor Analysis and Beyond',\n",
       "  'authors': ['Yujia Zheng',\n",
       "   'Yang Liu',\n",
       "   'Jiaxiong Yao',\n",
       "   'Yingyao Hu',\n",
       "   'Kun Zhang'],\n",
       "  'summary': 'Nearly all identifiability results in unsupervised representation learning\\ninspired by, e.g., independent component analysis, factor analysis, and causal\\nrepresentation learning, rely on assumptions of additive independent noise or\\nnoiseless regimes. In contrast, we study the more general case where noise can\\ntake arbitrary forms, depend on latent variables, and be non-invertibly\\nentangled within a nonlinear function. We propose a general framework for\\nidentifying latent variables in the nonparametric noisy settings. We first show\\nthat, under suitable conditions, the generative model is identifiable up to\\ncertain submanifold indeterminacies even in the presence of non-negligible\\nnoise. Furthermore, under the structural or distributional variability\\nconditions, we prove that latent variables of the general nonlinear models are\\nidentifiable up to trivial indeterminacies. Based on the proposed theoretical\\nframework, we have also developed corresponding estimation methods and\\nvalidated them in various synthetic and real-world settings. Interestingly, our\\nestimate of the true GDP growth from alternative measurements suggests more\\ninsightful information on the economies than official reports. We expect our\\nframework to provide new insight into how both researchers and practitioners\\ndeal with latent variables in real-world scenarios.',\n",
       "  'keywords': None},\n",
       " {'title': 'Optimal Nonlinear Online Learning under Sequential Price Competition via\\n  s-Concavity',\n",
       "  'authors': ['Daniele Bracale',\n",
       "   'Moulinath Banerjee',\n",
       "   'Cong Shi',\n",
       "   'Yuekai Sun'],\n",
       "  'summary': \"We consider price competition among multiple sellers over a selling horizon\\nof $T$ periods. In each period, sellers simultaneously offer their prices and\\nsubsequently observe their respective demand that is unobservable to\\ncompetitors. The demand function for each seller depends on all sellers' prices\\nthrough a private, unknown, and nonlinear relationship. To address this\\nchallenge, we propose a semi-parametric least-squares estimation of the\\nnonlinear mean function, which does not require sellers to communicate demand\\ninformation. We show that when all sellers employ our policy, their prices\\nconverge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers\\nwould reach if they were fully informed. Each seller incurs a regret of\\n$O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution\\nof our work is proving the existence of equilibrium under shape-constrained\\ndemand functions via the concept of $s$-concavity and establishing regret\\nbounds of our proposed policy. Technically, we also establish new concentration\\nresults for the least squares estimator under shape constraints. Our findings\\noffer significant insights into dynamic competition-aware pricing and\\ncontribute to the broader study of non-parametric learning in strategic\\ndecision-making.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Uniformly consistent proportion estimation for composite hypotheses via\\n  integral equations: \"the case of location-shift families\"',\n",
       "  'authors': ['Xiongzhi Chen'],\n",
       "  'summary': 'We consider estimating the proportion of random variables for two types of\\ncomposite null hypotheses: (i) the means or medians of the random variables\\nbelonging to a non-empty, bounded interval; (ii) the means or medians of the\\nrandom variables belonging to an unbounded interval that is not the whole real\\nline. For each type of composite null hypotheses, uniformly consistent\\nestimators of the proportion of false null hypotheses are constructed for\\nrandom variables whose distributions are members of a Type I location-shift\\nfamily. Further, uniformly consistent estimators of certain functions of a\\nbounded null on the means or medians are provided for the random variables\\nmentioned earlier; these functions are continuous and of bounded variation. The\\nestimators are constructed via solutions to Lebesgue-Stieltjes integral\\nequations and harmonic analysis, do not rely on a concept of p-value, and have\\nvarious applications.',\n",
       "  'keywords': None},\n",
       " {'title': 'A Statistical Analysis for Per-Instance Evaluation of Stochastic\\n  Optimizers: How Many Repeats Are Enough?',\n",
       "  'authors': ['Moslem Noori',\n",
       "   'Elisabetta Valiante',\n",
       "   'Thomas Van Vaerenbergh',\n",
       "   'Masoud Mohseni',\n",
       "   'Ignacio Rozada'],\n",
       "  'summary': \"A key trait of stochastic optimizers is that multiple runs of the same\\noptimizer in attempting to solve the same problem can produce different\\nresults. As a result, their performance is evaluated over several repeats, or\\nruns, on the problem. However, the accuracy of the estimated performance\\nmetrics depends on the number of runs and should be studied using statistical\\ntools. We present a statistical analysis of the common metrics, and develop\\nguidelines for experiment design to measure the optimizer's performance using\\nthese metrics to a high level of confidence and accuracy. To this end, we first\\ndiscuss the confidence interval of the metrics and how they are related to the\\nnumber of runs of an experiment. We then derive a lower bound on the number of\\nrepeats in order to guarantee achieving a given accuracy in the metrics. Using\\nthis bound, we propose an algorithm to adaptively adjust the number of repeats\\nneeded to ensure the accuracy of the evaluated metric. Our simulation results\\ndemonstrate the utility of our analysis and how it allows us to conduct\\nreliable benchmarking as well as hyperparameter tuning and prevent us from\\ndrawing premature conclusions regarding the performance of stochastic\\noptimizers.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Statistical accuracy of the ensemble Kalman filter in the near-linear\\n  setting',\n",
       "  'authors': ['E. Calvello',\n",
       "   'J. A. Carrillo',\n",
       "   'F. Hoffmann',\n",
       "   'P. Monmarché',\n",
       "   'A. M. Stuart',\n",
       "   'U. Vaes'],\n",
       "  'summary': 'Estimating the state of a dynamical system from partial and noisy\\nobservations is a ubiquitous problem in a large number of applications, such as\\nprobabilistic weather forecasting and prediction of epidemics. Particle filters\\nare a widely adopted approach to the problem and provide provably accurate\\napproximations of the statistics of the state, but they perform poorly in high\\ndimensions because of weight collapse. The ensemble Kalman filter does not\\nsuffer from this issue, as it relies on an interacting particle system with\\nequal weights. Despite its wide adoption in the geophysical sciences,\\nmathematical analysis of the accuracy of this filter is predominantly confined\\nto the setting of linear dynamical models and linear observations operators,\\nand analysis beyond the linear Gaussian setting is still in its infancy. In\\nthis short note, we provide an accessible overview of recent work in which the\\nauthors take first steps to analyze the accuracy of the filter beyond the\\nlinear Gaussian setting.',\n",
       "  'keywords': None},\n",
       " {'title': 'Sequential Monte Carlo with Gaussian Mixture Approximation for\\n  Infinite-Dimensional Statistical Inverse Problems',\n",
       "  'authors': ['Haoyu Lu', 'Junxiong Jia', 'Deyu Meng'],\n",
       "  'summary': 'By formulating the inverse problem of partial differential equations (PDEs)\\nas a statistical inference problem, the Bayesian approach provides a general\\nframework for quantifying uncertainties. In the inverse problem of PDEs,\\nparameters are defined on an infinite-dimensional function space, and the PDEs\\ninduce a computationally intensive likelihood function. Additionally, sparse\\ndata tends to lead to a multi-modal posterior. These features make it difficult\\nto apply existing sequential Monte Carlo (SMC) algorithms. To overcome these\\ndifficulties, we propose new conditions for the likelihood functions, construct\\na Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and\\ndemonstrate the universal approximation property of the infinite-dimensional\\nGaussian mixture probability measure. By combining these three novel tools, we\\npropose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a\\nconvergence theorem that allows Gaussian priors, illustrating that the\\nsequential particle filter actually reproduces the true posterior distribution.\\nFurthermore, the proposed new algorithm is rigorously defined on the\\ninfinite-dimensional function space, naturally exhibiting the\\ndiscretization-invariant property. Numerical experiments demonstrate that the\\nnew approach has a strong ability to probe the multi-modality of the posterior,\\nsignificantly reduces the computational burden, and numerically exhibits the\\ndiscretization-invariant property (important for large-scale problems).',\n",
       "  'keywords': ['infinitedimensionalBayesianinference;inverseproblemsofPDEs;sequential']},\n",
       " {'title': 'General reproducing properties in RKHS with application to derivative\\n  and integral operators',\n",
       "  'authors': ['Fatima-Zahrae El-Boukkouri',\n",
       "   'Josselin Garnier',\n",
       "   'Olivier Roustant'],\n",
       "  'summary': 'In this paper, we consider the reproducing property in Reproducing Kernel\\nHilbert Spaces (RKHS). We establish a reproducing property for the closure of\\nthe class of combinations of composition operators under minimal conditions.\\nThis allows to revisit the sufficient conditions for the reproducing property\\nto hold for the derivative operator, as well as for the existence of the mean\\nembedding function. These results provide a framework of application of the\\nrepresenter theorem for regularized learning algorithms that involve data for\\nfunction values, gradients, or any other operator from the considered class.',\n",
       "  'keywords': None},\n",
       " {'title': 'The Gaussian central limit theorem for a stationary time series with\\n  infinite variance',\n",
       "  'authors': ['Muneya Matsui', 'Thomas Mikosch'],\n",
       "  'summary': 'We consider a borderline case: the central limit theorem for a strictly\\nstationary time series with infinite variance but a Gaussian limit. In the iid\\ncase a well-known sufficient condition for this central limit theorem is\\nregular variation of the marginal distribution with tail index $\\\\alpha=2$. In\\nthe dependent case we assume the stronger condition of sequential regular\\nvariation of the time series with tail index $\\\\alpha=2$. We assume that a\\nsample of size $n$ from this time series can be split into $k_n$ blocks of size\\n$r_n\\\\to\\\\infty$ such that $r_n/n\\\\to 0$ as $n\\\\to\\\\infty$ and that the block sums\\nare asymptotically independent. Then we apply classical central limit theory\\nfor row-wise iid triangular arrays. The necessary and sufficient conditions for\\nsuch independent block sums will be verified by using large deviation results\\nfor the time series. We derive the central limit theorem for $m$-dependent\\nsequences, linear processes, stochastic volatility processes and solutions to\\naffine stochastic recurrence equations whose marginal distributions have\\ninfinite variance and are regularly varying with tail index $\\\\alpha=2$.',\n",
       "  'keywords': None},\n",
       " {'title': 'The Fundamental Limits of Recovering Planted Subgraphs',\n",
       "  'authors': ['Daniel Lee',\n",
       "   'Francisco Pernice',\n",
       "   'Amit Rajaraman',\n",
       "   'Ilias Zadik'],\n",
       "  'summary': 'Given an arbitrary subgraph $H=H_n$ and $p=p_n \\\\in (0,1)$, the planted\\nsubgraph model is defined as follows. A statistician observes the union a\\nrandom copy $H^*$ of $H$, together with random noise in the form of an instance\\nof an Erdos-Renyi graph $G(n,p)$. Their goal is to recover the planted $H^*$\\nfrom the observed graph. Our focus in this work is to understand the minimum\\nmean squared error (MMSE) for sufficiently large $n$.\\n  A recent paper [MNSSZ23] characterizes the graphs for which the limiting MMSE\\ncurve undergoes a sharp phase transition from $0$ to $1$ as $p$ increases, a\\nbehavior known as the all-or-nothing phenomenon, up to a mild density\\nassumption on $H$. In this paper, we provide a formula for the limiting MMSE\\ncurve for any graph $H=H_n$, up to the same mild density assumption. This curve\\nis expressed in terms of a variational formula over pairs of subgraphs of $H$,\\nand is inspired by the celebrated subgraph expectation thresholds from the\\nprobabilistic combinatorics literature [KK07]. Furthermore, we give a\\npolynomial-time description of the optimizers of this variational problem. This\\nallows one to efficiently approximately compute the MMSE curve for any dense\\ngraph $H$ when $n$ is large enough. The proof relies on a novel graph\\ndecomposition of $H$ as well as a new minimax theorem which may be of\\nindependent interest.\\n  Our results generalize to the setting of minimax rates of recovering\\narbitrary monotone boolean properties planted in random noise, where the\\nstatistician observes the union of a planted minimal element $A \\\\subseteq [N]$\\nof a monotone property and a random $Ber(p)^{\\\\otimes N}$ vector. In this\\nsetting, we provide a variational formula inspired by the so-called\\n\"fractional\" expectation threshold [Tal10], again describing the MMSE curve (in\\nthis case up to a multiplicative constant) for large enough $n$.',\n",
       "  'keywords': None},\n",
       " {'title': 'On the Functoriality of Belief Propagation Algorithms on finite\\n  Partially Ordered Sets',\n",
       "  'authors': ['Grégoire Sergeant-Perthuis',\n",
       "   'Toby St Clere Smithe',\n",
       "   'Léo Boitel'],\n",
       "  'summary': 'Undirected graphical models are a widely used class of probabilistic models\\nin machine learning that capture prior knowledge or putative pairwise\\ninteractions between variables. Those interactions are encoded in a graph for\\npairwise interactions; however, generalizations such as factor graphs account\\nfor higher-degree interactions using hypergraphs. Inference on such models,\\nwhich is performed by conditioning on some observed variables, is typically\\ndone approximately by optimizing a free energy, which is an instance of\\nvariational inference. The Belief Propagation algorithm is a dynamic\\nprogramming algorithm that finds critical points of that free energy. Recent\\nefforts have been made to unify and extend inference on graphical models and\\nfactor graphs to more expressive probabilistic models. A synthesis of these\\nworks shows that inference on graphical models, factor graphs, and their\\ngeneralizations relies on the introduction of presheaves and associated\\ninvariants (homology and cohomology groups).We propose to study the impact of\\nthe transformation of the presheaves onto the associated message passing\\nalgorithms. We show that natural transformations between presheaves associated\\nwith graphical models and their generalizations, which can be understood as\\ncoherent binning of the set of values of the variables, induce morphisms\\nbetween associated message-passing algorithms. It is, to our knowledge, the\\nfirst result on functoriality of the Loopy Belief Propagation.',\n",
       "  'keywords': None},\n",
       " {'title': 'Optimal Data Splitting for Holdout Cross-Validation in Large Covariance\\n  Matrix Estimation',\n",
       "  'authors': ['Lamia Lamrani', 'Christian Bongiorno', 'Marc Potters'],\n",
       "  'summary': 'Cross-validation is a statistical tool that can be used to improve large\\ncovariance matrix estimation. Although its efficiency is observed in practical\\napplications, the theoretical reasons behind it remain largely intuitive, with\\nformal proofs currently lacking. To carry on analytical analysis, we focus on\\nthe holdout method, a single iteration of cross-validation, rather than the\\ntraditional $k$-fold approach. We derive a closed-form expression for the\\nestimation error when the population matrix follows a white inverse Wishart\\ndistribution, and we observe the optimal train-test split scales as the square\\nroot of the matrix dimension. For general population matrices, we connected the\\nerror to the variance of eigenvalues distribution, but approximations are\\nnecessary. Interestingly, in the high-dimensional asymptotic regime, both the\\nholdout and $k$-fold cross-validation methods converge to the optimal estimator\\nwhen the train-test ratio scales with the square root of the matrix dimension.',\n",
       "  'keywords': None},\n",
       " {'title': 'Nonlinear Bayesian Update via Ensemble Kernel Regression with Clustering\\n  and Subsampling',\n",
       "  'authors': ['Yoonsang Lee'],\n",
       "  'summary': 'Nonlinear Bayesian update for a prior ensemble is proposed to extend\\ntraditional ensemble Kalman filtering to settings characterized by non-Gaussian\\npriors and nonlinear measurement operators. In this framework, the observed\\ncomponent is first denoised via a standard Kalman update, while the unobserved\\ncomponent is estimated using a nonlinear regression approach based on kernel\\ndensity estimation. The method incorporates a subsampling strategy to ensure\\nstability and, when necessary, employs unsupervised clustering to refine the\\nconditional estimate. Numerical experiments on Lorenz systems and a\\nPDE-constrained inverse problem illustrate that the proposed nonlinear update\\ncan reduce estimation errors compared to standard linear updates, especially in\\nhighly nonlinear scenarios.',\n",
       "  'keywords': None},\n",
       " {'title': 'Dynamic Investment Strategies Through Market Classification and\\n  Volatility: A Machine Learning Approach',\n",
       "  'authors': ['Jinhui Li', 'Wenjia Xie', 'Luis Seco'],\n",
       "  'summary': 'This study introduces a dynamic investment framework to enhance portfolio\\nmanagement in volatile markets, offering clear advantages over traditional\\nstatic strategies. Evaluates four conventional approaches : equal weighted,\\nminimum variance, maximum diversification, and equal risk contribution under\\ndynamic conditions. Using K means clustering, the market is segmented into ten\\nvolatility-based states, with transitions forecasted by a Bayesian Markov\\nswitching model employing Dirichlet priors and Gibbs sampling. This enables\\nreal-time asset allocation adjustments. Tested across two asset sets, the\\ndynamic portfolio consistently achieves significantly higher risk-adjusted\\nreturns and substantially higher total returns, outperforming most static\\nmethods. By integrating classical optimization with machine learning and\\nBayesian techniques, this research provides a robust strategy for optimizing\\ninvestment outcomes in unpredictable market environments.',\n",
       "  'keywords': ['Machine Learning',\n",
       "   'Dynamic Portfolio Allocation',\n",
       "   'Market Clas-']},\n",
       " {'title': 'A Bivariate Poisson-Gamma Distribution: Statistical Properties and\\n  Practical Applications',\n",
       "  'authors': ['Indranil Ghosh', 'Mina Norouzirad', 'Filipe J. Marques'],\n",
       "  'summary': \"Although the specification of bivariate probability models using a collection\\nof assumed conditional distributions is not a novel concept, it has received\\nconsiderable attention in the last decade. In this study, a bivariate\\ndistribution-the bivariate Poisson-Gamma conditional distribution-is\\nintroduced, combining both univariate continuous and discrete distributions.\\nThis work explores aspects of this model's structure and statistical inference\\nthat have not been studied before. This paper contributes to the field of\\nstatistical modeling and distribution theory through the use of maximum\\nlikelihood estimation, along with simulations and analyses of real data.\",\n",
       "  'keywords': ['Infinitedivisibility',\n",
       "   'Log-convexity',\n",
       "   'Mixturedistribution',\n",
       "   'Poisson-Gamma']},\n",
       " {'title': 'Finite sample expansions for semiparametric plug-in estimation and\\n  inference for BTL model',\n",
       "  'authors': ['Vladimir Spokoiny'],\n",
       "  'summary': 'The recent paper \\\\cite{GSZ2023} on estimation and inference for top-ranking\\nproblem in Bradley-Terry-Lice (BTL) model presented a surprising result:\\ncomponentwise estimation and inference can be done under much weaker conditions\\non the number of comparison then it is required for the full dimensional\\nestimation. The present paper revisits this finding from completely different\\nviewpoint. Namely, we show how a theoretical study of estimation in sup-norm\\ncan be reduced to the analysis of plug-in semiparametric estimation. For the\\nlatter, we adopt and extend the general approach from \\\\cite{Sp2024} for\\nhigh-dimensional estimation. The main tool of the analysis is a theory of\\nperturbed marginal optimization when an objective function depends on a\\nlow-dimensional target parameter along with a high-dimensional nuisance\\nparameter. A particular focus of the study is the critical dimension condition.\\nFull-dimensional estimation requires in general the condition \\\\( \\\\mathbbmsl{N}\\n\\\\gg \\\\mathbb{p} \\\\) between the effective parameter dimension \\\\( \\\\mathbb{p} \\\\)\\nand the effective sample size \\\\( \\\\mathbbmsl{N} \\\\) corresponding to the smallest\\neigenvalue of the Fisher information matrix \\\\( \\\\mathbbmsl{F} \\\\). Inference on\\nthe estimated parameter is even more demanding: the condition \\\\( \\\\mathbbmsl{N}\\n\\\\gg \\\\mathbb{p}^{2} \\\\) cannot be generally avoided; see \\\\cite{Sp2024}. However,\\nfor the sup-norm estimation, the critical dimension condition can be reduced to\\n\\\\( \\\\mathbbmsl{N} \\\\geq \\\\CONST \\\\log(\\\\dimp) \\\\). Compared to \\\\cite{GSZ2023}, the\\nproposed approach works for the classical MLE and does not require any\\nresampling procedure, applies to more general structure of the comparison\\ngraph, and yields more accurate expansions for each component of the parameter\\nvector.',\n",
       "  'keywords': ['Fisher and Wilks expansions',\n",
       "   'risk bounds',\n",
       "   'critical dimension']},\n",
       " {'title': 'Asymptotic Normality in LAD Polynomial Regression and Hilbert Matrices',\n",
       "  'authors': ['Saïd Maanan', 'Azzouz Dermoune', 'Ahmed El Ghini'],\n",
       "  'summary': \"This paper investigates the asymptotic properties of least absolute deviation\\n(LAD) regression for linear models with polynomial regressors, highlighting its\\nrobustness against heavy-tailed noise and outliers. Assuming independent and\\nidentically distributed (i.i.d.) errors, we establish the multiscale asymptotic\\nnormality of LAD estimators. A central result is the derivation of the\\nasymptotic precision matrix, shown to be proportional to Hilbert matrices, with\\nthe proportionality coefficient depending on the asymptotic variance of the\\nsample median of the noise distribution. We further explore the estimator's\\nconvergence properties, both in probability and almost surely, under varying\\nmodel specifications. Through comprehensive simulations, we evaluate the speed\\nof convergence of the LAD estimator and the empirical coverage probabilities of\\nconfidence intervals constructed under different scaling factors (T 1/2 and T\\n$\\\\alpha$ ). These experiments incorporate a range of noise distributions,\\nincluding Laplace, Gaussian, and Cauchy, to demonstrate the estimator's\\nrobustness and efficiency. The findings underscore the versatility and\\npractical relevance of LAD regression in handling non-standard data\\nenvironments. By connecting the statistical properties of LAD estimators to\\nclassical mathematical structures, such as Hilbert matrices, this study offers\\nboth theoretical insights and practical tools for robust statistical modeling.\",\n",
       "  'keywords': None},\n",
       " {'title': 'A Note on Local Linear Regression for Time Series in Banach Spaces',\n",
       "  'authors': ['Florian Heinrichs'],\n",
       "  'summary': 'This work extends local linear regression to Banach space-valued time series\\nfor estimating smoothly varying means and their derivatives in non-stationary\\ndata. The asymptotic properties of both the standard and bias-reduced Jackknife\\nestimators are analyzed under mild moment conditions, establishing their\\nconvergence rates. Simulation studies assess the finite sample performance of\\nthese estimators and compare them with the Nadaraya-Watson estimator.\\nAdditionally, the proposed methods are applied to smooth EEG recordings for\\nreconstructing eye movements and to video analysis for detecting pedestrians\\nand abandoned objects.',\n",
       "  'keywords': ['Local linear regression',\n",
       "   'Functional time series',\n",
       "   'Non-stationary time series',\n",
       "   '']},\n",
       " {'title': 'Inferring diffusivity from killed diffusion',\n",
       "  'authors': ['Richard Nickl', 'Fanny Seizilles'],\n",
       "  'summary': 'We consider diffusion of independent molecules in an insulated Euclidean\\ndomain with unknown diffusivity parameter. At a random time and position, the\\nmolecules may bind and stop diffusing in dependence of a given `binding\\npotential\\'. The binding process can be modeled by an additive random functional\\ncorresponding to the canonical construction of a `killed\\' diffusion Markov\\nprocess. We study the problem of conducting inference on the\\ninfinite-dimensional diffusion parameter from a histogram plot of the `killing\\'\\npositions of the process. We show first that these positions follow a Poisson\\npoint process whose intensity measure is determined by the solution of a\\ncertain Schr\\\\\"odinger equation. The inference problem can then be re-cast as a\\nnon-linear inverse problem for this PDE, which we show to be consistently\\nsolvable in a Bayesian way under natural conditions on the initial state of the\\ndiffusion, provided the binding potential is not too `aggressive\\'. In the\\ncourse of our proofs we obtain novel posterior contraction rate results for\\nhigh-dimensional Poisson count data that are of independent interest. A\\nnumerical illustration of the algorithm by standard MCMC methods is also\\nprovided.',\n",
       "  'keywords': None},\n",
       " {'title': 'The Field Equations of Penalized non-Parametric Regression',\n",
       "  'authors': ['Sven Pappert'],\n",
       "  'summary': 'We view penalized risks through the lens of the calculus of variations. We\\nconsider risks comprised of a fitness-term (e.g. MSE) and a gradient-based\\npenalty. After establishing the Euler-Lagrange field equations as a systematic\\napproach to finding minimizers of risks involving only first derivatives, we\\nproceed to exemplify this approach to the MSE penalized by the integral over\\nthe squared l2-norm of the gradient of the regression function. The minimizer\\nof this risk is given as the solution to a second order inhomogeneous PDE,\\nwhere the inhomogeneity is given as the conditional expectation of the target\\nvariable conditioned on the features. We discuss properties of the field\\nequations and practical implications thereof, which also apply to the classical\\nRidge penalty for linear models, and embed our findings into the existing\\nliterature. In particular, we find that we can recover the Rudin-Osher-Fatemi\\nmodel for image-denoising, if we consider the features as deterministic and\\nevenly distributed. Last, we outline several directions for future research.',\n",
       "  'keywords': ['Artificial Intelligence',\n",
       "   'Calculus of Variations',\n",
       "   'LASSO',\n",
       "   'Non-Parametric Regression',\n",
       "   'Penalized']},\n",
       " {'title': 'Hazard Rate for Associated Data in Deconvolution Problems: Asymptotic\\n  Normality',\n",
       "  'authors': ['Benjrada Mohammed Essalih'],\n",
       "  'summary': 'In reliability theory and survival analysis, observed data are often weakly\\ndependent and subject to additive measurement errors. Such contamination arises\\nwhen the underlying data are neither independent nor strongly mixed but instead\\nexhibit association. This paper focuses on estimating the hazard rate by\\ndeconvolving the density function and constructing an estimator of the\\ndistribution function. We assume that the data originate from a strictly\\nstationary sequence satisfying association conditions. Under appropriate\\nsmoothness assumptions on the error distribution, we establish the\\nquadratic-mean convergence and asymptotic normality of the proposed estimators.\\nThe finite-sample performance of both the hazard rate and distribution function\\nestimators is evaluated through a simulation study. We conclude with a\\ndiscussion of open problems and potential future research directions.',\n",
       "  'keywords': ['Hazard rate',\n",
       "   'Deconvolution',\n",
       "   'Asymptotic Normality',\n",
       "   'Positively Associated']},\n",
       " {'title': 'Testing Conditional Stochastic Dominance at Target Points',\n",
       "  'authors': ['Federico A. Bugni', 'Ivan A. Canay', 'Deborah Kim'],\n",
       "  'summary': \"This paper introduces a novel test for conditional stochastic dominance (CSD)\\nat specific values of the conditioning covariates, referred to as target\\npoints. The test is relevant for analyzing income inequality, evaluating\\ntreatment effects, and studying discrimination. We propose a\\nKolmogorov-Smirnov-type test statistic that utilizes induced order statistics\\nfrom independent samples. Notably, the test features a data-independent\\ncritical value, eliminating the need for resampling techniques such as the\\nbootstrap. Our approach avoids kernel smoothing and parametric assumptions,\\ninstead relying on a tuning parameter to select relevant observations. We\\nestablish the asymptotic properties of our test, showing that the induced order\\nstatistics converge to independent draws from the true conditional\\ndistributions and that the test controls asymptotic size under weak regularity\\nconditions. While our results apply to both continuous and discrete data, in\\nthe discrete case, the critical value only provides a valid upper bound. To\\naddress this, we propose a refined critical value that significantly enhances\\npower, requiring only knowledge of the support size of the distributions.\\nAdditionally, we analyze the test's behavior in the limit experiment,\\ndemonstrating that it reduces to a problem analogous to testing unconditional\\nstochastic dominance in finite samples. This framework allows us to prove the\\nvalidity of permutation-based tests for stochastic dominance when the random\\nvariables are continuous. Monte Carlo simulations confirm the strong\\nfinite-sample performance of our method.\",\n",
       "  'keywords': ['stochasticdominance',\n",
       "   'regressiondiscontinuitydesign',\n",
       "   'inducedorderstatis-']},\n",
       " {'title': 'On the Precise Asymptotics of Universal Inference',\n",
       "  'authors': ['Kenta Takatsu'],\n",
       "  'summary': 'In statistical inference, confidence set procedures are typically evaluated\\nbased on their validity and width properties. Even when procedures achieve\\nrate-optimal widths, confidence sets can still be excessively wide in practice\\ndue to elusive constants, leading to extreme conservativeness, where the\\nempirical coverage probability of nominal $1-\\\\alpha$ level confidence sets\\napproaches one. This manuscript studies this gap between validity and\\nconservativeness, using universal inference (Wasserman et al., 2020) with a\\nregular parametric model under model misspecification as a running example. We\\nidentify the source of asymptotic conservativeness and propose a general remedy\\nbased on studentization and bias correction. The resulting method attains exact\\nasymptotic coverage at the nominal $1-\\\\alpha$ level, even under model\\nmisspecification, provided that the product of the estimation errors of two\\nunknowns is negligible, exhibiting an intriguing resemblance to double\\nrobustness in semiparametric theory.',\n",
       "  'keywords': None},\n",
       " {'title': 'The broken sample problem revisited: Proof of a conjecture by Bai-Hsing\\n  and high-dimensional extensions',\n",
       "  'authors': ['Simiao Jiao', 'Yihong Wu', 'Jiaming Xu'],\n",
       "  'summary': \"We revisit the classical broken sample problem: Two samples of i.i.d. data\\npoints $\\\\mathbf{X}=\\\\{X_1,\\\\cdots, X_n\\\\}$ and $\\\\mathbf{Y}=\\\\{Y_1,\\\\cdots,Y_m\\\\}$ are\\nobserved without correspondence with $m\\\\leq n$. Under the null hypothesis,\\n$\\\\mathbf{X}$ and $\\\\mathbf{Y}$ are independent. Under the alternative\\nhypothesis, $\\\\mathbf{Y}$ is correlated with a random subsample of $\\\\mathbf{X}$,\\nin the sense that $(X_{\\\\pi(i)},Y_i)$'s are drawn independently from some\\nbivariate distribution for some latent injection $\\\\pi:[m] \\\\to [n]$. Originally\\nintroduced by DeGroot, Feder, and Goel (1971) to model matching records in\\ncensus data, this problem has recently gained renewed interest due to its\\napplications in data de-anonymization, data integration, and target tracking.\\nDespite extensive research over the past decades, determining the precise\\ndetection threshold has remained an open problem even for equal sample sizes\\n($m=n$). Assuming $m$ and $n$ grow proportionally, we show that the sharp\\nthreshold is given by a spectral and an $L_2$ condition of the likelihood ratio\\noperator, resolving a conjecture of Bai and Hsing (2005) in the positive. These\\nresults are extended to high dimensions and settle the sharp detection\\nthresholds for Gaussian and Bernoulli models.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Minimizers of U-processes and their domains of attraction',\n",
       "  'authors': ['Dietmar Ferger'],\n",
       "  'summary': \"In this paper, we study the minimizers of U-processes and their domains of\\nattraction. U-processes arise in various statistical contexts, particularly in\\nM-estimation, where estimators are defined as minimizers of certain objective\\nfunctions. Our main results establish necessary and sufficient conditions for\\nthe distributional convergence of these minimizers, identifying a broad class\\nof normalizing sequences that go beyond the standard square-root asymptotics\\nwith normal limits. We show that the limit distribution belongs to exactly one\\nof the four classes introduced by Smirnov. These results do not only extend\\nSmirnov's theory but also generalize existing asymptotic theories for\\nM-estimators, including classical results by Huber and extensions to\\nhigher-degree U-statistics. Furthermore, we analyze the domain of attraction\\nfor each class, providing alternative characterizations that determine which\\ntypes of statistical estimators fall into a given asymptotic regime.\",\n",
       "  'keywords': ['M-estimators',\n",
       "   'convexU-processes',\n",
       "   'distributionalconvergence',\n",
       "   'domainsof']},\n",
       " {'title': 'Robust tests for log-logistic models based on minimum density power\\n  divergence estimators',\n",
       "  'authors': ['A. Felipe', 'M. Jaenada', 'P. Miranda', 'L. Pardo'],\n",
       "  'summary': 'The log-logistic distribution is a versatile parametric family widely used\\nacross various applied fields, including survival analysis, reliability\\nengineering, and econometrics. When estimating parameters of the log-logistic\\ndistribution, hypothesis testing is necessary to verify assumptions about these\\nparameters. The Wald test and Rao test provide formal methods for testing\\nhypotheses about these parameters. However, these test statistics are not\\nrobust, and their rejection decisions may be affected by data contamination. In\\nthis paper we develop new families of Wald-type test statistics and Rao-type\\ntest statistics based on minimum density power divergence estimators (MDPDEs)\\nfor the parameters of the log-logistic distribution. These new families\\ngeneralize the Wald and Rao test statistics, inheriting the robustness\\nproperties from the MDPDEs and thus addressing the lack of robustness of the\\nclassical tests. Explicit expressions for the test statistics under the\\nlog-logistic model for both simple and composite null hypotheses are derived,\\nand their properties are analyzed in detail. An extensive simulation study\\nempirically demonstrates the robustness of these families and compares their\\nperformance with the classical methods.',\n",
       "  'keywords': ['Log-logisticdistribution',\n",
       "   'minimumdensitypowerdivergenceestimator',\n",
       "   'Wald-type']},\n",
       " {'title': 'Optimizing High-Dimensional Oblique Splits',\n",
       "  'authors': ['Chien-Ming Chi'],\n",
       "  'summary': 'Orthogonal-split trees perform well, but evidence suggests oblique splits can\\nenhance their performance. This paper explores optimizing high-dimensional\\n$s$-sparse oblique splits from $\\\\{(\\\\vec{w}, \\\\vec{w}^{\\\\top}\\\\boldsymbol{X}_{i}) :\\ni\\\\in \\\\{1,\\\\dots, n\\\\}, \\\\vec{w} \\\\in \\\\mathbb{R}^p, \\\\| \\\\vec{w} \\\\|_{2} = 1, \\\\|\\n\\\\vec{w} \\\\|_{0} \\\\leq s \\\\}$ for growing oblique trees, where $ s $ is a\\nuser-defined sparsity parameter. We establish a connection between SID\\nconvergence and $s_0$-sparse oblique splits with $s_0\\\\ge 1$, showing that the\\nSID function class expands as $s_0$ increases, enabling the capture of more\\ncomplex data-generating functions such as the $s_0$-dimensional XOR function.\\nThus, $s_0$ represents the unknown potential complexity of the underlying\\ndata-generating function. Learning these complex functions requires an\\n$s$-sparse oblique tree with $s \\\\geq s_0$ and greater computational resources.\\nThis highlights a trade-off between statistical accuracy, governed by the SID\\nfunction class size depending on $s_0$, and computational cost. In contrast,\\nprevious studies have explored the problem of SID convergence using orthogonal\\nsplits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we\\nintroduce a practical framework for oblique trees that integrates optimized\\noblique splits alongside orthogonal splits into random forests. The proposed\\napproach is assessed through simulations and real-data experiments, comparing\\nits performance against various oblique tree models.',\n",
       "  'keywords': ['oblique trees',\n",
       "   'trade-off between computation time and statistical accuracy',\n",
       "   '']},\n",
       " {'title': 'A New Proof of Sub-Gaussian Norm Concentration Inequality',\n",
       "  'authors': ['Zishun Liu', 'Yongxin Chen'],\n",
       "  'summary': 'We present a new proof of the sub-Gaussian norm concentration inequality. Our\\nproof is based on an averaged version of the moment generating function termed\\nthe averaged moment generating function. Compared with the widely adopted\\n$\\\\varepsilon$-net technique-based proof of the sub-Gaussian norm concentration\\ninequality, our method does not rely on the union bound and promises a tighter\\nconcentration bound.',\n",
       "  'keywords': None},\n",
       " {'title': \"Confidence Intervals Using Turing's Estimator: Simulations and\\n  Applications\",\n",
       "  'authors': ['Jie Chang', 'Michael Grabchak', 'Jialin Zhang'],\n",
       "  'summary': \"Turing's estimator allows one to estimate the probabilities of outcomes that\\neither do not appear or only rarely appear in a given random sample. We perform\\na simulation study to understand the finite sample performance of several\\nrelated confidence intervals (CIs) and introduce an approach for selecting the\\nappropriate CI for a given sample. We give an application to the problem of\\nauthorship attribution and apply it to a dataset comprised of tweets from users\\non X (Twitter). Further, we derive several theoretical results about asymptotic\\nnormality and asymptotic Poissonity of Turing's estimator for two important\\ndiscrete distributions.\",\n",
       "  'keywords': ['Turing’s estimator; Good-Turing estimation; authorship attribution; confidence']},\n",
       " {'title': 'Asymptotic properties of the MLE in distributional regression under\\n  random censoring',\n",
       "  'authors': ['Gitte Kremling', 'Gerhard Dikta'],\n",
       "  'summary': 'The aim of distributional regression is to find the best candidate in a given\\nparametric family of conditional distributions to model a given dataset. As\\neach candidate in the distribution family can be identified by the\\ncorresponding distribution parameters, a common approach for this task is using\\nthe maximum likelihood estimator (MLE) for the parameters. In this paper, we\\nestablish theoretical results for this estimator in case the response variable\\nis subject to random right censoring. In particular, we provide proofs of\\nalmost sure consistency and asymptotic normality of the MLE under censoring.\\nFurther, the finite-sample behavior is exemplarily demonstrated in a simulation\\nstudy.',\n",
       "  'keywords': None},\n",
       " {'title': 'The covariance of causal effect estimators for binary v-structures',\n",
       "  'authors': ['Jack Kuipers', 'Giusi Moffa'],\n",
       "  'summary': 'Previously [Journal of Causal Inference, 10, 90-105 (2022)], we computed the\\nvariance of two estimators of causal effects for a v-structure of binary\\nvariables. Here we show that a linear combination of these estimators has lower\\nvariance than either. Furthermore, we show that this holds also when the\\ntreatment variable is block randomised with a predefined number receiving\\ntreatment, with analogous results to when it is sampled randomly.',\n",
       "  'keywords': ['Causality;CovariateAdjustment;StructureLearning;BayesianNetworks;Probability']},\n",
       " {'title': 'Stratified Permutational Berry--Esseen Bounds and Their Applications to\\n  Statistics',\n",
       "  'authors': ['Pengfei Tian', 'Fan Yang', 'Peng Ding'],\n",
       "  'summary': \"The stratified linear permutation statistic arises in various statistics\\nproblems, including stratified and post-stratified survey sampling, stratified\\nand post-stratified experiments, conditional permutation tests, etc. Although\\nwe can derive the Berry--Esseen bounds for the stratified linear permutation\\nstatistic based on existing bounds for the non-stratified statistics, those\\nbounds are not sharp, and moreover, this strategy does not work in general\\nsettings with heterogeneous strata with varying sizes. We first use Stein's\\nmethod to obtain a unified stratified permutational Berry--Esseen bound that\\ncan accommodate heterogeneous strata. We then apply the bound to various\\nstatistics problems, leading to stronger theoretical quantifications and\\nthereby facilitating statistical inference in those problems.\",\n",
       "  'keywords': ['Causal inference; Design-based inference; Experimental design; Stein’s method;']},\n",
       " {'title': 'Spectrally-Corrected and Regularized QDA Classifier for Spiked\\n  Covariance Model',\n",
       "  'authors': ['Wenya Luo', 'Hua Li', 'Zhidong Bai', 'Zhijun Liu'],\n",
       "  'summary': 'Quadratic discriminant analysis (QDA) is a widely used method for\\nclassification problems, particularly preferable over Linear Discriminant\\nAnalysis (LDA) for heterogeneous data. However, QDA loses its effectiveness in\\nhigh-dimensional settings, where the data dimension and sample size tend to\\ninfinity. To address this issue, we propose a novel QDA method utilizing\\nspectral correction and regularization techniques, termed SR-QDA. The\\nregularization parameters in our method are selected by maximizing the\\nFisher-discriminant ratio. We compare SR-QDA with QDA, regularized quadratic\\ndiscriminant analysis (R-QDA), and several other competitors. The results\\nindicate that SR-QDA performs exceptionally well, especially in moderate and\\nhigh-dimensional situations. Empirical experiments across diverse datasets\\nfurther support this conclusion.',\n",
       "  'keywords': None},\n",
       " {'title': \"Stein's method of moment estimators for local dependency exponential\\n  random graph models\",\n",
       "  'authors': ['Adrian Fischer', 'Gesine Reinert', 'Wenkai Xu'],\n",
       "  'summary': 'Providing theoretical guarantees for parameter estimation in exponential\\nrandom graph models is a largely open problem. While maximum likelihood\\nestimation has theoretical guarantees in principle, verifying the assumptions\\nfor these guarantees to hold can be very difficult. Moreover, in complex\\nnetworks, numerical maximum likelihood estimation is computer-intensive and may\\nnot converge in reasonable time. To ameliorate this issue, local dependency\\nexponential random graph models have been introduced, which assume that the\\nnetwork consists of many independent exponential random graphs. In this\\nsetting, progress towards maximum likelihood estimation has been made. However\\nthe estimation is still computer-intensive. Instead, we propose to use\\nso-called Stein estimators: we use the Stein characterizations to obtain new\\nestimators for local dependency exponential random graph models.',\n",
       "  'keywords': ['Localdependencyexponentialrandomgraphmodel;Pointestimation;Stein’smethod']},\n",
       " {'title': \"Spearman's rho for bivariate zero-inflated data\",\n",
       "  'authors': ['Jasper Arends', 'Elisa Perrone'],\n",
       "  'summary': \"Quantifying the association between two random variables is crucial in\\napplications. Traditional estimation techniques for common association\\nmeasures, such as Spearman's rank correlation coefficient, $\\\\rho_S$, often fail\\nwhen data contain ties. This is particularly problematic in zero-inflated\\ncontexts and fields like insurance, healthcare, and weather forecasting, where\\nzeros are more frequent and require an extra probability mass. In this paper,\\nwe provide a new formulation of Spearman's rho specifically designed for\\nzero-inflated data and propose a novel estimator of Spearman's rho based on our\\nderived expression. Besides, we make our proposed estimator useful in practice\\nby deriving its achievable bounds and suggest how to estimate them. We analyze\\nour method in a comprehensive simulation study and show that our approach\\novercomes state-of-the-art methods in all the simulated scenarios.\\nAdditionally, we illustrate how the proposed theory can be used in practice for\\na more accurate quantification of association by considering two real-life\\napplications.\",\n",
       "  'keywords': ['Spearman’s rho',\n",
       "   'bivariate zero-inflated data',\n",
       "   'Fr´echet-Hoeffding bounds']},\n",
       " {'title': 'On a conjecture of Roverato regarding G-Wishart normalising constants',\n",
       "  'authors': ['Ching Wong', 'Giusi Moffa', 'Jack Kuipers'],\n",
       "  'summary': 'The evaluation of G-Wishart normalising constants is a core component for\\nBayesian analyses for Gaussian graphical models, but remains a computationally\\nintensive task in general. Based on empirical evidence, Roverato [Scandinavian\\nJournal of Statistics, 29:391--411 (2002)] observed and conjectured that such\\nconstants can be simplified and rewritten in terms of constants with an\\nidentity scale matrix. In this note, we disprove this conjecture for general\\ngraphs by showing that the conjecture instead implies an independently-derived\\napproximation for certain ratios of normalising constants.',\n",
       "  'keywords': None},\n",
       " {'title': 'Parameter estimation for generalized mixed fractional stochastic heat\\n  equation',\n",
       "  'authors': ['B. L. S. Prakasa Rao'],\n",
       "  'summary': 'We study the properties of a stochastic heat equation with a generalized\\nmixed fractional Brownian noise. We obtain the covariance structure,\\nstationarity and obtain bounds for the asymptotic behaviour of the solution. We\\nsuggest estimators for the unknown parameters based on discrete time\\nobservations and study their asymptotic properties.',\n",
       "  'keywords': ['Stochastic partial differential equation ; Generalized mixed fractional Brownian']},\n",
       " {'title': 'Estimating stationary mass, frequency by frequency',\n",
       "  'authors': ['Milind Nakul', 'Vidya Muthukumar', 'Ashwin Pananjady'],\n",
       "  'summary': 'Suppose we observe a trajectory of length $n$ from an $\\\\alpha$-mixing\\nstochastic process over a finite but potentially large state space. We consider\\nthe problem of estimating the probability mass placed by the stationary\\ndistribution of any such process on elements that occur with a certain\\nfrequency in the observed sequence. We estimate this vector of probabilities in\\ntotal variation distance, showing universal consistency in $n$ and recovering\\nknown results for i.i.d. sequences as special cases. Our proposed methodology\\ncarefully combines the plug-in (or empirical) estimator with a\\nrecently-proposed modification of the Good--Turing estimator called WingIt,\\nwhich was originally developed for Markovian sequences. En route to controlling\\nthe error of our estimator, we develop new performance bounds on WingIt and the\\nplug-in estimator for $\\\\alpha$-mixing stochastic processes. Importantly, the\\nextensively used method of Poissonization can no longer be applied in our non\\ni.i.d. setting, and so we develop complementary tools -- including\\nconcentration inequalities for a natural self-normalized statistic of mixing\\nsequences -- that may prove independently useful in the design and analysis of\\nestimators for related problems.',\n",
       "  'keywords': None},\n",
       " {'title': 'Asymptotic Expansions of Gaussian and Laguerre Ensembles at the Soft\\n  Edge II: Level Densities',\n",
       "  'authors': ['Folkmar Bornemann'],\n",
       "  'summary': 'We continue our work [arXiv:2403.07628] on asymptotic expansions at the soft\\nedge for the classical $n$-dimensional Gaussian and Laguerre random matrix\\nensembles. By revisiting the construction of the associated skew-orthogonal\\npolynomials in terms of wave functions, we obtain concise expressions for the\\nlevel densities that are well suited for proving asymptotic expansions in\\npowers of a certain parameter $h \\\\asymp n^{-2/3}$. In the unitary case, the\\nexpansion for the level density can be used to reconstruct the first correction\\nterm in an established asymptotic expansion of the associated generating\\nfunction. In the orthogonal and symplectic cases, we can even reconstruct the\\nconjectured first and second correction terms.',\n",
       "  'keywords': None},\n",
       " {'title': 'On self-training of summary data with genetic applications',\n",
       "  'authors': ['Buxin Su', 'Jiaoyang Huang', 'Jin Jin', 'Bingxin Zhao'],\n",
       "  'summary': 'Prediction model training is often hindered by limited access to\\nindividual-level data due to privacy concerns and logistical challenges,\\nparticularly in biomedical research. Resampling-based self-training presents a\\npromising approach for building prediction models using only summary-level\\ndata. These methods leverage summary statistics to sample pseudo datasets for\\nmodel training and parameter optimization, allowing for model development\\nwithout individual-level data. Although increasingly used in precision\\nmedicine, the general behaviors of self-training remain unexplored. In this\\npaper, we leverage a random matrix theory framework to establish the\\nstatistical properties of self-training algorithms for high-dimensional\\nsparsity-free summary data. We demonstrate that, within a class of linear\\nestimators, resampling-based self-training achieves the same asymptotic\\npredictive accuracy as conventional training methods that require\\nindividual-level datasets. These results suggest that self-training with only\\nsummary data incurs no additional cost in prediction accuracy, while offering\\nsignificant practical convenience. Our analysis provides several valuable\\ninsights and counterintuitive findings. For example, while pseudo-training and\\nvalidation datasets are inherently dependent, their interdependence\\nunexpectedly cancels out when calculating prediction accuracy measures,\\npreventing overfitting in self-training algorithms. Furthermore, we extend our\\nanalysis to show that the self-training framework maintains this no-cost\\nadvantage when combining multiple methods or when jointly training on data from\\ndifferent distributions. We numerically validate our findings through\\nsimulations and real data analyses using the UK Biobank. Our study highlights\\nthe potential of resampling-based self-training to advance genetic risk\\nprediction and other fields that make summary data publicly available.',\n",
       "  'keywords': ['Genetic prediction; Prediction models; Precision medicine; Random matrix theory;']},\n",
       " {'title': 'Optimal ANOVA-based emulators of models with(out) derivatives',\n",
       "  'authors': ['Matieyendou Lamboni'],\n",
       "  'summary': \"This paper proposes new ANOVA-based approximations of functions and emulators\\nof high-dimensional models using either available derivatives or local\\nstochastic evaluations of such models. Our approach makes use of sensitivity\\nindices to design adequate structures of emulators. For high-dimensional models\\nwith available derivatives, our derivative-based emulators reach dimension-free\\nmean squared errors (MSEs) and parametric rate of convergence (i.e.,\\n$\\\\mathsf{O}(N^{-1})$). This approach is extended to cope with every model\\n(without available derivatives) by deriving global emulators that account for\\nthe local properties of models or simulators. Such generic emulators enjoy\\ndimension-free biases, parametric rates of convergence and MSEs that depend on\\nthe dimensionality. Dimension-free MSEs are obtained for high-dimensional\\nmodels with particular inputs' distributions. Our emulators are also\\ncompetitive in dealing with different distributions of the input variables and\\nfor selecting inputs and interactions. Simulations show the efficiency of our\\napproach.\",\n",
       "  'keywords': ['Derivative-based ANOVA',\n",
       "   'Emulators',\n",
       "   'High-dimensional models',\n",
       "   '']},\n",
       " {'title': 'Two statistical problems for multivariate mixture distributions',\n",
       "  'authors': ['Ricardo Fraiman', 'Leonardo Moreno', 'Thomas Ransford'],\n",
       "  'summary': 'In an earlier work arXiv:2410.22038, it was shown that mixtures of\\nmultivariate Gaussian or $t$-distributions can be distinguished by projecting\\nthem onto a certain predetermined finite set of lines, the number of lines\\ndepending only on the total number of distributions involved and on the ambient\\ndimension. Using this work, we address the following two important statistical\\nproblems: that of testing and measuring the agreement between two different\\nrandom partitions, and that of estimating for mixtures of multivariate normal\\ndistributions and mixtures of $t$-distributions based of univariate\\nprojections. We also compare our proposal with robust versions of the\\nexpectation-maximization method EM. In each case, we present algorithms for\\neffecting the task, and compare them with existing methods by carrying out some\\nsimulations.',\n",
       "  'keywords': None},\n",
       " {'title': 'Training Diagonal Linear Networks with Stochastic Sharpness-Aware\\n  Minimization',\n",
       "  'authors': ['Gabriel Clara', 'Sophie Langer', 'Johannes Schmidt-Hieber'],\n",
       "  'summary': 'We analyze the landscape and training dynamics of diagonal linear networks in\\na linear regression task, with the network parameters being perturbed by small\\nisotropic normal noise. The addition of such noise may be interpreted as a\\nstochastic form of sharpness-aware minimization (SAM) and we prove several\\nresults that relate its action on the underlying landscape and training\\ndynamics to the sharpness of the loss. In particular, the noise changes the\\nexpected gradient to force balancing of the weight matrices at a fast rate\\nalong the descent trajectory. In the diagonal linear model, we show that this\\nequates to minimizing the average sharpness, as well as the trace of the\\nHessian matrix, among all possible factorizations of the same matrix. Further,\\nthe noise forces the gradient descent iterates towards a shrinkage-thresholding\\nof the underlying true parameter, with the noise level explicitly regulating\\nboth the shrinkage factor and the threshold.',\n",
       "  'keywords': None},\n",
       " {'title': 'Excess Mean Squared Error of Empirical Bayes Estimators',\n",
       "  'authors': ['Yue Ju', 'Bo Wahlberg', 'Håkan Hjalmarsson'],\n",
       "  'summary': 'Empirical Bayes estimators are based on minimizing the average risk with the\\nhyper-parameters in the weighting function being estimated from observed data.\\nThe performance of an empirical Bayes estimator is typically evaluated by its\\nmean squared error (MSE). However, the explicit expression for its MSE is\\ngenerally unavailable for finite sample sizes. To address this issue, we define\\na high-order analytical criterion: the excess MSE. It quantifies the\\nperformance difference between the maximum likelihood and empirical Bayes\\nestimators. An explicit expression for the excess MSE of an empirical Bayes\\nestimator employing a general data-dependent hyper-parameter estimator is\\nderived. As specific instances, we provide excess MSE expressions for\\nkernel-based regularized estimators using the scaled empirical Bayes, Stein\\nunbiased risk estimation, and generalized cross-validation hyper-parameter\\nestimators. Moreover, we propose a modification to the excess MSE expressions\\nfor regularized estimators for moderate sample sizes and show its improvement\\non accuracy in numerical simulations.',\n",
       "  'keywords': None},\n",
       " {'title': 'Proposal for the Application of Fractional Operators in Polynomial\\n  Regression Models to Enhance the Determination Coefficient $R^2$ on Unseen\\n  Data',\n",
       "  'authors': ['Anthony Torres-Hernandez'],\n",
       "  'summary': 'Since polynomial regression models are generally quite reliable for data with\\na linear trend, it is important to note that, in some cases, they may encounter\\noverfitting issues during the training phase, which could result in negative\\nvalues of the coefficient of determination $R^2$ for unseen data. For this\\nreason, this work proposes the partial implementation of fractional operators\\nin polynomial regression models to generate a fractional regression model. The\\ngoal of this proposal is to attempt to mitigate overfitting, which could\\nimprove the value of the coefficient of determination for unseen data, compared\\nto the polynomial model, under the assumption that this would contribute to\\ngenerating predictive models with better performance. The methodology for\\nconstructing these fractional regression models is detailed, and examples\\napplicable to both Riemann-Liouville and Caputo fractional operators are\\npresented.',\n",
       "  'keywords': ['FractionalOperators',\n",
       "   'FractionalCalculusofSets',\n",
       "   'PolynomialRegressionModels.']},\n",
       " {'title': 'Towards practical PDMP sampling: Metropolis adjustments, locally\\n  adaptive step-sizes, and NUTS-based time lengths',\n",
       "  'authors': ['Augustin Chevallier', 'Sam Power', 'Matthew Sutton'],\n",
       "  'summary': \"Piecewise-Deterministic Markov Processes (PDMPs) hold significant promise for\\nsampling from complex probability distributions. However, their practical\\nimplementation is hindered by the need to compute model-specific bounds.\\nConversely, while Hamiltonian Monte Carlo (HMC) offers a generally efficient\\napproach to sampling, its inability to adaptively tune step sizes impedes its\\nperformance when sampling complex distributions like funnels.\\n  To address these limitations, we introduce three innovative concepts: (a) a\\nMetropolis-adjusted approximation for PDMP simulation that eliminates the need\\nfor explicit bounds without compromising the invariant measure, (b) an adaptive\\nstep size mechanism compatible with the Metropolis correction, and (c) a No\\nU-Turn Sampler (NUTS)-inspired scheme for dynamically selecting path lengths in\\nPDMPs. These three ideas can be seamlessly integrated into a single,\\n`doubly-adaptive' PDMP sampler with favourable robustness and efficiency\\nproperties.\",\n",
       "  'keywords': None},\n",
       " {'title': \"On continuity of Chatterjee's rank correlation and related dependence\\n  measures\",\n",
       "  'authors': ['Jonathan Ansari', 'Sebastian Fuchs'],\n",
       "  'summary': 'While measures of concordance -- such as Spearman\\'s rho, Kendall\\'s tau, and\\nBlomqvist\\'s beta -- are continuous with respect to weak convergence,\\nChatterjee\\'s rank correlation xi recently introduced in Azadkia and Chatterjee\\n[5] does not share this property, causing drawbacks in statistical inference as\\npointed out in B\\\\\"ucher and Dette [7]. As we study in this paper, xi is instead\\nweakly continuous with respect to conditionally independent copies -- the\\nMarkov products. To establish weak continuity of Markov products, we provide\\nseveral sufficient conditions, including copula-based criteria and conditions\\nrelying on the concept of conditional weak convergence in Sweeting [36]. As a\\nconsequence, we also obtain continuity results for xi and related dependence\\nmeasures and verify their continuity in the parameters of standard models such\\nas multivariate elliptical and l1-norm symmetric distributions.',\n",
       "  'keywords': None},\n",
       " {'title': 'The pushed beta distribution and contaminated binary sampling',\n",
       "  'authors': [\"Ben O'Neill\"],\n",
       "  'summary': 'We examine a generalisation of the beta distribution that we call the pushed\\nbeta distribution. This is a continuous univariate distribution on the unit\\ninterval which generalises the beta distribution by \"pushing\" the density in a\\nparticular direction using an additional multiplicative term in the density\\nkernel. We examine the properties of this distribution and compare it to the\\nbeta distribution. We also examine the use of this distribution in contaminated\\nbinary sampling using Bayesian inference. We find that this distribution arises\\nas the appropriate posterior distribution for inference in certain kinds of\\ncontaminated binary models. We derive a broad range of properties of the\\ndistribution and we also establish some computational methods to compute\\nvarious functions for the distribution.',\n",
       "  'keywords': None},\n",
       " {'title': 'Statistical Impossibility and Possibility of Aligning LLMs with Human\\n  Preferences: From Condorcet Paradox to Nash Equilibrium',\n",
       "  'authors': ['Kaizhao Liu',\n",
       "   'Qi Long',\n",
       "   'Zhekun Shi',\n",
       "   'Weijie J. Su',\n",
       "   'Jiancong Xiao'],\n",
       "  'summary': 'Aligning large language models (LLMs) with diverse human preferences is\\ncritical for ensuring fairness and informed outcomes when deploying these\\nmodels for decision-making. In this paper, we seek to uncover fundamental\\nstatistical limits concerning aligning LLMs with human preferences, with a\\nfocus on the probabilistic representation of human preferences and the\\npreservation of diverse preferences in aligned LLMs. We first show that human\\npreferences can be represented by a reward model if and only if the preference\\namong LLM-generated responses is free of any Condorcet cycle. Moreover, we\\nprove that Condorcet cycles exist with probability converging to one\\nexponentially fast under a probabilistic preference model, thereby\\ndemonstrating the impossibility of fully aligning human preferences using\\nreward-based approaches such as reinforcement learning from human feedback.\\nNext, we explore the conditions under which LLMs would employ mixed strategies\\n-- meaning they do not collapse to a single response -- when aligned in the\\nlimit using a non-reward-based approach, such as Nash learning from human\\nfeedback (NLHF). We identify a necessary and sufficient condition for mixed\\nstrategies: the absence of a response that is preferred over all others by a\\nmajority. As a blessing, we prove that this condition holds with high\\nprobability under the probabilistic preference model, thereby highlighting the\\nstatistical possibility of preserving minority preferences without explicit\\nregularization in aligning LLMs. Finally, we leverage insights from our\\nstatistical results to design a novel, computationally efficient algorithm for\\nfinding Nash equilibria in aligning LLMs with NLHF. Our experiments show that\\nLlama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\\\\%\\nagainst the base model.',\n",
       "  'keywords': None},\n",
       " {'title': 'A New Design-Based Variance Estimator for Finely Stratified Experiments',\n",
       "  'authors': ['Yuehao Bai',\n",
       "   'Xun Huang',\n",
       "   'Joseph P. Romano',\n",
       "   'Azeem M. Shaikh',\n",
       "   'Max Tabord-Meehan'],\n",
       "  'summary': 'This paper considers the problem of design-based inference for the average\\ntreatment effect in finely stratified experiments. Here, by \"design-based\\'\\' we\\nmean that the only source of uncertainty stems from the randomness in treatment\\nassignment; by \"finely stratified\\'\\' we mean units are first stratified into\\ngroups of size k according to baseline covariates and then, within each group,\\na fixed number l < k are assigned uniformly at random to treatment and the\\nremainder to control. In this setting, we first show under mild conditions that\\ninference using the difference-in-means estimator requires an estimator of its\\nvariance that is at least asymptotically upward-biased. We then present a novel\\nestimator of the variance and show that it is upward-biased; furthermore, the\\nmagnitude of the bias depends in a natural way on the quality of the\\nstratification. Importantly, this estimator remains well-defined even in the\\nsetting in which l = 1 or k - l = 1. We then compare our estimator with some\\nwell-known estimators that have been proposed previously for this case. We\\nfirst show that, while these estimators are also upward-biased, the magnitude\\nof their bias does not change in the natural way with the quality of\\nstratification. To further discriminate among these estimators, we introduce a\\nframework motivated by a thought experiment in which the finite population can\\nbe modeled as having been drawn once in an i.i.d. fashion from a well-behaved\\nprobability distribution. In this framework, we argue that our estimator\\ndominates the others in terms of limiting bias, and that these improvements are\\nstrict except under exceptionally strong restrictions on the treatment effects.\\nFinally, we illustrate our theoretical results through a simulation study,\\nwhich reveals that our estimator can lead to substantially more precise\\ninferences, especially when the quality of stratification is high.',\n",
       "  'keywords': ['Experiments',\n",
       "   'Finite Population',\n",
       "   'Average Treatment Effect',\n",
       "   'Matched Pairs',\n",
       "   'Stratification']},\n",
       " {'title': 'Nonparametric Exponential Family Regression Under Star-Shaped\\n  Constraints',\n",
       "  'authors': ['Guanghong Yi', 'Matey Neykov'],\n",
       "  'summary': 'We study the minimax rate of estimation in nonparametric exponential family\\nregression under star-shaped constraints. Specifically, the parameter space $K$\\nis a star-shaped set contained within a bounded box $[-M, M]^n$, where $M$ is a\\nknown positive constant. Moreover, we assume that the exponential family is\\nnonsingular and that its cumulant function is twice continuously\\ndifferentiable. Our main result shows that the minimax rate for this problem is\\n$\\\\varepsilon^{*2} \\\\wedge \\\\operatorname{diam}(K)^2$, up to absolute constants,\\nwhere $\\\\varepsilon^*$ is defined as\\n  \\\\[ \\\\varepsilon^* = \\\\sup \\\\{\\\\varepsilon: \\\\varepsilon^2 \\\\kappa(M) \\\\leq \\\\log\\nN^{\\\\operatorname{loc}}(\\\\varepsilon)\\\\}, \\\\]\\n  with $N^{\\\\operatorname{loc}}(\\\\varepsilon)$ denoting the local entropy and\\n$\\\\kappa(M)$ is an absolute constant allowed to depend on $M$. We also provide\\nan example and derive its corresponding minimax optimal rate.',\n",
       "  'keywords': None},\n",
       " {'title': 'On the Injective Norm of Sums of Random Tensors and the Moments of\\n  Gaussian Chaoses',\n",
       "  'authors': ['Ishaq Aden-Ali'],\n",
       "  'summary': \"We prove an upper bound on the expected $\\\\ell_p$ injective norm of sums of\\nsubgaussian random tensors. Our proof is simple and does not rely on any\\nexplicit geometric or chaining arguments. Instead, it follows from a simple\\napplication of the PAC-Bayesian lemma, a tool that has proven effective at\\ncontrolling the suprema of certain ``smooth'' empirical processes in recent\\nyears. Our bound strictly improves a very recent result of Bandeira, Gopi,\\nJiang, Lucca, and Rothvoss. In the Euclidean case ($p=2$), our bound sharpens a\\nresult of Lata{\\\\l}a that was central to proving his estimates on the moments of\\nGaussian chaoses. As a consequence, we obtain an elementary proof of this\\nfundamental result.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Generalized network autoregressive modelling of longitudinal networks\\n  with application to presidential elections in the USA',\n",
       "  'authors': ['Guy Nason', 'Daniel Salnikov', 'Mario Cortina-Borja'],\n",
       "  'summary': 'Longitudinal networks are becoming increasingly relevant in the study of\\ndynamic processes characterised by known or inferred community structure.\\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\\nframework for exploiting the underlying network and multivariate time series.\\nWe introduce the community-$\\\\alpha$ GNAR model with interactions that exploits\\nprior knowledge or exogenous variables for analysing interactions within and\\nbetween communities, and can describe serial correlation in longitudinal\\nnetworks. We derive new explicit finite-sample error bounds that validate\\nanalysing high-dimensional longitudinal network data with GNAR models, and\\nprovide insights into their attractive properties. We further illustrate our\\napproach by analysing the dynamics of $\\\\textit{Red, Blue}$ and $\\\\textit{Swing}$\\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\\na time series of length twelve on 51 time series (US states and Washington DC).\\nOur analysis connects network autocorrelation to eight-year long terms,\\nhighlights a possible change in the system after the 2016 election, and a\\ndifference in behaviour between $\\\\textit{Red}$ and $\\\\textit{Blue}$ states.',\n",
       "  'keywords': ['high-dimensional time series',\n",
       "   'network time series',\n",
       "   'finite sample bounds',\n",
       "   'longi-']},\n",
       " {'title': 'Batch List-Decodable Linear Regression via Higher Moments',\n",
       "  'authors': ['Ilias Diakonikolas',\n",
       "   'Daniel M. Kane',\n",
       "   'Sushrut Karmalkar',\n",
       "   'Sihan Liu',\n",
       "   'Thanasis Pittas'],\n",
       "  'summary': 'We study the task of list-decodable linear regression using batches. A batch\\nis called clean if it consists of i.i.d. samples from an unknown linear\\nregression distribution. For a parameter $\\\\alpha \\\\in (0, 1/2)$, an unknown\\n$\\\\alpha$-fraction of the batches are clean and no assumptions are made on the\\nremaining ones. The goal is to output a small list of vectors at least one of\\nwhich is close to the true regressor vector in $\\\\ell_2$-norm. [DJKS23] gave an\\nefficient algorithm, under natural distributional assumptions, with the\\nfollowing guarantee. Assuming that the batch size $n$ satisfies $n \\\\geq\\n\\\\tilde{\\\\Omega}(\\\\alpha^{-1})$ and the number of batches is $m = \\\\mathrm{poly}(d,\\nn, 1/\\\\alpha)$, their algorithm runs in polynomial time and outputs a list of\\n$O(1/\\\\alpha^2)$ vectors at least one of which is\\n$\\\\tilde{O}(\\\\alpha^{-1/2}/\\\\sqrt{n})$ close to the target regressor. Here we\\ndesign a new polynomial time algorithm with significantly stronger guarantees\\nunder the assumption that the low-degree moments of the covariates distribution\\nare Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant\\n$\\\\delta>0$, as long as the batch size is $n \\\\geq\\n\\\\Omega_{\\\\delta}(\\\\alpha^{-\\\\delta})$ and the degree-$\\\\Theta(1/\\\\delta)$ moments of\\nthe covariates are SoS certifiably bounded, our algorithm uses $m =\\n\\\\mathrm{poly}((dn)^{1/\\\\delta}, 1/\\\\alpha)$ batches, runs in polynomial-time, and\\noutputs an $O(1/\\\\alpha)$-sized list of vectors one of which is\\n$O(\\\\alpha^{-\\\\delta/2}/\\\\sqrt{n})$ close to the target. That is, our algorithm\\nachieves substantially smaller minimum batch size and final error, while\\nachieving the optimal list size. Our approach uses higher-order moment\\ninformation by carefully combining the SoS paradigm interleaved with an\\niterative method and a novel list pruning procedure. In the process, we give an\\nSoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader\\napplicability.',\n",
       "  'keywords': None},\n",
       " {'title': 'Minimax Optimality of the Probability Flow ODE for Diffusion Models',\n",
       "  'authors': ['Changxiao Cai', 'Gen Li'],\n",
       "  'summary': 'Score-based diffusion models have become a foundational paradigm for modern\\ngenerative modeling, demonstrating exceptional capability in generating samples\\nfrom complex high-dimensional distributions. Despite the dominant adoption of\\nprobability flow ODE-based samplers in practice due to their superior sampling\\nefficiency and precision, rigorous statistical guarantees for these methods\\nhave remained elusive in the literature. This work develops the first\\nend-to-end theoretical framework for deterministic ODE-based samplers that\\nestablishes near-minimax optimal guarantees under mild assumptions on target\\ndata distributions. Specifically, focusing on subgaussian distributions with\\n$\\\\beta$-H\\\\\"older smooth densities for $\\\\beta\\\\leq 2$, we propose a smooth\\nregularized score estimator that simultaneously controls both the $L^2$ score\\nerror and the associated mean Jacobian error. Leveraging this estimator within\\na refined convergence analysis of the ODE-based sampling process, we\\ndemonstrate that the resulting sampler achieves the minimax rate in total\\nvariation distance, modulo logarithmic factors. Notably, our theory\\ncomprehensively accounts for all sources of error in the sampling process and\\ndoes not require strong structural conditions such as density lower bounds or\\nLipschitz/smooth scores on target distributions, thereby covering a broad range\\nof practical data distributions.',\n",
       "  'keywords': ['diffusion model',\n",
       "   'probability flow ODE',\n",
       "   'sampling',\n",
       "   'minimax optimality']},\n",
       " {'title': 'Parameter estimation for the stochastic Burgers equation driven by white\\n  noise from local measurements',\n",
       "  'authors': ['Josef Janák', 'Enrico Priola'],\n",
       "  'summary': 'For one dimensional stochastic Burgers equation driven by space-time white\\nnoise we consider the problem of estimation of the diffusivity parameter in\\nfront of the second-order spatial derivative. Based on local observations in\\nspace, we study the estimator derived in [Altmeyer, Rei{\\\\ss}, Ann. Appl.\\nProbab.(2021)] for linear stochastic heat equation that has also been used in\\n[Altmeyer, Cialenco, Pasemann, Bernoulli (2023)] to cover large class of\\nsemilinear SPDEs and has been examined for the stochastic Burgers equation\\ndriven by trace class noise. We extend the achieved results by considering the\\nspace-time white noise case which has also relevant physical motivations. After\\nwe establish new regularity results for the solution, we are able to show that\\nour proposed estimator is strongly consistent and asymptotically normal.',\n",
       "  'keywords': ['Stochastic Burgers equation; space-time white noise; local measurements;']},\n",
       " {'title': 'Competing-risk Weibull survival model with multiple causes',\n",
       "  'authors': ['Kai Wang',\n",
       "   'Yuqin Mu',\n",
       "   'Shenyi Zhang',\n",
       "   'Zhengjun Zhang',\n",
       "   'Chengxiu Ling'],\n",
       "  'summary': \"The failure of a system can result from the simultaneous effects of multiple\\ncauses, where assigning a specific cause may be inappropriate or unavailable.\\nExamples include contributing causes of death in epidemiology and the aetiology\\nof neurodegenerative diseases like Alzheimer's. We propose a parametric Weibull\\naccelerated failure time model for multiple causes, incorporating a\\ndata-driven, individualized, and time-varying winning probability (relative\\nimportance) matrix. Using maximum likelihood estimation and the\\nexpectation-maximization (EM) algorithm, our approach enables simultaneous\\nestimation of regression coefficients and relative cause importance, ensuring\\nconsistency and asymptotic normality. A simulation study and an application to\\nAlzheimer's disease demonstrate its effectiveness in addressing cause-mixture\\nproblems and identifying informative biomarker combinations, with comparisons\\nto Weibull and Cox proportional hazards models.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Low-Rank Graphon Estimation: Theory and Applications to Graphon Games',\n",
       "  'authors': ['Olga Klopp', 'Fedor Noskov'],\n",
       "  'summary': \"This paper tackles the challenge of estimating a low-rank graphon from\\nsampled network data, employing a singular value thresholding (SVT) estimator\\nto create a piecewise-constant graphon based on the network's adjacency matrix.\\nUnder certain assumptions about the graphon's structural properties, we\\nestablish bounds on the operator norm distance between the true graphon and its\\nestimator, as well as on the rank of the estimated graphon. In the second part\\nof the paper, we apply our estimator to graphon games. We derive bounds on the\\nsuboptimality of interventions in the social welfare problem in graphon games\\nwhen the intervention is based on the estimated graphon. These bounds are\\nexpressed in terms of the operator norm of the difference between the true and\\nestimated graphons. We also emphasize the computational benefits of using the\\nlow-rank estimated graphon to solve these problems.\",\n",
       "  'keywords': ['Networks',\n",
       "   'NetworkGames',\n",
       "   'TargetedInterventions',\n",
       "   'GraphonModel',\n",
       "   'RankEstimation']},\n",
       " {'title': 'Addressing pitfalls in implicit unobserved confounding synthesis using\\n  explicit block hierarchical ancestral sampling',\n",
       "  'authors': ['Xudong Sun', 'Alex Markham', 'Pratik Misra', 'Carsten Marr'],\n",
       "  'summary': 'Unbiased data synthesis is crucial for evaluating causal discovery algorithms\\nin the presence of unobserved confounding, given the scarcity of real-world\\ndatasets. A common approach, implicit parameterization, encodes unobserved\\nconfounding by modifying the off-diagonal entries of the idiosyncratic\\ncovariance matrix while preserving positive definiteness. Within this approach,\\nwe identify that state-of-the-art protocols have two distinct issues that\\nhinder unbiased sampling from the complete space of causal models: first, we\\ngive a detailed analysis of use of diagonally dominant constructions restricts\\nthe spectrum of partial correlation matrices; and second, the restriction of\\npossible graphical structures when sampling bidirected edges, unnecessarily\\nruling out valid causal models. To address these limitations, we propose an\\nimproved explicit modeling approach for unobserved confounding, leveraging\\nblock-hierarchical ancestral generation of ground truth causal graphs.\\nAlgorithms for converting the ground truth DAG into ancestral graph is provided\\nso that the output of causal discovery algorithms could be compared with. We\\ndraw connections between implicit and explicit parameterization, prove that our\\napproach fully covers the space of causal models, including those generated by\\nthe implicit parameterization, thus enabling more robust evaluation of methods\\nfor causal discovery and inference.',\n",
       "  'keywords': None},\n",
       " {'title': '\"All-Something-Nothing\" Phase Transitions in Planted k-Factor Recovery',\n",
       "  'authors': ['Julia Gaudio', 'Colin Sandon', 'Jiaming Xu', 'Dana Yang'],\n",
       "  'summary': 'This paper studies the problem of inferring a $k$-factor, specifically a\\nspanning $k$-regular graph, planted within an Erdos-Renyi random graph\\n$G(n,\\\\lambda/n)$. We uncover an interesting \"all-something-nothing\" phase\\ntransition. Specifically, we show that as the average degree $\\\\lambda$\\nsurpasses the critical threshold of $1/k$, the inference problem undergoes a\\ntransition from almost exact recovery (\"all\" phase) to partial recovery\\n(\"something\" phase). Moreover, as $\\\\lambda$ tends to infinity, the accuracy of\\nrecovery diminishes to zero, leading to the onset of the \"nothing\" phase. This\\nfinding complements the recent result by Mossel, Niles-Weed, Sohn, Sun, and\\nZadik who established that for certain sufficiently dense graphs, the problem\\nundergoes an \"all-or-nothing\" phase transition, jumping from near-perfect to\\nnear-zero recovery. In addition, we characterize the recovery accuracy of a\\nlinear-time iterative pruning algorithm and show that it achieves almost exact\\nrecovery when $\\\\lambda < 1/k$. A key component of our analysis is a two-step\\ncycle construction: we first build trees through local neighborhood exploration\\nand then connect them by sprinkling using reserved edges. Interestingly, for\\nproving impossibility of almost exact recovery, we construct $\\\\Theta(n)$ many\\nsmall trees of size $\\\\Theta(1)$, whereas for establishing the algorithmic lower\\nbound, a single large tree of size $\\\\Theta(\\\\sqrt{n\\\\log n})$ suffices.',\n",
       "  'keywords': None},\n",
       " {'title': 'Distribution and Moments of a Normalized Dissimilarity Ratio for two\\n  Correlated Gamma Variables',\n",
       "  'authors': ['Elise Colin', 'Razvigor Ossikovski'],\n",
       "  'summary': 'We consider two random variables $X$ and $Y$ following correlated Gamma\\ndistributions, characterized by identical scale and shape parameters and a\\nlinear correlation coefficient $\\\\rho$. Our focus is on the parameter: \\\\[\\n  D(X,Y) = \\\\frac{|X - Y|}{X + Y}, \\\\] which appears in applied contexts such as\\ndynamic speckle imaging, where it is known as the \\\\textit{Fujii index}. In this\\nwork, we derive a closed-form expression for the probability density function\\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\\nderivation starts by representing $X$ and $Y$ as two correlated exponential\\nrandom variables, obtained from the squared magnitudes of circular complex\\nGaussian variables. By considering the sum of $k$ independent exponential\\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\\ncorrelated Gamma variables. Through appropriate varable transformations, we\\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\\nanalytically. These theoretical findings are validated through numerical\\nsimulations, with particular attention to two specific cases: zero correlation\\nand unit shape parameter.',\n",
       "  'keywords': None},\n",
       " {'title': 'TransPCA for Large-dimensional Factor Analysis with Weak Factors: Power\\n  Enhancement via Knowledge Transfer',\n",
       "  'authors': ['Yong He', 'Dong Liu', 'Yunjing Sun', 'Yalin Wang'],\n",
       "  'summary': 'Early work established convergence of the principal component estimators of\\nthe factors and loadings up to a rotation for large dimensional approximate\\nfactor models with weak factors in that the factor loading $\\\\Lambda^{(0)}$\\nscales sublinearly in the number $N$ of cross-section units, i.e.,\\n$\\\\Lambda^{(0)\\\\top}\\\\Lambda^{(0)}/N^{\\\\alpha}$ is positive definite in the limit\\nfor some $\\\\alpha\\\\in (0,1)$. However, the established convergence rates for weak\\nfactors can be much slower especially for small $\\\\alpha$. This article proposes\\na Transfer Principal Component Analysis (TransPCA) method for enhancing the\\nconvergence rates for weak factors by transferring knowledge from large number\\nof available informative panel datasets, which should not be turned a blind eye\\non in this big data era. We aggregate useful information by analyzing a\\nweighted average projection matrix of the estimated loading spaces from all\\ninformative datasets which is highly flexible and computationally efficient.\\nTheoretically, we derive the convergence rates of the estimators of weak/strong\\nloading spaces and factor scores. The results indicate that as long as the\\nauxiliary datasets are similar enough to the target dataset and the auxiliary\\nsample size is sufficiently large, TransPCA estimators can achieve faster\\nconvergence rates in contrast to performing PCA solely on the target dataset.\\nTo avoid negative transfer, we also investigate the case that the informative\\ndatasets are unknown and provide a criterion for selecting useful datasets.\\nThorough simulation studies and {empirical analysis on real datasets in areas\\nof macroeconomic and finance} are conducted to illustrate the usefulness of our\\nproposed methods where large number of source panel datasets are naturally\\navailable.',\n",
       "  'keywords': ['Average Projection Matrix; Factor Strength; Transfer Learning; Weak Factors.']},\n",
       " {'title': 'Pointwise Minimax Vector Field Reconstruction from Noisy ODE',\n",
       "  'authors': ['Hugo Henneuse'],\n",
       "  'summary': \"This work addresses the problem of estimating a vector field from a noisy\\nOrdinary Differential Equation (ODE) in a non-parametric regression setting\\nwith a random design for initial values. More specifically, given a vector\\nfield $ f:\\\\mathbb{R}^{D}\\\\rightarrow \\\\mathbb{R}^{D}$ governing a dynamical\\nsystem defined by the autonomous ODE: $y' = f(y)$, we assume that the\\nobservations are $\\\\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) +\\n\\\\varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time\\n$t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a\\nprobability distribution $\\\\mu$, and $\\\\varepsilon_{i,j}$ some noise. In this\\ncontext, we investigate, from a minimax perspective, the pointwise\\nreconstruction of $f$ within the envelope of trajectories originating from the\\nsupport of $\\\\mu$. We propose an estimation strategy based on preliminary flow\\nreconstruction and techniques from derivative estimation in non-parametric\\nregression. Under mild assumptions on $f$, we establish convergence rates that\\ndepend on the temporal resolution, the number of sampled initial values and the\\nmass concentration of $\\\\mu$. Importantly, we show that these rates are minimax\\noptimal. Furthermore, we discuss the implications of our results in a manifold\\nlearning setting, providing insights into how our approach can mitigate the\\ncurse of dimensionality.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Generation and Balancing Capacity in Future Electric Power Systems --\\n  Scenario Analysis Using Bayesian Networks',\n",
       "  'authors': ['Seppo Borenius',\n",
       "   'Pekka Kekolahti',\n",
       "   'Petri Mähönen',\n",
       "   'Matti Lehtonen'],\n",
       "  'summary': \"This paper examines the evolution of the Finnish electric energy system up to\\n2035, focusing on the likelihood of different development paths. The primary\\ncontribution of this paper is the development of an extensive Bayesian Network,\\ndesigned to model and analyse the evolution of power generation capacity mix,\\nassess the likelihood of different grid management scenarios, and understand\\nthe causal relationships underlying these scenarios. A target optimisation was\\ncarried out using the constructed Bayesian Network to explore possibilities to\\nminimise grid management complexity. The results of the optimisation reveal\\nthat the authorities and stakeholders should prioritise increasing demand\\nresponse, gas power, and battery storage capacities. These mature technologies\\nare well-suited to guarantee energy adequacy during peak consumption periods,\\nwhich in Finland typically occur during consecutive cold, dark and windless\\nwinter weeks. Although this study focuses on the evolution of the Finnish power\\ngrid, the constructed Bayesian Network approach is broadly applicable and can\\nbe utilised to explore causal relationships in other countries by employing the\\ndesigned questionnaire and engaging a panel of experts specific to the\\ncountry's energy infrastructure.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Empirical Error Estimates for Graph Sparsification',\n",
       "  'authors': ['Siyao Wang', 'Miles E. Lopes'],\n",
       "  'summary': 'Graph sparsification is a well-established technique for accelerating\\ngraph-based learning algorithms, which uses edge sampling to approximate dense\\ngraphs with sparse ones. Because the sparsification error is random and\\nunknown, users must contend with uncertainty about the reliability of\\ndownstream computations. Although it is possible for users to obtain conceptual\\nguidance from theoretical error bounds in the literature, such results are\\ntypically impractical at a numerical level. Taking an alternative approach, we\\npropose to address these issues from a data-driven perspective by computing\\nempirical error estimates. The proposed error estimates are highly versatile,\\nand we demonstrate this in four use cases: Laplacian matrix approximation,\\ngraph cut queries, graph-structured regression, and spectral clustering.\\nMoreover, we provide two theoretical guarantees for the error estimates, and\\nexplain why the cost of computing them is manageable in comparison to the\\noverall cost of a typical graph sparsification workflow.',\n",
       "  'keywords': None},\n",
       " {'title': 'The Hidden Toll of COVID-19 on Opioid Mortality in Georgia: A Bayesian\\n  Excess Opioid Mortality Analysis',\n",
       "  'authors': ['Cyen J. Peterkin', 'Lance A. Waller', 'Emily N. Peterson'],\n",
       "  'summary': 'COVID-19 has had a large scale negative impact on the health of opioid users\\nexacerbating the health of an already vulnerable population. Critical\\ninformation on the total impact of COVID-19 on opioid users is unknown due to a\\nlack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and\\nlack of data coverage. To assess the impact of COVID-19 on small-area opioid\\nmortality, we developed a Bayesian hierarchical excess opioid mortality\\nmodeling approach. We incorporate spatio-temporal autocorrelation structures to\\nallow for sharing of information across small areas and time to reduce\\nuncertainty in small area estimates. Excess mortality is defined as the\\ndifference between observed trends after a crisis and expected trends based on\\nobserved historical trends, which captures the total increase in observed\\nmortality rates compared to what was expected prior to the crisis. We\\nillustrate the application of our approach to assess excess opioid mortality\\nrisk estimates for 159 counties in GA. Using our proposed approach will help\\ninform interventions in opioid-related public health responses, policies, and\\nresource allocation. The application of this work also provides a general\\nframework for improving the estimation and mapping of health indicators during\\ncrisis periods for the opioid user population.',\n",
       "  'keywords': ['BayesianHierarchicalmodels',\n",
       "   'COVID-19',\n",
       "   'Diseasemapping',\n",
       "   'ExcessOpioid']},\n",
       " {'title': 'Concentration via Metastable Mixing, with Applications to the\\n  Supercritical Exponential Random Graph Model',\n",
       "  'authors': ['Vilas Winstein'],\n",
       "  'summary': \"It is a folklore belief that metastable wells in low-temperature statistical\\nmechanics models exhibit high-temperature behavior. We prove a rigorous version\\nof this phenomenon in the setting of the exponential random graph model (ERGM)\\nthrough the lens of concentration of measure. To do this, we first present a\\nnew general result deriving concentration inequalities in a metastable well\\nfrom the metastable mixing of a Markov chain with the appropriate stationary\\ndistribution, extending a result of Chatterjee [Cha05] which is suited for more\\ntraditional forms of global mixing. We then apply this result to the\\nsupercritical (low-temperature) ERGM which was recently proven to exhibit\\nmetastable mixing by Bresler, Nagaraj, and Nichani [BNN24], and obtain a novel\\nconcentration inequality for Lipschitz observables of the supercritical ERGM\\nconditioned on a large metastable well, answering a question posed by [BNN24].\\nThis extends a result of Ganguly and Nam [GN24] from the subcritical\\n(high-temperature) regime to a metastable well in the supercritical regime, and\\nwe are also able to extend the applications of their concentration inequality\\nto these metastable wells. Namely, we obtain an upper bound on the Wasserstein\\ndistance between the ERGM conditioned on a metastable well and an appropriate\\nErd\\\\H{o}s-R\\\\'enyi model, as well as derive a central limit theorem for the\\ncount of edges in certain small subcollections of possible edges. Finally, to\\nsupplement the mathematical content of the article, we also discuss the results\\nof what appears to be the first simulation study of a metastable well in the\\nsupercritical ERGM.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Is a Good Foundation Necessary for Efficient Reinforcement Learning? The\\n  Computational Role of the Base Model in Exploration',\n",
       "  'authors': ['Dylan J. Foster', 'Zakaria Mhammedi', 'Dhruv Rohatgi'],\n",
       "  'summary': 'Language model alignment (or, reinforcement learning) techniques that\\nleverage active exploration -- deliberately encouraging the model to produce\\ndiverse, informative responses -- offer the promise of super-human\\ncapabilities. However, current understanding of algorithm design primitives for\\ncomputationally efficient exploration with language models is limited. To\\nbetter understand how to leverage access to powerful pre-trained generative\\nmodels to improve the efficiency of exploration, we introduce a new\\ncomputational framework for RL with language models, in which the learner\\ninteracts with the model through a sampling oracle. Focusing on the linear\\nsoftmax model parameterization, we provide new results that reveal the\\ncomputational-statistical tradeoffs of efficient exploration:\\n  1. Necessity of coverage: Coverage refers to the extent to which the\\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\\nWe show that coverage, while not necessary for data efficiency, lower bounds\\nthe runtime of any algorithm in our framework.\\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\\nwhich obtains optimal data efficiency and is computationally efficient whenever\\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\\nSpannerSampling leverages inference-time computation with the pre-trained model\\nto reduce the effective search space for exploration.\\n  3. Insufficiency of training-time interventions: We contrast the result above\\nby showing that training-time interventions that produce proper policies cannot\\nachieve similar guarantees in polynomial time.\\n  4. Computational benefits of multi-turn exploration: Finally, we show that\\nunder additional representational assumptions, one can achieve improved runtime\\n(replacing sequence-level coverage with token-level coverage) through\\nmulti-turn exploration.',\n",
       "  'keywords': None},\n",
       " {'title': 'Personalized Convolutional Dictionary Learning of Physiological Time\\n  Series',\n",
       "  'authors': ['Axel Roques',\n",
       "   'Samuel Gruffaz',\n",
       "   'Kyurae Kim',\n",
       "   'Alain Oliviero-Durmus',\n",
       "   'Laurent Oudre'],\n",
       "  'summary': 'Human physiological signals tend to exhibit both global and local structures:\\nthe former are shared across a population, while the latter reflect\\ninter-individual variability. For instance, kinetic measurements of the gait\\ncycle during locomotion present common characteristics, although idiosyncrasies\\nmay be observed due to biomechanical disposition or pathology. To better\\nrepresent datasets with local-global structure, this work extends Convolutional\\nDictionary Learning (CDL), a popular method for learning interpretable\\nrepresentations, or dictionaries, of time-series data. In particular, we\\npropose Personalized CDL (PerCDL), in which a local dictionary models local\\ninformation as a personalized spatiotemporal transformation of a global\\ndictionary. The transformation is learnable and can combine operations such as\\ntime warping and rotation. Formal computational and statistical guarantees for\\nPerCDL are provided and its effectiveness on synthetic and real human\\nlocomotion data is demonstrated.',\n",
       "  'keywords': None},\n",
       " {'title': 'Estimation of Local Geometric Structure on Manifolds from Noisy Data',\n",
       "  'authors': ['Yariv Aizenbud', 'Barak Sober'],\n",
       "  'summary': 'A common observation in data-driven applications is that high-dimensional\\ndata have a low intrinsic dimension, at least locally. In this work, we\\nconsider the problem of point estimation for manifold-valued data. Namely,\\ngiven a finite set of noisy samples of $\\\\mathcal{M}$, a $d$ dimensional\\nsubmanifold of $\\\\mathbb{R}^D$, and a point $r$ near the manifold we aim to\\nproject $r$ onto the manifold. Assuming that the data was sampled uniformly\\nfrom a tubular neighborhood of a $k$-times smooth boundaryless and compact\\nmanifold, we present an algorithm that takes $r$ from this neighborhood and\\noutputs $\\\\hat p_n\\\\in \\\\mathbb{R}^D$, and $\\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ an\\nelement in the Grassmannian $Gr(d, D)$. We prove that as the number of samples\\n$n\\\\to\\\\infty$, the point $\\\\hat p_n$ converges to $\\\\mathbf{p}\\\\in \\\\mathcal{M}$,\\nthe projection of $r$ onto $\\\\mathcal{M}$, and $\\\\widehat{T_{\\\\hat\\np_n}\\\\mathcal{M}}$ converges to $T_{\\\\mathbf{p}}\\\\mathcal{M}$ (the tangent space\\nat that point) with high probability. Furthermore, we show that $\\\\hat p_n$\\napproaches the manifold with an asymptotic rate of $n^{-\\\\frac{k}{2k + d}}$, and\\nthat $\\\\hat p_n, \\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ approach $\\\\mathbf{p}$ and\\n$T_{\\\\mathbf{p}}\\\\mathcal{M}$ correspondingly with asymptotic rates of\\n$n^{-\\\\frac{k-1}{2k + d}}$.',\n",
       "  'keywords': None},\n",
       " {'title': 'Asymptotic normality and strong consistency of kernel regression\\n  estimation in q-calculus',\n",
       "  'authors': ['Emmanuel De Dieu Nkou', 'Fridolin Melong'],\n",
       "  'summary': 'We construct a family of estimators for a regression function based on a\\nsample following a qdistribution. Our approach is nonparametric, using kernel\\nmethods built from operations that leverage the properties of q-calculus.\\nFurthermore, under appropriate assumptions, we establish the weak convergence\\nand strong consistency of this family of estimators.',\n",
       "  'keywords': ['q-calculus',\n",
       "   'nonparametric estimation',\n",
       "   'regressionestimation.']},\n",
       " {'title': 'The level of self-organized criticality in oscillating Brownian motion:\\n  $n$-consistency and stable Poisson-type convergence of the MLE',\n",
       "  'authors': ['Johannes Brutsche', 'Angelika Rohde'],\n",
       "  'summary': 'For some discretely observed path of oscillating Brownian motion with level\\nof self-organized criticality $\\\\rho_0$, we prove in the infill asymptotics that\\nthe MLE is $n$-consistent, where $n$ denotes the sample size, and derive its\\nlimit distribution with respect to stable convergence. As the transition\\ndensity of this homogeneous Markov process is not even continuous in $\\\\rho_0$,\\nthe analysis is highly non-standard. Therefore, interesting and somewhat\\nunexpected phenomena occur: The likelihood function splits into several\\ncomponents, each of them contributing very differently depending on how close\\nthe argument $\\\\rho$ is to $\\\\rho_0$. Correspondingly, the MLE is successively\\nexcluded to lay outside a compact set, a $1/\\\\sqrt{n}$-neighborhood and finally\\na $1/n$-neigborhood of $\\\\rho_0$ asymptotically. The crucial argument to derive\\nthe stable convergence is to exploit the semimartingale structure of the\\nsequential suitably rescaled local log-likelihood function (as a process in\\ntime). Both sequentially and as a process in $\\\\rho$, it exhibits a bivariate\\nPoissonian behavior in the stable limit with its intensity being a multiple of\\nthe local time at $\\\\rho_0$.',\n",
       "  'keywords': None},\n",
       " {'title': 'BASIC: Bipartite Assisted Spectral-clustering for Identifying\\n  Communities in Large-scale Networks',\n",
       "  'authors': ['Tianchen Gao', 'Jingyuan Liu', 'Rui Pan', 'Ao Sun'],\n",
       "  'summary': 'Community detection, which focuses on recovering the group structure within\\nnetworks, is a crucial and fundamental task in network analysis. However, the\\ndetection process can be quite challenging and unstable when community signals\\nare weak. Motivated by a newly collected large-scale academic network dataset\\nfrom the Web of Science, which includes multi-layer network information, we\\npropose a Bipartite Assisted Spectral-clustering approach for Identifying\\nCommunities (BASIC), which incorporates the bipartite network information into\\nthe community structure learning of the primary network. The accuracy and\\nstability enhancement of BASIC is validated theoretically on the basis of the\\ndegree-corrected stochastic block model framework, as well as numerically\\nthrough extensive simulation studies. We rigorously study the convergence rate\\nof BASIC even under weak signal scenarios and prove that BASIC yields a tighter\\nupper error bound than that based on the primary network information alone. We\\nutilize the proposed BASIC method to analyze the newly collected large-scale\\nacademic network dataset from statistical papers. During the author\\ncollaboration network structure learning, we incorporate the bipartite network\\ninformation from author-paper, author-institution, and author-region\\nrelationships. From both statistical and interpretative perspectives, these\\nbipartite networks greatly aid in identifying communities within the primary\\ncollaboration network.',\n",
       "  'keywords': ['CommunityDetection',\n",
       "   'SpectralClustering',\n",
       "   'BipartiteNetwork',\n",
       "   'WeakSignal',\n",
       "   '']},\n",
       " {'title': 'Limit Theorems for One-Dimensional Homogenized Diffusion Processes',\n",
       "  'authors': ['Jaroslav I. Borodavka', 'Sebastian Krumscheid'],\n",
       "  'summary': 'We present two limit theorems, a mean ergodic and a central limit theorem,\\nfor a specific class of one-dimensional diffusion processes that depend on a\\nsmall-scale parameter $\\\\varepsilon$ and converge weakly to a homogenized\\ndiffusion process in the limit $\\\\varepsilon \\\\rightarrow 0$. In these results,\\nwe allow for the time horizon to blow up such that $T_\\\\varepsilon \\\\rightarrow\\n\\\\infty$ as $\\\\varepsilon \\\\rightarrow 0$. The novelty of the results arises from\\nthe circumstance that many quantities are unbounded for $\\\\varepsilon\\n\\\\rightarrow 0$, so that formerly established theory is not directly applicable\\nhere and a careful investigation of all relevant $\\\\varepsilon$-dependent terms\\nis required. As a mathematical application, we then use these limit theorems to\\nprove asymptotic properties of a minimum distance estimator for parameters in a\\nhomogenized diffusion equation.',\n",
       "  'keywords': None},\n",
       " {'title': 'Extremes of structural causal models',\n",
       "  'authors': ['Sebastian Engelke', 'Nicola Gnecco', 'Frank Röttger'],\n",
       "  'summary': 'The behavior of extreme observations is well-understood for time series or\\nspatial data, but little is known if the data generating process is a\\nstructural causal model (SCM). We study the behavior of extremes in this model\\nclass, both for the observational distribution and under extremal\\ninterventions. We show that under suitable regularity conditions on the\\nstructure functions, the extremal behavior is described by a multivariate\\nPareto distribution, which can be represented as a new SCM on an extremal\\ngraph. Importantly, the latter is a sub-graph of the graph in the original SCM,\\nwhich means that causal links can disappear in the tails. We further introduce\\na directed version of extremal graphical models and show that an extremal SCM\\nsatisfies the corresponding Markov properties. Based on a new test of extremal\\nconditional independence, we propose two algorithms for learning the extremal\\ncausal structure from data. The first is an extremal version of the\\nPC-algorithm, and the second is a pruning algorithm that removes edges from the\\noriginal graph to consistently recover the extremal graph. The methods are\\nillustrated on river data with known causal ground truth.',\n",
       "  'keywords': ['causality',\n",
       "   'extremevaluetheory',\n",
       "   'graphicalmodel',\n",
       "   'structurelearning.']},\n",
       " {'title': \"Detecting correlation efficiently in stochastic block models: breaking\\n  Otter's threshold by counting decorated trees\",\n",
       "  'authors': ['Guanyi Chen', 'Jian Ding', 'Shuyang Gong', 'Zhangsong Li'],\n",
       "  'summary': \"Consider a pair of sparse correlated stochastic block models $\\\\mathcal\\nS(n,\\\\tfrac{\\\\lambda}{n},\\\\epsilon;s)$ subsampled from a common parent stochastic\\nblock model with two symmetric communities, average degree $\\\\lambda=O(1)$ and\\ndivergence parameter $\\\\epsilon \\\\in (0,1)$. For all $\\\\epsilon\\\\in(0,1)$, we\\nconstruct a statistic based on the combination of two low-degree polynomials\\nand show that there exists a sufficiently small constant\\n$\\\\delta=\\\\delta(\\\\epsilon)>0$ and a sufficiently large constant\\n$\\\\Delta=\\\\Delta(\\\\epsilon,\\\\delta)$ such that when $\\\\lambda>\\\\Delta$ and\\n$s>\\\\sqrt{\\\\alpha}-\\\\delta$ where $\\\\alpha\\\\approx 0.338$ is Otter's constant, this\\nstatistic can distinguish this model and a pair of independent stochastic block\\nmodels $\\\\mathcal S(n,\\\\tfrac{\\\\lambda s}{n},\\\\epsilon)$ with probability $1-o(1)$.\\nWe also provide an efficient algorithm that approximates this statistic in\\npolynomial time. The crux of our statistic's construction lies in a carefully\\ncurated family of multigraphs called \\\\emph{decorated trees}, which enables\\neffective aggregation of the community signal and graph correlation from the\\ncounts of the same decorated tree while suppressing the undesirable\\ncorrelations among counts of different decorated trees.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Parameter Estimation for Partially Observed Affine and Polynomial\\n  Processes',\n",
       "  'authors': ['Jan Kallsen', 'Ivo Richert'],\n",
       "  'summary': 'This paper is devoted to parameter estimation for partially observed\\npolynomial state space models. This class includes discretely observed affine\\nor more generally polynomial Markov processes. The polynomial structure allows\\nfor the explicit computation of a Gaussian quasi-likelihood estimator and its\\nasymptotic covariance matrix. We show consistency and asymptotic normality of\\nthe estimating sequence and provide explicitly computable expressions for the\\ncorresponding asymptotic covariance matrix.',\n",
       "  'keywords': ['polynomialprocesses',\n",
       "   'polynomialstatespacemodels',\n",
       "   'parameterestimation',\n",
       "   '']},\n",
       " {'title': 'Filtering of partially observed polynomial processes in discrete and\\n  continuous time',\n",
       "  'authors': ['Jan Kallsen', 'Ivo Richert'],\n",
       "  'summary': 'This paper is devoted to filtering, smoothing, and prediction of polynomial\\nprocesses that are partially observed. These problems are known to allow for an\\nexplicit solution in the simpler case of linear Gaussian state space models.\\nThe key insight underlying the present piece of research is that in filtering\\napplications polynomial processes and their discrete counterpart are\\nindistinguishable from Gaussian processes sharing their first two moments. We\\ndescribe the construction of these Gaussian equivalents of polynomial processes\\nand explicitly compute optimal linear filters, predictors and smoothers for\\npolynomial processes in discrete and continuous time. The consideration of\\nGaussian equivalents also opens the door to parameter estimation and\\nlinear-quadratic optimal control in the context of polynomial processes.',\n",
       "  'keywords': None},\n",
       " {'title': 'Comparing regularisation paths of (conjugate) gradient estimators in\\n  ridge regression',\n",
       "  'authors': ['Laura Hucker', 'Markus Reiß', 'Thomas Stark'],\n",
       "  'summary': 'We consider standard gradient descent, gradient flow and conjugate gradients\\nas iterative algorithms for minimizing a penalized ridge criterion in linear\\nregression. While it is well known that conjugate gradients exhibit fast\\nnumerical convergence, the statistical properties of their iterates are more\\ndifficult to assess due to inherent nonlinearities and dependencies. On the\\nother hand, standard gradient flow is a linear method with well known\\nregularizing properties when stopped early. By an explicit non-standard error\\ndecomposition we are able to bound the prediction error for conjugate gradient\\niterates by a corresponding prediction error of gradient flow at transformed\\niteration indices. This way, the risk along the entire regularisation path of\\nconjugate gradient iterations can be compared to that for regularisation paths\\nof standard linear methods like gradient flow and ridge regression. In\\nparticular, the oracle conjugate gradient iterate shares the optimality\\nproperties of the gradient flow and ridge regression oracles up to a constant\\nfactor. Numerical examples show the similarity of the regularisation paths in\\npractice.',\n",
       "  'keywords': None},\n",
       " {'title': 'Stochastic dominance of sums of risks under dependence conditions',\n",
       "  'authors': ['Jorge Navarro', 'José M. Zapata'],\n",
       "  'summary': 'We provide conditions for the stochastic dominance comparisons of a risk $X$\\nand an associated risk $X+Z$, where $Z$ represents the uncertainty due to the\\nenvironment and where $X$ and $Z$ can be dependent. The comparisons depend on\\nboth the copula $C$ between the distributions of $X$ and $Z$ and on the\\ndistribution of $Z$. We provide two different conditions for $C$ which\\nrepresents new positive dependence properties. Regarding $Z$, we need some\\nsymmetry or asymmetry (skew) properties. Some illustrative examples are\\nprovided.',\n",
       "  'keywords': ['Dependenceproperties·Stochasticorders·Copula·C-convolutions']},\n",
       " {'title': 'Graph Alignment via Birkhoff Relaxation',\n",
       "  'authors': ['Sushil Mahavir Varma',\n",
       "   'Irène Waldspurger',\n",
       "   'Laurent Massoulié'],\n",
       "  'summary': 'We consider the graph alignment problem, wherein the objective is to find a\\nvertex correspondence between two graphs that maximizes the edge overlap. The\\ngraph alignment problem is an instance of the quadratic assignment problem\\n(QAP), known to be NP-hard in the worst case even to approximately solve. In\\nthis paper, we analyze Birkhoff relaxation, a tight convex relaxation of QAP,\\nand present theoretical guarantees on its performance when the inputs follow\\nthe Gaussian Wigner Model. More specifically, the weighted adjacency matrices\\nare correlated Gaussian Orthogonal Ensemble with correlation\\n$1/\\\\sqrt{1+\\\\sigma^2}$. Denote the optimal solutions of the QAP and Birkhoff\\nrelaxation by $\\\\Pi^\\\\star$ and $X^\\\\star$ respectively. We show that\\n$\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = o(n)$ when $\\\\sigma = o(n^{-1.25})$ and\\n$\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = \\\\Omega(n)$ when $\\\\sigma = \\\\Omega(n^{-0.5})$. Thus,\\nthe optimal solution $X^\\\\star$ transitions from a small perturbation of\\n$\\\\Pi^\\\\star$ for small $\\\\sigma$ to being well separated from $\\\\Pi^\\\\star$ as\\n$\\\\sigma$ becomes larger than $n^{-0.5}$. This result allows us to guarantee\\nthat simple rounding procedures on $X^\\\\star$ align $1-o(1)$ fraction of\\nvertices correctly whenever $\\\\sigma = o(n^{-1.25})$. This condition on $\\\\sigma$\\nto ensure the success of the Birkhoff relaxation is state-of-the-art.',\n",
       "  'keywords': ['Graph Alignment',\n",
       "   'Quadratic Assignment Problem',\n",
       "   'Convex Relaxation',\n",
       "   'Sensitivity']},\n",
       " {'title': 'Optimal and fast online change point estimation in linear regression',\n",
       "  'authors': ['Annika Hüselitz', 'Housen Li', 'Axel Munk'],\n",
       "  'summary': 'We consider the problem of sequential estimation of a single change point in\\na piecewise linear regression model under a Gaussian setup. We demonstrate that\\na certain CUSUM-type statistic attains the minimax optimal rates for localizing\\nthe change point. Our minimax analysis unveils an interesting phase transition\\nfrom a jump (discontinuity in values) to a kink (change in slope).\\nSpecifically, for a jump, the minimax rate is of order $\\\\log (n) / n$, whereas\\nfor a kink it scales as $\\\\bigl(\\\\log (n) / n\\\\bigr)^{1/3}$, given that the\\nsampling rate is of order $1/n$. We further introduce an algorithm for the\\nproposed online change point detector, which requires constant computational\\nsteps and constant memory per incoming sample. Finally, the empirical\\nperformance of our method is examined on both simulated and real-world data\\nsets. An implementation is available in the R package FLOC on GitHub.',\n",
       "  'keywords': ['Sequential detection',\n",
       "   'minimax rate',\n",
       "   'efficient computation',\n",
       "   'Covid-19',\n",
       "   'two phase regression.']},\n",
       " {'title': 'Kernel-based estimators for functional causal effects',\n",
       "  'authors': ['Yordan P. Raykov',\n",
       "   'Hengrui Luo',\n",
       "   'Justin D. Strait',\n",
       "   'Wasiur R. KhudaBukhsh'],\n",
       "  'summary': \"We propose causal effect estimators based on empirical Fr\\\\'{e}chet means and\\noperator-valued kernels, tailored to functional data spaces. These methods\\naddress the challenges of high-dimensionality, sequential ordering, and model\\ncomplexity while preserving robustness to treatment misspecification. Using\\nstructural assumptions, we obtain compact representations of potential\\noutcomes, enabling scalable estimation of causal effects over time and across\\ncovariates. We provide both theoretical, regarding the consistency of\\nfunctional causal effects, as well as empirical comparison of a range of\\nproposed causal effect estimators.\\n  Applications to binary treatment settings with functional outcomes illustrate\\nthe framework's utility in biomedical monitoring, where outcomes exhibit\\ncomplex temporal dynamics. Our estimators accommodate scenarios with registered\\ncovariates and outcomes, aligning them to the Fr\\\\'{e}chet means, as well as\\ncases requiring higher-order representations to capture intricate\\ncovariate-outcome interactions. These advancements extend causal inference to\\ndynamic and non-linear domains, offering new tools for understanding complex\\ntreatment effects in functional data settings.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Estimation of relative risk, odds ratio and their logarithms with\\n  guaranteed accuracy and controlled sample size ratio',\n",
       "  'authors': ['Luis Mendo'],\n",
       "  'summary': \"Given two populations from which independent binary observations are taken\\nwith parameters $p_1$ and $p_2$ respectively, estimators are proposed for the\\nrelative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their\\nlogarithms. The estimators guarantee that the relative mean-square error, or\\nthe mean-square error for the logarithmic versions, is less than a target value\\nfor any $p_1, p_2 \\\\in (0,1)$, and the ratio of average sample sizes from the\\ntwo populations is close to a prescribed value. The estimators can also be used\\nwith group sampling, whereby samples are taken in batches of fixed size from\\nthe two populations. The efficiency of the estimators with respect to the\\nCram\\\\'er-Rao bound is good, and in particular it is close to $1$ for small\\nvalues of the target error.\",\n",
       "  'keywords': ['Estimation',\n",
       "   'sequentialsampling',\n",
       "   'groupsampling',\n",
       "   'relativerisk',\n",
       "   'oddsratio',\n",
       "   'logoddsratio',\n",
       "   '']},\n",
       " {'title': 'Learning Causal Response Representations through Direct Effect Analysis',\n",
       "  'authors': ['Homer Durand', 'Gherardo Varando', 'Gustau Camps-Valls'],\n",
       "  'summary': 'We propose a novel approach for learning causal response representations. Our\\nmethod aims to extract directions in which a multidimensional outcome is most\\ndirectly caused by a treatment variable. By bridging conditional independence\\ntesting with causal representation learning, we formulate an optimisation\\nproblem that maximises the evidence against conditional independence between\\nthe treatment and outcome, given a conditioning set. This formulation employs\\nflexible regression models tailored to specific applications, creating a\\nversatile framework. The problem is addressed through a generalised eigenvalue\\ndecomposition. We show that, under mild assumptions, the distribution of the\\nlargest eigenvalue can be bounded by a known $F$-distribution, enabling\\ntestable conditional independence. We also provide theoretical guarantees for\\nthe optimality of the learned representation in terms of signal-to-noise ratio\\nand Fisher information maximisation. Finally, we demonstrate the empirical\\neffectiveness of our approach in simulation and real-world experiments. Our\\nresults underscore the utility of this framework in uncovering direct causal\\neffects within complex, multivariate settings.',\n",
       "  'keywords': None},\n",
       " {'title': 'Provable Robust Overfitting Mitigation in Wasserstein Distributionally\\n  Robust Optimization',\n",
       "  'authors': ['Shuang Liu',\n",
       "   'Yihan Wang',\n",
       "   'Yifan Zhu',\n",
       "   'Yibo Miao',\n",
       "   'Xiao-Shan Gao'],\n",
       "  'summary': 'Wasserstein distributionally robust optimization (WDRO) optimizes against\\nworst-case distributional shifts within a specified uncertainty set, leading to\\nenhanced generalization on unseen adversarial examples, compared to standard\\nadversarial training which focuses on pointwise adversarial perturbations.\\nHowever, WDRO still suffers fundamentally from the robust overfitting problem,\\nas it does not consider statistical error. We address this gap by proposing a\\nnovel robust optimization framework under a new uncertainty set for adversarial\\nnoise via Wasserstein distance and statistical error via Kullback-Leibler\\ndivergence, called the Statistically Robust WDRO. We establish a robust\\ngeneralization bound for the new optimization framework, implying that\\nout-of-distribution adversarial performance is at least as good as the\\nstatistically robust training loss with high probability. Furthermore, we\\nderive conditions under which Stackelberg and Nash equilibria exist between the\\nlearner and the adversary, giving an optimal robust model in certain sense.\\nFinally, through extensive experiments, we demonstrate that our method\\nsignificantly mitigates robust overfitting and enhances robustness within the\\nframework of WDRO.',\n",
       "  'keywords': None},\n",
       " {'title': 'Generalizability of Neural Networks Minimizing Empirical Risk Based on\\n  Expressive Ability',\n",
       "  'authors': ['Lijia Yu',\n",
       "   'Yibo Miao',\n",
       "   'Yifan Zhu',\n",
       "   'Xiao-Shan Gao',\n",
       "   'Lijun Zhang'],\n",
       "  'summary': 'The primary objective of learning methods is generalization. Classic uniform\\ngeneralization bounds, which rely on VC-dimension or Rademacher complexity,\\nfail to explain the significant attribute that over-parameterized models in\\ndeep learning exhibit nice generalizability. On the other hand,\\nalgorithm-dependent generalization bounds, like stability bounds, often rely on\\nstrict assumptions. To establish generalizability under less stringent\\nassumptions, this paper investigates the generalizability of neural networks\\nthat minimize or approximately minimize empirical risk. We establish a lower\\nbound for population accuracy based on the expressiveness of these networks,\\nwhich indicates that with an adequate large number of training samples and\\nnetwork sizes, these networks, including over-parameterized ones, can\\ngeneralize effectively. Additionally, we provide a necessary condition for\\ngeneralization, demonstrating that, for certain data distributions, the\\nquantity of training data required to ensure generalization exceeds the network\\nsize needed to represent the corresponding data distribution. Finally, we\\nprovide theoretical insights into several phenomena in deep learning, including\\nrobust generalization, importance of over-parameterization, and effect of loss\\nfunction on generalization.',\n",
       "  'keywords': None},\n",
       " {'title': 'Improving discrepancy by moving a few points',\n",
       "  'authors': ['Gleb Smirnov', 'Roman Vershynin'],\n",
       "  'summary': 'We show how to improve the discrepancy of an iid sample by moving only a few\\npoints. Specifically, modifying \\\\( O(m) \\\\) sample points on average reduces the\\nKolmogorov-Smirnov distance to the population distribution to \\\\(1/m\\\\).',\n",
       "  'keywords': None},\n",
       " {'title': 'Visual tests using several safe confidence intervals',\n",
       "  'authors': ['Timothée Mathieu'],\n",
       "  'summary': 'We propose a new statistical hypothesis testing framework which decides\\nvisually, using confidence intervals, whether the means of two samples are\\nequal or if one is larger than the other. With our method, the user can at the\\nsame time visualize the confidence region of the means and do a test to decide\\nif the means of the two populations are significantly different or not by\\nlooking whether the two confidence intervals overlap. To design this test we\\nuse confidence intervals constructed using e-variables, which provide a measure\\nof evidence in hypothesis testing. We propose both a sequential test and a\\nnon-sequential test based on the overlap of confidence intervals and for each\\nof these tests we give finite-time error bounds on the probabilities of error.\\nWe also illustrate the practicality of our method by applying it to the\\ncomparison of sequential learning algorithms.',\n",
       "  'keywords': ['Confidenceintervals',\n",
       "   'hypothesistesting',\n",
       "   'E-values',\n",
       "   'Anytimeconfidenceintervals']},\n",
       " {'title': 'Early-Stopped Mirror Descent for Linear Regression over Convex Bodies',\n",
       "  'authors': ['Tobias Wegel', 'Gil Kur', 'Patrick Rebeschini'],\n",
       "  'summary': 'Early-stopped iterative optimization methods are widely used as alternatives\\nto explicit regularization, and direct comparisons between early-stopping and\\nexplicit regularization have been established for many optimization geometries.\\nHowever, most analyses depend heavily on the specific properties of the\\noptimization geometry or strong convexity of the empirical objective, and it\\nremains unclear whether early-stopping could ever be less statistically\\nefficient than explicit regularization for some particular shape constraint,\\nespecially in the overparameterized regime. To address this question, we study\\nthe setting of high-dimensional linear regression under additive Gaussian noise\\nwhen the ground truth is assumed to lie in a known convex body and the task is\\nto minimize the in-sample mean squared error. Our main result shows that for\\nany convex body and any design matrix, up to an absolute constant factor, the\\nworst-case risk of unconstrained early-stopped mirror descent with an\\nappropriate potential is at most that of the least squares estimator\\nconstrained to the convex body. We achieve this by constructing algorithmic\\nregularizers based on the Minkowski functional of the convex body.',\n",
       "  'keywords': None},\n",
       " {'title': 'Statistical Limits in Random Tensors with Multiple Correlated Spikes',\n",
       "  'authors': ['Yang Qi', 'Alexis Decurninge'],\n",
       "  'summary': 'We use tools from random matrix theory to study the multi-spiked tensor\\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\\nparticular, thanks to the nature of local optimization methods used to find the\\nmaximum likelihood estimator of this model, we propose to study the phase\\ntransition phenomenon for finding critical points of the corresponding\\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\\nestimated signals corresponding to a critical point of the likelihood and the\\nground truth signals. With the help of these results, we propose a new\\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\\nequations, which is asymptotically unbiased contrary the maximum likelihood\\nestimator.',\n",
       "  'keywords': None},\n",
       " {'title': 'Drift estimation for rough processes under small noise asymptotic:\\n  trajectory fitting method',\n",
       "  'authors': ['Arnaud Gloter', 'Nakahiro Yoshida'],\n",
       "  'summary': 'We consider a process $X^\\\\varepsilon$ solution of a stochastic Volterra\\nequation with an unknown parameter $\\\\theta^\\\\star$ in the drift function. The\\nVolterra kernel is singular and given by $K(u)=c u^{\\\\alpha-1/2}\\n\\\\mathbb{1}_{u>0}$ with $\\\\alpha \\\\in (0,1/2)$. It is assumed that the diffusion\\ncoefficient is proportional to $\\\\varepsilon \\\\to 0$. From an observation of the\\npath $(X^\\\\varepsilon_s)_{s\\\\in[0,T]}$, we construct a Trajectory Fitting\\nEstimator, which is shown to be consistent and asymptotically normal. We also\\nspecify identifiability conditions insuring the $L^p$ convergence of the\\nestimator.',\n",
       "  'keywords': None},\n",
       " {'title': 'Safety Verification of Nonlinear Stochastic Systems via Probabilistic\\n  Tube',\n",
       "  'authors': ['Zishun Liu', 'Saber Jafarpour', 'Yongxin Chen'],\n",
       "  'summary': 'We address the problem of safety verification for nonlinear stochastic\\nsystems, specifically the task of certifying that system trajectories remain\\nwithin a safe set with high probability. To tackle this challenge, we adopt a\\nset-erosion strategy, which decouples the effects of stochastic disturbances\\nfrom deterministic dynamics. This approach converts the stochastic safety\\nverification problem on a safe set into a deterministic safety verification\\nproblem on an eroded subset of the safe set. The success of this strategy\\nhinges on the depth of erosion, which is determined by a probabilistic tube\\nthat bounds the deviation of stochastic trajectories from their corresponding\\ndeterministic trajectories. Our main contribution is the establishment of a\\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\\nobtain a probabilistic bound for stochastic trajectories, we adopt a\\nmartingale-based approach. The core innovation lies in the design of a novel\\nenergy function associated with the averaged moment generating function, which\\nforms an affine martingale, a generalization of the traditional c-martingale.\\nUsing this energy function, we derive a precise bound for the probabilistic\\ntube. Furthermore, we enhance this bound by incorporating the union-bound\\ninequality for strictly contractive dynamics. By integrating the derived\\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\\nsafety verification problem for nonlinear stochastic systems can be reduced to\\na deterministic safety verification problem. Our theoretical results are\\nvalidated through applications in reachability-based safety verification and\\nsafe controller synthesis, accompanied by several numerical examples that\\nillustrate their effectiveness.',\n",
       "  'keywords': None},\n",
       " {'title': 'Estimating weak Markov-switching AR(1) models',\n",
       "  'authors': ['Yacouba Boubacar Mainassara', 'Landy Rabehasaina', 'Armel Bra'],\n",
       "  'summary': 'In this paper, we present the asymptotic properties of the moment estimator\\nfor autoregressive (AR for short) models subject to Markovian changes in regime\\nunder the assumption that the errors are uncorrelated but not necessarily\\nindependent. We relax the standard independence assumption on the innovation\\nprocess to extend considerably the range of application of the Markov-switching\\nAR models. We provide necessary conditions to prove the consistency and\\nasymptotic normality of the moment estimator in a specific case. Particular\\nattention is paid to the estimation of the asymptotic covariance matrix.\\nFinally, some simulation studies and an application to the hourly\\nmeteorological data are presented to corroborate theoretical work.',\n",
       "  'keywords': ['Weak AR models',\n",
       "   'Regime-switching models',\n",
       "   'Markov-switching models',\n",
       "   'Times series with']},\n",
       " {'title': 'A Near Complete Nonasymptotic Generalization Theory For Multilayer\\n  Neural Networks: Beyond the Bias-Variance Tradeoff',\n",
       "  'authors': ['Hao Yu', 'Xiangyang Ji'],\n",
       "  'summary': \"We propose a first near complete (that will make explicit sense in the main\\ntext) nonasymptotic generalization theory for multilayer neural networks with\\narbitrary Lipschitz activations and general Lipschitz loss functions (with some\\nvery mild conditions). In particular, it doens't require the boundness of loss\\nfunction, as commonly assumed in the literature. Our theory goes beyond the\\nbias-variance tradeoff, aligned with phenomenon typically encountered in deep\\nlearning. It is therefore sharp different with other existing nonasymptotic\\ngeneralization error bounds for neural networks. More explicitly, we propose an\\nexplicit generalization error upper bound for multilayer neural networks with\\narbitrary Lipschitz activations $\\\\sigma$ with $\\\\sigma(0)=0$ and broad enough\\nLipschitz loss functions, without requiring either the width, depth or other\\nhyperparameters of the neural network approaching infinity, a specific neural\\nnetwork architect (e.g. sparsity, boundness of some norms), a particular\\nactivation function, a particular optimization algorithm or boundness of the\\nloss function, and with taking the approximation error into consideration.\\nGeneral Lipschitz activation can also be accommodated into our framework. A\\nfeature of our theory is that it also considers approximation errors.\\nFurthermore, we show the near minimax optimality of our theory for multilayer\\nReLU networks for regression problems. Notably, our upper bound exhibits the\\nfamous double descent phenomenon for such networks, which is the most\\ndistinguished characteristic compared with other existing results. This work\\nemphasizes a view that many classical results should be improved to embrace the\\nunintuitive characteristics of deep learning to get a better understanding of\\nit.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Pseudo-Maximum Likelihood Theory for High-Dimensional Rank One Inference',\n",
       "  'authors': ['Curtis Grant', 'Aukosh Jagannath', 'Justin Ko'],\n",
       "  'summary': \"We develop a pseudo-likelihood theory for rank one matrix estimation problems\\nin the high dimensional limit. We prove a variational principle for the\\nlimiting pseudo-maximum likelihood which also characterizes the performance of\\nthe corresponding pseudo-maximum likelihood estimator. We show that this\\nvariational principle is universal and depends only on four parameters\\ndetermined by the corresponding null model. Through this universality, we\\nintroduce a notion of equivalence for estimation problems of this type and, in\\nparticular, show that a broad class of estimation tasks, including community\\ndetection, sparse submatrix detection, and non-linear spiked matrix models, are\\nequivalent to spiked matrix models. As an application, we obtain a complete\\ndescription of the performance of the least-squares (or ``best rank one'')\\nestimator for any rank one matrix estimation problem.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Asymmetric Cross-Correlation in Multivariate Spatial Stochastic\\n  Processes: A Primer',\n",
       "  'authors': ['Xiaoqing Chen'],\n",
       "  'summary': 'Multivariate spatial phenomena are ubiquitous, spanning domains such as\\nclimate, pandemics, air quality, and social economy. Cross-correlation between\\ndifferent quantities of interest at different locations is asymmetric in\\ngeneral. This paper provides the visualization, structure, and properties of\\nasymmetric cross-correlation as well as symmetric auto-correlation. It reviews\\nmainstream multivariate spatial models and analyzes their capability to\\naccommodate asymmetric cross-correlation. It also illustrates the difference in\\nmodel accuracy with and without asymmetric accommodation using a 1D simulated\\nexample.',\n",
       "  'keywords': ['asymmetric cross-correlation',\n",
       "   'symmetric auto-correlation',\n",
       "   'multivariate spatial']},\n",
       " {'title': 'Systemic Risk Management via Maximum Independent Set in Extremal\\n  Dependence Networks',\n",
       "  'authors': ['Qian Hui', 'Tiandong Wang'],\n",
       "  'summary': 'The failure of key financial institutions may accelerate risk contagion due\\nto their interconnections within the system. In this paper, we propose a robust\\nportfolio strategy to mitigate systemic risks during extreme events. We use the\\nstock returns of key financial institutions as an indicator of their\\nperformance, apply extreme value theory to assess the extremal dependence among\\nstocks of financial institutions, and construct a network model based on a\\nthreshold approach that captures extremal dependence. Our analysis reveals\\ndifferent dependence structures in the Chinese and U.S. financial systems. By\\napplying the maximum independent set (MIS) from graph theory, we identify a\\nsubset of institutions with minimal extremal dependence, facilitating the\\nconstruction of diversified portfolios resilient to risk contagion. We also\\ncompare the performance of our proposed portfolios with that of the market\\nportfolios in the two economies.',\n",
       "  'keywords': ['Extremal dependence measure',\n",
       "   'Complex network',\n",
       "   'Maximum independent set',\n",
       "   'Systemic risk']},\n",
       " {'title': 'Powerful rank verification for multivariate Gaussian data with any\\n  covariance structure',\n",
       "  'authors': ['Anav Sood'],\n",
       "  'summary': 'Upon observing $n$-dimensional multivariate Gaussian data, when can we infer\\nthat the largest $K$ observations came from the largest $K$ means? When $K=1$\\nand the covariance is isotropic, \\\\cite{Gutmann} argue that this inference is\\njustified when the two-sided difference-of-means test comparing the largest and\\nsecond largest observation rejects. Leveraging tools from selective inference,\\nwe provide a generalization of their procedure that applies for both any $K$\\nand any covariance structure. We show that our procedure draws the desired\\ninference whenever the two-sided difference-of-means test comparing the pair of\\nobservations inside and outside the top $K$ with the smallest standardized\\ndifference rejects, and sometimes even when this test fails to reject. Using\\nthis insight, we argue that our procedure renders existing simultaneous\\ninference approaches inadmissible when $n > 2$. When the observations are\\nindependent (with possibly unequal variances) or equicorrelated, our procedure\\ncorresponds exactly to running the two-sided difference-of-means test comparing\\nthe pair of observations inside and outside the top $K$ with the smallest\\nstandardized difference.',\n",
       "  'keywords': None},\n",
       " {'title': 'Asymptotic Theory of Eigenvectors for Latent Embeddings with Generalized\\n  Laplacian Matrices',\n",
       "  'authors': ['Jianqing Fan',\n",
       "   'Yingying Fan',\n",
       "   'Jinchi Lv',\n",
       "   'Fan Yang',\n",
       "   'Diwen Yu'],\n",
       "  'summary': 'Laplacian matrices are commonly employed in many real applications, encoding\\nthe underlying latent structural information such as graphs and manifolds. The\\nuse of the normalization terms naturally gives rise to random matrices with\\ndependency. It is well-known that dependency is a major bottleneck of new\\nrandom matrix theory (RMT) developments. To this end, in this paper, we\\nformally introduce a class of generalized (and regularized) Laplacian matrices,\\nwhich contains the Laplacian matrix and the random adjacency matrix as a\\nspecific case, and suggest the new framework of the asymptotic theory of\\neigenvectors for latent embeddings with generalized Laplacian matrices\\n(ATE-GL). Our new theory is empowered by the tool of generalized quadratic\\nvector equation for dealing with RMT under dependency, and delicate high-order\\nasymptotic expansions of the empirical spiked eigenvectors and eigenvalues\\nbased on local laws. The asymptotic normalities established for both spiked\\neigenvectors and eigenvalues will enable us to conduct precise inference and\\nuncertainty quantification for applications involving the generalized Laplacian\\nmatrices with flexibility. We discuss some applications of the suggested ATE-GL\\nframework and showcase its validity through some numerical examples.',\n",
       "  'keywords': None},\n",
       " {'title': 'Semi-Parametric Batched Global Multi-Armed Bandits with Covariates',\n",
       "  'authors': ['Sakshi Arya', 'Hyebin Song'],\n",
       "  'summary': 'The multi-armed bandits (MAB) framework is a widely used approach for\\nsequential decision-making, where a decision-maker selects an arm in each round\\nwith the goal of maximizing long-term rewards. Moreover, in many practical\\napplications, such as personalized medicine and recommendation systems,\\nfeedback is provided in batches, contextual information is available at the\\ntime of decision-making, and rewards from different arms are related rather\\nthan independent. We propose a novel semi-parametric framework for batched\\nbandits with covariates and a shared parameter across arms, leveraging the\\nsingle-index regression (SIR) model to capture relationships between arm\\nrewards while balancing interpretability and flexibility. Our algorithm,\\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\\nemploys a batched successive arm elimination strategy with a dynamic binning\\nmechanism guided by the single-index direction. We consider two settings: one\\nwhere a pilot direction is available and another where the direction is\\nestimated from data, deriving theoretical regret bounds for both cases. When a\\npilot direction is available with sufficient accuracy, our approach achieves\\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\\ncircumventing the curse of dimensionality. Extensive experiments on simulated\\nand real-world datasets demonstrate the effectiveness of our algorithm compared\\nto the nonparametric batched bandit method introduced by\\n\\\\cite{jiang2024batched}.',\n",
       "  'keywords': ['single-index regression',\n",
       "   'contextual bandits',\n",
       "   'batches',\n",
       "   'dynamic binning',\n",
       "   'regret']},\n",
       " {'title': 'Geometric Ergodicity of a Gibbs Algorithm for a Normal Model With a\\n  Horseshoe Prior',\n",
       "  'authors': ['Yasuyuki Hamura'],\n",
       "  'summary': 'In this paper, we consider a two-stage Gibbs sampler for a normal linear\\nregression model with a horseshoe prior. Under some assumptions, we show that\\nit produces a geometrically ergodic Markov chain. In particular, we prove\\ngeometric ergodicity under some three-parameter beta global prior which does\\nnot have a finite $(p / 5)$-th negative moment, where $p$ is the number of\\nregression coefficients. This is in contrast to the case of a known general\\nresult which is applicable if the global parameter has a finite approximately\\n$(p / 2)$-th negative moment.',\n",
       "  'keywords': None},\n",
       " {'title': 'Uniform Limit Theory for Network Data',\n",
       "  'authors': ['Yuya Sasaki'],\n",
       "  'summary': 'I present a novel uniform law of large numbers (ULLN) for network-dependent\\ndata. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive\\nsuite of limit theorems and a robust variance estimator for network-dependent\\nprocesses, their analysis focuses on pointwise convergence. On the other hand,\\nuniform convergence is essential for nonlinear estimators such as M and GMM\\nestimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I\\nestablish the ULLN under network dependence and demonstrate its utility by\\nproving the consistency of both M and GMM estimators. A byproduct of this work\\nis a novel maximal inequality for network data, which may prove useful for\\nfuture research beyond the scope of this paper.',\n",
       "  'keywords': ['GMM estimation',\n",
       "   'M estimation',\n",
       "   'maximal inequality',\n",
       "   'network',\n",
       "   'uniform']},\n",
       " {'title': 'A Few Observations on Sample-Conditional Coverage in Conformal\\n  Prediction',\n",
       "  'authors': ['John C. Duchi'],\n",
       "  'summary': \"We revisit the problem of constructing predictive confidence sets for which\\nwe wish to obtain some type of conditional validity. We provide new arguments\\nshowing how ``split conformal'' methods achieve near desired coverage levels\\nwith high probability, a guarantee conditional on the validation data rather\\nthan marginal over it. In addition, we directly consider (approximate)\\nconditional coverage, where, e.g., conditional on a covariate $X$ belonging to\\nsome group of interest, we would like a guarantee that a predictive set covers\\nthe true outcome $Y$. We show that the natural method of performing quantile\\nregression on a held-out (validation) dataset yields minimax optimal guarantees\\nof coverage here. Complementing these positive results, we also provide\\nexperimental evidence that interesting work remains to be done to develop\\ncomputationally efficient but valid predictive inference methods.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Aspects of a Generalized Theory of Sparsity based Inference in Linear\\n  Inverse Problems',\n",
       "  'authors': [\"Ryan O'Dowd\", 'Raghu G. Raj', 'Hrushikesh N. Mhaskar'],\n",
       "  'summary': 'Linear inverse problems are ubiquitous in various science and engineering\\ndisciplines. Of particular importance in the past few decades, is the\\nincorporation of sparsity based priors, in particular $\\\\ell_1$ priors, into\\nlinear inverse problems, which led to the flowering of fields of compressive\\nsensing (CS) and sparsity based signal processing. More recently, methods based\\non a Compound Gaussian (CG) prior have been investigated and demonstrate\\nimproved results over CS in practice. This paper is the first attempt to\\nidentify and elucidate the fundamental structures underlying the success of CG\\nmethods by studying CG in the context of a broader framework of\\ngeneralized-sparsity-based-inference. After defining our notion of generalized\\nsparsity we introduce a weak null space property and proceed to generalize two\\nwell-known methods in CS, basis pursuit and iteratively reweighted least\\nsquares (IRLS). We show how a subset of CG-induced regularizers fits into this\\nframework.',\n",
       "  'keywords': None},\n",
       " {'title': 'Halfspace Representations of Path Polytopes of Trees',\n",
       "  'authors': ['Amer Goel', 'Aida Maraj', 'Alvaro Ribot'],\n",
       "  'summary': 'Given a tree $T$, its path polytope is the convex hull of the edge indicator\\nvectors for the paths between any two distinct leaves in $T$. These polytopes\\narise naturally in polyhedral geometry and applications, such as phylogenetics,\\ntropical geometry, and algebraic statistics. We provide a minimal halfspace\\nrepresentation of these polytopes. The construction is made inductively using\\ntoric fiber products.',\n",
       "  'keywords': None},\n",
       " {'title': 'Modeling discrete common-shock risks through matrix distributions',\n",
       "  'authors': ['Martin Bladt',\n",
       "   'Eric C. K. Cheung',\n",
       "   'Oscar Peralta',\n",
       "   'Jae-Kyung Woo'],\n",
       "  'summary': 'We introduce a novel class of bivariate common-shock discrete phase-type\\n(CDPH) distributions to describe dependencies in loss modeling, with an\\nemphasis on those induced by common shocks. By constructing two jointly\\nevolving terminating Markov chains that share a common evolution up to a random\\ntime corresponding to the common shock component, and then proceed\\nindependently, we capture the essential features of risk events influenced by\\nshared and individual-specific factors. We derive explicit expressions for the\\njoint distribution of the termination times and prove various class and\\ndistributional properties, facilitating tractable analysis of the risks.\\nExtending this framework, we model random sums where aggregate claims are sums\\nof continuous phase-type random variables with counts determined by these\\ntermination times, and show that their joint distribution belongs to the\\nmultivariate phase-type or matrix-exponential class. We develop estimation\\nprocedures for the CDPH distributions using the expectation-maximization\\nalgorithm and demonstrate the applicability of our models through simulation\\nstudies and an application to bivariate insurance claim frequency data.',\n",
       "  'keywords': ['Phase-type distribution; Discrete bivariate distribution; Common shocks;']},\n",
       " {'title': 'Location Characteristics of Conditional Selective Confidence Intervals\\n  via Polyhedral Methods',\n",
       "  'authors': ['Andreas Dzemski', 'Ryo Okui', 'Wenjie Wang'],\n",
       "  'summary': 'We examine the location characteristics of a conditional selective confidence\\ninterval based on the polyhedral method. This interval is constructed from the\\ndistribution of a test statistic conditional upon the event of statistical\\nsignificance. In the case of a one-sided test, the behavior of the interval\\nvaries depending on whether the parameter is highly significant or only\\nmarginally significant. When the parameter is highly significant, the interval\\nis similar to the usual confidence interval derived without considering\\nselection. However, when the parameter is only marginally significant, the\\ninterval falls into an extreme range and deviates greatly from the estimated\\nvalue of the parameter. In contrast, an interval conditional on two-sided\\nsignificance does not yield extreme results, although it may exclude the\\nestimated parameter value.',\n",
       "  'keywords': ['Selective inference',\n",
       "   'statistical significance',\n",
       "   'confidence inter-']},\n",
       " {'title': 'Minimax Optimal Kernel Two-Sample Tests with Random Features',\n",
       "  'authors': ['Soumya Mukherjee', 'Bharath K. Sriperumbudur'],\n",
       "  'summary': 'Reproducing Kernel Hilbert Space (RKHS) embedding of probability\\ndistributions has proved to be an effective approach, via MMD (maximum mean\\ndiscrepancy) for nonparametric hypothesis testing problems involving\\ndistributions defined over general (non-Euclidean) domains. While a substantial\\namount of work has been done on this topic, only recently, minimax optimal\\ntwo-sample tests have been constructed that incorporate, unlike MMD, both the\\nmean element and a regularized version of the covariance operator. However, as\\nwith most kernel algorithms, the computational complexity of the optimal test\\nscales cubically in the sample size, limiting its applicability. In this paper,\\nwe propose a spectral regularized two-sample test based on random Fourier\\nfeature (RFF) approximation and investigate the trade-offs between statistical\\noptimality and computational efficiency. We show the proposed test to be\\nminimax optimal if the approximation order of RFF (which depends on the\\nsmoothness of the likelihood ratio and the decay rate of the eigenvalues of the\\nintegral operator) is sufficiently large. We develop a practically\\nimplementable permutation-based version of the proposed test with a\\ndata-adaptive strategy for selecting the regularization parameter and the\\nkernel. Finally, through numerical experiments on simulated and benchmark\\ndatasets, we demonstrate that the proposed RFF-based test is computationally\\nefficient and performs almost similar (with a small drop in power) to the exact\\ntest.',\n",
       "  'keywords': None},\n",
       " {'title': 'Characterizing the Training-Conditional Coverage of Full Conformal\\n  Inference in High Dimensions',\n",
       "  'authors': ['Isaac Gibbs', 'Emmanuel J. Candès'],\n",
       "  'summary': 'We study the coverage properties of full conformal regression in the\\nproportional asymptotic regime where the ratio of the dimension and the sample\\nsize converges to a constant. In this setting, existing theory tells us only\\nthat full conformal inference is unbiased, in the sense that its average\\ncoverage lies at the desired level when marginalized over both the new test\\npoint and the training data. Considerably less is known about the behaviour of\\nthese methods conditional on the training set. As a result, the exact benefits\\nof full conformal inference over much simpler alternative methods is unclear.\\nThis paper investigates the behaviour of full conformal inference and natural\\nuncorrected alternatives for a broad class of $L_2$-regularized linear\\nregression models. We show that in the proportional asymptotic regime the\\ntraining-conditional coverage of full conformal inference concentrates at the\\ntarget value. On the other hand, simple alternatives that directly compare test\\nand training residuals realize constant undercoverage bias. While these results\\ndemonstrate the necessity of full conformal in correcting for high-dimensional\\noverfitting, we also show that this same methodology is redundant for the\\nrelated task of tuning the regularization level. In particular, we show that\\nfull conformal inference still yields asymptotically valid coverage when the\\nregularization level is selected using only the training set, without\\nconsideration of the test point. Simulations show that our asymptotic\\napproximations are accurate in finite samples and can be readily extended to\\nother popular full conformal variants, such as full conformal quantile\\nregression and the LASSO, that do not directly meet our assumptions.',\n",
       "  'keywords': None},\n",
       " {'title': 'The Kolmogorov-Smirnov Statistic Revisited',\n",
       "  'authors': ['Elvis Han Cui', 'Yihao Li', 'Zhuang Liu'],\n",
       "  'summary': 'The Kolmogorov-Smirnov (KS) statistic is a classical nonparametric test\\nwidely used for comparing an empirical distribution function with a reference\\ndistribution or for comparing two empirical distributions. Despite its broad\\napplicability in statistical hypothesis testing and model validation, certain\\naspects of the KS statistic remain under-explored among the young generation,\\nparticularly under finite sample conditions. This paper revisits the KS\\nstatistic in both one-sample and two-sample scenarios, considering one-sided\\nand two-sided variants. We derive exact probabilities for the supremum of the\\nempirical process and present a unified treatment of the KS statistic under\\ndiverse settings. Additionally, we explore the discrete nature of the hitting\\ntimes of the normalized empirical process, providing practical insights into\\nthe computation of KS test p-values. The study also discusses the\\nDvoretzky-Kiefer-Wolfowitz-Massart (DKWM) inequality, highlighting its role in\\nconstructing confidence bands for distribution functions. Using empirical\\nprocess theory, we establish the limit distribution of the KS statistic when\\nthe true distribution includes unknown parameters. Our findings extend existing\\nresults, offering improved methodologies for statistical analysis and\\nhypothesis testing using the KS statistic, particularly in finite sample\\nscenarios.',\n",
       "  'keywords': None},\n",
       " {'title': 'Robust statistical inference for accelerated life-tests with one-shot\\n  devices under log-logistic distributions',\n",
       "  'authors': ['María González-Calderón', 'María Jaenada', 'Leandro Pardo'],\n",
       "  'summary': 'A one-shot device is a unit that operates only once, after which it is either\\ndestroyed or needs to be rebuilt. For this type of device, the operational\\nstatus can only be assessed at a specific inspection time, determining whether\\nfailure occurred before or after it. Consequently, lifetimes are subject to\\nleft- or right-censoring. One-shot devices are usually highly reliables. To\\nanalyze the reliability of such products, an accelerated life test (ALT) plan\\nis typically employed by subjecting the devices to increased levels of stress\\nfactors, thus allowing life characteristics observed under high-stress\\nconditions to be extrapolated to normal operating conditions. By accelerating\\nthe degradation process, ALT significantly reduces both the time required for\\ntesting and the associated experimental costs.\\n  Recently, robust inferential methods have gained considerable interest in\\nstatistical analysis. Among them, weighted minimum density power divergence\\nestimators (WMDPDEs) are widely recognized for their robust statistical\\nproperties with small loss of efficiency. In this work, robust WMDPDE and\\nassociated statistical tests are developed under a log-logistic lifetime\\ndistribution with multiple stresses. Explicit expressions for the estimating\\nequations and asymptotic distribution of the estimators are obtained. Further,\\na Monte Carlo simulation study is presented to evaluate the performance of the\\nWMDPDE in practical applications.',\n",
       "  'keywords': None},\n",
       " {'title': 'Minimax rate for learning kernels in operators',\n",
       "  'authors': ['Sichong Zhang', 'Xiong Wang', 'Fei Lu'],\n",
       "  'summary': 'Learning kernels in operators from data lies at the intersection of inverse\\nproblems and statistical learning, offering a powerful framework for capturing\\nnonlocal dependency in function spaces and high-dimensional settings. In\\ncontrast to classical nonparametric regression, where the inverse problem is\\nwell-posed, kernel estimation involves a compact normal operator and an\\nill-posed deconvolution. To address these challenges, we introduce adaptive\\nspectral Sobolev spaces, unifying Sobolev spaces and reproducing kernel Hilbert\\nspaces, that automatically discard non-identifiable components and control\\nterms with small eigenvalues. Within this framework, we establish the minimax\\nconvergence rates for the mean squared error under both polynomial and\\nexponential spectral decay regimes. Methodologically, we develop a tamed least\\nsquares estimator achieving the minimax upper rates via controlling the\\nleft-tail probability for eigenvalues of the random normal matrix; and for the\\nminimax lower rates, we resolve challenges from infinite-dimensional measures\\nthrough their projections.',\n",
       "  'keywords': None},\n",
       " {'title': 'Linear type conditional specifications for multivariate count variables',\n",
       "  'authors': ['Yang Lu', 'Wei Sun'],\n",
       "  'summary': 'This paper investigates conditional specifications for multivariate count\\nvariables. Recently, the spatial count data literature has proposed several\\nconditional models such that the conditional expectations are linear in the\\nconditioning variables. These models are much easier to estimate than existing\\nspatial count models based on Gaussian random field. However, whether or not\\nsuch conditional specifications are compatible have not been addressed. We\\ninvestigate two large families of conditional models, that are the compound\\nautoregressive model and the random coefficient integer autoregressive model.\\nWe characterize all the solutions to these two families of models at arbitrary\\ndimensions, and find that only a handful of them admit non-trivial solutions.\\nWe then show that if we focus on the linearity condition of the conditional\\nexpectations only, a considerable larger family of solutions can be obtained.\\nThis suggests that for spatial count data modeling, semi-parametric type\\nspecifications that impose the conditional expectation structure is preferable.',\n",
       "  'keywords': ['Random coefficient count model',\n",
       "   'compound autoregressive model',\n",
       "   'conjugate']},\n",
       " {'title': 'On the Glivenko-Cantelli theorem for real-valued empirical functions of\\n  stationary $α$-mixing and $β$-mixing sequences',\n",
       "  'authors': ['Ousmane Coulibaly', 'Harouna Sangaré'],\n",
       "  'summary': 'In this paper we extend the classical Glivenko-Cantelli theorem to\\nreal-valued empirical functions under dependence structures characterised by\\n$\\\\alpha$-mixing and $\\\\beta$-mixing conditions. We investigate sufficient\\nconditions ensuring that families of real-valued functions exhibit the\\nGlivenko-Cantelli (GC) property in these dependence settings. Our analysis\\nfocuses on function classes satisfying uniform entropy conditions and\\nestablishes deviation bounds under mixing coefficients that decay at\\nappropriate rates. Our results refine the existing literature by relaxing the\\nindependence assumptions and highlighting the role of dependence in empirical\\nprocess convergence.',\n",
       "  'keywords': None},\n",
       " {'title': \"Stein's unbiased risk estimate and Hyvärinen's score matching\",\n",
       "  'authors': ['Sulagna Ghosh',\n",
       "   'Nikolaos Ignatiadis',\n",
       "   'Frederic Koehler',\n",
       "   'Amber Lee'],\n",
       "  'summary': 'We study two G-modeling strategies for estimating the signal distribution\\n(the empirical Bayesian\\'s prior) from observations corrupted with normal noise.\\nFirst, we choose the signal distribution by minimizing Stein\\'s unbiased risk\\nestimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach\\nmotivated by optimal empirical Bayesian shrinkage estimation of the signals.\\nSecond, we select the signal distribution by minimizing Hyv\\\\\"arinen\\'s score\\nmatching objective for the implied score (derivative of log-marginal density),\\ntargeting minimal Fisher divergence between estimated and true marginal\\ndensities. While these strategies appear distinct, they are known to be\\nmathematically equivalent. We provide a unified analysis of SURE and score\\nmatching under both well-specified signal distribution classes and\\nmisspecification. In the classical well-specified setting with homoscedastic\\nnoise and compactly supported signal distribution, we establish nearly\\nparametric rates of convergence of the empirical Bayes regret and the Fisher\\ndivergence. In a commonly studied misspecified model, we establish fast rates\\nof convergence to the oracle denoiser and corresponding oracle inequalities.\\nOur empirical results demonstrate competitiveness with nonparametric maximum\\nlikelihood in well-specified settings, while showing superior performance under\\nmisspecification, particularly in settings involving heteroscedasticity and\\nside information.',\n",
       "  'keywords': None},\n",
       " {'title': 'Modeling Extreme Events in the Presence of Inlier: A Mixture Approach',\n",
       "  'authors': ['Shivshankar Nila', 'Ishapathik Das', 'N. Balakrishna'],\n",
       "  'summary': \"In many random phenomena, such as life-testing experiments and environmental\\ndata (like rainfall data), there are often positive values and an excess of\\nzeros, which create modeling challenges. In life testing, immediate failures\\nresult in zero lifetimes, often due to defects or poor quality, especially in\\nelectronics and clinical trials. These failures, called zero inliers, are\\ndifficult to model using standard approaches. When studying extreme values in\\nthe above scenarios, a key issue is selecting an appropriate threshold for\\naccurate tail approximation of the population using asymptotic models. While\\nsome extreme value mixture models address threshold estimation and tail\\napproximation, conventional parametric and non-parametric bulk and generalised\\nPareto distribution (GPD) approaches often neglect inliers, leading to\\nsuboptimal results. This paper introduces a framework for modeling extreme\\nevents and inliers using the GPD, addressing threshold uncertainty and\\neffectively capturing inliers at zero. The model's parameters are estimated\\nusing the maximum likelihood estimation (MLE) method, ensuring optimal\\nprecision. Through simulation studies and real-world applications, we\\ndemonstrate that the proposed model significantly outperforms the traditional\\nmethods, which typically neglect inliers at the origin.\",\n",
       "  'keywords': ['Extreme value theory',\n",
       "   'Extreme value mixture model',\n",
       "   'Inliers',\n",
       "   'Threshold estimation.']},\n",
       " {'title': 'Set and functional prediction: randomness, exchangeability, and\\n  conformal',\n",
       "  'authors': ['Vladimir Vovk'],\n",
       "  'summary': 'This paper continues the study of the efficiency of conformal prediction as\\ncompared with more general randomness prediction and exchangeability\\nprediction. It does not restrict itself to the case of classification, and our\\nresults will also be applicable to the case of regression. The price to pay is\\nthat efficiency will be attained only on average, albeit with respect to a wide\\nrange of probability measures on the label space.',\n",
       "  'keywords': None},\n",
       " {'title': 'A Matsuoka-Based GARMA Model for Hydrological Forecasting: Theory,\\n  Estimation, and Applications',\n",
       "  'authors': ['Guilherme Pumi',\n",
       "   'Danilo Hiroshi Matsuoka',\n",
       "   'Taiane Schaedler Prass',\n",
       "   'Bruna Gregory Palm'],\n",
       "  'summary': 'Time series in natural sciences, such as hydrology and climatology, and other\\nenvironmental applications, often consist of continuous observations\\nconstrained to the unit interval (0,1). Traditional Gaussian-based models fail\\nto capture these bounds, requiring more flexible approaches. This paper\\nintroduces the Matsuoka Autoregressive Moving Average (MARMA) model, extending\\nthe GARMA framework by assuming a Matsuoka-distributed random component taking\\nvalues in (0,1) and an ARMA-like systematic structure allowing for random\\ntime-dependent covariates. Parameter estimation is performed via partial\\nmaximum likelihood (PMLE), for which we present the asymptotic theory. It\\nenables statistical inference, including confidence intervals and model\\nselection. To construct prediction intervals, we propose a novel\\nbootstrap-based method that accounts for dependence structure uncertainty. A\\ncomprehensive Monte Carlo simulation study assesses the finite sample\\nperformance of the proposed methodologies, while an application to forecasting\\nthe useful water volume of the Guarapiranga Reservoir in Brazil showcases their\\npractical usefulness.',\n",
       "  'keywords': ['time series analysis',\n",
       "   'regression models',\n",
       "   'partial maximum likelihood',\n",
       "   'non-']},\n",
       " {'title': 'Kernel Estimation for Nonlinear Dynamics',\n",
       "  'authors': ['Marie-Christine Düker', 'Adam Waterbury'],\n",
       "  'summary': 'Many scientific problems involve data exhibiting both temporal and\\ncross-sectional dependencies. While linear dependencies have been extensively\\nstudied, the theoretical analysis of regression estimators under nonlinear\\ndependencies remains scarce. This work studies a kernel-based estimation\\nprocedure for nonlinear dynamics within the reproducing kernel Hilbert space\\nframework, focusing on nonlinear vector autoregressive models. We derive\\nnonasymptotic probabilistic bounds on the deviation between a regularized\\nkernel estimator and the nonlinear regression function. A key technical\\ncontribution is a concentration bound for quadratic forms of stochastic\\nmatrices in the presence of dependent data, which is of independent interest.\\nAdditionally, we characterize conditions on multivariate kernels that guarantee\\noptimal convergence rates.',\n",
       "  'keywords': ['kernel regression',\n",
       "   'reproducing kernel Hilbert space',\n",
       "   'method of regularization',\n",
       "   'nonlinear time series',\n",
       "   '']},\n",
       " {'title': 'Learning sparse generalized linear models with binary outcomes via\\n  iterative hard thresholding',\n",
       "  'authors': ['Namiko Matsumoto', 'Arya Mazumdar'],\n",
       "  'summary': \"In statistics, generalized linear models (GLMs) are widely used for modeling\\ndata and can expressively capture potential nonlinear dependence of the model's\\noutcomes on its covariates. Within the broad family of GLMs, those with binary\\noutcomes, which include logistic and probit regressions, are motivated by\\ncommon tasks such as binary classification with (possibly) non-separable data.\\nIn addition, in modern machine learning and statistics, data is often\\nhigh-dimensional yet has a low intrinsic dimension, making sparsity constraints\\nin models another reasonable consideration. In this work, we propose to use and\\nanalyze an iterative hard thresholding (projected gradient descent on the ReLU\\nloss) algorithm, called binary iterative hard thresholding (BIHT), for\\nparameter estimation in sparse GLMs with binary outcomes. We establish that\\nBIHT is statistically efficient and converges to the correct solution for\\nparameter estimation in a general class of sparse binary GLMs. Unlike many\\nother methods for learning GLMs, including maximum likelihood estimation,\\ngeneralized approximate message passing, and GLM-tron (Kakade et al. 2011;\\nBahmani et al. 2016), BIHT does not require knowledge of the GLM's link\\nfunction, offering flexibility and generality in allowing the algorithm to\\nlearn arbitrary binary GLMs. As two applications, logistic and probit\\nregression are additionally studied. In this regard, it is shown that in\\nlogistic regression, the algorithm is in fact statistically optimal in the\\nsense that the order-wise sample complexity matches (up to logarithmic factors)\\nthe lower bound obtained previously. To the best of our knowledge, this is the\\nfirst work achieving statistical optimality for logistic regression in all\\nnoise regimes with a computationally efficient algorithm. Moreover, for probit\\nregression, our sample complexity is on the same order as that obtained for\\nlogistic regression.\",\n",
       "  'keywords': None},\n",
       " {'title': 'Testing Thresholds and Spectral Properties of High-Dimensional Random\\n  Toroidal Graphs via Edgeworth-Style Expansions',\n",
       "  'authors': ['Samuel Baguley',\n",
       "   'Andreas Göbel',\n",
       "   'Marcus Pappik',\n",
       "   'Leon Schiller'],\n",
       "  'summary': \"We study high-dimensional random geometric graphs (RGGs) of edge-density $p$\\nwith vertices uniformly distributed on the $d$-dimensional torus and edges\\ninserted between sufficiently close vertices with respect to an $L_q$-norm. We\\nfocus on distinguishing an RGG from an Erd\\\\H{o}s--R\\\\'enyi (ER) graph if both\\nmodels have edge probability $p$. So far, most results considered either\\nspherical RGGs with $L_2$-distance or toroidal RGGs under $L_\\\\infty$-distance.\\nHowever, for general $L_q$-distances, many questions remain open, especially if\\n$p$ is allowed to depend on $n$. The main reason for this is that RGGs under\\n$L_q$-distances can not easily be represented as the logical AND of their\\n1-dimensional counterparts, as for $L_\\\\infty$ geometries. To overcome this, we\\ndevise a novel technique for quantifying the dependence between edges based on\\nmodified Edgeworth expansions.\\n  Our technique yields the first tight algorithmic upper bounds for\\ndistinguishing toroidal RGGs under general $L_q$ norms from ER-graphs for fixed\\n$p$ and $q$. We achieve this by showing that signed triangles can distinguish\\nthe two models when $d\\\\ll n^3p^3$ for the whole regime of $c/n<p<1$.\\nAdditionally, our technique yields an improved information-theoretic lower\\nbound for this task, showing that the two distributions converge whenever\\n$d=\\\\tilde{\\\\Omega}(n^3p^2)$, which is just as strong as the currently best known\\nlower bound for spherical RGGs in case of general $p$ from Liu et al.\\n[STOC'22]. Finally, our expansions allow us to tightly characterize the\\nspectral properties of toroidal RGGs both under $L_q$-distances for fixed $1\\\\le\\nq<\\\\infty$, and $L_\\\\infty$-distance. Our results partially resolve a conjecture\\nof Bangachev and Bresler [COLT'24] and prove that the distance metric, rather\\nthan the underlying space, is responsible for the observed differences in the\\nbehavior of spherical and toroidal RGGs.\",\n",
       "  'keywords': None},\n",
       " {'title': 'A Unified Bayesian Perspective for Conventional and Robust Adaptive\\n  Filters',\n",
       "  'authors': ['Leszek Szczecinski', 'Jacob Benesty', 'Eduardo Vinicius Kuhn'],\n",
       "  'summary': 'In this work, we present a new perspective on the origin and interpretation\\nof adaptive filters. By applying Bayesian principles of recursive inference\\nfrom the state-space model and using a series of simplifications regarding the\\nstructure of the solution, we can present, in a unified framework, derivations\\nof many adaptive filters which depend on the probabilistic model of the\\nobservational noise. In particular, under a Gaussian model, we obtain solutions\\nwell-known in the literature (such as LMS, NLMS, or Kalman filter), while using\\nnon-Gaussian noise, we obtain new families of adaptive filter. Notably, under\\nassumption of Laplacian noise, we obtain a family of robust filters of which\\nthe signed-error algorithm is a well-known member, while other algorithms,\\nderived effortlessly in the proposed framework, are entirely new. Numerical\\nexamples are shown to illustrate the properties and provide a better insight\\ninto the performance of the derived adaptive filters.',\n",
       "  'keywords': ['Adaptive filters',\n",
       "   'Robust adaptive filters',\n",
       "   'LMS',\n",
       "   'NLMS',\n",
       "   'Kalman filter',\n",
       "   'Bayesian']},\n",
       " {'title': 'Sequential Outlier Detection in Non-Stationary Time Series',\n",
       "  'authors': ['Florian Heinrichs', 'Patrick Bastian', 'Holger Dette'],\n",
       "  'summary': \"A novel method for sequential outlier detection in non-stationary time series\\nis proposed. The method tests the null hypothesis of ``no outlier'' at each\\ntime point, addressing the multiple testing problem by bounding the error\\nprobability of successive tests, using extreme value theory. The asymptotic\\nproperties of the test statistic are studied under the null hypothesis and\\nalternative. The finite sample properties of the new detection scheme are\\ninvestigated by means of a simulation study, and the method is compared with\\nalternative procedures which have recently been proposed in the statistics and\\nmachine learning literature.\",\n",
       "  'keywords': ['Outlier detection',\n",
       "   'Non-stationary time series',\n",
       "   'Local linear regression',\n",
       "   '']},\n",
       " {'title': 'Generating Correlation Matrices with Graph Structures Using Convex\\n  Optimization',\n",
       "  'authors': ['Ali Fakhar', 'Kévin Polisano', 'Irène Gannaz', 'Sophie Achard'],\n",
       "  'summary': 'This work deals with the generation of theoretical correlation matrices with\\nspecific sparsity patterns, associated to graph structures. We present a novel\\napproach based on convex optimization, offering greater flexibility compared to\\nexisting techniques, notably by controlling the mean of the entry distribution\\nin the generated correlation matrices. This allows for the generation of\\ncorrelation matrices that better represent realistic data and can be used to\\nbenchmark statistical methods for graph inference.',\n",
       "  'keywords': None},\n",
       " {'title': 'Certified Decisions',\n",
       "  'authors': ['Isaiah Andrews', 'Jiafeng Chen'],\n",
       "  'summary': 'Hypothesis tests and confidence intervals are ubiquitous in empirical\\nresearch, yet their connection to subsequent decision-making is often unclear.\\nWe develop a theory of certified decisions that pairs recommended decisions\\nwith inferential guarantees. Specifically, we attach P-certificates -- upper\\nbounds on loss that hold with probability at least $1-\\\\alpha$ -- to recommended\\nactions. We show that such certificates allow \"safe,\" risk-controlling adoption\\ndecisions for ambiguity-averse downstream decision-makers. We further prove\\nthat it is without loss to limit attention to P-certificates arising as minimax\\ndecisions over confidence sets, or what Manski (2021) terms \"as-if decisions\\nwith a set estimate.\" A parallel argument applies to E-certified decisions\\nobtained from e-values in settings with unbounded loss.',\n",
       "  'keywords': None},\n",
       " {'title': 'Conformal Prediction Under Generalized Covariate Shift with Posterior\\n  Drift',\n",
       "  'authors': ['Baozhen Wang', 'Xingye Qiao'],\n",
       "  'summary': 'In many real applications of statistical learning, collecting sufficiently\\nmany training data is often expensive, time-consuming, or even unrealistic. In\\nthis case, a transfer learning approach, which aims to leverage knowledge from\\na related source domain to improve the learning performance in the target\\ndomain, is more beneficial. There have been many transfer learning methods\\ndeveloped under various distributional assumptions. In this article, we study a\\nparticular type of classification problem, called conformal prediction, under a\\nnew distributional assumption for transfer learning. Classifiers under the\\nconformal prediction framework predict a set of plausible labels instead of one\\nsingle label for each data instance, affording a more cautious and safer\\ndecision. We consider a generalization of the \\\\textit{covariate shift with\\nposterior drift} setting for transfer learning. Under this setting, we propose\\na weighted conformal classifier that leverages both the source and target\\nsamples, with a coverage guarantee in the target domain. Theoretical studies\\ndemonstrate favorable asymptotic properties. Numerical studies further\\nillustrate the usefulness of the proposed method.',\n",
       "  'keywords': None},\n",
       " {'title': 'A Unified Framework for Semiparametrically Efficient Semi-Supervised\\n  Learning',\n",
       "  'authors': ['Zichun Xu', 'Daniela Witten', 'Ali Shojaie'],\n",
       "  'summary': 'We consider statistical inference under a semi-supervised setting where we\\nhave access to both a labeled dataset consisting of pairs $\\\\{X_i, Y_i\\n\\\\}_{i=1}^n$ and an unlabeled dataset $\\\\{ X_i \\\\}_{i=n+1}^{n+N}$. We ask the\\nquestion: under what circumstances, and by how much, can incorporating the\\nunlabeled dataset improve upon inference using the labeled data? To answer this\\nquestion, we investigate semi-supervised learning through the lens of\\nsemiparametric efficiency theory. We characterize the efficiency lower bound\\nunder the semi-supervised setting for an arbitrary inferential problem, and\\nshow that incorporating unlabeled data can potentially improve efficiency if\\nthe parameter is not well-specified. We then propose two types of\\nsemi-supervised estimators: a safe estimator that imposes minimal assumptions,\\nis simple to compute, and is guaranteed to be at least as efficient as the\\ninitial supervised estimator; and an efficient estimator, which -- under\\nstronger assumptions -- achieves the semiparametric efficiency bound. Our\\nfindings unify existing semiparametric efficiency results for particular\\nspecial cases, and extend these results to a much more general class of\\nproblems. Moreover, we show that our estimators can flexibly incorporate\\npredicted outcomes arising from ``black-box\" machine learning models, and\\nthereby achieve the same goal as prediction-powered inference (PPI), but with\\nsuperior theoretical guarantees. We also provide a complete understanding of\\nthe theoretical basis for the existing set of PPI methods. Finally, we apply\\nthe theoretical framework developed to derive and analyze efficient\\nsemi-supervised estimators in a number of settings, including M-estimation,\\nU-statistics, and average treatment effect estimation, and demonstrate the\\nperformance of the proposed estimators via simulations.',\n",
       "  'keywords': None},\n",
       " {'title': 'Learning Density Evolution from Snapshot Data',\n",
       "  'authors': ['Rentian Yao', 'Atsushi Nitanda', 'Xiaohui Chen', 'Yun Yang'],\n",
       "  'summary': 'Motivated by learning dynamical structures from static snapshot data, this\\npaper presents a distribution-on-scalar regression approach for estimating the\\ndensity evolution of a stochastic process from its noisy temporal point clouds.\\nWe propose an entropy-regularized nonparametric maximum likelihood estimator\\n(E-NPMLE), which leverages the entropic optimal transport as a smoothing\\nregularizer for the density flow. We show that the E-NPMLE has almost\\ndimension-free statistical rates of convergence to the ground truth\\ndistributions, which exhibit a striking phase transition phenomenon in terms of\\nthe number of snapshots and per-snapshot sample size. To efficiently compute\\nthe E-NPMLE, we design a novel particle-based and grid-free coordinate KL\\ndivergence gradient descent (CKLGD) algorithm and prove its polynomial\\niteration complexity. Moreover, we provide numerical evidence on synthetic data\\nto support our theoretical findings. This work contributes to the theoretical\\nunderstanding and practical computation of estimating density evolution from\\nnoisy observations in arbitrary dimensions.',\n",
       "  'keywords': None},\n",
       " {'title': 'Optimal Recovery Meets Minimax Estimation',\n",
       "  'authors': ['Ronald DeVore',\n",
       "   'Robert D. Nowak',\n",
       "   'Rahul Parhi',\n",
       "   'Guergana Petrova',\n",
       "   'Jonathan W. Siegel'],\n",
       "  'summary': 'A fundamental problem in statistics and machine learning is to estimate a\\nfunction $f$ from possibly noisy observations of its point samples. The goal is\\nto design a numerical algorithm to construct an approximation $\\\\hat f$ to $f$\\nin a prescribed norm that asymptotically achieves the best possible error (as a\\nfunction of the number $m$ of observations and the variance $\\\\sigma^2$ of the\\nnoise). This problem has received considerable attention in both nonparametric\\nstatistics (noisy observations) and optimal recovery (noiseless observations).\\nQuantitative bounds require assumptions on $f$, known as model class\\nassumptions. Classical results assume that $f$ is in the unit ball of a Besov\\nspace. In nonparametric statistics, the best possible performance of an\\nalgorithm for finding $\\\\hat f$ is known as the minimax rate and has been\\nstudied in this setting under the assumption that the noise is Gaussian. In\\noptimal recovery, the best possible performance of an algorithm is known as the\\noptimal recovery rate and has also been determined in this setting. While one\\nwould expect that the minimax rate recovers the optimal recovery rate when the\\nnoise level $\\\\sigma$ tends to zero, it turns out that the current results on\\nminimax rates do not carefully determine the dependence on $\\\\sigma$ and the\\nlimit cannot be taken. This paper handles this issue and determines the\\nnoise-level-aware (NLA) minimax rates for Besov classes when error is measured\\nin an $L_q$-norm with matching upper and lower bounds. The end result is a\\nreconciliation between minimax rates and optimal recovery rates. The NLA\\nminimax rate continuously depends on the noise level and recovers the optimal\\nrecovery rate when $\\\\sigma$ tends to zero.',\n",
       "  'keywords': None},\n",
       " {'title': 'Stronger Neyman Regret Guarantees for Adaptive Experimental Design',\n",
       "  'authors': ['Georgy Noarov',\n",
       "   'Riccardo Fogliato',\n",
       "   'Martin Bertran',\n",
       "   'Aaron Roth'],\n",
       "  'summary': 'We study the design of adaptive, sequential experiments for unbiased average\\ntreatment effect (ATE) estimation in the design-based potential outcomes\\nsetting. Our goal is to develop adaptive designs offering sublinear Neyman\\nregret, meaning their efficiency must approach that of the hindsight-optimal\\nnonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first\\nmethod achieving $\\\\widetilde{O}(\\\\sqrt{T})$ expected Neyman regret under mild\\nconditions. In this work, we propose adaptive designs with substantially\\nstronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain\\nanytime $\\\\widetilde{O}(\\\\log T)$ Neyman regret under natural boundedness\\nassumptions. Further, in the setting where experimental units have\\npre-treatment covariates, we introduce and study a class of contextual\\n\"multigroup\" Neyman regret guarantees: Given any set of possibly overlapping\\ngroups based on the covariates, the adaptive design outperforms each group\\'s\\nbest non-adaptive designs. In particular, we develop a contextual adaptive\\ndesign with $\\\\widetilde{O}(\\\\sqrt{T})$ anytime multigroup Neyman regret. We\\nempirically validate the proposed designs through an array of experiments.',\n",
       "  'keywords': None},\n",
       " {'title': 'Invariance principle for the Gaussian Multiplicative Chaos via a high\\n  dimensional CLT with low rank increments',\n",
       "  'authors': ['Mriganka Basu Roy Chowdhury', 'Shirshendu Ganguly'],\n",
       "  'summary': 'Gaussian multiplicative chaos (GMC) is a canonical random fractal measure\\nobtained by exponentiating log-correlated Gaussian processes, first constructed\\nin the seminal work of Kahane (1985). Since then it has served as an important\\nbuilding block in constructions of quantum field theories and Liouville quantum\\ngravity. However, in many natural settings, non-Gaussian log-correlated\\nprocesses arise. In this paper, we investigate the universality of GMC through\\nan invariance principle. We consider the model of a random Fourier series, a\\nprocess known to be log-correlated. While the Gaussian Fourier series has been\\na classical object of study, recently, the non-Gaussian counterpart was\\ninvestigated and the associated multiplicative chaos constructed by Junnila in\\n2016. We show that the Gaussian and non-Gaussian variables can be coupled so\\nthat the associated chaos measures are almost surely mutually absolutely\\ncontinuous throughout the entire sub-critical regime. This solves the main open\\nproblem from Kim and Kriechbaum (2024) who had earlier established such a\\nresult for a part of the regime. The main ingredient is a new high dimensional\\nCLT for a sum of independent (but not i.i.d.) random vectors belonging to rank\\none subspaces with error bounds involving the isotropic properties of the\\ncovariance matrix of the sum, which we expect will find other applications. The\\nproof relies on a path-wise analysis of Skorokhod embeddings as well as a\\nperturbative result about square roots of positive semi-definite matrices\\nwhich, surprisingly, appears to be new.',\n",
       "  'keywords': None},\n",
       " {'title': 'On a class of high dimensional linear regression methods with debiasing\\n  and thresholding',\n",
       "  'authors': ['Ying-Ao Wang', 'Yunyi Zhang', 'Ye Zhang'],\n",
       "  'summary': 'In this paper, we introduce a unified framework, inspired by classical\\nregularization theory, for designing and analyzing a broad class of linear\\nregression approaches. Our framework encompasses traditional methods like least\\nsquares regression and Ridge regression, as well as innovative techniques,\\nincluding seven novel regression methods such as Landweber and Showalter\\nregressions. Within this framework, we further propose a class of debiased and\\nthresholded regression methods to promote feature selection, particularly in\\nterms of sparsity. These methods may offer advantages over conventional\\nregression techniques, including Lasso, due to their ease of computation via a\\nclosed-form expression. Theoretically, we establish consistency results and\\nGaussian approximation theorems for this new class of regularization methods.\\nExtensive numerical simulations further demonstrate that the debiased and\\nthresholded counterparts of linear regression methods exhibit favorable finite\\nsample performance and may be preferable in certain settings.',\n",
       "  'keywords': ['Linearregression',\n",
       "   'regularization',\n",
       "   'consistency',\n",
       "   'Gaussianapproximation',\n",
       "   'spar-']},\n",
       " {'title': 'On the admissibility of bounds on the mean of discrete, scalar\\n  probability distributions from an iid sample',\n",
       "  'authors': ['Erik Learned-Miller'],\n",
       "  'summary': 'We address the problem of producing a lower bound for the mean of a discrete\\nprobability distribution, with known support over a finite set of real numbers,\\nfrom an iid sample of that distribution. Up to a constant, this is equivalent\\nto bounding the mean of a multinomial distribution (with known support) from a\\nsample of that distribution. Our main contribution is to characterize the\\ncomplete set of admissible bound functions for any sample space, and to show\\nthat certain previously published bounds are admissible. We prove that the\\nsolution to each one of a set of simple-to-state optimization problems yields\\nsuch an admissible bound. Single examples of such bounds, such as the trinomial\\nbound by Miratrix and Stark [2009] have been previously published, but without\\nan analysis of admissibility, and without a discussion of the full set of\\nalternative admissible bounds. In addition to a variety of results about\\nadmissible bounds, we prove the non-existence of optimal bounds for sample\\nspaces with supports of size greater than 1 and samples sizes greater than 1.',\n",
       "  'keywords': None}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a20e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего найдено статей с ключевыми словами: 90\n"
     ]
    }
   ],
   "source": [
    "print(f\"Всего найдено статей с ключевыми словами: {sum(1 for p in papers if p['keywords'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ba3eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e985af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52daef72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Rolled Gaussian process models for curves on m...</td>\n",
       "      <td>[Simon Preston, Karthik Bharath, Pablo Lopez-C...</td>\n",
       "      <td>Given a planar curve, imagine rolling a sphere...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Locally minimax optimal and dimension-agnostic...</td>\n",
       "      <td>[Ilmun Kim, Aaditya Ramdas]</td>\n",
       "      <td>We revisit the discrete argmin inference probl...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Power comparison of sequential testing by bett...</td>\n",
       "      <td>[Amaury Durand, Olivier Wintenberger]</td>\n",
       "      <td>In this paper, we derive power guarantees of s...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Testing Thresholds and Spectral Properties of ...</td>\n",
       "      <td>[Samuel Baguley, Andreas Göbel, Marcus Pappik,...</td>\n",
       "      <td>We study high-dimensional random geometric gra...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Debiasing Continuous-time Nonlinear Autoregres...</td>\n",
       "      <td>[Simon Kuang, Xinfan Lin]</td>\n",
       "      <td>We study how to identify a class of continuous...</td>\n",
       "      <td>[systemidentification]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "70   Rolled Gaussian process models for curves on m...   \n",
       "72   Locally minimax optimal and dimension-agnostic...   \n",
       "46   Power comparison of sequential testing by bett...   \n",
       "237  Testing Thresholds and Spectral Properties of ...   \n",
       "3    Debiasing Continuous-time Nonlinear Autoregres...   \n",
       "\n",
       "                                               authors  \\\n",
       "70   [Simon Preston, Karthik Bharath, Pablo Lopez-C...   \n",
       "72                         [Ilmun Kim, Aaditya Ramdas]   \n",
       "46               [Amaury Durand, Olivier Wintenberger]   \n",
       "237  [Samuel Baguley, Andreas Göbel, Marcus Pappik,...   \n",
       "3                            [Simon Kuang, Xinfan Lin]   \n",
       "\n",
       "                                               summary                keywords  \n",
       "70   Given a planar curve, imagine rolling a sphere...                    None  \n",
       "72   We revisit the discrete argmin inference probl...                    None  \n",
       "46   In this paper, we derive power guarantees of s...                    None  \n",
       "237  We study high-dimensional random geometric gra...                    None  \n",
       "3    We study how to identify a class of continuous...  [systemidentification]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af70b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(160)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keywords.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079d2047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('data.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb224860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e95574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yake in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (0.4.8)\n",
      "Requirement already satisfied: networkx in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (3.4.2)\n",
      "Requirement already satisfied: matplotlib in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: scipy in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (1.15.1)\n",
      "Requirement already satisfied: seaborn in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: textblob in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (0.19.0)\n",
      "Requirement already satisfied: stanza in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: tabulate in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from yake) (0.9.0)\n",
      "Requirement already satisfied: click>=6.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from yake) (8.1.8)\n",
      "Requirement already satisfied: numpy in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from yake) (2.2.2)\n",
      "Requirement already satisfied: segtok in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from yake) (1.5.11)\n",
      "Requirement already satisfied: jellyfish in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from yake) (1.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: nltk>=3.9 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: emoji in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from stanza) (5.29.3)\n",
      "Requirement already satisfied: requests in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from stanza) (2.6.0)\n",
      "Requirement already satisfied: tqdm in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: joblib in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: filelock in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->stanza) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->stanza) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->stanza) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install yake networkx matplotlib scipy seaborn textblob stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ebdc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from textblob import Word\n",
    "import stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e37ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_extractor = yake.KeywordExtractor(lan=\"en\", n=4, dedupLim=0.85, top=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d3d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    text = text.lower()\n",
    "    keywords = yake_extractor.extract_keywords(text)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "def update_keywords(row):\n",
    "    existing_keywords = set(row[\"keywords\"]) if isinstance(row[\"keywords\"], list) else set()\n",
    "    new_keywords = set(extract_keywords(row[\"title\"] + \" \" + row[\"summary\"]))\n",
    "    \n",
    "    updated_keywords = list(existing_keywords | new_keywords) \n",
    "    return updated_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b223181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"keywords\"] = df.apply(update_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3af68c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nonparametric local polynomial regression for ...</td>\n",
       "      <td>[space, polynomial, nonparametric local, hilbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revisiting poverty measures using quantile fun...</td>\n",
       "      <td>[flexible quantile function models, measures i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online Bernstein-von Mises theorem</td>\n",
       "      <td>[sequentially updated posterior, updated poste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Debiasing Continuous-time Nonlinear Autoregres...</td>\n",
       "      <td>[debiasing continuous-time nonlinear, ordinary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Generalized Tangent Approximation Framework ...</td>\n",
       "      <td>[framework for strongly super-gaussian, analys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Nonparametric local polynomial regression for ...   \n",
       "1  Revisiting poverty measures using quantile fun...   \n",
       "2                 Online Bernstein-von Mises theorem   \n",
       "3  Debiasing Continuous-time Nonlinear Autoregres...   \n",
       "4  A Generalized Tangent Approximation Framework ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [space, polynomial, nonparametric local, hilbe...  \n",
       "1  [flexible quantile function models, measures i...  \n",
       "2  [sequentially updated posterior, updated poste...  \n",
       "3  [debiasing continuous-time nonlinear, ordinary...  \n",
       "4  [framework for strongly super-gaussian, analys...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title', 'keywords']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e97123b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flexible quantile function models',\n",
       " 'measures in literature',\n",
       " 'revisiting poverty measures',\n",
       " 'revisiting poverty',\n",
       " 'quantile functions',\n",
       " 'distribution functions',\n",
       " 'measures using quantile functions',\n",
       " 'redefine various poverty measures',\n",
       " 'terms of quantile functions',\n",
       " 'poverty measures in literature',\n",
       " 'poverty',\n",
       " 'measures',\n",
       " 'article we redefine',\n",
       " 'literature in terms',\n",
       " 'flexible quantile function',\n",
       " 'poverty measures',\n",
       " 'quantile function models',\n",
       " 'poverty measurement and analysis',\n",
       " 'prevailing approach',\n",
       " 'poverty measures using quantile']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1].keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e4e8d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee4cf4c3b1440a7aa8234320332d407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:23:28 INFO: Downloaded file to /Users/maksvell/stanza_resources/resources.json\n",
      "2025-04-09 16:23:28 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b47a82480645149a901afd7baa6209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/default.zip:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:24:25 INFO: Downloaded file to /Users/maksvell/stanza_resources/en/default.zip\n",
      "2025-04-09 16:24:26 INFO: Finished downloading models and saved to /Users/maksvell/stanza_resources\n",
      "2025-04-09 16:24:26 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71850733782f4df38678d944eca95511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:24:27 INFO: Downloaded file to /Users/maksvell/stanza_resources/resources.json\n",
      "2025-04-09 16:24:28 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2025-04-09 16:24:28 INFO: Using device: cpu\n",
      "2025-04-09 16:24:28 INFO: Loading: tokenize\n",
      "2025-04-09 16:24:28 INFO: Loading: mwt\n",
      "2025-04-09 16:24:28 INFO: Loading: pos\n",
      "2025-04-09 16:24:29 INFO: Loading: lemma\n",
      "2025-04-09 16:24:29 INFO: Loading: constituency\n",
      "2025-04-09 16:24:29 INFO: Loading: depparse\n",
      "2025-04-09 16:24:30 INFO: Loading: sentiment\n",
      "2025-04-09 16:24:30 INFO: Loading: ner\n",
      "2025-04-09 16:24:31 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')\n",
    "\n",
    "def lemmatize_keywords_stanza(keywords):\n",
    "    lemmatized_phrases = []\n",
    "    \n",
    "    for phrase in keywords:\n",
    "        doc = nlp(phrase) \n",
    "        lemmatized_words = [word.lemma for word in doc.iter_words()]\n",
    "        lemmatized_phrases.append(\" \".join(lemmatized_words)) \n",
    "    \n",
    "    return lemmatized_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc71972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_keywords'] = df['keywords'].apply(lambda row: lemmatize_keywords_stanza(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54640f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "      <th>lemmatized_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nonparametric local polynomial regression for ...</td>\n",
       "      <td>[Moritz Jirak, Alois Kneip, Alexander Meister,...</td>\n",
       "      <td>We consider nonparametric regression with func...</td>\n",
       "      <td>[space, polynomial, nonparametric local, hilbe...</td>\n",
       "      <td>[space, polynomial, nonparametric local, hilbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revisiting poverty measures using quantile fun...</td>\n",
       "      <td>[N. Unnikrishnan Nair, S. M. Sunoj]</td>\n",
       "      <td>In this article we redefine various poverty me...</td>\n",
       "      <td>[flexible quantile function models, measures i...</td>\n",
       "      <td>[flexible quantile function model, measure in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online Bernstein-von Mises theorem</td>\n",
       "      <td>[Jeyong Lee, Junhyeok Choi, Minwoo Chae]</td>\n",
       "      <td>Online learning is an inferential paradigm in ...</td>\n",
       "      <td>[sequentially updated posterior, updated poste...</td>\n",
       "      <td>[sequentially update posterior, update posteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Debiasing Continuous-time Nonlinear Autoregres...</td>\n",
       "      <td>[Simon Kuang, Xinfan Lin]</td>\n",
       "      <td>We study how to identify a class of continuous...</td>\n",
       "      <td>[debiasing continuous-time nonlinear, ordinary...</td>\n",
       "      <td>[debiase continuous - time nonlinear, ordinary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Generalized Tangent Approximation Framework ...</td>\n",
       "      <td>[Somjit Roy, Pritam Dey, Debdeep Pati, Bani K....</td>\n",
       "      <td>Tangent approximation form a popular class of ...</td>\n",
       "      <td>[framework for strongly super-gaussian, analys...</td>\n",
       "      <td>[framework for strongly super - gaussian, anal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Nonparametric local polynomial regression for ...   \n",
       "1  Revisiting poverty measures using quantile fun...   \n",
       "2                 Online Bernstein-von Mises theorem   \n",
       "3  Debiasing Continuous-time Nonlinear Autoregres...   \n",
       "4  A Generalized Tangent Approximation Framework ...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Moritz Jirak, Alois Kneip, Alexander Meister,...   \n",
       "1                [N. Unnikrishnan Nair, S. M. Sunoj]   \n",
       "2           [Jeyong Lee, Junhyeok Choi, Minwoo Chae]   \n",
       "3                          [Simon Kuang, Xinfan Lin]   \n",
       "4  [Somjit Roy, Pritam Dey, Debdeep Pati, Bani K....   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We consider nonparametric regression with func...   \n",
       "1  In this article we redefine various poverty me...   \n",
       "2  Online learning is an inferential paradigm in ...   \n",
       "3  We study how to identify a class of continuous...   \n",
       "4  Tangent approximation form a popular class of ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [space, polynomial, nonparametric local, hilbe...   \n",
       "1  [flexible quantile function models, measures i...   \n",
       "2  [sequentially updated posterior, updated poste...   \n",
       "3  [debiasing continuous-time nonlinear, ordinary...   \n",
       "4  [framework for strongly super-gaussian, analys...   \n",
       "\n",
       "                                 lemmatized_keywords  \n",
       "0  [space, polynomial, nonparametric local, hilbe...  \n",
       "1  [flexible quantile function model, measure in ...  \n",
       "2  [sequentially update posterior, update posteri...  \n",
       "3  [debiase continuous - time nonlinear, ordinary...  \n",
       "4  [framework for strongly super - gaussian, anal...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ebf2cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['flexible quantile function models', 'measures in literature', 'revisiting poverty measures', 'revisiting poverty', 'quantile functions', 'distribution functions', 'measures using quantile functions', 'redefine various poverty measures', 'terms of quantile functions', 'poverty measures in literature', 'poverty', 'measures', 'article we redefine', 'literature in terms', 'flexible quantile function', 'poverty measures', 'quantile function models', 'poverty measurement and analysis', 'prevailing approach', 'poverty measures using quantile']),\n",
       "       list(['flexible quantile function model', 'measure in literature', 'revisit poverty measure', 'revisit poverty', 'quantile function', 'distribution function', 'measure use quantile function', 'redefine various poverty measure', 'term of quantile function', 'poverty measure in literature', 'poverty', 'measure', 'article we redefine', 'literature in term', 'flexible quantile function', 'poverty measure', 'quantile function model', 'poverty measurement and analysis', 'prevail approach', 'poverty measure use quantile'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['keywords', 'lemmatized_keywords']].iloc[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4448c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maksvell/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bfba654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_keywords(keywords):\n",
    "    cleaned_keywords = []\n",
    "    \n",
    "    for phrase in keywords:\n",
    "        words = phrase.split() \n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        if filtered_words:\n",
    "            cleaned_keywords.append(\" \".join(filtered_words))\n",
    "    \n",
    "    return cleaned_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0f6da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_keywords'] = df['lemmatized_keywords'].apply(clean_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bca9c011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flexible quantile function model',\n",
       " 'measure literature',\n",
       " 'revisit poverty measure',\n",
       " 'revisit poverty',\n",
       " 'quantile function',\n",
       " 'distribution function',\n",
       " 'measure use quantile function',\n",
       " 'redefine various poverty measure',\n",
       " 'term quantile function',\n",
       " 'poverty measure literature',\n",
       " 'poverty',\n",
       " 'measure',\n",
       " 'article redefine',\n",
       " 'literature term',\n",
       " 'flexible quantile function',\n",
       " 'poverty measure',\n",
       " 'quantile function model',\n",
       " 'poverty measurement analysis',\n",
       " 'prevail approach',\n",
       " 'poverty measure use quantile']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1].cleaned_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3d67987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "      <th>lemmatized_keywords</th>\n",
       "      <th>cleaned_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Dynamic Investment Strategies Through Market C...</td>\n",
       "      <td>[Jinhui Li, Wenjia Xie, Luis Seco]</td>\n",
       "      <td>This study introduces a dynamic investment fra...</td>\n",
       "      <td>[learning approach this study, enhance portfol...</td>\n",
       "      <td>[learn approach this study, enhance portfolio ...</td>\n",
       "      <td>[learn approach study, enhance portfolio manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Generalized Tangent Approximation Framework ...</td>\n",
       "      <td>[Somjit Roy, Pritam Dey, Debdeep Pati, Bani K....</td>\n",
       "      <td>Tangent approximation form a popular class of ...</td>\n",
       "      <td>[framework for strongly super-gaussian, analys...</td>\n",
       "      <td>[framework for strongly super - gaussian, anal...</td>\n",
       "      <td>[framework strongly super - gaussian, analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Training Diagonal Linear Networks with Stochas...</td>\n",
       "      <td>[Gabriel Clara, Sophie Langer, Johannes Schmid...</td>\n",
       "      <td>We analyze the landscape and training dynamics...</td>\n",
       "      <td>[training dynamics, perturbed by small isotrop...</td>\n",
       "      <td>[training dynamic, perturb by small isotropic,...</td>\n",
       "      <td>[training dynamic, perturb small isotropic, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Existence and non-existence of consistent esti...</td>\n",
       "      <td>[Peter Braunsteins, Sophie Hautphenne, James K...</td>\n",
       "      <td>We consider the problem of estimating the para...</td>\n",
       "      <td>[non-existence of consistent, consistently est...</td>\n",
       "      <td>[non-existence of consistent, consistently est...</td>\n",
       "      <td>[non-existence consistent, consistently estima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Identifiability of VAR(1) model in a stationar...</td>\n",
       "      <td>[Bixuan Liu]</td>\n",
       "      <td>We consider a classical First-order Vector Aut...</td>\n",
       "      <td>[weighted directed, components of the var, vec...</td>\n",
       "      <td>[weight direct, component of the var, vector a...</td>\n",
       "      <td>[weight direct, component var, vector autoregr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "129  Dynamic Investment Strategies Through Market C...   \n",
       "4    A Generalized Tangent Approximation Framework ...   \n",
       "158  Training Diagonal Linear Networks with Stochas...   \n",
       "21   Existence and non-existence of consistent esti...   \n",
       "17   Identifiability of VAR(1) model in a stationar...   \n",
       "\n",
       "                                               authors  \\\n",
       "129                 [Jinhui Li, Wenjia Xie, Luis Seco]   \n",
       "4    [Somjit Roy, Pritam Dey, Debdeep Pati, Bani K....   \n",
       "158  [Gabriel Clara, Sophie Langer, Johannes Schmid...   \n",
       "21   [Peter Braunsteins, Sophie Hautphenne, James K...   \n",
       "17                                        [Bixuan Liu]   \n",
       "\n",
       "                                               summary  \\\n",
       "129  This study introduces a dynamic investment fra...   \n",
       "4    Tangent approximation form a popular class of ...   \n",
       "158  We analyze the landscape and training dynamics...   \n",
       "21   We consider the problem of estimating the para...   \n",
       "17   We consider a classical First-order Vector Aut...   \n",
       "\n",
       "                                              keywords  \\\n",
       "129  [learning approach this study, enhance portfol...   \n",
       "4    [framework for strongly super-gaussian, analys...   \n",
       "158  [training dynamics, perturbed by small isotrop...   \n",
       "21   [non-existence of consistent, consistently est...   \n",
       "17   [weighted directed, components of the var, vec...   \n",
       "\n",
       "                                   lemmatized_keywords  \\\n",
       "129  [learn approach this study, enhance portfolio ...   \n",
       "4    [framework for strongly super - gaussian, anal...   \n",
       "158  [training dynamic, perturb by small isotropic,...   \n",
       "21   [non-existence of consistent, consistently est...   \n",
       "17   [weight direct, component of the var, vector a...   \n",
       "\n",
       "                                      cleaned_keywords  \n",
       "129  [learn approach study, enhance portfolio manag...  \n",
       "4    [framework strongly super - gaussian, analysis...  \n",
       "158  [training dynamic, perturb small isotropic, li...  \n",
       "21   [non-existence consistent, consistently estima...  \n",
       "17   [weight direct, component var, vector autoregr...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fb30fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('model', 10), ('time series', 10), ('process', 8), ('regression', 6), ('distribution', 6), ('gaussian', 6), ('random variable', 6), ('confidence', 6), ('statistical inference', 6), ('stochastic', 6), ('estimation', 5), ('probability distribution', 5), ('linear regression', 5), ('random', 5), ('confidence set', 5), ('causal', 5), ('datum', 5), ('network', 5), ('regression model', 4), ('sigma', 4), ('machine learning', 4), ('mathcal', 4), ('conformal prediction', 4), ('empirical', 4), ('time', 4), ('sample size', 4), ('theorem', 4), ('sample', 4), ('learning', 4), ('random graph', 4), ('estimator', 4), ('confidence interval', 4), ('hilbert space', 3), ('convergence rate', 3), ('functional', 3), ('measure', 3), ('differential equation', 3), ('play important role', 3), ('sequential', 3), ('testing', 3), ('white noise', 3), ('neural network', 3), ('hypothesis testing', 3), ('set', 3), ('delta', 3), ('paper introduce', 3), ('random vector', 3), ('test', 3), ('high - dimensional setting', 3), ('spectral density', 3)]\n"
     ]
    }
   ],
   "source": [
    "all_keywords = [word for keywords in df['cleaned_keywords'] for word in keywords]\n",
    "keyword_freq = Counter(all_keywords)\n",
    "\n",
    "most_common_keywords = keyword_freq.most_common(50)\n",
    "print(most_common_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0aacc3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pq/7p0c971j5k7dd6zbzt4kxsj40000gn/T/ipykernel_92616/3615611862.py:3: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=list(top_keywords.values()), y=list(top_keywords.keys()), palette=\"viridis\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_keywords = dict(most_common_keywords)\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=list(top_keywords.values()), y=list(top_keywords.keys()), palette=\"viridis\")\n",
    "plt.xlabel('Частота')\n",
    "plt.ylabel('Ключевые слова')\n",
    "plt.title('Топ-50 самых частых ключевых слов')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9225a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных лемм: 4629\n"
     ]
    }
   ],
   "source": [
    "unique_keywords = set([word for keywords in df['lemmatized_keywords'] for word in keywords])\n",
    "\n",
    "print(f\"Количество уникальных лемм: {len(unique_keywords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e13a70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"preprocessed_data.parquet.gzip\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdaf440f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "      <th>lemmatized_keywords</th>\n",
       "      <th>cleaned_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nonparametric local polynomial regression for ...</td>\n",
       "      <td>[Moritz Jirak, Alois Kneip, Alexander Meister,...</td>\n",
       "      <td>We consider nonparametric regression with func...</td>\n",
       "      <td>[space, polynomial, nonparametric local, hilbe...</td>\n",
       "      <td>[space, polynomial, nonparametric local, hilbe...</td>\n",
       "      <td>[space, polynomial, nonparametric local, hilbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revisiting poverty measures using quantile fun...</td>\n",
       "      <td>[N. Unnikrishnan Nair, S. M. Sunoj]</td>\n",
       "      <td>In this article we redefine various poverty me...</td>\n",
       "      <td>[flexible quantile function models, measures i...</td>\n",
       "      <td>[flexible quantile function model, measure in ...</td>\n",
       "      <td>[flexible quantile function model, measure lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Online Bernstein-von Mises theorem</td>\n",
       "      <td>[Jeyong Lee, Junhyeok Choi, Minwoo Chae]</td>\n",
       "      <td>Online learning is an inferential paradigm in ...</td>\n",
       "      <td>[sequentially updated posterior, updated poste...</td>\n",
       "      <td>[sequentially update posterior, update posteri...</td>\n",
       "      <td>[sequentially update posterior, update posteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Debiasing Continuous-time Nonlinear Autoregres...</td>\n",
       "      <td>[Simon Kuang, Xinfan Lin]</td>\n",
       "      <td>We study how to identify a class of continuous...</td>\n",
       "      <td>[debiasing continuous-time nonlinear, ordinary...</td>\n",
       "      <td>[debiase continuous - time nonlinear, ordinary...</td>\n",
       "      <td>[debiase continuous - time nonlinear, ordinary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Generalized Tangent Approximation Framework ...</td>\n",
       "      <td>[Somjit Roy, Pritam Dey, Debdeep Pati, Bani K....</td>\n",
       "      <td>Tangent approximation form a popular class of ...</td>\n",
       "      <td>[framework for strongly super-gaussian, analys...</td>\n",
       "      <td>[framework for strongly super - gaussian, anal...</td>\n",
       "      <td>[framework strongly super - gaussian, analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Optimal Recovery Meets Minimax Estimation</td>\n",
       "      <td>[Ronald DeVore, Robert D. Nowak, Rahul Parhi, ...</td>\n",
       "      <td>A fundamental problem in statistics and machin...</td>\n",
       "      <td>[recovery, estimation a fundamental problem, r...</td>\n",
       "      <td>[recovery, estimation a fundamental problem, r...</td>\n",
       "      <td>[recovery, estimation fundamental problem, rat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Stronger Neyman Regret Guarantees for Adaptive...</td>\n",
       "      <td>[Georgy Noarov, Riccardo Fogliato, Martin Bert...</td>\n",
       "      <td>We study the design of adaptive, sequential ex...</td>\n",
       "      <td>[potential outcomes, treatment effect, potenti...</td>\n",
       "      <td>[potential outcome, treatment effect, potentia...</td>\n",
       "      <td>[potential outcome, treatment effect, potentia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Invariance principle for the Gaussian Multipli...</td>\n",
       "      <td>[Mriganka Basu Roy Chowdhury, Shirshendu Ganguly]</td>\n",
       "      <td>Gaussian multiplicative chaos (GMC) is a canon...</td>\n",
       "      <td>[random fractal measure obtained, non-gaussian...</td>\n",
       "      <td>[random fractal measure obtain, non-gaussian l...</td>\n",
       "      <td>[random fractal measure obtain, non-gaussian l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>On a class of high dimensional linear regressi...</td>\n",
       "      <td>[Ying-Ao Wang, Yunyi Zhang, Ye Zhang]</td>\n",
       "      <td>In this paper, we introduce a unified framewor...</td>\n",
       "      <td>[inspired by classical, broad class of linear,...</td>\n",
       "      <td>[inspire by classical, broad class of linear, ...</td>\n",
       "      <td>[inspire classical, broad class linear, method...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>On the admissibility of bounds on the mean of ...</td>\n",
       "      <td>[Erik Learned-Miller]</td>\n",
       "      <td>We address the problem of producing a lower bo...</td>\n",
       "      <td>[distribution, bounds, finite set of real, sca...</td>\n",
       "      <td>[distribution, bound, finite set of real, scal...</td>\n",
       "      <td>[distribution, bound, finite set real, scalar ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Nonparametric local polynomial regression for ...   \n",
       "1    Revisiting poverty measures using quantile fun...   \n",
       "2                   Online Bernstein-von Mises theorem   \n",
       "3    Debiasing Continuous-time Nonlinear Autoregres...   \n",
       "4    A Generalized Tangent Approximation Framework ...   \n",
       "..                                                 ...   \n",
       "245          Optimal Recovery Meets Minimax Estimation   \n",
       "246  Stronger Neyman Regret Guarantees for Adaptive...   \n",
       "247  Invariance principle for the Gaussian Multipli...   \n",
       "248  On a class of high dimensional linear regressi...   \n",
       "249  On the admissibility of bounds on the mean of ...   \n",
       "\n",
       "                                               authors  \\\n",
       "0    [Moritz Jirak, Alois Kneip, Alexander Meister,...   \n",
       "1                  [N. Unnikrishnan Nair, S. M. Sunoj]   \n",
       "2             [Jeyong Lee, Junhyeok Choi, Minwoo Chae]   \n",
       "3                            [Simon Kuang, Xinfan Lin]   \n",
       "4    [Somjit Roy, Pritam Dey, Debdeep Pati, Bani K....   \n",
       "..                                                 ...   \n",
       "245  [Ronald DeVore, Robert D. Nowak, Rahul Parhi, ...   \n",
       "246  [Georgy Noarov, Riccardo Fogliato, Martin Bert...   \n",
       "247  [Mriganka Basu Roy Chowdhury, Shirshendu Ganguly]   \n",
       "248              [Ying-Ao Wang, Yunyi Zhang, Ye Zhang]   \n",
       "249                              [Erik Learned-Miller]   \n",
       "\n",
       "                                               summary  \\\n",
       "0    We consider nonparametric regression with func...   \n",
       "1    In this article we redefine various poverty me...   \n",
       "2    Online learning is an inferential paradigm in ...   \n",
       "3    We study how to identify a class of continuous...   \n",
       "4    Tangent approximation form a popular class of ...   \n",
       "..                                                 ...   \n",
       "245  A fundamental problem in statistics and machin...   \n",
       "246  We study the design of adaptive, sequential ex...   \n",
       "247  Gaussian multiplicative chaos (GMC) is a canon...   \n",
       "248  In this paper, we introduce a unified framewor...   \n",
       "249  We address the problem of producing a lower bo...   \n",
       "\n",
       "                                              keywords  \\\n",
       "0    [space, polynomial, nonparametric local, hilbe...   \n",
       "1    [flexible quantile function models, measures i...   \n",
       "2    [sequentially updated posterior, updated poste...   \n",
       "3    [debiasing continuous-time nonlinear, ordinary...   \n",
       "4    [framework for strongly super-gaussian, analys...   \n",
       "..                                                 ...   \n",
       "245  [recovery, estimation a fundamental problem, r...   \n",
       "246  [potential outcomes, treatment effect, potenti...   \n",
       "247  [random fractal measure obtained, non-gaussian...   \n",
       "248  [inspired by classical, broad class of linear,...   \n",
       "249  [distribution, bounds, finite set of real, sca...   \n",
       "\n",
       "                                   lemmatized_keywords  \\\n",
       "0    [space, polynomial, nonparametric local, hilbe...   \n",
       "1    [flexible quantile function model, measure in ...   \n",
       "2    [sequentially update posterior, update posteri...   \n",
       "3    [debiase continuous - time nonlinear, ordinary...   \n",
       "4    [framework for strongly super - gaussian, anal...   \n",
       "..                                                 ...   \n",
       "245  [recovery, estimation a fundamental problem, r...   \n",
       "246  [potential outcome, treatment effect, potentia...   \n",
       "247  [random fractal measure obtain, non-gaussian l...   \n",
       "248  [inspire by classical, broad class of linear, ...   \n",
       "249  [distribution, bound, finite set of real, scal...   \n",
       "\n",
       "                                      cleaned_keywords  \n",
       "0    [space, polynomial, nonparametric local, hilbe...  \n",
       "1    [flexible quantile function model, measure lit...  \n",
       "2    [sequentially update posterior, update posteri...  \n",
       "3    [debiase continuous - time nonlinear, ordinary...  \n",
       "4    [framework strongly super - gaussian, analysis...  \n",
       "..                                                 ...  \n",
       "245  [recovery, estimation fundamental problem, rat...  \n",
       "246  [potential outcome, treatment effect, potentia...  \n",
       "247  [random fractal measure obtain, non-gaussian l...  \n",
       "248  [inspire classical, broad class linear, method...  \n",
       "249  [distribution, bound, finite set real, scalar ...  \n",
       "\n",
       "[250 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c0a58f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'authors', 'summary', 'keywords', 'lemmatized_keywords',\n",
       "       'cleaned_keywords'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75ad9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \" \".join(df[\"summary\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68bb46b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We consider nonparametric regression with functional covariates, that is,\\nthey are elements of an infinite-dimensional Hilbert space. A locally\\npolynomial estimator is constructed, where an orthonormal basis and various\\ntuning parameters remain to be selected. We provide a general asymptotic upper\\nbound on the estimation error and show that this procedure achieves polynomial\\nconvergence rates under appropriate tuning and supersmoothness of the\\nregression function. Such polynomial convergence rates have usually been\\nconsidered to be non-attainable in nonparametric functional regression without\\nany additional strong structural constraints such as linearity of the\\nregression function. In this article we redefine various poverty measures in literature in terms\\nof quantile functions instead of distribution functions in the prevailing\\napproach. This enables provision for alternative methodology for poverty\\nmeasurement and analysis along with some new results that are difficult to\\nobtain in the existing framework. Several flexible quantile function models\\nthat can enrich the existing ones are proposed and their utility is\\ndemonstrated for real data. Online learning is an inferential paradigm in which parameters are updated\\nincrementally from sequentially available data, in contrast to batch learning,\\nwhere the entire dataset is processed at once. In this paper, we assume that\\nmini-batches from the full dataset become available sequentially. The Bayesian\\nframework, which updates beliefs about unknown parameters after observing each\\nmini-batch, is naturally suited for online learning. At each step, we update\\nthe posterior distribution using the current prior and new observations, with\\nthe updated posterior serving as the prior for the next step. However, this\\nrecursive Bayesian updating is rarely computationally tractable unless the\\nmodel and prior are conjugate. When the model is regular, the updated posterior\\ncan be approximated by a normal distribution, as justified by the Bernstein-von\\nMises theorem. We adopt a variational approximation at each step and\\ninvestigate the frequentist properties of the final posterior obtained through\\nthis sequential procedure. Under mild assumptions, we show that the accumulated\\napproximation error becomes negligible once the mini-batch size exceeds a\\nthreshold depending on the parameter dimension. As a result, the sequentially\\nupdated posterior is asymptotically indistinguishable from the full posterior. We study how to identify a class of continuous-time nonlinear systems defined\\nby an ordinary differential equation affine in the unknown parameter. We define\\na notion of asymptotic consistency as $(n, h) \\\\to (\\\\infty, 0)$, and we achieve\\nit using a family of direct methods where the first step is differentiating a\\nnoisy time series and the second step is a plug-in linear estimator. The first\\nstep, differentiation, is a signal processing adaptation of the nonparametric\\nstatistical technique of local polynomial regression. The second step,\\ngeneralized linear regression, can be consistent using a least squares\\nestimator, but we demonstrate two novel bias corrections that improve the\\naccuracy for finite $h$. These methods significantly broaden the class of\\ncontinuous-time systems that can be consistently estimated by direct methods. Tangent approximation form a popular class of variational inference (VI)\\ntechniques for Bayesian analysis in intractable non-conjugate models. It is\\nbased on the principle of convex duality to construct a minorant of the\\nmarginal likelihood, making the problem tractable. Despite its extensive\\napplications, a general methodology for tangent approximation encompassing a\\nlarge class of likelihoods beyond logit models with provable optimality\\nguarantees is still elusive. In this article, we propose a general Tangent\\nApproximation based Variational InferencE (TAVIE) framework for strongly\\nsuper-Gaussian (SSG) likelihood functions which includes a broad class of\\nflexible probability models. Specifically, TAVIE obtains a quadratic lower\\nbound of the corresponding log-likelihood, thus inducing conjugacy with\\nGaussian priors over the model parameters. Under mild assumptions on the\\ndata-generating process, we demonstrate the optimality of our proposed\\nmethodology in the fractional likelihood setup. Furthermore, we illustrate the\\nempirical performance of TAVIE through extensive simulations and an application\\non the U.S. 2000 Census real data. Diffusion models are distinguished by their exceptional generative\\nperformance, particularly in producing high-quality samples through iterative\\ndenoising. While current theory suggests that the number of denoising steps\\nrequired for accurate sample generation should scale linearly with data\\ndimension, this does not reflect the practical efficiency of widely used\\nalgorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper\\ninvestigates the effectiveness of diffusion models in sampling from complex\\nhigh-dimensional distributions that can be well-approximated by Gaussian\\nMixture Models (GMMs). For these distributions, our main result shows that DDPM\\ntakes at most $\\\\widetilde{O}(1/\\\\varepsilon)$ iterations to attain an\\n$\\\\varepsilon$-accurate distribution in total variation (TV) distance,\\nindependent of both the ambient dimension $d$ and the number of components $K$,\\nup to logarithmic factors. Furthermore, this result remains robust to score\\nestimation errors. These findings highlight the remarkable effectiveness of\\ndiffusion models in high-dimensional settings given the universal approximation\\ncapability of GMMs, and provide theoretical insights into their practical\\nsuccess. Score estimation is the backbone of score-based generative models (SGMs),\\nespecially denoising diffusion probabilistic models (DDPMs). A key result in\\nthis area shows that with accurate score estimates, SGMs can efficiently\\ngenerate samples from any realistic data distribution (Chen et al., ICLR\\'23;\\nLee et al., ALT\\'23). This distribution learning result, where the learned\\ndistribution is implicitly that of the sampler\\'s output, does not explain how\\nscore estimation relates to classical tasks of parameter and density\\nestimation.\\n  This paper introduces a framework that reduces score estimation to these two\\ntasks, with various implications for statistical and computational learning\\ntheory:\\n  Parameter Estimation: Koehler et al. (ICLR\\'23) demonstrate that a\\nscore-matching variant is statistically inefficient for the parametric\\nestimation of multimodal densities common in practice. In contrast, we show\\nthat under mild conditions, denoising score-matching in DDPMs is asymptotically\\nefficient.\\n  Density Estimation: By linking generation to score estimation, we lift\\nexisting score estimation guarantees to $(\\\\epsilon,\\\\delta)$-PAC density\\nestimation, i.e., a function approximating the target log-density within\\n$\\\\epsilon$ on all but a $\\\\delta$-fraction of the space. We provide (i) minimax\\nrates for density estimation over H\\\\\"older classes and (ii) a quasi-polynomial\\nPAC density estimation algorithm for the classical Gaussian location mixture\\nmodel, building on and addressing an open problem from Gatmiry et al.\\n(arXiv\\'24).\\n  Lower Bounds for Score Estimation: Our framework offers the first principled\\nmethod to prove computational lower bounds for score estimation across general\\ndistributions. As an application, we establish cryptographic lower bounds for\\nscore estimation in general Gaussian mixture models, conceptually recovering\\nSong\\'s (NeurIPS\\'24) result and advancing his key open problem. Exoplanets play an important role in understanding the mechanics of planetary\\nsystem formation and orbital evolution. In this context the correlations of\\ndifferent parameters of the planets and their host star are useful guides in\\nthe search for explanatory mechanisms. Based on a reanalysis of the data set\\nfrom \\\\cite{figueria14} we study the as of now still poorly understood\\ncorrelation between planetary surface gravity and stellar activity of Hot\\nJupiters. Unfortunately, data collection often suffers from measurement errors\\ndue to complicated and indirect measurement setups, rendering standard\\ninference techniques unreliable.\\n  We present new methods to estimate and test for correlations in a\\ndeconvolution framework and thereby improve the state of the art analysis of\\nthe data in two directions. First, we are now able to account for additive\\nmeasurement errors which facilitates reliable inference. Second we test for\\nrelevant changes, i.e. we are testing for correlations exceeding a certain\\nthreshold $\\\\Delta$. This reflects the fact that small nonzero correlations are\\nto be expected for real life data almost always and that standard statistical\\ntests will therefore always reject the null of no correlation given sufficient\\ndata. Our theory focuses on quantities that can be estimated by U-Statistics\\nwhich contain a variety of correlation measures. We propose a bootstrap test\\nand establish its theoretical validity. As a by product we also obtain\\nconfidence intervals. Applying our methods to the Hot Jupiter data set from\\n\\\\cite{figueria14}, we observe that taking into account the measurement errors\\nyields smaller point estimates and the null of no relevant correlation is\\nrejected only for very small $\\\\Delta$. This demonstrates the importance of\\nconsidering the impact of measurement errors to avoid misleading conclusions\\nfrom the resulting statistical analysis. We propose a neural-network based survival model (SurvSurf) specifically\\ndesigned for direct and simultaneous probabilistic prediction of the first\\nhitting time of sequential events from baseline. Unlike existing models,\\nSurvSurf is theoretically guaranteed to never violate the monotonic\\nrelationship between the cumulative incidence functions of sequential events,\\nwhile allowing nonlinear influence from predictors. It also incorporates\\nimplicit truths for unobserved intermediate events in model fitting, and\\nsupports both discrete and continuous time and events. We also identified a\\nvariant of the Integrated Brier Score (IBS) that showed robust correlation with\\nthe mean squared error (MSE) between the true and predicted probabilities by\\naccounting for implied truths about the missing intermediate events. We\\ndemonstrated the superiority of SurvSurf compared to modern and traditional\\npredictive survival models in two simulated datasets and two real-world\\ndatasets, using MSE, the more robust IBS and by measuring the extent of\\nmonotonicity violation. The drift sequential parameter estimation problems for the Cox-Ingersoll-Ross\\n(CIR) processes under the limited duration of observation are studied.\\nTruncated sequential estimation methods for both scalar and {two}-dimensional\\nparameter cases are proposed. In the non-asymptotic setting, for the proposed\\ntruncated estimators, the properties of guaranteed mean-square estimation\\naccuracy are established. In the asymptotic formulation, when the observation\\ntime tends to infinity, it is shown that the proposed sequential procedures are\\nasymptotically optimal among all possible sequential and non-sequential\\nestimates with an average estimation time less than the fixed observation\\nduration. It also turned out that asymptotically, without degrading the\\nestimation quality, they significantly reduce the observation duration compared\\nto classical non-sequential maximum likelihood estimations based on a fixed\\nobservation duration. Existing research on negations primarily focuses on entropy and extropy.\\nRecently, new functions such as varentropy and varextropy have been developed,\\nwhich can be considered as\\n  extensions of entropy and extropy. However, the impact of negation on these\\nextended measures, particularly varentropy and varextropy, has not been\\nextensively explored. To address\\n  this gap, this paper investigates the effect of negation on Shannon entropy,\\nvarentropy, and varextropy. We explore how the negation of a probability\\ndistribution influences these\\n  measures, showing that the negated distribution consistently leads to higher\\nvalues of Shannon entropy, varentropy, and varextropy compared to the original\\ndistribution.\\n  Additionally, we prove that the negation of a probability distribution\\nmaximizes these measures during the process. The paper provides theoretical\\nproofs and a detailed analysis of\\n  the behaviour of these measures, contributing to a better understanding of\\nthe interplay between probability distributions, negation, and\\ninformation-theoretic quantities. We consider the task of Gaussian mean testing, that is, of testing whether a\\nhigh-dimensional vector perturbed by white noise has large magnitude, or is the\\nzero vector. This question, originating from the signal processing community,\\nhas recently seen a surge of interest from the machine learning and theoretical\\ncomputer science community, and is by now fairly well understood. What is much\\nless understood, and the focus of our work, is how to perform this task under\\ntruncation: that is, when the observations (i.i.d.\\\\ samples from the underlying\\nhigh-dimensional Gaussian) are only observed when they fall in an given subset\\nof the domain $\\\\R^d$. This truncation model, previously studied in the context\\nof learning (instead of testing) the mean vector, has a range of applications,\\nin particular in Economics and Social Sciences. As our work shows, sample\\ntruncations affect the complexity of the testing task in a rather subtle and\\nsurprising way. This paper analyzes the Gini coefficient estimator for zero-truncated Poisson\\npopulations, revealing the presence of bias, and provides a mathematical\\nexpression for the bias, along with a bias-corrected estimator, which is\\nevaluated using Monte Carlo simulation methods. The challenge of clustering short text data lies in balancing informativeness\\nwith interpretability. Traditional evaluation metrics often overlook this\\ntrade-off. Inspired by linguistic principles of communicative efficiency, this\\npaper investigates the optimal number of clusters by quantifying the trade-off\\nbetween informativeness and cognitive simplicity. We use large language models\\n(LLMs) to generate cluster names and evaluate their effectiveness through\\nsemantic density, information theory, and clustering accuracy. Our results show\\nthat Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM,\\nincreases semantic density compared to random assignment, effectively grouping\\nsimilar bios. However, as clusters increase, interpretability declines, as\\nmeasured by a generative LLM\\'s ability to correctly assign bios based on\\ncluster names. A logistic regression analysis confirms that classification\\naccuracy depends on the semantic similarity between bios and their assigned\\ncluster names, as well as their distinction from alternatives.\\n  These findings reveal a \"Goldilocks zone\" where clusters remain distinct yet\\ninterpretable. We identify an optimal range of 16-22 clusters, paralleling\\nlinguistic efficiency in lexical categorization. These insights inform both\\ntheoretical models and practical applications, guiding future research toward\\noptimising cluster interpretability and usefulness. This article introduces Regression Discontinuity Design (RDD) with\\nDistribution-Valued Outcomes (R3D), extending the standard RDD framework to\\nsettings where the outcome is a distribution rather than a scalar. Such\\nsettings arise when treatment is assigned at a higher level of aggregation than\\nthe outcome-for example, when a subsidy is allocated based on a firm-level\\nrevenue cutoff while the outcome of interest is the distribution of employee\\nwages within the firm. Since standard RDD methods cannot accommodate such\\ntwo-level randomness, I propose a novel approach based on random distributions.\\nThe target estimand is a \"local average quantile treatment effect\", which\\naverages across random quantiles. To estimate this target, I introduce two\\nrelated approaches: one that extends local polynomial regression to random\\nquantiles and another based on local Fr\\\\\\'echet regression, a form of functional\\nregression. For both estimators, I establish asymptotic normality and develop\\nuniform, debiased confidence bands together with a data-driven bandwidth\\nselection procedure. Simulations validate these theoretical properties and show\\nexisting methods to be biased and inconsistent in this setting. I then apply\\nthe proposed methods to study the effects of gubernatorial party control on\\nwithin-state income distributions in the US, using a close-election design. The\\nresults suggest a classic equality-efficiency tradeoff under Democratic\\ngovernorship, driven by reductions in income at the top of the distribution. We study a multivariate Hawkes process as a model for time-continuous\\nrelational event networks. The model does not assume the network to be known,\\nit includes covariates, and it allows for both common drivers, parameters\\ncommon to all the actors in the network, and also local parameters specific for\\neach actor. We derive rates of convergence for all of the model parameters when\\nboth the number of actors and the time horizon tends to infinity. To prevent an\\nexploding network, sparseness is assumed. We also discuss numerical aspects. Importance Sampling (IS) is a widely used variance reduction technique for\\nenhancing the efficiency of Monte Carlo methods, particularly in rare-event\\nsimulation and related applications. Despite its power, the performance of IS\\nis often highly sensitive to the choice of the proposal distribution and\\nfrequently requires stochastic calibration techniques. While the design and\\nanalysis of IS have been extensively studied in estimation settings, applying\\nIS within stochastic optimization introduces a unique challenge: the decision\\nand the IS distribution are mutually dependent, creating a circular\\noptimization structure. This interdependence complicates both the analysis of\\nconvergence for decision iterates and the efficiency of the IS scheme. In this\\npaper, we propose an iterative gradient-based algorithm that jointly updates\\nthe decision variable and the IS distribution without requiring time-scale\\nseparation between the two. Our method achieves the lowest possible asymptotic\\nvariance and guarantees global convergence under convexity of the objective and\\nmild assumptions on the IS distribution family. Furthermore, we show that these\\nproperties are preserved under linear constraints by incorporating a recent\\nvariant of Nesterov\\'s dual averaging method. We consider a classical First-order Vector AutoRegressive (VAR(1)) model,\\nwhere we interpret the autoregressive interaction matrix as influence\\nrelationships among the components of the VAR(1) process that can be encoded by\\na weighted directed graph. A majority of previous work studies the structural\\nidentifiability of the graph based on time series observations and therefore\\nrelies on dynamical information. In this work we assume that an equilibrium\\nexists, and study instead the identifiability of the graph from the stationary\\ndistribution, meaning that we seek a way to reconstruct the influence graph\\nunderlying the dynamic network using only static information. We use an\\napproach from algebraic statistics that characterizes models using the Jacobian\\nmatroids associated with the parametrization of the models, and we introduce\\nsufficient graphical conditions under which different graphs yield distinct\\nsteady-state distributions. Additionally, we illustrate how our results could\\nbe applied to characterize networks inspired by ecological research. Given i.i.d. observations uniformly distributed on a closed submanifold of\\nthe Euclidean space, we study higher-order generalizations of graph Laplacians,\\nso-called Hodge Laplacians on graphs, as approximations of the Laplace-Beltrami\\noperator on differential forms. Our main result is a high-probability error\\nbound for the associated Dirichlet forms. This bound improves existing\\nDirichlet form error bounds for graph Laplacians in the context of Laplacian\\nEigenmaps, and it provides insights into the Betti numbers studied in\\ntopological data analysis and the complementing positive part of the spectrum. Nonparametric regression with random design is considered.\\n  The $L_2$ error with integration with respect to the design\\n  measure is used as the error criterion.\\n  An over-parametrized deep neural network\\n  regression estimate\\n  with logistic activation function\\n  is defined, where all weights are learned\\n  by gradient descent. It is shown that the estimate\\n  achieves a nearly optimal rate of convergence in case\\n  that the regression function is $(p,C)$--smooth. A new formula for Marchenko-Pastur inversion is derived and used for\\ninference of population linear spectral statistics. The formula allows for\\nestimation of the Stieltjes transform of the population spectral distribution\\n$s_H(z)$, when $z$ is sufficiently far from the support of the population\\nspectral distribution $H$. If the dimension $d$ and the sample size $n$ go to\\ninfinity simultaneously such that $\\\\frac{d}{n} \\\\rightarrow c>0$, the estimation\\nerror is shown to be asymptotically less than $\\\\frac{n^{\\\\varepsilon}}{n}$ for\\narbitrary $\\\\varepsilon > 0$. By integrating along a curve around the support of\\n$H$, estimators for population linear spectral statistics are constructed,\\nwhich benefit from this convergence speed of $\\\\frac{n^{\\\\varepsilon}}{n}$. We consider the problem of estimating the parameters of a supercritical\\ncontrolled branching process consistently from a single observed trajectory of\\npopulation size counts. Our goal is to establish which parameters can and\\ncannot be consistently estimated. When a parameter can be consistently\\nestimated, we derive an explicit expression for the estimator. We address these\\nquestions in three scenarios: when the distribution of the control function\\ndistribution is known, when it is unknown, and when progenitor numbers are\\nobserved alongside population size counts. Our results offer a theoretical\\njustification for the common practice in population ecology of estimating\\ndemographic and environmental stochasticity using separate observation schemes. Accurate tuning of hyperparameters is crucial to ensure that models can\\ngeneralise effectively across different settings. In this paper, we present\\ntheoretical guarantees for hyperparameter selection using variational Bayes in\\nthe nonparametric regression model. We construct a variational approximation to\\na hierarchical Bayes procedure, and derive upper bounds for the contraction\\nrate of the variational posterior in an abstract setting. The theory is applied\\nto various Gaussian process priors and variational classes, resulting in\\nminimax optimal rates. Our theoretical results are accompanied with numerical\\nanalysis both on synthetic and real world data sets. In this paper, we study the problem of multivariate shuffled linear\\nregression, where the correspondence between predictors and responses in a\\nlinear model is obfuscated by a latent permutation. Specifically, we\\ninvestigate the model $Y=\\\\tfrac{1}{\\\\sqrt{1+\\\\sigma^2}}(\\\\Pi_* X Q_* + \\\\sigma Z)$,\\nwhere $X$ is an $n*d$ standard Gaussian design matrix, $Z$ is an $n*m$ Gaussian\\nnoise matrix, $\\\\Pi_*$ is an unknown $n*n$ permutation matrix, and $Q_*$ is an\\nunknown $d*m$ on the Grassmanian manifold satisfying $Q_*^{\\\\top} Q_* = \\\\mathbb\\nI_m$.\\n  Consider the hypothesis testing problem of distinguishing this model from the\\ncase where $X$ and $Y$ are independent Gaussian random matrices of sizes $n*d$\\nand $n*m$, respectively. Our results reveal a phase transition phenomenon in\\nthe performance of low-degree polynomial algorithms for this task. (1) When\\n$m=o(d)$, we show that all degree-$D$ polynomials fail to distinguish these two\\nmodels even when $\\\\sigma=0$, provided with $D^4=o\\\\big( \\\\tfrac{d}{m} \\\\big)$. (2)\\nWhen $m=d$ and $\\\\sigma=\\\\omega(1)$, we show that all degree-$D$ polynomials fail\\nto distinguish these two models provided with $D=o(\\\\sigma)$. (3) When $m=d$ and\\n$\\\\sigma=o(1)$, we show that there exists a constant-degree polynomial that\\nstrongly distinguish these two models. These results establish a smooth\\ntransition in the effectiveness of low-degree polynomial algorithms for this\\nproblem, highlighting the interplay between the dimensions $m$ and $d$, the\\nnoise level $\\\\sigma$, and the computational complexity of the testing task. We introduce a new approach for estimating the number of spikes in a general\\nclass of spiked covariance models without directly computing the eigenvalues of\\nthe sample covariance matrix. This approach is based on the Lanczos algorithm\\nand the asymptotic properties of the associated Jacobi matrix and its Cholesky\\nfactorization. A key aspect of the analysis is interpreting the eigenvector\\nspectral distribution as a perturbation of its asymptotic counterpart. The\\nspecific exponential-type asymptotics of the Jacobi matrix enables an efficient\\napproximation of the Stieltjes transform of the asymptotic spectral\\ndistribution via a finite continued fraction. As a consequence, we also obtain\\nestimates for the density of the asymptotic distribution and the location of\\noutliers. We provide consistency guarantees for our proposed estimators,\\nproving their convergence in the high-dimensional regime. We demonstrate that,\\nwhen applied to standard spiked covariance models, our approach outperforms\\nexisting methods in computational efficiency and runtime, while still\\nmaintaining robustness to exotic population covariances. The behavior of the random feature model in the high-dimensional regression\\nframework has become a popular issue of interest in the machine learning\\nliterature}. This model is generally considered for feature vectors $x_i =\\n\\\\Sigma^{1/2} x_i\\'$, where $x_i\\'$ is a random vector made of independent and\\nidentically distributed (iid) entries, and $\\\\Sigma$ is a positive definite\\nmatrix representing the covariance of the features.\\n  In this paper, we move beyond {\\\\CB this standard assumption by studying the\\nperformances of the random features model in the setting of non-iid feature\\nvectors}. Our approach is related to the analysis of the spectrum of large\\nrandom matrices through random matrix theory (RMT) {\\\\CB and free probability}\\nresults. We turn to the analysis of non-iid data by using the notion of\\nvariance profile {\\\\CB which} is {\\\\CB well studied in RMT.} Our main\\ncontribution is then the study of the limits of the training and {\\\\CB\\nprediction} risks associated to the ridge estimator in the random features\\nmodel when its dimensions grow. We provide asymptotic equivalents of these\\nrisks that capture the behavior of ridge regression with random features in a\\n{\\\\CB high-dimensional} framework. These asymptotic equivalents, {\\\\CB which\\nprove to be sharp in numerical experiments}, are retrieved by adapting, to our\\nsetting, established results from operator-valued free probability theory.\\nMoreover, {\\\\CB for various classes of random feature vectors that have not been\\nconsidered so far in the literature}, our approach allows to show the\\nappearance of the double descent phenomenon when the ridge regularization\\nparameter is small enough. An e-variable for a family of distributions $\\\\mathcal{P}$ is a nonnegative\\nrandom variable whose expected value under every distribution in $\\\\mathcal{P}$\\nis at most one. E-variables have recently been recognized as fundamental\\nobjects in hypothesis testing, and a rapidly growing body of work has attempted\\nto derive admissible or optimal e-variables for various families $\\\\mathcal{P}$.\\nIn this paper, we study classes $\\\\mathcal{P}$ that are specified by\\nconstraints. Simple examples include bounds on the moments, but our general\\ntheory covers arbitrary sets of measurable constraints. Our main results\\ncharacterize the set of all e-variables for such classes, as well as maximal\\nones. Three case studies illustrate the scope of our theory: finite constraint\\nsets, one-sided sub-$\\\\psi$ distributions, and distributions invariant under a\\ngroup of symmetries. In particular, we generalize recent results of Clerico\\n(2024a) by dropping all assumptions on the constraints. We exploit the multiplicative structure of P\\\\\\'olya Tree priors for density\\nand differential entropy estimation in $p$-dimensions. We establish: (i) a\\nrepresentation theorem of entropy functionals and (ii) conditions on the\\nparameters of P\\\\\\'olya Trees to obtain Kullback-Leibler and Total Variation\\nconsistency for vectors with compact support. Those results motivate a novel\\ndifferential entropy estimator that is consistent in probability for compact\\nsupported vectors under mild conditions. In order to enable applications of\\nboth results, we also provide a theoretical motivation for the truncation of\\nUnivariate P\\\\\\'olya Trees at level $3 \\\\log_2 n $. We consider the problem of sequential hypothesis testing by betting. For a\\ngeneral class of composite testing problems -- which include bounded mean\\ntesting, equal mean testing for bounded random tuples, and some key ingredients\\nof two-sample and independence testing as special cases -- we show that any\\n$e$-process satisfying a certain sublinear regret bound is adaptively,\\nasymptotically, and almost surely log-optimal for a composite alternative. This\\nis a strong notion of optimality that has not previously been established for\\nthe aforementioned problems and we provide explicit test supermartingales and\\n$e$-processes satisfying this notion in the more general case. Furthermore, we\\nderive matching lower and upper bounds on the expected rejection time for the\\nresulting sequential tests in all of these cases. The proofs of these results\\nmake weak, algorithm-agnostic moment assumptions and rely on a general-purpose\\nproof technique involving the aforementioned regret and a family of numeraire\\nportfolios. Finally, we discuss how all of these theorems hold in a\\ndistribution-uniform sense, a notion of log-optimality that is stronger still\\nand seems to be new to the literature. We study the problem of learning a high-density region of an arbitrary\\ndistribution over $\\\\mathbb{R}^d$. Given a target coverage parameter $\\\\delta$,\\nand sample access to an arbitrary distribution $D$, we want to output a\\nconfidence set $S \\\\subset \\\\mathbb{R}^d$ such that $S$ achieves $\\\\delta$\\ncoverage of $D$, i.e., $\\\\mathbb{P}_{y \\\\sim D} \\\\left[ y \\\\in S \\\\right] \\\\ge\\n\\\\delta$, and the volume of $S$ is as small as possible. This is a central\\nproblem in high-dimensional statistics with applications in finding confidence\\nsets, uncertainty quantification, and support estimation.\\n  In the most general setting, this problem is statistically intractable, so we\\nrestrict our attention to competing with sets from a concept class $C$ with\\nbounded VC-dimension. An algorithm is competitive with class $C$ if, given\\nsamples from an arbitrary distribution $D$, it outputs in polynomial time a set\\nthat achieves $\\\\delta$ coverage of $D$, and whose volume is competitive with\\nthe smallest set in $C$ with the required coverage $\\\\delta$. This problem is\\ncomputationally challenging even in the basic setting when $C$ is the set of\\nall Euclidean balls. Existing algorithms based on coresets find in polynomial\\ntime a ball whose volume is $\\\\exp(\\\\tilde{O}( d/ \\\\log d))$-factor competitive\\nwith the volume of the best ball.\\n  Our main result is an algorithm that finds a confidence set whose volume is\\n$\\\\exp(\\\\tilde{O}(d^{2/3}))$ factor competitive with the optimal ball having the\\ndesired coverage. The algorithm is improper (it outputs an ellipsoid). Combined\\nwith our computational intractability result for proper learning balls within\\nan $\\\\exp(\\\\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results\\nprovide an interesting separation between proper and (improper) learning of\\nconfidence sets. This paper introduces a periodic multivariate Poisson autoregression with\\npotentially infinite memory, with a special focus on the network setting. Using\\ncontraction techniques, we study the stability of such a process and provide\\nupper bounds on how fast it reaches the periodically stationary regime. We then\\npropose a computationally efficient Markov approximation using the properties\\nof the exponential function and a density result. Furthermore, we prove the\\nstrong consistency of the maximum likelihood estimator for the Markov\\napproximation and empirically test its robustness in the case of\\nmisspecification. Our model is applied to the prediction of weekly Rotavirus\\ncases in Berlin, demonstrating superior performance compared to the existing\\nPNAR model. Given a probability measure with density, Fermat distances and density-driven\\nmetrics are conformal transformation of the Euclidean metric that shrink\\ndistances in high density areas and enlarge distances in low density areas.\\nAlthough they have been widely studied and have shown to be useful in various\\nmachine learning tasks, they are limited to measures with density (with respect\\nto Lebesgue measure, or volume form on manifold). In this paper, by replacing\\nthe density with the Distance-to-Measure, we introduce a new metric, the Fermat\\nDistance-to-Measure, defined for any probability measure in R^d. We derive\\nstrong stability properties for the Fermat Distance-to-Measure with respect to\\nthe measure and propose an estimator from random sampling of the measure,\\nfeaturing an explicit bound on its convergence speed. This paper presents a unified framework for understanding the methodology and\\ntheory behind several different methods in the conformal prediction literature,\\nwhich includes standard conformal prediction (CP), weighted conformal\\nprediction (WCP), nonexchangeable conformal prediction (NexCP), and\\nrandomly-localized conformal prediction (RLCP), among others. At the crux of\\nour framework is the idea that conformal methods are based on revealing partial\\ninformation about the data at hand, and positing a conditional distribution for\\nthe data given the partial information. Different methods arise from different\\nchoices of partial information, and of the corresponding (approximate)\\nconditional distribution. In addition to recovering and unifying existing\\nresults, our framework leads to both new theoretical guarantees for existing\\nmethods, and new extensions of the conformal methodology. We propose new statistical tests, in high-dimensional settings, for testing\\nthe independence of two random vectors and their conditional independence given\\na third random vector. The key idea is simple, i.e., we first transform each\\ncomponent variable to standard normal via its marginal empirical distribution,\\nand we then test for independence and conditional independence of the\\ntransformed random vectors using appropriate $L_\\\\infty$-type test statistics.\\nWhile we are testing some necessary conditions of the independence or the\\nconditional independence, the new tests outperform the 13 frequently used\\ntesting methods in a large scale simulation comparison. The advantage of the\\nnew tests can be summarized as follows: (i) they do not require any moment\\nconditions, (ii) they allow arbitrary dependence structures of the components\\namong the random vectors, and (iii) they allow the dimensions of random vectors\\ndiverge at the exponential rates of the sample size. The critical values of the\\nproposed tests are determined by a computationally efficient multiplier\\nbootstrap procedure. Theoretical analysis shows that the sizes of the proposed\\ntests can be well controlled by the nominal significance level, and the\\nproposed tests are also consistent under certain local alternatives. The finite\\nsample performance of the new tests is illustrated via extensive simulation\\nstudies and a real data application. We study the geometry of Receiver Operating Characteristic (ROC) and\\nPrecision-Recall (PR) curves in binary classification problems. The key finding\\nis that many of the most commonly used binary classification metrics are merely\\nfunctions of the composition function $G := F_p \\\\circ F_n^{-1}$, where\\n$F_p(\\\\cdot)$ and $F_n(\\\\cdot)$ are the class-conditional cumulative distribution\\nfunctions of the classifier scores in the positive and negative classes,\\nrespectively. This geometric perspective facilitates the selection of operating\\npoints, understanding the effect of decision thresholds, and comparison between\\nclassifiers. It also helps explain how the shapes and geometry of ROC/PR curves\\nreflect classifier behavior, providing objective tools for building classifiers\\noptimized for specific applications with context-specific constraints. We\\nfurther explore the conditions for classifier dominance, present analytical and\\nnumerical examples demonstrating the effects of class separability and variance\\non ROC and PR geometries, and derive a link between the positive-to-negative\\nclass leakage function $G(\\\\cdot)$ and the Kullback--Leibler divergence. The\\nframework highlights practical considerations, such as model calibration,\\ncost-sensitive optimization, and operating point selection under real-world\\ncapacity constraints, enabling more informed approaches to classifier\\ndeployment and decision-making. In this work, we are interested in studying the causal effect of an\\nendogenous binary treatment on a dependently censored duration outcome. By\\ndependent censoring, it is meant that the duration time ($T$) and right\\ncensoring time ($C$) are not statistically independent of each other, even\\nafter conditioning on the measured covariates. The endogeneity issue is handled\\nby making use of a binary instrumental variable for the treatment. To deal with\\nthe dependent censoring problem, it is assumed that on the stratum of\\ncompliers: (i) $T$ follows a semiparametric proportional hazards model; (ii)\\n$C$ follows a fully parametric model; and (iii) the relation between $T$ and\\n$C$ is modeled by a parametric copula, such that the association parameter can\\nbe left unspecified. In this framework, the treatment effect of interest is the\\ncomplier causal hazard ratio (CCHR). We devise an estimation procedure that is\\nbased on a weighted maximum likelihood approach, where the weights are the\\nprobabilities of an observation coming from a complier. The weights are\\nestimated non-parametrically in a first stage, followed by the estimation of\\nthe CCHR. Novel conditions under which the model is identifiable are given, a\\ntwo-step estimation procedure is proposed and some important asymptotic\\nproperties are established. Simulations are used to assess the validity and\\nfinite-sample performance of the estimation procedure. Finally, we apply the\\napproach to estimate the CCHR of both job training programs on unemployment\\nduration and periodic screening examinations on time until death from breast\\ncancer. The data come from the National Job Training Partnership Act study and\\nthe Health Insurance Plan of Greater New York experiment respectively. The de Bruijn identity states that Fisher information is the half of the\\nderivative of Shannon differential entropy along heat flow. In the same spirit,\\nin this paper we introduce a generalized version of Fisher information, named\\nas the R\\\\\\'enyi--Fisher information, which is the half of the derivative of\\nR\\\\\\'enyi information along heat flow. Based on this R\\\\\\'enyi--Fisher information,\\nwe establish sharp R\\\\\\'enyi-entropic isoperimetric inequalities, which\\ngeneralize the classic entropic isoperimetric inequality to the R\\\\\\'enyi\\nsetting. Utilizing these isoperimetric inequalities, we extend the classical\\nCram\\\\\\'er--Rao inequality from Fisher information to R\\\\\\'enyi--Fisher\\ninformation. Lastly, we use these generalized Cram\\\\\\'er--Rao inequalities to\\ndetermine the signs of derivatives of entropy along heat flow, strengthening\\nexisting results on the complete monotonicity of entropy. This paper focuses on nonparametric statistical inference of the hazard rate\\nfunction of discrete distributions based on $\\\\delta$-record data. We derive the\\nexplicit expression of the maximum likelihood estimator and determine its exact\\ndistribution, as well as some important characteristics such as its bias and\\nmean squared error. We then discuss the construction of confidence intervals\\nand goodness-of-fit tests. The performance of our proposals is evaluated using\\nsimulation methods. Applications to real data are given, as well. The\\nestimation of the hazard rate function based on usual records has been studied\\nin the literature, although many procedures require several samples of records.\\nIn contrast, our approach relies on a single sequence of $\\\\delta$-records,\\nsimplifying the experimental design and increasing the applicability of the\\nmethods. Proper scoring rules have been a subject of growing interest in recent years,\\nnot only as tools for evaluation of probabilistic forecasts but also as methods\\nfor estimating probability distributions. In this article, we review the\\nmathematical foundations of proper scoring rules including general\\ncharacterization results and important families of scoring rules. We discuss\\ntheir role in statistics and machine learning for estimation and forecast\\nevaluation. Furthermore, we comment on interesting developments of their usage\\nin applications. The goal of this paper is to propose a new approach to asymptotic analysis of\\nthe finite predictor for stationary sequences. It produces the exact\\nasymptotics of the relative prediction error and the partial correlation\\ncoefficients. The assumptions are analytic in nature and applicable to\\nprocesses with long range dependence. The ARIMA type process driven by the\\nfractional Gaussian noise (fGn), which previously remained elusive, serves as\\nour study case. Empirical likelihood serves as a powerful tool for constructing confidence\\nintervals in nonparametric regression and regression discontinuity designs\\n(RDD). The original empirical likelihood framework can be naturally extended to\\nthese settings using local linear smoothers, with Wilks\\' theorem holding only\\nwhen an undersmoothed bandwidth is selected. However, the generalization of\\nbias-corrected versions of empirical likelihood under more realistic conditions\\nis non-trivial and has remained an open challenge in the literature. This paper\\nprovides a satisfactory solution by proposing a novel approach, referred to as\\nrobust empirical likelihood, designed for nonparametric regression and RDD. The\\ncore idea is to construct robust weights which simultaneously achieve bias\\ncorrection and account for the additional variability introduced by the\\nestimated bias, thereby enabling valid confidence interval construction without\\nextra estimation steps involved. We demonstrate that the Wilks\\' phenomenon\\nstill holds under weaker conditions in nonparametric regression, sharp and\\nfuzzy RDD settings. Extensive simulation studies confirm the effectiveness of\\nour proposed approach, showing superior performance over existing methods in\\nterms of coverage probabilities and interval lengths. Moreover, the proposed\\nprocedure exhibits robustness to bandwidth selection, making it a flexible and\\nreliable tool for empirical analyses. The practical usefulness is further\\nillustrated through applications to two real datasets. In this paper, we prove exponential tail bounds for canonical (or degenerate)\\n$U$-statistics and $U$-processes under exponential-type tail assumptions on the\\nkernels. Most of the existing results in the relevant literature often assume\\nbounded kernels or obtain sub-optimal tail behavior under unbounded kernels. We\\nobtain sharp rates and optimal tail behavior under sub-Weibull kernel\\nfunctions. Some examples from nonparametric and semiparametric statistics\\nliterature are considered. Multiple works regarding convergence analysis of Markov chains have led to\\nspectral gap decomposition formulas of the form \\\\[ \\\\mathrm{Gap}(S) \\\\geq c_0\\n\\\\left[\\\\inf_z \\\\mathrm{Gap}(Q_z)\\\\right] \\\\mathrm{Gap}(\\\\bar{S}), \\\\] where $c_0$ is\\na constant, $\\\\mathrm{Gap}$ denotes the right spectral gap of a reversible\\nMarkov operator, $S$ is the Markov transition kernel (Mtk) of interest,\\n$\\\\bar{S}$ is an idealized or simplified version of $S$, and $\\\\{Q_z\\\\}$ is a\\ncollection of Mtks characterizing the differences between $S$ and $\\\\bar{S}$.\\n  This type of relationship has been established in various contexts,\\nincluding: 1. decomposition of Markov chains based on a finite cover of the\\nstate space, 2. hybrid Gibbs samplers, and 3. spectral independence and\\nlocalization schemes.\\n  We show that multiple key decomposition results across these domains can be\\nconnected within a unified framework, rooted in a simple sandwich structure of\\n$S$. Within the general framework, we establish new instances of spectral gap\\ndecomposition for hybrid hit-and-run samplers and hybrid data augmentation\\nalgorithms with two intractable conditional distributions. Additionally, we\\nexplore several other properties of the sandwich structure, and derive\\nextensions of the spectral gap decomposition formula. Multiparameter persistent homology is a generalization of classical\\npersistent homology, a central and widely-used methodology from topological\\ndata analysis, which takes into account density estimation and is an effective\\ntool for data analysis in the presence of noise. Similar to its classical\\nsingle-parameter counterpart, however, it is challenging to compute and use in\\npractice due to its complex algebraic construction. In this paper, we study a\\npopular and tractable invariant for multiparameter persistent homology in a\\nstatistical setting: the multiparameter persistence landscape. We derive a\\nfunctional central limit theorem for multiparameter persistence landscapes,\\nfrom which we compute confidence bands, giving rise to one of the first\\nstatistical inference methodologies for multiparameter persistence landscapes.\\nWe provide an implementation of confidence bands and demonstrate their\\napplication in a machine learning task on synthetic data. Real-world networks grow over time; statistical models based on node\\nexchangeability are not appropriate. Instead of constraining the structure of\\nthe \\\\textit{distribution} of edges, we propose that the relevant symmetries\\nrefer to the \\\\textit{causal structure} between them. We first enumerate the 96\\ncausal directed acyclic graph (DAG) models over pairs of nodes (dyad variables)\\nin a growing network with finite ancestral sets that are invariant to node\\ndeletion. We then partition them into 21 classes with ancestral sets that are\\nclosed under node marginalization. Several of these classes are remarkably\\namenable to distributed and asynchronous evaluation. As an example, we\\nhighlight a simple model that exhibits flexible power-law degree distributions\\nand emergent phase transitions in sparsity, which we characterize analytically.\\nWith few parameters and much conditional independence, our proposed framework\\nprovides natural baseline models for causal inference in relational data. We address the problem of nonparametric estimation of the spectral density\\nfor a centered stationary Gaussian time series under local differential privacy\\nconstraints. Specifically, we propose new interactive privacy mechanisms for\\nthree tasks: estimating a single covariance coefficient, estimating the\\nspectral density at a fixed frequency, and estimating the entire spectral\\ndensity function. Our approach achieves faster rates through a two-stage\\nprocess: we apply first the Laplace mechanism to the truncated value and then\\nuse the former privatized sample to gain knowledge on the dependence mechanism\\nin the time series. For spectral densities belonging to H\\\\\"older and Sobolev\\nsmoothness classes, we demonstrate that our estimators improve upon the\\nnon-interactive mechanism of Kroll (2024) for small privacy parameter $\\\\alpha$,\\nsince the pointwise rates depend on $n\\\\alpha^2$ instead of $n\\\\alpha^4$.\\nMoreover, we show that the rate $(n\\\\alpha^4)^{-1}$ is optimal for estimating a\\ncovariance coefficient with non-interactive mechanisms. However, the $L_2$ rate\\nof our interactive estimator is slower than the pointwise rate. We show how to\\nuse these estimators to provide a bona-fide locally differentially private\\ncovariance matrix estimator. In this paper, we derive power guarantees of some sequential tests for\\nbounded mean under general alternatives. We focus on testing procedures using\\nnonnegative supermartingales which are anytime valid and consider alternatives\\nwhich coincide asymptotically with the null (e.g. vanishing mean) while still\\nallowing to reject in finite time. Introducing variance constraints, we show\\nthat the alternative can be broaden while keeping power guarantees for certain\\nsecond-order testing procedures. We also compare different test procedures in\\nmultidimensional setting using characteristics of the rejection times. Finally,\\nwe extend our analysis to other functionals as well as testing and comparing\\nforecasters. Our results are illustrated with numerical simulations including\\nbounded mean testing and comparison of forecasters. Multivariate phase relationships are important to characterize and understand\\nnumerous physical, biological, and chemical systems, from electromagnetic waves\\nto neural oscillations. These systems exhibit complex spatiotemporal dynamics\\nand intricate interdependencies among their constituent elements. While\\nclassical models of multivariate phase relationships, such as the wave equation\\nand Kuramoto model, give theoretical models to describe phenomena, the\\ndevelopment of statistical tools for hypothesis testing and inference for\\nmultivariate phase relationships in complex systems remains limited. This paper\\nintroduces a novel probabilistic modeling framework to characterize\\nmultivariate phase relationships, with wave-like phenomena serving as a key\\nexample. This approach describes spatial patterns and interactions between\\noscillators through a pairwise exponential family distribution. Building upon\\nthe literature of graphical model inference, including methods like Ising\\nmodels, graphical lasso, and interaction screening, this work bridges the gap\\nbetween classical wave dynamics and modern statistical approaches. Efficient\\ninference methods are introduced, leveraging the Chow-Liu algorithm for\\ndirected tree approximations and interaction screening for general graphical\\nmodels. Simulated experiments demonstrate the utility of these methods for\\nuncovering wave properties and sparse interaction structures, highlighting\\ntheir applicability to diverse scientific domains. This framework establishes a\\nnew paradigm for statistical modeling of multivariate phase relationships,\\nproviding a powerful toolset for exploring the complexity of these systems. In survival analysis, the estimation of the proportion of subjects who will\\nnever experience the event of interest, termed the cure rate, has received\\nconsiderable attention recently. Its estimation can be a particularly difficult\\ntask when follow-up is not sufficient, that is when the censoring mechanism has\\na smaller support than the distribution of the target data. In the latter case,\\nnon-parametric estimators were recently proposed using extreme value\\nmethodology, assuming that the distribution of the susceptible population is in\\nthe Fr\\\\\\'echet or Gumbel max-domains of attraction. In this paper, we take the\\nextreme value techniques one step further, to jointly estimate the cure rate\\nand the extreme value index, using probability plotting methodology, and in\\nparticular using the full information contained in the top order statistics. In\\nother words, under sufficient or insufficient follow-up, we reconstruct the\\nimmune proportion. To this end, a Peaks-over-Threshold approach is proposed\\nunder the Gumbel max-domain assumption. Next, the approach is also transferred\\nto more specific models such as Pareto, log-normal and Weibull tail models,\\nallowing to recognize the most important tail characteristics of the\\nsusceptible population. We establish the asymptotic behavior of our estimators\\nunder regularization. Though simulation studies, our estimators are show to\\nrival and often outperform established models, even when purely considering\\ncure rate estimation. Finally, we provide an application of our method to\\nNorwegian birth registry data. Spectral estimation is an important tool in time series analysis, with\\napplications including economics, astronomy, and climatology. The asymptotic\\ntheory for non-parametric estimation is well-known but the development of\\nnon-asymptotic theory is still ongoing. Our recent work obtained the first\\nnon-asymptotic error bounds on the Bartlett and Welch methods for $L$-mixing\\nstochastic processes. The class of $L$-mixing processes contains common models\\nin time series analysis, including autoregressive processes and measurements of\\ngeometrically ergodic Markov chains. Our prior analysis assumes that the\\nprocess has zero mean. While zero-mean assumptions are common, real-world\\ntime-series data often has unknown, non-zero mean. In this work, we derive\\nnon-asymptotic error bounds for both Bartlett and Welch estimators for\\n$L$-mixing time-series data with unknown means. The obtained error bounds are\\nof $O(\\\\frac{1}{\\\\sqrt{k}})$, where $k$ is the number of data segments used in\\nthe algorithm, which are tighter than our previous results under the zero-mean\\nassumption. Exact eigendecomposition of large matrices is very expensive, and it is\\npractically impossible to compute exact eigenvalues. Instead, one may set a\\nmore modest goal of approaching the empirical distribution of the eigenvalues,\\nrecovering the overall shape of the eigenspectrum. Current approaches to\\nspectral estimation typically work with \\\\emph{moments} of the spectral\\ndistribution. These moments are first estimated using Monte Carlo trace\\nestimators, then the estimates are combined to approximate the spectral\\ndensity. In this article we show how \\\\emph{Kirchhoff forests}, which are random\\nforests on graphs, can be used to estimate certain non-linear moments of very\\nlarge graph Laplacians. We show how to combine these moments into an estimate\\nof the spectral density. If the estimate\\'s desired precision isn\\'t too high,\\nour approach paves the way to the estimation of a graph\\'s spectrum in time\\nsublinear in the number of links. In this work, we construct optimal low-rank approximations for the Gaussian\\nposterior distribution in linear Gaussian inverse problems. The parameter space\\nis a separable Hilbert space of possibly infinite dimension, and the data space\\nis assumed to be finite-dimensional. We consider various types of approximation\\nfamilies for the posterior. We first consider approximate posteriors in which\\nthe means vary among a class of either structure-preserving or\\nstructure-ignoring low-rank transformations of the data, and in which the\\nposterior covariance is kept fixed. We give necessary and sufficient conditions\\nfor these approximating posteriors to be equivalent to the exact posterior, for\\nall possible realisations of the data simultaneously. For such approximations,\\nwe measure approximation error with the Kullback-Leibler, R\\\\\\'enyi and Amari\\n$\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, and with the Hellinger distance, all\\naveraged over the data distribution. With these losses, we find the optimal\\napproximations and formulate an equivalent condition for their uniqueness,\\nextending the work in finite dimensions of Spantini et al. (SIAM J. Sci.\\nComput. 2015). We then consider joint approximation of the mean and covariance,\\nby also varying the posterior covariance over the low-rank updates considered\\nin Part I of this work. For the reverse Kullback-Leibler divergence, we show\\nthat the separate optimal approximations of the mean and of the covariance can\\nbe combined to yield an optimal joint approximation of the mean and covariance.\\nIn addition, we interpret the joint approximation with the optimal\\nstructure-ignoring approximate mean in terms of an optimal projector in\\nparameter space. Consider an observation of a multivariate temporal point process $N$ with law\\n$\\\\mathcal P$ on the time interval $[0,T]$. To test the null hypothesis that\\n$\\\\mathcal P$ belongs to a given parametric family, we construct a convergent\\ncompensated counting process to which we apply an innovation martingale\\ntransformation. We prove that the resulting process converges weakly to a\\nstandard Wiener process. Consequently, taking a suitable functional of this\\nprocess yields an asymptotically distribution-free goodness-of-fit test for\\npoint processes. For several standard tests based on the increments of this\\ntransformed process, we establish consistency under alternative hypotheses.\\nFinally, we assess the performance of the proposed testing procedure through a\\nMonte Carlo simulation study and illustrate its practical utility with two\\nreal-data examples. In this paper, in a multivariate setting we derive near optimal rates of\\nconvergence in the minimax sense for estimating partial derivatives of the mean\\nfunction for functional data observed under a fixed synchronous design over\\nH\\\\\"older smoothness classes. We focus on the supremum norm since it corresponds\\nto the visualisation of the estimation error, and is closely related to the\\nconstruction of uniform confidence bands. In contrast to mean function\\nestimation, for derivative estimation the smoothness of the paths of the\\nprocesses is crucial for the rates of convergence. On the one hand, if the\\npaths have higher-order smoothness than the order of the partial derivative to\\nbe estimated, the parametric $\\\\sqrt n$ rate can be achieved under sufficiently\\ndense design. On the other hand, for processes with rough paths of lower-order\\nsmoothness, we show that the rates of convergence are necessarily slower than\\nthe parametric rate, and determine a near-optimal rate at which estimation is\\nstill possible. We implement a multivariate local polynomial derivative\\nestimator and illustrate its finite-sample performance in a simulation as well\\nas for two real-data sets. To assess the smoothness of the sample paths in the\\napplications we further discuss a method based on comparing restricted\\nestimates of the partial derivatives of the covariance kernel. We introduce a new version of the KL-divergence for Gaussian distributions\\nwhich is based on Wasserstein geometry and referred to as WKL-divergence. We\\nshow that this version is consistent with the geometry of the sample space\\n${\\\\Bbb R}^n$. In particular, we can evaluate the WKL-divergence of the Dirac\\nmeasures concentrated in two points which turns out to be proportional to the\\nsquared distance between these points. For linear inverse problems with Gaussian priors and Gaussian observation\\nnoise, the posterior is Gaussian, with mean and covariance determined by the\\nconditioning formula. Using the Feldman-Hajek theorem, we analyse the\\nprior-to-posterior update and its low-rank approximation for\\ninfinite-dimensional Hilbert parameter spaces and finite-dimensional\\nobservations. We show that the posterior distribution differs from the prior on\\na finite-dimensional subspace, and construct low-rank approximations to the\\nposterior covariance, while keeping the mean fixed. Since in infinite\\ndimensions, not all low-rank covariance approximations yield approximate\\nposterior distributions which are equivalent to the posterior and prior\\ndistribution, we characterise the low-rank covariance approximations which do\\nyield this equivalence, and their respective inverses, or `precisions\\'. For\\nsuch approximations, a family of measure approximation problems is solved by\\nidentifying the low-rank approximations which are optimal for various losses\\nsimultaneously. These loss functions include the family of R\\\\\\'enyi divergences,\\nthe Amari $\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, the Hellinger metric and\\nthe Kullback-Leibler divergence. Our results extend those of Spantini et al.\\n(SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide\\ntheoretical underpinning for the construction of low-rank approximations of\\ndiscretised versions of the infinite-dimensional inverse problem, by\\nformulating discretization independent results. Species sampling processes have long served as the framework for studying\\nrandom discrete distributions. However, their statistical applicability is\\nlimited when partial exchangeability is assumed as probabilistic invariance for\\nthe observables. Despite numerous discrete models for partially exchangeable\\nobservations, a unifying framework is currently missing, leaving many questions\\nabout the induced learning mechanisms unanswered in this setting. To fill this\\ngap, we consider the natural extension of species sampling models to a\\nmultivariate framework, obtaining a general class of models characterized by\\ntheir partially exchangeable partition probability function. A notable\\nsubclass, named regular multivariate species sampling models, exists among\\nthese models. In the subclass, dependence across processes is accurately\\ncaptured by the correlation among them: a correlation of one equals full\\nexchangeability and a null correlation corresponds to independence. Regular\\nmultivariate species sampling models encompass discrete processes for partial\\nexchangeable data used in Bayesian models, thereby highlighting their core\\ndistributional properties and providing a means for developing new models. We consider the Wigner matrix $W_{n}$ of dimension $n \\\\times n$ as $n \\\\to\\n\\\\infty$. The objective of this paper is two folds: first we construct an\\noperator $\\\\mathcal{W}$ on a suitable Hilbert space $\\\\mathcal{H}$ and then\\ndefine a suitable notion of convergence such that the matrices $W_{n}$ converge\\nin that notion of convergence to $\\\\mathcal{W}$. We further investigate some\\nproperties of $\\\\mathcal{W}$ and $\\\\mathcal{H}$. We show that $\\\\mathcal{H}$ is a\\nnontrivial extension of $L^{2}[0,1]$ with respect to the Lebesgue measure and\\nthe spectral measure of $\\\\mathcal{W}$ at any function $f \\\\in L^{2}[0,1]$ is\\nalmost surely the semicircular law. Selective prediction, where a model has the option to abstain from making a\\ndecision, is crucial for machine learning applications in which mistakes are\\ncostly. In this work, we focus on distributional regression and introduce a\\nframework that enables the model to abstain from estimation in situations of\\nhigh uncertainty. We refer to this approach as distributional regression with\\nreject option, inspired by similar concepts in classification and regression\\nwith reject option. We study the scenario where the rejection rate is fixed. We\\nderive a closed-form expression for the optimal rule, which relies on\\nthresholding the entropy function of the Continuous Ranked Probability Score\\n(CRPS). We propose a semi-supervised estimation procedure for the optimal rule,\\nusing two datasets: the first, labeled, is used to estimate both the\\nconditional distribution function and the entropy function of the CRPS, while\\nthe second, unlabeled, is employed to calibrate the desired rejection rate.\\nNotably, the control of the rejection rate is distribution-free. Under mild\\nconditions, we show that our procedure is asymptotically as effective as the\\noptimal rule, both in terms of error rate and rejection rate. Additionally, we\\nestablish rates of convergence for our approach based on distributional\\nk-nearest neighbor. A numerical analysis on real-world datasets demonstrates\\nthe strong performance of our procedure Estimating the mode of a unimodal distribution is a classical problem in\\nstatistics. Although there are several approaches for point-estimation of mode\\nin the literature, very little has been explored about the interval-estimation\\nof mode. Our work proposes a collection of novel methods of obtaining finite\\nsample valid confidence set of the mode of a unimodal distribution. We analyze\\nthe behaviour of the width of the proposed confidence sets under some\\nregularity assumptions of the density about the mode and show that the width of\\nthese confidence sets shrink to zero near optimally. Simply put, we show that\\nit is possible to build finite sample valid confidence sets for the mode that\\nshrink to a singleton as sample size increases. We support the theoretical\\nresults by showing the performance of the proposed methods on some synthetic\\ndata-sets. We believe that our confidence sets can be improved both in\\nconstruction and in terms of rate. We study the bias and the mean-squared error of the maximum likelihood\\nestimators (MLE) of parameters associated with a two-parameter mean-reverting\\nprocess for a finite time $T$. Using the likelihood ratio process, we derive\\nthe expressions for MLEs, then compute the bias and the MSE via the change of\\nmeasure and Ito\\'s formula. We apply the derived expressions to the general\\nOrnstein-Uhlenbeck process, where the bias and the MSE are numerically computed\\nthrough a joint moment-generating function of key functionals of the O-U\\nprocess. A numerical study is provided to illustrate the behaviour of bias and\\nthe MSE for the MLE of the mean-reverting speed parameter. In multivariate time series analysis, understanding the underlying causal\\nrelationships among variables is often of interest for various applications.\\nDirected acyclic graphs (DAGs) provide a powerful framework for representing\\ncausal dependencies. This paper proposes a novel Bayesian approach for modeling\\nmultivariate time series where conditional independencies and causal structure\\nare encoded by a DAG. The proposed model allows structural properties such as\\nstationarity to be easily accommodated. Given the application, we further\\nextend the model for matrix-variate time series. We take a Bayesian approach to\\ninference, and a ``projection-posterior\\'\\' based efficient computational\\nalgorithm is developed. The posterior convergence properties of the proposed\\nmethod are established along with two identifiability results for the\\nunrestricted structural equation models. The utility of the proposed method is\\ndemonstrated through simulation studies and real data analysis. We propose to model the records of the maximum Drawdown in capital markets by\\nmeans a Piecewise Deterministic Markov Process (PDMP). We derive statistical\\nresults such as the mean and variance that describes the sequence of maximum\\nDrawdown records. In addition, we developed a simulation study and techniques\\nfor estimating the parameters governing the stochastic process, using a\\npractical example in the capital market to illustrate the procedure. This paper addresses the problem of detecting change points in the spectral\\ndensity of time series, motivated by EEG analysis of seizure patients. Seizures\\ndisrupt coherence and functional connectivity, necessitating precise detection.\\nDeparting from traditional parametric approaches, we utilize the Wold\\ndecomposition, representing general time series as autoregressive processes\\nwith infinite lags, which are truncated and estimated around the change point.\\nOur detection procedure employs an initial estimator that systematically\\nsearches across time points. We examine the localization error and its\\ndependence on time series properties and sample size. To enhance accuracy, we\\nintroduce an optimal rate method with an asymptotic distribution, facilitating\\nthe construction of confidence intervals. The proposed method effectively\\nidentifies seizure onset in EEG data and extends to event detection in video\\ndata. Comprehensive numerical experiments demonstrate its superior performance\\ncompared to existing techniques. Under certain conditions, the largest eigenvalue of a sample covariance\\nmatrix undergoes a well-known phase transition when the sample size $n$ and\\ndata dimension $p$ diverge proportionally. In the subcritical regime, this\\neigenvalue has fluctuations of order $n^{-2/3}$ that can be approximated by a\\nTracy-Widom distribution, while in the supercritical regime, it has\\nfluctuations of order $n^{-1/2}$ that can be approximated with a Gaussian\\ndistribution. However, the statistical problem of determining which regime\\nunderlies a given dataset is far from resolved. We develop a new testing\\nframework and procedure to address this problem. In particular, we demonstrate\\nthat the procedure has an asymptotically controlled level, and that it is power\\nconsistent for certain alternatives. Also, this testing procedure enables the\\ndesign a new bootstrap method for approximating the distributions of\\nfunctionals of the leading sample eigenvalues within the subcritical regime --\\nwhich is the first such method that is supported by theoretical guarantees. We developed a mathematical setup inspired by Buyse\\'s generalized pairwise\\ncomparisons to define a notion of optimal individualized treatment rule (ITR)\\nin the presence of prioritized outcomes in a randomized controlled trial,\\nterming such an ITR pairwise optimal. We present two approaches to estimate\\npairwise optimal ITRs. The first is a variant of the k-nearest neighbors\\nalgorithm. The second is a meta-learner based on a randomized bagging scheme,\\nallowing the use of any classification algorithm for constructing an ITR. We\\nstudy the behavior of these estimation schemes from a theoretical standpoint\\nand through Monte Carlo simulations and illustrate their use on trial data. Significant treatment effects are often emphasized when interpreting and\\nsummarizing empirical findings in studies that estimate multiple, possibly\\nmany, treatment effects. Under this kind of selective reporting, conventional\\ntreatment effect estimates may be biased and their corresponding confidence\\nintervals may undercover the true effect sizes. We propose new estimators and\\nconfidence intervals that provide valid inferences on the effect sizes of the\\nsignificant effects after multiple hypothesis testing. Our methods are based on\\nthe principle of selective conditional inference and complement a wide range of\\ntests, including step-up tests and bootstrap-based step-down tests. Our\\napproach is scalable, allowing us to study an application with over 370\\nestimated effects. We justify our procedure for asymptotically normal treatment\\neffect estimators. We provide two empirical examples that demonstrate bias\\ncorrection and confidence interval adjustments for significant effects. The\\nmagnitude and direction of the bias correction depend on the correlation\\nstructure of the estimated effects and whether the interpretation of the\\nsignificant effects depends on the (in)significance of other effects. We study the consistency and weak convergence of the conditional tail\\nfunction and conditional Hill estimators under broad dependence assumptions for\\na heavy-tailed response sequence and a covariate sequence. Consistency is\\nestablished under $\\\\alpha$-mixing, while asymptotic normality follows from\\n$\\\\beta$-mixing and second-order conditions. A key aspect of our approach is its\\nversatile functional formulation in terms of the conditional tail process.\\nSimulations demonstrate its performance across dependence scenarios. We apply\\nour method to extreme event modeling in the oil industry, revealing distinct\\ntail behaviors under varying conditioning values. The extremal dependence structure of a regularly varying $d$-dimensional\\nrandom vector can be described by its angular measure. The standard\\nnonparametric estimator of this measure is the empirical measure of the\\nobserved angles of the $k$ random vectors with largest norm, for a suitably\\nchosen number $k$. Due to the curse of dimensionality, for moderate or large\\n$d$, this estimator is often inaccurate. If the angular measure is concentrated\\non a vicinity of a lower dimensional subspace, then first projecting the data\\non a lower dimensional subspace obtained by a principal component analysis of\\nthe angles of extreme observations can substantially improve the performance of\\nthe estimator.\\n  We derive the asymptotic behavior of such PCA projections and the resulting\\nexcess risk. In particular, it is shown that, under mild conditions, the excess\\nrisk (as a function of $k$) decreases much faster than it was suggested by\\nempirical risk bounds obtained in \\\\cite{DS21}. Moreover, functional limit\\ntheorems for local empirical processes of the (empirical) reconstruction error\\nof projections uniformly over neighborhoods of the true optimal projection are\\nestablished. Based on these asymptotic results, we propose a data-driven method\\nto select the dimension of the projection space. Finally, the finite sample\\nperformance of resulting estimators is examined in a simulation study. This article presents an improved approximation for the effective degrees of\\nfreedom in the Satterthwaite (1941, 1946) method which estimates the\\ndistribution of a weighted combination of variance components The standard\\nSatterthwaite approximation assumes a scaled chisquare distribution for the\\ncomposite variance estimator but is known to be biased downward when component\\ndegrees of freedom are small. Building on recent work by von Davier (2025) we\\npropose an adjusted estimator that corrects this bias by modifying both the\\nnumerator and denominator of the traditional formula. The new approximation\\nincorporates a weighted average of component degrees of freedom and a scaling\\nfactor that ensures consistency as the number of components or their degrees of\\nfreedom increases. We demonstrate the utility of this adjustment in practical\\nsettings including Rubins (1987) total variance estimation in multiple\\nimputations where weighted variance combinations are common. The proposed\\nestimator generalizes von Daviers (2025) unweighted case and more accurately\\napproximates synthetic variance estimators with arbitrary weights. Given a planar curve, imagine rolling a sphere along that curve without\\nslipping or twisting, and by this means tracing out a curve on the sphere. It\\nis well known that such a rolling operation induces a local isometry between\\nthe sphere and the plane so that the two curves uniquely determine each other,\\nand moreover, the operation extends to a general class of manifolds in any\\ndimension. We use rolling to construct an analogue of a Gaussian process on a\\nmanifold starting from a Euclidean Gaussian process. The resulting model is\\ngenerative, and is amenable to statistical inference given data as curves on a\\nmanifold. We illustrate with examples on the unit sphere, symmetric\\npositive-definite matrices, and with a robotics application involving 3D\\norientations. Most Kalman filters for non-linear systems, such as the unscented Kalman\\nfilter, are based on Gaussian approximations. We use Poincar\\\\\\'e inequalities to\\nbound the Wasserstein distance between the true joint distribution of the\\nprediction and measurement and its Gaussian approximation. The bounds can be\\nused to assess the performance of non-linear Gaussian filters and determine\\nthose filtering approximations that are most likely to induce error. We revisit the discrete argmin inference problem in high-dimensional\\nsettings. Given $n$ observations from a $d$ dimensional vector, the goal is to\\ntest whether the $r$th component of the mean vector is the smallest among all\\ncomponents. We propose dimension-agnostic tests that maintain validity\\nregardless of how $d$ scales with $n$, and regardless of arbitrary ties in the\\nmean vector. Notably, our validity holds under mild moment conditions,\\nrequiring little more than finiteness of a second moment, and permitting\\npossibly strong dependence between coordinates. In addition, we establish the\\nlocal minimax separation rate for this problem, which adapts to the cardinality\\nof a confusion set, and show that the proposed tests attain this rate. Our\\nmethod uses the sample splitting and self-normalization approach of Kim and\\nRamdas (2024). Our tests can be easily inverted to yield confidence sets for\\nthe argmin index. Empirical results illustrate the strong performance of our\\napproach in terms of type I error control and power compared to existing\\nmethods. The Glivenko-Cantelli theorem is a uniform version of the strong law of large\\nnumbers. It states that for every IID sequence of random variables, the\\nempirical measure converges to the underlying distribution (in the sense of\\nuniform convergence of the CDF). In this work, we provide tools to study such\\nlimits of empirical measures in categorical probability.\\n  We propose two axioms, permutation invariance and empirical adequacy, that a\\nmorphism of type $X^\\\\mathbb{N} \\\\to X$ should satisfy to be interpretable as\\ntaking an infinite sequence as input and producing a sample from its empirical\\nmeasure as output. Since not all sequences have a well-defined empirical\\nmeasure, ``such empirical sampling morphisms\\'\\' live in quasi-Markov categories,\\nwhich, unlike Markov categories, allow partial morphisms. Given an empirical\\nsampling morphism and a few other properties, we prove representability as well\\nas abstract versions of the de Finetti theorem, the Glivenko-Cantelli theorem\\nand the strong law of large numbers.\\n  We provide several concrete constructions of empirical sampling morphisms as\\npartially defined Markov kernels on standard Borel spaces. Instantiating our\\nabstract results then recovers the standard Glivenko-Cantelli theorem and the\\nstrong law of large numbers for random variables with finite first moment. Our\\nwork thus provides a joint proof of these two theorems in conjunction with the\\nde Finetti theorem from first principles. In this paper we consider the use of tiered background knowledge within\\nconstraint based causal discovery. Our focus is on settings relaxing causal\\nsufficiency, i.e. allowing for latent variables which may arise because\\nrelevant information could not be measured at all, or not jointly, as in the\\ncase of multiple overlapping datasets. We first present novel insights into the\\nproperties of the \\'tiered FCI\\' (tFCI) algorithm. Building on this, we introduce\\na new extension of the IOD (integrating overlapping datasets) algorithm\\nincorporating tiered background knowledge, the \\'tiered IOD\\' (tIOD) algorithm.\\nWe show that under full usage of the tiered background knowledge tFCI and tIOD\\nare sound, while simple versions of the tIOD and tFCI are sound and complete.\\nWe further show that the tIOD algorithm can often be expected to be\\nconsiderably more efficient and informative than the IOD algorithm even beyond\\nthe obvious restriction of the Markov equivalence classes. We provide a formal\\nresult on the conditions for this gain in efficiency and informativeness. Our\\nresults are accompanied by a series of examples illustrating the exact role and\\nusefulness of tiered background knowledge. Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology\\nthat images the heart at up to 50 frames per second, offering insight into the\\nrespiratory effects on the heartbeat. However, this method significantly\\nincreases the number of images that must be segmented to derive critical health\\nindicators. Although neural networks perform well on inner slices, predictions\\non outer slices are often unreliable.\\n  This work proposes sparse Bayesian learning (SBL) to predict the ventricular\\nvolume on outer slices with minimal manual labeling to address this challenge.\\nThe ventricular volume over time is assumed to be dominated by sparse\\nfrequencies corresponding to the heart and respiratory rates. Moreover, SBL\\nidentifies these sparse frequencies on well-segmented inner slices by\\noptimizing hyperparameters via type -II likelihood, automatically pruning\\nirrelevant components. The identified sparse frequencies guide the selection of\\nouter slice images for labeling, minimizing posterior variance.\\n  This work provides performance guarantees for the greedy algorithm. Testing\\non patient data demonstrates that only a few labeled images are necessary for\\naccurate volume prediction. The labeling procedure effectively avoids selecting\\ninefficient images. Furthermore, the Bayesian approach provides uncertainty\\nestimates, highlighting unreliable predictions (e.g., when choosing suboptimal\\nlabels). We consider the problem of constructing a least conservative estimator of the\\nexpected value $\\\\mu$ of a non-negative heavy-tailed random variable. We require\\nthat the probability of overestimating the expected value $\\\\mu$ is kept\\nappropriately small; a natural requirement if its subsequent use in a decision\\nprocess is anticipated. In this setting, we show it is optimal to estimate\\n$\\\\mu$ by solving a distributionally robust optimization (DRO) problem using the\\nKullback-Leibler (KL) divergence. We further show that the statistical\\nproperties of KL-DRO compare favorably with other estimators based on\\ntruncation, variance regularization, or Wasserstein DRO. Particle filters (PFs) is a class of Monte Carlo algorithms that propagate\\nover time a set of $N\\\\in\\\\mathbb{N}$ particles which can be used to estimate, in\\nan online fashion, the sequence of filtering distributions\\n$(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ defined by a state-space model. Despite the\\npopularity of PFs, the study of the time evolution of their estimates has only\\nreceived very little attention in the literature. Denoting by\\n$(\\\\hat{\\\\eta}_t^N)_{t\\\\geq 1}$ the PF estimate of $(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ and\\nletting $\\\\kappa\\\\in (0,1)$, in this work we first show that for any number of\\nparticles $N$ it holds that, with probability one, we have $\\\\|\\\\hat{\\\\eta}_t^N-\\n\\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa$ for infinitely many $t\\\\geq 1$, with $\\\\|\\\\cdot\\\\|$ a\\nmeasure of distance between probability distributions. Considering a simple\\nfiltering problem we then provide reassuring results concerning the ability of\\nPFs to estimate jointly a finite set $\\\\{\\\\hat{\\\\eta}_t\\\\}_{t=1}^T$ of filtering\\ndistributions by studying\\n$\\\\P(\\\\sup_{t\\\\in\\\\{1,\\\\dots,T\\\\}}\\\\|\\\\hat{\\\\eta}_t^{N}-\\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa)$.\\nFinally, on the same toy filtering problem, we prove that sequential\\nquasi-Monte Carlo, a randomized quasi-Monte Carlo version of PF algorithms,\\noffers greater safety guarantees than PFs in the sense that, for this\\nalgorithm, it holds that $\\\\lim_{N\\\\rightarrow\\\\infty}\\\\sup_{t\\\\geq\\n1}\\\\|\\\\hat{\\\\eta}_t^N-\\\\hat{\\\\eta}_t\\\\|=0$ with probability one. This work deals with the generation of theoretical correlation matrices with\\nspecific sparsity patterns, associated to graph structures. We present a novel\\napproach based on convex optimization, offering greater flexibility compared to\\nexisting techniques, notably by controlling the mean of the entry distribution\\nin the generated correlation matrices. This allows for the generation of\\ncorrelation matrices that better represent realistic data and can be used to\\nbenchmark statistical methods for graph inference. In this paper, we analyze the relative errors in various reliability measures\\ndue to the tacit assumption that the components associated with a $n$-component\\nseries system or a parallel system are independently working where the\\ncomponents are dependent. We use Copula functions in said error analysis. This\\ntechnique generalizes the existing work on error assessment for many wide class\\nof distributions. In this paper, we analyze the relative errors that crop up in the various\\nreliability measures due to the tacit assumption that the components are\\nindependently working associated with a $n$-component series system or a\\nparallel system where the components are dependent and follow a well-defined\\nmultivariate Weibull or exponential distribution. We also list some important\\nobservations which the previous authors have not noted in their earlier works.\\nIn this paper, we focus on the incurred error in multi-component series and\\nparallel systems having multivariate Weibull distributions. In the upcoming\\nsections, we establish that the present study has relevance with stochastic\\norders and statistical dependence which were not previously pointed out by\\nprevious authors. Evaluation is critical to advance decision making across domains, yet\\nexisting methodologies often struggle to balance theoretical rigor and\\npractical scalability. In order to reduce the cost of experimental evaluation,\\nwe introduce a computational theory of evaluation for parameterisable subjects.\\nWe prove upper bounds of generalized evaluation error and generalized causal\\neffect error of evaluation metric on subject. We also prove efficiency, and\\nconsistency to estimated causal effect of subject on metric by prediction. To\\noptimize evaluation models, we propose a meta-learner to handle heterogeneous\\nevaluation subjects space. Comparing with other computational approaches, our\\n(conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12\\nscenes, including individual medicine, scientific simulation, business\\nactivities, and quantum trade. The evaluation time is reduced 3-7 order of\\nmagnitude comparing with experiments or simulations. Variable selection comprises an important step in many modern statistical\\ninference procedures. In the regression setting, when estimators cannot shrink\\nirrelevant signals to zero, covariates without relationships to the response\\noften manifest small but non-zero regression coefficients. The ad hoc procedure\\nof discarding variables whose coefficients are smaller than some threshold is\\noften employed in practice. We formally analyze a version of such thresholding\\nprocedures and develop a simple thresholding method that consistently estimates\\nthe set of relevant variables under mild regularity assumptions. Using this\\nthresholding procedure, we propose a sparse, $\\\\sqrt{n}$-consistent and\\nasymptotically normal estimator whose non-zero elements do not exhibit\\nshrinkage. The performance and applicability of our approach are examined via\\nnumerical studies of simulated and real data. This document is an extended version of an abstract for a talk, with\\napproximately the same title, to be held at the 7th Joint Statistical Meeting\\nof the Deutsche Arbeitsgemeinschaft Statistik, from 24 to 28 March 2025 in\\nBerlin.\\n  Here ``teachable\\'\\' is meant to apply to people ranging from sufficiently\\nadvanced high school pupils to university students in mathematics or\\nstatistics: For understanding most of the proposed approximation results, it\\nshould suffice to know binomial laws, their means and variances, and the\\nstandard normal distribution function (but not necessarily the concept of a\\ncorresponding normal random variable).\\n  Of the proposed approximations, some are well-known (at least to experts),\\nand some are based on teaching experience and research at Trier University. This paper introduces a new kind of periodic fractional autoregressive\\nprocess (PFAR) driven by fractional Gaussian noise (fGn). The new model is a\\nspecialized varying coefficient fractional autoregressive model, where the\\ncoefficients adhere to a periodic structure. In this working, Generalized least\\nsquares estimation and GPH method are employed to construct an initial\\nestimator to estimate the joint estimation of the parameters of these models.\\nThen one-step procedure is used to obtain a more asymptotically-efficient\\nestimator. The paper proves that both estimators are consistent and\\nasymptotically normal, and their performance is demonstrated through a\\nsimulation study using finite-size samples via Monte Carlo simulations.\\nSimulation studies suggests that, while both estimation methods can accurately\\nestimate the model, the one-step estimator outperforms the initial estimator. In this work, we present a theoretical and computational framework for\\nconstructing stochastic transport maps between probability distributions using\\ndiffusion processes. We begin by proving that the time-marginal distribution of\\nthe sum of two independent diffusion processes satisfies a Fokker-Planck\\nequation. Building on this result and applying Ambrosio-Figalli-Trevisan\\'s\\nsuperposition principle, we establish the existence and uniqueness of solutions\\nto the associated stochastic differential equation (SDE). Leveraging these\\ntheoretical foundations, we develop a method to construct (stochastic)\\ntransport maps between arbitrary probability distributions using dynamical\\nordinary differential equations (ODEs) and SDEs. Furthermore, we introduce a\\nunified framework that generalizes and extends a broad class of diffusion-based\\ngenerative models and sampling techniques. Finally, we analyze the convergence\\nproperties of particle approximations for the SDEs underlying our framework,\\nproviding theoretical guarantees for their practical implementation. This work\\nbridges theoretical insights with practical applications, offering new tools\\nfor generative modeling and sampling in high-dimensional spaces. In Learning Theory, the smoothness assumption on the target function (known\\nas source condition) is a key factor in establishing theoretical convergence\\nrates for an estimator. The existing general form of the source condition, as\\ndiscussed in learning theory literature, has traditionally been restricted to a\\nclass of functions that can be expressed as a product of an operator monotone\\nfunction and a Lipschitz continuous function. In this note, we remove these\\nrestrictions on the index function and establish optimal convergence rates for\\nleast-square regression over a Hilbert space with general regularization under\\na general source condition, thereby significantly broadening the scope of\\nexisting theoretical results. Let $P=(x_1,\\\\ldots,x_n)$ be a population consisting of $n\\\\ge 2$ real numbers\\nwhose sum is zero, and let $k <n$ be a positive integer. We sample $k$ elements\\nfrom $P$ without replacement and denote by $X_P$ the sum of the elements in our\\nsample. In this article, using ideas from the theory of majorization, we deduce\\nnon-asymptotic lower and upper bounds on the probability that $X_P$ exceeds its\\nexpected value. We consider nonlinear mixed effects models including high-dimensional\\ncovariates to model individual parameters. The objective is to identify\\nrelevant covariates and estimate model parameters. We combine a penalized\\nLASSO-type estimator with an eBIC model choice criterion to select the\\ncovariates of interest. Then we estimate the parameters by maximum likelihood\\nin the reduced model. We calculate the LASSO-type penalized estimator by a\\nweighted proximal gradient descent algorithm with an adaptive learning rate.\\nThis choice allows us in particular to consider models that do not necessarily\\nbelong to the curved exponential family. We compare first the performance of\\nthe proposed methodology with those of the glmmLasso procedure in a linear\\nmixed effects model in a simulation study. We then illustrate its performance\\nin a nonlinear mixed-effects logistic growth model through simulation. We study the nonparametric maximum likelihood estimator $\\\\widehat{\\\\pi}$ for\\nGaussian location mixtures in one dimension. It has been known since (Lindsay,\\n1983) that given an $n$-point dataset, this estimator always returns a mixture\\nwith at most $n$ components, and more recently (Wu-Polyanskiy, 2020) gave a\\nsharp $O(\\\\log n)$ bound for subgaussian data. In this work we study\\ncomputational aspects of $\\\\widehat{\\\\pi}$. We provide an algorithm which for\\nsmall enough $\\\\varepsilon>0$ computes an $\\\\varepsilon$-approximation of\\n$\\\\widehat\\\\pi$ in Wasserstein distance in time $K+Cnk^2\\\\log\\\\log(1/\\\\varepsilon)$.\\nHere $K$ is data-dependent but independent of $\\\\varepsilon$, while $C$ is an\\nabsolute constant and $k=|supp(\\\\widehat{\\\\pi})|\\\\leq n$ is the number of atoms in\\n$\\\\widehat\\\\pi$. We also certifiably compute the exact value of\\n$|supp(\\\\widehat\\\\pi)|$ in finite time. These guarantees hold almost surely\\nwhenever the dataset $(x_1,\\\\dots,x_n)\\\\in [-cn^{1/4},cn^{1/4}]$ consists of\\nindependent points from a probability distribution with a density (relative to\\nLebesgue measure). We also show the distribution of $\\\\widehat\\\\pi$ conditioned\\nto be $k$-atomic admits a density on the associated $2k-1$ dimensional\\nparameter space for all $k\\\\leq \\\\sqrt{n}/3$, and almost sure locally linear\\nconvergence of the EM algorithm. One key tool is a classical Fourier analytic\\nestimate for non-degenerate curves. Statistical learning methods typically assume that the training and test data\\noriginate from the same distribution, enabling effective risk minimization.\\nHowever, real-world applications frequently involve distributional shifts,\\nleading to poor model generalization. To address this, recent advances in\\ncausal inference and robust learning have introduced strategies such as\\ninvariant causal prediction and anchor regression. While these approaches have\\nbeen explored for traditional structural equation models (SEMs), their\\nextension to functional systems remains limited. This paper develops a risk\\nminimization framework for functional SEMs using linear, potentially unbounded\\noperators. We introduce a functional worst-risk minimization approach, ensuring\\nrobust predictive performance across shifted environments. Our key contribution\\nis a novel worst-risk decomposition theorem, which expresses the maximum\\nout-of-sample risk in terms of observed environments. We establish conditions\\nfor the existence and uniqueness of the worst-risk minimizer and provide\\nconsistent estimation procedures. Empirical results on functional systems\\nillustrate the advantages of our method in mitigating distributional shifts.\\nThese findings contribute to the growing literature on robust functional\\nregression and causal learning, offering practical guarantees for out-of-sample\\ngeneralization in dynamic environments. We investigate the asymptotic behavior of the number of parts $K_n$ in the\\nEwens--Pitman partition model under the regime where the diversity parameter is\\nscaled linearly with the sample size, that is, $\\\\theta = \\\\lambda n$ for\\nsome~$\\\\lambda > 0$. While recent work has established a law of large numbers\\n(LLN) and a central limit theorem (CLT) for $K_n$ in this regime, we revisit\\nthese results through a martingale-based approach. Our method yields\\nsignificantly shorter proofs, and leads to sharper convergence rates in the\\nCLT, including improved Berry--Esseen bounds in the case $\\\\alpha = 0$, and a\\nnew result for the regime $\\\\alpha \\\\in (0,1)$, filling a gap in the literature. At present, in the theory of stochastic process modeling a problem of\\nassessment of reliability and accuracy of stochastic process model in $C(T)$\\nspace wasn\\'t studied for the case of implicit decomposition of process in the\\nform of a series with independent terms. The goal is to study reliability and\\naccuracy in $C(T)$ of models of processes from $Sub_\\\\varphi(\\\\Omega)$ that\\ncannot be decomposed in a series with independent elements explicitly. Using\\nprevious research in the field of modeling of stochastic processes, assumption\\nis considered about possibility of decomposition of a stochastic process in the\\nseries with independent elements that can be found using approximations. Impact\\nof approximation error of process decomposition in series with independent\\nelements on reliability and accuracy of modeling of stochastic process in\\n$C(T)$ is studied. Theorems are proved that allow estimation of reliability and\\naccuracy of a model in $C(T)$ of a stochastic process from\\n$Sub_\\\\varphi(\\\\Omega)$ in the case when decomposition of this process in a\\nseries with independent elements can be found only with some error, for\\nexample, using numerical approximations. Deep neural networks (DNNs) have become powerful tools for modeling complex\\ndata structures through sequentially integrating simple functions in each\\nhidden layer. In survival analysis, recent advances of DNNs primarily focus on\\nenhancing model capabilities, especially in exploring nonlinear covariate\\neffects under right censoring. However, deep learning methods for\\ninterval-censored data, where the unobservable failure time is only known to\\nlie in an interval, remain underexplored and limited to specific data type or\\nmodel. This work proposes a general regression framework for interval-censored\\ndata with a broad class of partially linear transformation models, where key\\ncovariate effects are modeled parametrically while nonlinear effects of\\nnuisance multi-modal covariates are approximated via DNNs, balancing\\ninterpretability and flexibility. We employ sieve maximum likelihood estimation\\nby leveraging monotone splines to approximate the cumulative baseline hazard\\nfunction. To ensure reliable and tractable estimation, we develop an EM\\nalgorithm incorporating stochastic gradient descent. We establish the\\nasymptotic properties of parameter estimators and show that the DNN estimator\\nachieves minimax-optimal convergence. Extensive simulations demonstrate\\nsuperior estimation and prediction accuracy over state-of-the-art methods.\\nApplying our method to the Alzheimer\\'s Disease Neuroimaging Initiative dataset\\nyields novel insights and improved predictive performance compared to\\ntraditional approaches. When prior information is lacking, the go-to strategy for probabilistic\\ninference is to combine a \"default prior\" and the likelihood via Bayes\\'s\\ntheorem. Objective Bayes, (generalized) fiducial inference, etc. fall under\\nthis umbrella. This construction is natural, but the corresponding posterior\\ndistributions generally only offer limited, approximately valid uncertainty\\nquantification. The present paper takes a reimagined approach offering\\nposterior distributions with stronger reliability properties. The proposed\\nconstruction starts with an inferential model (IM), one that takes the\\nmathematical form of a data-driven possibility measure and features exactly\\nvalid uncertainty quantification, and then returns a so-called inner\\nprobabilistic approximation thereof. This inner probabilistic approximation\\ninherits many of the original IM\\'s desirable properties, including credible\\nsets with exact coverage and asymptotic efficiency. The approximation also\\nagrees with the familiar Bayes/fiducial solution obtained in applications where\\nthe model has a group transformation structure. A Monte Carlo method for\\nevaluating the probabilistic approximation is presented, along with numerical\\nillustrations. We formalize the generalization error bound using Rademacher complexity in\\nthe Lean 4 theorem prover. Generalization error quantifies the gap between a\\nlearning machine\\'s performance on given training data versus unseen test data,\\nand Rademacher complexity serves as an estimate of this error based on the\\ncomplexity of learning machines, or hypothesis class. Unlike traditional\\nmethods such as PAC learning and VC dimension, Rademacher complexity is\\napplicable across diverse machine learning scenarios including deep learning\\nand kernel methods. We formalize key concepts and theorems, including the\\nempirical and population Rademacher complexities, and establish generalization\\nerror bounds through formal proofs of McDiarmid\\'s inequality, Hoeffding\\'s\\nlemma, and symmetrization arguments. The problems of detecting and recovering planted structures/subgraphs in\\nErd\\\\H{o}s-R\\\\\\'{e}nyi random graphs, have received significant attention over the\\npast three decades, leading to many exciting results and mathematical\\ntechniques. However, prior work has largely focused on specific ad hoc planted\\nstructures and inferential settings, while a general theory has remained\\nelusive. In this paper, we bridge this gap by investigating the detection of an\\n\\\\emph{arbitrary} planted subgraph $\\\\Gamma = \\\\Gamma_n$ in an Erd\\\\H{o}s-R\\\\\\'{e}nyi\\nrandom graph $\\\\mathcal{G}(n, q_n)$, where the edge probability within $\\\\Gamma$\\nis $p_n$. We examine both the statistical and computational aspects of this\\nproblem and establish the following results. In the dense regime, where the\\nedge probabilities $p_n$ and $q_n$ are fixed, we tightly characterize the\\ninformation-theoretic and computational thresholds for detecting $\\\\Gamma$, and\\nprovide conditions under which a computational-statistical gap arises. Most\\nnotably, these thresholds depend on $\\\\Gamma$ only through its number of edges,\\nmaximum degree, and maximum subgraph density. Our lower and upper bounds are\\ngeneral and apply to any value of $p_n$ and $q_n$ as functions of $n$.\\nAccordingly, we also analyze the sparse regime where $q_n =\\n\\\\Theta(n^{-\\\\alpha})$ and $p_n-q_n =\\\\Theta(q_n)$, with $\\\\alpha\\\\in[0,2]$, as well\\nas the critical regime where $p_n=1-o(1)$ and $q_n = \\\\Theta(n^{-\\\\alpha})$, both\\nof which have been widely studied, for specific choices of $\\\\Gamma$. For these\\nregimes, we show that our bounds are tight for all planted subgraphs\\ninvestigated in the literature thus far\\\\textemdash{}and many more. Finally, we\\nidentify conditions under which detection undergoes sharp phase transition,\\nwhere the boundaries at which algorithms succeed or fail shift abruptly as a\\nfunction of $q_n$. A statistical model is said to be calibrated if the resulting mean estimates\\nperfectly match the true means of the underlying responses. Aiming for\\ncalibration is often not achievable in practice as one has to deal with finite\\nsamples of noisy observations. A weaker notion of calibration is\\nauto-calibration. An auto-calibrated model satisfies that the expected value of\\nthe responses being given the same mean estimate matches this estimate. Testing\\nfor auto-calibration has only been considered recently in the literature and we\\npropose a new approach based on calibration bands. Calibration bands denote a\\nset of lower and upper bounds such that the probability that the true means lie\\nsimultaneously inside those bounds exceeds some given confidence level. Such\\nbands were constructed by Yang-Barber (2019) for sub-Gaussian distributions.\\nDimitriadis et al. (2023) then introduced narrower bands for the Bernoulli\\ndistribution and we use the same idea in order to extend the construction to\\nthe entire exponential dispersion family that contains for example the\\nbinomial, Poisson, negative binomial, gamma and normal distributions. Moreover,\\nwe show that the obtained calibration bands allow us to construct various tests\\nfor calibration and auto-calibration, respectively. Optimal transport theory has become a fundamental tool for handling diverse\\ntypes of data, with growing applications across various fields. However, the\\nWasserstein distance presents significant computational and statistical\\nchallenges in high-dimensional settings. To address these issues, alternative\\ndistances such as the sliced Wasserstein distance, which leverages\\none-dimensional projections, have been introduced. In this work, we establish a\\nnovel central limit theorem for the p-sliced Wasserstein distance, for p>1,\\nusing the Efron-Stein inequality-a technique that has proven effective in\\nrelated problems. This approach yields a central limit theorem centered at the\\nexpected value of the empirical cost, under mild regularity conditions.\\nNotably, unlike the general Wasserstein distance in arbitrary dimensions, we\\ndemonstrate that, under specific assumptions, the centering constants can be\\nreplaced by the population cost, which is essential for statistical inference.\\nThis generalizes and significantly refines existing results for the\\none-dimensional case. Consequently, we present the first asymptotically valid\\ninference framework for the sliced Wasserstein distance applicable to measures\\nthat are not necessarily compactly supported, for p>1. Finally, we address key\\npractical aspects for inference, including Monte Carlo estimation of the\\nintegral and estimation of the asymptotic variance, ensuring applicability in\\nreal-world scenarios. We study the nonconvex optimization landscapes of synchronization problems on\\nspheres. First, we present new results for the statistical problem of\\nsynchronization over the two-element group $\\\\mathbf{Z}_2$. We consider the\\nnonconvex least-squares problem with $\\\\mathbf{Z}_2 = \\\\{\\\\pm 1\\\\}$ relaxed to the\\nunit sphere in $\\\\mathbf{R}^r$ for $r \\\\geq 2$; for several popular models,\\nincluding graph clustering under the binary stochastic block model, we show\\nthat, for any $r \\\\geq 2$, every second-order critical point recovers the ground\\ntruth in the asymptotic regimes where exact recovery is\\ninformation-theoretically possible. Such statistical optimality via spherical\\nrelaxations had previously only been shown for (potentially arbitrarily) larger\\nrelaxation dimension $r$. Second, we consider the global synchronization of\\nnetworks of coupled oscillators under the (homogeneous) Kuramoto model. We\\nprove new and optimal asymptotic results for random signed networks on an\\nErd\\\\H{o}s--R\\\\\\'enyi graph, and we give new and simple proofs for several\\nexisting state-of-the-art results. Our key tool is a deterministic landscape\\ncondition that extends a recent result of Rakoto Endor and Waldspurger. This\\nresult says that, if a certain problem-dependent Laplacian matrix has small\\nenough condition number, the nonconvex landscape is benign. Our extension\\nallows the condition number to include an arbitrary diagonal preconditioner,\\nwhich gives tighter results for many problems. We show that, for the\\nsynchronization of Kuramoto oscillator networks on nearest-neighbor circulant\\ngraphs as studied by Wiley, Strogatz, and Girvan, this condition is optimal. We\\nalso prove a natural complex extension that may be of interest for\\nsynchronization on the special orthogonal group $\\\\operatorname{SO}(2)$. A fundamental challenge in the application of finite mixture models is\\nselecting the number of mixture components, also known as order. Traditional\\napproaches rely on selecting a single best model using information criteria.\\nHowever, in the presence of noisy data, and when models with different orders\\nyield similar fits, model selection uncertainty can be substantial, making it\\nchallenging to confidently identify the true number of components. In this\\npaper, we introduce the Model Selection Confidence Set (MSCS) for order\\nselection - a set-valued estimator that, with a predefined confidence level,\\nincludes the true mixture order across repeated samples. Rather than selecting\\na single model, our MSCS identifies all plausible orders by determining whether\\neach candidate model is at least as plausible as the best-selected one, using a\\nscreening test based on a penalized likelihood ratio statistic. We provide\\ntheoretical guarantees for the asymptotic coverage of our confidence set and\\ndemonstrate its practical advantages through simulations and real data\\nanalysis. Identification of joint dependence among more than two random vectors plays\\nan important role in many statistical applications, where the data may contain\\nsensitive or confidential information. In this paper, we consider the the\\nd-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of\\ndifferential privacy. Given the limiting distribution of the empirical estimate\\nof dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy\\nregime is typically based on permutation and bootstrap. To detect joint\\ndependence in privacy, we propose a dHSIC-based testing procedure by employing\\na differentially private permutation methodology. Our method enjoys privacy\\nguarantee, valid level and pointwise consistency, while the bootstrap\\ncounterpart suffers inconsistent power. We further investigate the uniform\\npower of the proposed test in dHSIC metric and $L_2$ metric, indicating that\\nthe proposed test attains the minimax optimal power across different privacy\\nregimes. As a byproduct, our results also contain the pointwise and uniform\\npower of the non-private permutation dHSIC, addressing an unsolved question\\nremained in Pfister et al. (2018). We introduce a new compositional framework for generalized variational\\ninference, clarifying the different parts of a model, how they interact, and\\nhow they compose. We explain that both exact Bayesian inference and the loss\\nfunctions typical of variational inference (such as variational free energy and\\nits generalizations) satisfy chain rules akin to that of reverse-mode automatic\\ndifferentiation, and we advocate for exploiting this to build and optimize\\nmodels accordingly. To this end, we construct a series of compositional tools:\\nfor building models; for constructing their inversions; for attaching local\\nloss functions; and for exposing parameters. Finally, we explain how the\\nresulting parameterized statistical games may be optimized locally, too. We\\nillustrate our framework with a number of classic examples, pointing to new\\nareas of extensibility that are revealed. The quantification of treatment effects plays an important role in a wide\\nrange of applications, including policy making and bio-pharmaceutical research.\\nIn this article, we study the quantile treatment effect (QTE) while addressing\\ntwo specific types of heterogeneities: (a) personalized heterogeneity, which\\ncaptures the varying treatment effects for different individuals, and (b)\\nquantile heterogeneity, which accounts for how the impact of covariates varies\\nacross different quantile levels. A well-designed debiased estimator for the\\nindividualized quantile treatment effect (IQTE) is proposed to capture such\\nheterogeneities effectively. We show that this estimator converges weakly to a\\nGaussian process as a function of the quantile levels and propose valid\\nstatistical inference methods, including the construction of confidence\\nintervals and the development of hypothesis testing decision rules. In\\naddition, the minimax optimality frameworks for these inference procedures are\\nestablished. Specifically, we derive the minimax optimal rates for the expected\\nlength of confidence intervals and the magnitude of the detection boundary for\\nhypothesis testing procedures, illustrating the superiority of the proposed\\nestimator. The effectiveness of our methods is demonstrated through extensive\\nsimulations and an analysis of the National Health and Nutrition Examination\\nSurvey (NHANES) datasets. We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave\\nequation on $\\\\mathbb R_+$ with zero initial conditions is excited with a\\nNeumann boundary data modelled as a white noise process. Given also the\\nDirichlet data at the same point, determine the unknown first order coefficient\\nfunction of the system.\\n  We first establish that direct problem is well-posed. The inverse problem is\\nthen solved by showing that correlations of the boundary data determine the\\nNeumann-to-Dirichlet operator in the sense of distributions, which is known to\\nuniquely identify the coefficient. This approach has applications in acoustic\\nmeasurements of internal cross-sections of fluid pipes such as pressurised\\nwater supply pipes and vocal tract shape determination. This paper introduces a quasi-likelihood ratio testing procedure for\\ndiffusion processes observed under nonsynchronous sampling schemes.\\nHigh-frequency data, particularly in financial econometrics, are often recorded\\nat irregular time points, challenging conventional synchronous methods for\\nparameter estimation and hypothesis testing. To address these challenges, we\\ndevelop a quasi-likelihood framework that accommodates irregular sampling while\\nintegrating adaptive estimation techniques for both drift and diffusion\\ncoefficients, thereby enhancing optimization stability and reducing\\ncomputational burden. We rigorously derive the asymptotic properties of the\\nproposed test statistic, showing that it converges to a chi-squared\\ndistribution under the null hypothesis and exhibits consistency under\\nalternatives. Moreover, we establish that the resulting tests are\\nasymptotically uniformly most powerful. Extensive numerical experiments\\ncorroborate the theoretical findings and demonstrate that our method\\noutperforms existing nonparametric approaches. Deviations from Bayesian updating are traditionally categorized as biases,\\nerrors, or fallacies, thus implying their inherent ``sub-optimality.\\'\\' We offer\\na more nuanced view. We demonstrate that, in learning problems with\\nmisspecified models, non-Bayesian updating can outperform Bayesian updating. We consider a quotient of a complete Riemannian manifold modulo an\\nisometrically and properly acting Lie group and lifts of the quotient to the\\nmanifolds in optimal position to a reference point on the manifold. With\\nrespect to the pushed forward Riemannian volume onto the quotient we derive\\ncontinuity and uniqueness a.e. and smoothness to large extents also with\\nrespect to the reference point. In consequence we derive a general manifold\\nstability theorem: the Fr\\\\\\'echet mean lies in the highest dimensional stratum\\nassumed with positive probability, and a strong law for optimal lifts. This\\nallows to define new two-sample tests utilizing individual optimal lifts which\\noutperform existing two-sample tests on simulated data. They also outperform\\nexisting tests on a newly derived reverse labeling reflection shape space, that\\nis used to model filament data of microtubules within cells in a biological\\napplication. Topic modeling is traditionally applied to word counts without accounting for\\nthe context in which words appear. Recent advancements in large language models\\n(LLMs) offer contextualized word embeddings, which capture deeper meaning and\\nrelationships between words. We aim to leverage such embeddings to improve\\ntopic modeling.\\n  We use a pre-trained LLM to convert each document into a sequence of word\\nembeddings. This sequence is then modeled as a Poisson point process, with its\\nintensity measure expressed as a convex combination of $K$ base measures, each\\ncorresponding to a topic. To estimate these topics, we propose a flexible\\nalgorithm that integrates traditional topic modeling methods, enhanced by\\nnet-rounding applied before and kernel smoothing applied after. One advantage\\nof this framework is that it treats the LLM as a black box, requiring no\\nfine-tuning of its parameters. Another advantage is its ability to seamlessly\\nintegrate any traditional topic modeling approach as a plug-in module, without\\nthe need for modifications\\n  Assuming each topic is a $\\\\beta$-H\\\\\"{o}lder smooth intensity measure on the\\nembedded space, we establish the rate of convergence of our method. We also\\nprovide a minimax lower bound and show that the rate of our method matches with\\nthe lower bound when $\\\\beta\\\\leq 1$. Additionally, we apply our method to\\nseveral datasets, providing evidence that it offers an advantage over\\ntraditional topic modeling approaches. We construct a new tail bound for the sum of independent random variables for\\nsituations in which the expected value of the sum is known and each random\\nvariable lies within a specified interval, which may be different for each\\nvariable. This new bound can be computed by solving a two-dimensional convex\\noptimization problem. Simulations demonstrate that the new bound is often\\nsubstantially tighter than Hoeffding\\'s inequality for cases in which both\\nbounds are applicable. Contrastive learning -- a modern approach to extract useful representations\\nfrom unlabeled data by training models to distinguish similar samples from\\ndissimilar ones -- has driven significant progress in foundation models. In\\nthis work, we develop a new theoretical framework for analyzing data\\naugmentation-based contrastive learning, with a focus on SimCLR as a\\nrepresentative example. Our approach is based on the concept of\\n\\\\emph{approximate sufficient statistics}, which we extend beyond its original\\ndefinition in \\\\cite{oko2025statistical} for contrastive language-image\\npretraining (CLIP) using KL-divergence. We generalize it to equivalent forms\\nand general f-divergences, and show that minimizing SimCLR and other\\ncontrastive losses yields encoders that are approximately sufficient.\\nFurthermore, we demonstrate that these near-sufficient encoders can be\\neffectively adapted to downstream regression and classification tasks, with\\nperformance depending on their sufficiency and the error induced by data\\naugmentation in contrastive learning. Concrete examples in linear regression\\nand topic classification are provided to illustrate the broad applicability of\\nour results. In this paper, models that approximate stochastic processes from the space\\n$Sub_\\\\varphi(\\\\Omega)$ with given reliability and accuracy in $L_p(T)$ are\\nconsidered for some specific functions $\\\\varphi(t)$. For processes that are\\ndecomposited in series using orthonormal bases, such models are constructed in\\nthe case where elements of such decomposition cannot be found explicitly. The Generalized Mallows Model (GMM) is a well known family of models for\\nranking data. A GMM is a distribution over $\\\\mathbb{S}_n$, the set of\\npermutations of n objects, characterized by a location parameter $\\\\sigma \\\\in\\n\\\\mathbb{S}_n$, known as central permutation and a set of dispersion parameters\\n$\\\\theta_{1:n-1}\\\\in(0,1]$. The GMM shares many properties, such as having\\nsufficient statistics, with exponential models, thus it can be seen as an\\nexponential family with a discrete parameter $\\\\sigma$. This paper shows that\\ncomputing entropy, crossentropy and Kullback-Leibler divergence in the the\\nclass of GMM is tractable, paving the way for a better understanding of this\\nexponential family. Knowledge distillation is a technique used to train a small student network\\nusing the output generated by a large teacher network, and has many empirical\\nadvantages~\\\\citep{Hinton2015DistillingTK}. While the standard one-shot approach\\nto distillation only uses the output of the final teacher network, recent\\nwork~\\\\citep{panigrahi2024progressive} has shown that using intermediate\\ncheckpoints from the teacher\\'s training process as an implicit ``curriculum\\'\\'\\nfor progressive distillation can significantly speed up training. However, such\\nschemes require storing these checkpoints, and often require careful selection\\nof the intermediate checkpoints to train on, which can be impractical for\\nlarge-scale training.\\n  In this paper, we show that a curriculum can be \\\\emph{extracted} from just\\nthe fully trained teacher network, and that this extracted curriculum can give\\nsimilar efficiency benefits to those of progressive distillation. Our\\nextraction scheme is natural; we use a random projection of the hidden\\nrepresentations of the teacher network to progressively train the student\\nnetwork, before training using the output of the full network. We show that our\\nscheme significantly outperforms one-shot distillation and achieves a\\nperformance similar to that of progressive distillation for learning sparse\\nparities with two-layer networks, and provide theoretical guarantees for this\\nsetting. Additionally, we show that our method outperforms one-shot\\ndistillation even when using transformer-based architectures, both for\\nsparse-parity learning, and language modeling tasks. We extend the celebrated Glivenko-Cantelli theorem, sometimes called the\\nfundamental theorem of statistics, from its standard setting of total variation\\ndistance to all $f$-divergences. A key obstacle in this endeavor is to define\\n$f$-divergence on a subcollection of a $\\\\sigma$-algebra that forms a\\n$\\\\pi$-system but not a $\\\\sigma$-subalgebra. This is a side contribution of our\\nwork. We will show that this notion of $f$-divergence on the $\\\\pi$-system of\\nrays preserves nearly all known properties of standard $f$-divergence, yields a\\nnovel integral representation of the Kolmogorov-Smirnov distance, and has a\\nGlivenko-Cantelli theorem. We will also discuss the prospects of a\\nVapnik-Chervonenkis theory for $f$-divergence. We investigate differentially private estimators for individual parameters\\nwithin larger parametric models. While generic private estimators exist, the\\nestimators we provide repose on new local notions of estimand stability, and\\nthese notions allow procedures that provide private certificates of their own\\nstability. By leveraging these private certificates, we provide computationally\\nand statistical efficient mechanisms that release private statistics that are,\\nat least asymptotically in the sample size, essentially unimprovable: they\\nachieve instance optimal bounds. Additionally, we investigate the practicality\\nof the algorithms both in simulated data and in real-world data from the\\nAmerican Community Survey and US Census, highlighting scenarios in which the\\nnew procedures are successful and identifying areas for future work. The Friedman test has been extensively applied as a nonparametric alternative\\nto the conventional F procedure for comparing treatment effects in randomized\\ncomplete block designs. A chi-square distribution provides a convenient\\napproximation to determining the critical values for the Friedman procedure in\\nhypothesis testing. However, the chi-square approximation is generally\\nconservative and the accuracy declines with increasing number of treatments.\\nThis paper describes an alternative transformation of the Friedman statistic\\nalong with an approximate F distribution that has the same numerator degrees of\\nfreedom as the ANOVA F test. Moreover, two approximate noncentral F\\ndistributions are presented for the proposed F-transformation under the\\nalternative hypothesis of heterogeneous location shifts. Explicit power\\nfunctions are derived when the underlying populations have the uniform, normal,\\nLaplace, and exponential distributions. Theoretical examination and empirical\\nassessment are presented to validate the advantages of the proposed approaches\\nover the existing methods of the Friedman test. The developed test and power\\nprocedures are recommended due to their consistently acceptable Type I error\\nrates and accurate power calculations for the location shift structures and\\npopulation distributions considered here. Nearly all identifiability results in unsupervised representation learning\\ninspired by, e.g., independent component analysis, factor analysis, and causal\\nrepresentation learning, rely on assumptions of additive independent noise or\\nnoiseless regimes. In contrast, we study the more general case where noise can\\ntake arbitrary forms, depend on latent variables, and be non-invertibly\\nentangled within a nonlinear function. We propose a general framework for\\nidentifying latent variables in the nonparametric noisy settings. We first show\\nthat, under suitable conditions, the generative model is identifiable up to\\ncertain submanifold indeterminacies even in the presence of non-negligible\\nnoise. Furthermore, under the structural or distributional variability\\nconditions, we prove that latent variables of the general nonlinear models are\\nidentifiable up to trivial indeterminacies. Based on the proposed theoretical\\nframework, we have also developed corresponding estimation methods and\\nvalidated them in various synthetic and real-world settings. Interestingly, our\\nestimate of the true GDP growth from alternative measurements suggests more\\ninsightful information on the economies than official reports. We expect our\\nframework to provide new insight into how both researchers and practitioners\\ndeal with latent variables in real-world scenarios. We consider price competition among multiple sellers over a selling horizon\\nof $T$ periods. In each period, sellers simultaneously offer their prices and\\nsubsequently observe their respective demand that is unobservable to\\ncompetitors. The demand function for each seller depends on all sellers\\' prices\\nthrough a private, unknown, and nonlinear relationship. To address this\\nchallenge, we propose a semi-parametric least-squares estimation of the\\nnonlinear mean function, which does not require sellers to communicate demand\\ninformation. We show that when all sellers employ our policy, their prices\\nconverge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers\\nwould reach if they were fully informed. Each seller incurs a regret of\\n$O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution\\nof our work is proving the existence of equilibrium under shape-constrained\\ndemand functions via the concept of $s$-concavity and establishing regret\\nbounds of our proposed policy. Technically, we also establish new concentration\\nresults for the least squares estimator under shape constraints. Our findings\\noffer significant insights into dynamic competition-aware pricing and\\ncontribute to the broader study of non-parametric learning in strategic\\ndecision-making. We consider estimating the proportion of random variables for two types of\\ncomposite null hypotheses: (i) the means or medians of the random variables\\nbelonging to a non-empty, bounded interval; (ii) the means or medians of the\\nrandom variables belonging to an unbounded interval that is not the whole real\\nline. For each type of composite null hypotheses, uniformly consistent\\nestimators of the proportion of false null hypotheses are constructed for\\nrandom variables whose distributions are members of a Type I location-shift\\nfamily. Further, uniformly consistent estimators of certain functions of a\\nbounded null on the means or medians are provided for the random variables\\nmentioned earlier; these functions are continuous and of bounded variation. The\\nestimators are constructed via solutions to Lebesgue-Stieltjes integral\\nequations and harmonic analysis, do not rely on a concept of p-value, and have\\nvarious applications. A key trait of stochastic optimizers is that multiple runs of the same\\noptimizer in attempting to solve the same problem can produce different\\nresults. As a result, their performance is evaluated over several repeats, or\\nruns, on the problem. However, the accuracy of the estimated performance\\nmetrics depends on the number of runs and should be studied using statistical\\ntools. We present a statistical analysis of the common metrics, and develop\\nguidelines for experiment design to measure the optimizer\\'s performance using\\nthese metrics to a high level of confidence and accuracy. To this end, we first\\ndiscuss the confidence interval of the metrics and how they are related to the\\nnumber of runs of an experiment. We then derive a lower bound on the number of\\nrepeats in order to guarantee achieving a given accuracy in the metrics. Using\\nthis bound, we propose an algorithm to adaptively adjust the number of repeats\\nneeded to ensure the accuracy of the evaluated metric. Our simulation results\\ndemonstrate the utility of our analysis and how it allows us to conduct\\nreliable benchmarking as well as hyperparameter tuning and prevent us from\\ndrawing premature conclusions regarding the performance of stochastic\\noptimizers. Estimating the state of a dynamical system from partial and noisy\\nobservations is a ubiquitous problem in a large number of applications, such as\\nprobabilistic weather forecasting and prediction of epidemics. Particle filters\\nare a widely adopted approach to the problem and provide provably accurate\\napproximations of the statistics of the state, but they perform poorly in high\\ndimensions because of weight collapse. The ensemble Kalman filter does not\\nsuffer from this issue, as it relies on an interacting particle system with\\nequal weights. Despite its wide adoption in the geophysical sciences,\\nmathematical analysis of the accuracy of this filter is predominantly confined\\nto the setting of linear dynamical models and linear observations operators,\\nand analysis beyond the linear Gaussian setting is still in its infancy. In\\nthis short note, we provide an accessible overview of recent work in which the\\nauthors take first steps to analyze the accuracy of the filter beyond the\\nlinear Gaussian setting. By formulating the inverse problem of partial differential equations (PDEs)\\nas a statistical inference problem, the Bayesian approach provides a general\\nframework for quantifying uncertainties. In the inverse problem of PDEs,\\nparameters are defined on an infinite-dimensional function space, and the PDEs\\ninduce a computationally intensive likelihood function. Additionally, sparse\\ndata tends to lead to a multi-modal posterior. These features make it difficult\\nto apply existing sequential Monte Carlo (SMC) algorithms. To overcome these\\ndifficulties, we propose new conditions for the likelihood functions, construct\\na Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and\\ndemonstrate the universal approximation property of the infinite-dimensional\\nGaussian mixture probability measure. By combining these three novel tools, we\\npropose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a\\nconvergence theorem that allows Gaussian priors, illustrating that the\\nsequential particle filter actually reproduces the true posterior distribution.\\nFurthermore, the proposed new algorithm is rigorously defined on the\\ninfinite-dimensional function space, naturally exhibiting the\\ndiscretization-invariant property. Numerical experiments demonstrate that the\\nnew approach has a strong ability to probe the multi-modality of the posterior,\\nsignificantly reduces the computational burden, and numerically exhibits the\\ndiscretization-invariant property (important for large-scale problems). In this paper, we consider the reproducing property in Reproducing Kernel\\nHilbert Spaces (RKHS). We establish a reproducing property for the closure of\\nthe class of combinations of composition operators under minimal conditions.\\nThis allows to revisit the sufficient conditions for the reproducing property\\nto hold for the derivative operator, as well as for the existence of the mean\\nembedding function. These results provide a framework of application of the\\nrepresenter theorem for regularized learning algorithms that involve data for\\nfunction values, gradients, or any other operator from the considered class. We consider a borderline case: the central limit theorem for a strictly\\nstationary time series with infinite variance but a Gaussian limit. In the iid\\ncase a well-known sufficient condition for this central limit theorem is\\nregular variation of the marginal distribution with tail index $\\\\alpha=2$. In\\nthe dependent case we assume the stronger condition of sequential regular\\nvariation of the time series with tail index $\\\\alpha=2$. We assume that a\\nsample of size $n$ from this time series can be split into $k_n$ blocks of size\\n$r_n\\\\to\\\\infty$ such that $r_n/n\\\\to 0$ as $n\\\\to\\\\infty$ and that the block sums\\nare asymptotically independent. Then we apply classical central limit theory\\nfor row-wise iid triangular arrays. The necessary and sufficient conditions for\\nsuch independent block sums will be verified by using large deviation results\\nfor the time series. We derive the central limit theorem for $m$-dependent\\nsequences, linear processes, stochastic volatility processes and solutions to\\naffine stochastic recurrence equations whose marginal distributions have\\ninfinite variance and are regularly varying with tail index $\\\\alpha=2$. Given an arbitrary subgraph $H=H_n$ and $p=p_n \\\\in (0,1)$, the planted\\nsubgraph model is defined as follows. A statistician observes the union a\\nrandom copy $H^*$ of $H$, together with random noise in the form of an instance\\nof an Erdos-Renyi graph $G(n,p)$. Their goal is to recover the planted $H^*$\\nfrom the observed graph. Our focus in this work is to understand the minimum\\nmean squared error (MMSE) for sufficiently large $n$.\\n  A recent paper [MNSSZ23] characterizes the graphs for which the limiting MMSE\\ncurve undergoes a sharp phase transition from $0$ to $1$ as $p$ increases, a\\nbehavior known as the all-or-nothing phenomenon, up to a mild density\\nassumption on $H$. In this paper, we provide a formula for the limiting MMSE\\ncurve for any graph $H=H_n$, up to the same mild density assumption. This curve\\nis expressed in terms of a variational formula over pairs of subgraphs of $H$,\\nand is inspired by the celebrated subgraph expectation thresholds from the\\nprobabilistic combinatorics literature [KK07]. Furthermore, we give a\\npolynomial-time description of the optimizers of this variational problem. This\\nallows one to efficiently approximately compute the MMSE curve for any dense\\ngraph $H$ when $n$ is large enough. The proof relies on a novel graph\\ndecomposition of $H$ as well as a new minimax theorem which may be of\\nindependent interest.\\n  Our results generalize to the setting of minimax rates of recovering\\narbitrary monotone boolean properties planted in random noise, where the\\nstatistician observes the union of a planted minimal element $A \\\\subseteq [N]$\\nof a monotone property and a random $Ber(p)^{\\\\otimes N}$ vector. In this\\nsetting, we provide a variational formula inspired by the so-called\\n\"fractional\" expectation threshold [Tal10], again describing the MMSE curve (in\\nthis case up to a multiplicative constant) for large enough $n$. Undirected graphical models are a widely used class of probabilistic models\\nin machine learning that capture prior knowledge or putative pairwise\\ninteractions between variables. Those interactions are encoded in a graph for\\npairwise interactions; however, generalizations such as factor graphs account\\nfor higher-degree interactions using hypergraphs. Inference on such models,\\nwhich is performed by conditioning on some observed variables, is typically\\ndone approximately by optimizing a free energy, which is an instance of\\nvariational inference. The Belief Propagation algorithm is a dynamic\\nprogramming algorithm that finds critical points of that free energy. Recent\\nefforts have been made to unify and extend inference on graphical models and\\nfactor graphs to more expressive probabilistic models. A synthesis of these\\nworks shows that inference on graphical models, factor graphs, and their\\ngeneralizations relies on the introduction of presheaves and associated\\ninvariants (homology and cohomology groups).We propose to study the impact of\\nthe transformation of the presheaves onto the associated message passing\\nalgorithms. We show that natural transformations between presheaves associated\\nwith graphical models and their generalizations, which can be understood as\\ncoherent binning of the set of values of the variables, induce morphisms\\nbetween associated message-passing algorithms. It is, to our knowledge, the\\nfirst result on functoriality of the Loopy Belief Propagation. Cross-validation is a statistical tool that can be used to improve large\\ncovariance matrix estimation. Although its efficiency is observed in practical\\napplications, the theoretical reasons behind it remain largely intuitive, with\\nformal proofs currently lacking. To carry on analytical analysis, we focus on\\nthe holdout method, a single iteration of cross-validation, rather than the\\ntraditional $k$-fold approach. We derive a closed-form expression for the\\nestimation error when the population matrix follows a white inverse Wishart\\ndistribution, and we observe the optimal train-test split scales as the square\\nroot of the matrix dimension. For general population matrices, we connected the\\nerror to the variance of eigenvalues distribution, but approximations are\\nnecessary. Interestingly, in the high-dimensional asymptotic regime, both the\\nholdout and $k$-fold cross-validation methods converge to the optimal estimator\\nwhen the train-test ratio scales with the square root of the matrix dimension. Nonlinear Bayesian update for a prior ensemble is proposed to extend\\ntraditional ensemble Kalman filtering to settings characterized by non-Gaussian\\npriors and nonlinear measurement operators. In this framework, the observed\\ncomponent is first denoised via a standard Kalman update, while the unobserved\\ncomponent is estimated using a nonlinear regression approach based on kernel\\ndensity estimation. The method incorporates a subsampling strategy to ensure\\nstability and, when necessary, employs unsupervised clustering to refine the\\nconditional estimate. Numerical experiments on Lorenz systems and a\\nPDE-constrained inverse problem illustrate that the proposed nonlinear update\\ncan reduce estimation errors compared to standard linear updates, especially in\\nhighly nonlinear scenarios. This study introduces a dynamic investment framework to enhance portfolio\\nmanagement in volatile markets, offering clear advantages over traditional\\nstatic strategies. Evaluates four conventional approaches : equal weighted,\\nminimum variance, maximum diversification, and equal risk contribution under\\ndynamic conditions. Using K means clustering, the market is segmented into ten\\nvolatility-based states, with transitions forecasted by a Bayesian Markov\\nswitching model employing Dirichlet priors and Gibbs sampling. This enables\\nreal-time asset allocation adjustments. Tested across two asset sets, the\\ndynamic portfolio consistently achieves significantly higher risk-adjusted\\nreturns and substantially higher total returns, outperforming most static\\nmethods. By integrating classical optimization with machine learning and\\nBayesian techniques, this research provides a robust strategy for optimizing\\ninvestment outcomes in unpredictable market environments. Although the specification of bivariate probability models using a collection\\nof assumed conditional distributions is not a novel concept, it has received\\nconsiderable attention in the last decade. In this study, a bivariate\\ndistribution-the bivariate Poisson-Gamma conditional distribution-is\\nintroduced, combining both univariate continuous and discrete distributions.\\nThis work explores aspects of this model\\'s structure and statistical inference\\nthat have not been studied before. This paper contributes to the field of\\nstatistical modeling and distribution theory through the use of maximum\\nlikelihood estimation, along with simulations and analyses of real data. The recent paper \\\\cite{GSZ2023} on estimation and inference for top-ranking\\nproblem in Bradley-Terry-Lice (BTL) model presented a surprising result:\\ncomponentwise estimation and inference can be done under much weaker conditions\\non the number of comparison then it is required for the full dimensional\\nestimation. The present paper revisits this finding from completely different\\nviewpoint. Namely, we show how a theoretical study of estimation in sup-norm\\ncan be reduced to the analysis of plug-in semiparametric estimation. For the\\nlatter, we adopt and extend the general approach from \\\\cite{Sp2024} for\\nhigh-dimensional estimation. The main tool of the analysis is a theory of\\nperturbed marginal optimization when an objective function depends on a\\nlow-dimensional target parameter along with a high-dimensional nuisance\\nparameter. A particular focus of the study is the critical dimension condition.\\nFull-dimensional estimation requires in general the condition \\\\( \\\\mathbbmsl{N}\\n\\\\gg \\\\mathbb{p} \\\\) between the effective parameter dimension \\\\( \\\\mathbb{p} \\\\)\\nand the effective sample size \\\\( \\\\mathbbmsl{N} \\\\) corresponding to the smallest\\neigenvalue of the Fisher information matrix \\\\( \\\\mathbbmsl{F} \\\\). Inference on\\nthe estimated parameter is even more demanding: the condition \\\\( \\\\mathbbmsl{N}\\n\\\\gg \\\\mathbb{p}^{2} \\\\) cannot be generally avoided; see \\\\cite{Sp2024}. However,\\nfor the sup-norm estimation, the critical dimension condition can be reduced to\\n\\\\( \\\\mathbbmsl{N} \\\\geq \\\\CONST \\\\log(\\\\dimp) \\\\). Compared to \\\\cite{GSZ2023}, the\\nproposed approach works for the classical MLE and does not require any\\nresampling procedure, applies to more general structure of the comparison\\ngraph, and yields more accurate expansions for each component of the parameter\\nvector. This paper investigates the asymptotic properties of least absolute deviation\\n(LAD) regression for linear models with polynomial regressors, highlighting its\\nrobustness against heavy-tailed noise and outliers. Assuming independent and\\nidentically distributed (i.i.d.) errors, we establish the multiscale asymptotic\\nnormality of LAD estimators. A central result is the derivation of the\\nasymptotic precision matrix, shown to be proportional to Hilbert matrices, with\\nthe proportionality coefficient depending on the asymptotic variance of the\\nsample median of the noise distribution. We further explore the estimator\\'s\\nconvergence properties, both in probability and almost surely, under varying\\nmodel specifications. Through comprehensive simulations, we evaluate the speed\\nof convergence of the LAD estimator and the empirical coverage probabilities of\\nconfidence intervals constructed under different scaling factors (T 1/2 and T\\n$\\\\alpha$ ). These experiments incorporate a range of noise distributions,\\nincluding Laplace, Gaussian, and Cauchy, to demonstrate the estimator\\'s\\nrobustness and efficiency. The findings underscore the versatility and\\npractical relevance of LAD regression in handling non-standard data\\nenvironments. By connecting the statistical properties of LAD estimators to\\nclassical mathematical structures, such as Hilbert matrices, this study offers\\nboth theoretical insights and practical tools for robust statistical modeling. This work extends local linear regression to Banach space-valued time series\\nfor estimating smoothly varying means and their derivatives in non-stationary\\ndata. The asymptotic properties of both the standard and bias-reduced Jackknife\\nestimators are analyzed under mild moment conditions, establishing their\\nconvergence rates. Simulation studies assess the finite sample performance of\\nthese estimators and compare them with the Nadaraya-Watson estimator.\\nAdditionally, the proposed methods are applied to smooth EEG recordings for\\nreconstructing eye movements and to video analysis for detecting pedestrians\\nand abandoned objects. We consider diffusion of independent molecules in an insulated Euclidean\\ndomain with unknown diffusivity parameter. At a random time and position, the\\nmolecules may bind and stop diffusing in dependence of a given `binding\\npotential\\'. The binding process can be modeled by an additive random functional\\ncorresponding to the canonical construction of a `killed\\' diffusion Markov\\nprocess. We study the problem of conducting inference on the\\ninfinite-dimensional diffusion parameter from a histogram plot of the `killing\\'\\npositions of the process. We show first that these positions follow a Poisson\\npoint process whose intensity measure is determined by the solution of a\\ncertain Schr\\\\\"odinger equation. The inference problem can then be re-cast as a\\nnon-linear inverse problem for this PDE, which we show to be consistently\\nsolvable in a Bayesian way under natural conditions on the initial state of the\\ndiffusion, provided the binding potential is not too `aggressive\\'. In the\\ncourse of our proofs we obtain novel posterior contraction rate results for\\nhigh-dimensional Poisson count data that are of independent interest. A\\nnumerical illustration of the algorithm by standard MCMC methods is also\\nprovided. We view penalized risks through the lens of the calculus of variations. We\\nconsider risks comprised of a fitness-term (e.g. MSE) and a gradient-based\\npenalty. After establishing the Euler-Lagrange field equations as a systematic\\napproach to finding minimizers of risks involving only first derivatives, we\\nproceed to exemplify this approach to the MSE penalized by the integral over\\nthe squared l2-norm of the gradient of the regression function. The minimizer\\nof this risk is given as the solution to a second order inhomogeneous PDE,\\nwhere the inhomogeneity is given as the conditional expectation of the target\\nvariable conditioned on the features. We discuss properties of the field\\nequations and practical implications thereof, which also apply to the classical\\nRidge penalty for linear models, and embed our findings into the existing\\nliterature. In particular, we find that we can recover the Rudin-Osher-Fatemi\\nmodel for image-denoising, if we consider the features as deterministic and\\nevenly distributed. Last, we outline several directions for future research. In reliability theory and survival analysis, observed data are often weakly\\ndependent and subject to additive measurement errors. Such contamination arises\\nwhen the underlying data are neither independent nor strongly mixed but instead\\nexhibit association. This paper focuses on estimating the hazard rate by\\ndeconvolving the density function and constructing an estimator of the\\ndistribution function. We assume that the data originate from a strictly\\nstationary sequence satisfying association conditions. Under appropriate\\nsmoothness assumptions on the error distribution, we establish the\\nquadratic-mean convergence and asymptotic normality of the proposed estimators.\\nThe finite-sample performance of both the hazard rate and distribution function\\nestimators is evaluated through a simulation study. We conclude with a\\ndiscussion of open problems and potential future research directions. This paper introduces a novel test for conditional stochastic dominance (CSD)\\nat specific values of the conditioning covariates, referred to as target\\npoints. The test is relevant for analyzing income inequality, evaluating\\ntreatment effects, and studying discrimination. We propose a\\nKolmogorov-Smirnov-type test statistic that utilizes induced order statistics\\nfrom independent samples. Notably, the test features a data-independent\\ncritical value, eliminating the need for resampling techniques such as the\\nbootstrap. Our approach avoids kernel smoothing and parametric assumptions,\\ninstead relying on a tuning parameter to select relevant observations. We\\nestablish the asymptotic properties of our test, showing that the induced order\\nstatistics converge to independent draws from the true conditional\\ndistributions and that the test controls asymptotic size under weak regularity\\nconditions. While our results apply to both continuous and discrete data, in\\nthe discrete case, the critical value only provides a valid upper bound. To\\naddress this, we propose a refined critical value that significantly enhances\\npower, requiring only knowledge of the support size of the distributions.\\nAdditionally, we analyze the test\\'s behavior in the limit experiment,\\ndemonstrating that it reduces to a problem analogous to testing unconditional\\nstochastic dominance in finite samples. This framework allows us to prove the\\nvalidity of permutation-based tests for stochastic dominance when the random\\nvariables are continuous. Monte Carlo simulations confirm the strong\\nfinite-sample performance of our method. In statistical inference, confidence set procedures are typically evaluated\\nbased on their validity and width properties. Even when procedures achieve\\nrate-optimal widths, confidence sets can still be excessively wide in practice\\ndue to elusive constants, leading to extreme conservativeness, where the\\nempirical coverage probability of nominal $1-\\\\alpha$ level confidence sets\\napproaches one. This manuscript studies this gap between validity and\\nconservativeness, using universal inference (Wasserman et al., 2020) with a\\nregular parametric model under model misspecification as a running example. We\\nidentify the source of asymptotic conservativeness and propose a general remedy\\nbased on studentization and bias correction. The resulting method attains exact\\nasymptotic coverage at the nominal $1-\\\\alpha$ level, even under model\\nmisspecification, provided that the product of the estimation errors of two\\nunknowns is negligible, exhibiting an intriguing resemblance to double\\nrobustness in semiparametric theory. We revisit the classical broken sample problem: Two samples of i.i.d. data\\npoints $\\\\mathbf{X}=\\\\{X_1,\\\\cdots, X_n\\\\}$ and $\\\\mathbf{Y}=\\\\{Y_1,\\\\cdots,Y_m\\\\}$ are\\nobserved without correspondence with $m\\\\leq n$. Under the null hypothesis,\\n$\\\\mathbf{X}$ and $\\\\mathbf{Y}$ are independent. Under the alternative\\nhypothesis, $\\\\mathbf{Y}$ is correlated with a random subsample of $\\\\mathbf{X}$,\\nin the sense that $(X_{\\\\pi(i)},Y_i)$\\'s are drawn independently from some\\nbivariate distribution for some latent injection $\\\\pi:[m] \\\\to [n]$. Originally\\nintroduced by DeGroot, Feder, and Goel (1971) to model matching records in\\ncensus data, this problem has recently gained renewed interest due to its\\napplications in data de-anonymization, data integration, and target tracking.\\nDespite extensive research over the past decades, determining the precise\\ndetection threshold has remained an open problem even for equal sample sizes\\n($m=n$). Assuming $m$ and $n$ grow proportionally, we show that the sharp\\nthreshold is given by a spectral and an $L_2$ condition of the likelihood ratio\\noperator, resolving a conjecture of Bai and Hsing (2005) in the positive. These\\nresults are extended to high dimensions and settle the sharp detection\\nthresholds for Gaussian and Bernoulli models. In this paper, we study the minimizers of U-processes and their domains of\\nattraction. U-processes arise in various statistical contexts, particularly in\\nM-estimation, where estimators are defined as minimizers of certain objective\\nfunctions. Our main results establish necessary and sufficient conditions for\\nthe distributional convergence of these minimizers, identifying a broad class\\nof normalizing sequences that go beyond the standard square-root asymptotics\\nwith normal limits. We show that the limit distribution belongs to exactly one\\nof the four classes introduced by Smirnov. These results do not only extend\\nSmirnov\\'s theory but also generalize existing asymptotic theories for\\nM-estimators, including classical results by Huber and extensions to\\nhigher-degree U-statistics. Furthermore, we analyze the domain of attraction\\nfor each class, providing alternative characterizations that determine which\\ntypes of statistical estimators fall into a given asymptotic regime. The log-logistic distribution is a versatile parametric family widely used\\nacross various applied fields, including survival analysis, reliability\\nengineering, and econometrics. When estimating parameters of the log-logistic\\ndistribution, hypothesis testing is necessary to verify assumptions about these\\nparameters. The Wald test and Rao test provide formal methods for testing\\nhypotheses about these parameters. However, these test statistics are not\\nrobust, and their rejection decisions may be affected by data contamination. In\\nthis paper we develop new families of Wald-type test statistics and Rao-type\\ntest statistics based on minimum density power divergence estimators (MDPDEs)\\nfor the parameters of the log-logistic distribution. These new families\\ngeneralize the Wald and Rao test statistics, inheriting the robustness\\nproperties from the MDPDEs and thus addressing the lack of robustness of the\\nclassical tests. Explicit expressions for the test statistics under the\\nlog-logistic model for both simple and composite null hypotheses are derived,\\nand their properties are analyzed in detail. An extensive simulation study\\nempirically demonstrates the robustness of these families and compares their\\nperformance with the classical methods. Orthogonal-split trees perform well, but evidence suggests oblique splits can\\nenhance their performance. This paper explores optimizing high-dimensional\\n$s$-sparse oblique splits from $\\\\{(\\\\vec{w}, \\\\vec{w}^{\\\\top}\\\\boldsymbol{X}_{i}) :\\ni\\\\in \\\\{1,\\\\dots, n\\\\}, \\\\vec{w} \\\\in \\\\mathbb{R}^p, \\\\| \\\\vec{w} \\\\|_{2} = 1, \\\\|\\n\\\\vec{w} \\\\|_{0} \\\\leq s \\\\}$ for growing oblique trees, where $ s $ is a\\nuser-defined sparsity parameter. We establish a connection between SID\\nconvergence and $s_0$-sparse oblique splits with $s_0\\\\ge 1$, showing that the\\nSID function class expands as $s_0$ increases, enabling the capture of more\\ncomplex data-generating functions such as the $s_0$-dimensional XOR function.\\nThus, $s_0$ represents the unknown potential complexity of the underlying\\ndata-generating function. Learning these complex functions requires an\\n$s$-sparse oblique tree with $s \\\\geq s_0$ and greater computational resources.\\nThis highlights a trade-off between statistical accuracy, governed by the SID\\nfunction class size depending on $s_0$, and computational cost. In contrast,\\nprevious studies have explored the problem of SID convergence using orthogonal\\nsplits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we\\nintroduce a practical framework for oblique trees that integrates optimized\\noblique splits alongside orthogonal splits into random forests. The proposed\\napproach is assessed through simulations and real-data experiments, comparing\\nits performance against various oblique tree models. We present a new proof of the sub-Gaussian norm concentration inequality. Our\\nproof is based on an averaged version of the moment generating function termed\\nthe averaged moment generating function. Compared with the widely adopted\\n$\\\\varepsilon$-net technique-based proof of the sub-Gaussian norm concentration\\ninequality, our method does not rely on the union bound and promises a tighter\\nconcentration bound. Turing\\'s estimator allows one to estimate the probabilities of outcomes that\\neither do not appear or only rarely appear in a given random sample. We perform\\na simulation study to understand the finite sample performance of several\\nrelated confidence intervals (CIs) and introduce an approach for selecting the\\nappropriate CI for a given sample. We give an application to the problem of\\nauthorship attribution and apply it to a dataset comprised of tweets from users\\non X (Twitter). Further, we derive several theoretical results about asymptotic\\nnormality and asymptotic Poissonity of Turing\\'s estimator for two important\\ndiscrete distributions. The aim of distributional regression is to find the best candidate in a given\\nparametric family of conditional distributions to model a given dataset. As\\neach candidate in the distribution family can be identified by the\\ncorresponding distribution parameters, a common approach for this task is using\\nthe maximum likelihood estimator (MLE) for the parameters. In this paper, we\\nestablish theoretical results for this estimator in case the response variable\\nis subject to random right censoring. In particular, we provide proofs of\\nalmost sure consistency and asymptotic normality of the MLE under censoring.\\nFurther, the finite-sample behavior is exemplarily demonstrated in a simulation\\nstudy. Previously [Journal of Causal Inference, 10, 90-105 (2022)], we computed the\\nvariance of two estimators of causal effects for a v-structure of binary\\nvariables. Here we show that a linear combination of these estimators has lower\\nvariance than either. Furthermore, we show that this holds also when the\\ntreatment variable is block randomised with a predefined number receiving\\ntreatment, with analogous results to when it is sampled randomly. The stratified linear permutation statistic arises in various statistics\\nproblems, including stratified and post-stratified survey sampling, stratified\\nand post-stratified experiments, conditional permutation tests, etc. Although\\nwe can derive the Berry--Esseen bounds for the stratified linear permutation\\nstatistic based on existing bounds for the non-stratified statistics, those\\nbounds are not sharp, and moreover, this strategy does not work in general\\nsettings with heterogeneous strata with varying sizes. We first use Stein\\'s\\nmethod to obtain a unified stratified permutational Berry--Esseen bound that\\ncan accommodate heterogeneous strata. We then apply the bound to various\\nstatistics problems, leading to stronger theoretical quantifications and\\nthereby facilitating statistical inference in those problems. Quadratic discriminant analysis (QDA) is a widely used method for\\nclassification problems, particularly preferable over Linear Discriminant\\nAnalysis (LDA) for heterogeneous data. However, QDA loses its effectiveness in\\nhigh-dimensional settings, where the data dimension and sample size tend to\\ninfinity. To address this issue, we propose a novel QDA method utilizing\\nspectral correction and regularization techniques, termed SR-QDA. The\\nregularization parameters in our method are selected by maximizing the\\nFisher-discriminant ratio. We compare SR-QDA with QDA, regularized quadratic\\ndiscriminant analysis (R-QDA), and several other competitors. The results\\nindicate that SR-QDA performs exceptionally well, especially in moderate and\\nhigh-dimensional situations. Empirical experiments across diverse datasets\\nfurther support this conclusion. Providing theoretical guarantees for parameter estimation in exponential\\nrandom graph models is a largely open problem. While maximum likelihood\\nestimation has theoretical guarantees in principle, verifying the assumptions\\nfor these guarantees to hold can be very difficult. Moreover, in complex\\nnetworks, numerical maximum likelihood estimation is computer-intensive and may\\nnot converge in reasonable time. To ameliorate this issue, local dependency\\nexponential random graph models have been introduced, which assume that the\\nnetwork consists of many independent exponential random graphs. In this\\nsetting, progress towards maximum likelihood estimation has been made. However\\nthe estimation is still computer-intensive. Instead, we propose to use\\nso-called Stein estimators: we use the Stein characterizations to obtain new\\nestimators for local dependency exponential random graph models. Quantifying the association between two random variables is crucial in\\napplications. Traditional estimation techniques for common association\\nmeasures, such as Spearman\\'s rank correlation coefficient, $\\\\rho_S$, often fail\\nwhen data contain ties. This is particularly problematic in zero-inflated\\ncontexts and fields like insurance, healthcare, and weather forecasting, where\\nzeros are more frequent and require an extra probability mass. In this paper,\\nwe provide a new formulation of Spearman\\'s rho specifically designed for\\nzero-inflated data and propose a novel estimator of Spearman\\'s rho based on our\\nderived expression. Besides, we make our proposed estimator useful in practice\\nby deriving its achievable bounds and suggest how to estimate them. We analyze\\nour method in a comprehensive simulation study and show that our approach\\novercomes state-of-the-art methods in all the simulated scenarios.\\nAdditionally, we illustrate how the proposed theory can be used in practice for\\na more accurate quantification of association by considering two real-life\\napplications. The evaluation of G-Wishart normalising constants is a core component for\\nBayesian analyses for Gaussian graphical models, but remains a computationally\\nintensive task in general. Based on empirical evidence, Roverato [Scandinavian\\nJournal of Statistics, 29:391--411 (2002)] observed and conjectured that such\\nconstants can be simplified and rewritten in terms of constants with an\\nidentity scale matrix. In this note, we disprove this conjecture for general\\ngraphs by showing that the conjecture instead implies an independently-derived\\napproximation for certain ratios of normalising constants. We study the properties of a stochastic heat equation with a generalized\\nmixed fractional Brownian noise. We obtain the covariance structure,\\nstationarity and obtain bounds for the asymptotic behaviour of the solution. We\\nsuggest estimators for the unknown parameters based on discrete time\\nobservations and study their asymptotic properties. Suppose we observe a trajectory of length $n$ from an $\\\\alpha$-mixing\\nstochastic process over a finite but potentially large state space. We consider\\nthe problem of estimating the probability mass placed by the stationary\\ndistribution of any such process on elements that occur with a certain\\nfrequency in the observed sequence. We estimate this vector of probabilities in\\ntotal variation distance, showing universal consistency in $n$ and recovering\\nknown results for i.i.d. sequences as special cases. Our proposed methodology\\ncarefully combines the plug-in (or empirical) estimator with a\\nrecently-proposed modification of the Good--Turing estimator called WingIt,\\nwhich was originally developed for Markovian sequences. En route to controlling\\nthe error of our estimator, we develop new performance bounds on WingIt and the\\nplug-in estimator for $\\\\alpha$-mixing stochastic processes. Importantly, the\\nextensively used method of Poissonization can no longer be applied in our non\\ni.i.d. setting, and so we develop complementary tools -- including\\nconcentration inequalities for a natural self-normalized statistic of mixing\\nsequences -- that may prove independently useful in the design and analysis of\\nestimators for related problems. We continue our work [arXiv:2403.07628] on asymptotic expansions at the soft\\nedge for the classical $n$-dimensional Gaussian and Laguerre random matrix\\nensembles. By revisiting the construction of the associated skew-orthogonal\\npolynomials in terms of wave functions, we obtain concise expressions for the\\nlevel densities that are well suited for proving asymptotic expansions in\\npowers of a certain parameter $h \\\\asymp n^{-2/3}$. In the unitary case, the\\nexpansion for the level density can be used to reconstruct the first correction\\nterm in an established asymptotic expansion of the associated generating\\nfunction. In the orthogonal and symplectic cases, we can even reconstruct the\\nconjectured first and second correction terms. Prediction model training is often hindered by limited access to\\nindividual-level data due to privacy concerns and logistical challenges,\\nparticularly in biomedical research. Resampling-based self-training presents a\\npromising approach for building prediction models using only summary-level\\ndata. These methods leverage summary statistics to sample pseudo datasets for\\nmodel training and parameter optimization, allowing for model development\\nwithout individual-level data. Although increasingly used in precision\\nmedicine, the general behaviors of self-training remain unexplored. In this\\npaper, we leverage a random matrix theory framework to establish the\\nstatistical properties of self-training algorithms for high-dimensional\\nsparsity-free summary data. We demonstrate that, within a class of linear\\nestimators, resampling-based self-training achieves the same asymptotic\\npredictive accuracy as conventional training methods that require\\nindividual-level datasets. These results suggest that self-training with only\\nsummary data incurs no additional cost in prediction accuracy, while offering\\nsignificant practical convenience. Our analysis provides several valuable\\ninsights and counterintuitive findings. For example, while pseudo-training and\\nvalidation datasets are inherently dependent, their interdependence\\nunexpectedly cancels out when calculating prediction accuracy measures,\\npreventing overfitting in self-training algorithms. Furthermore, we extend our\\nanalysis to show that the self-training framework maintains this no-cost\\nadvantage when combining multiple methods or when jointly training on data from\\ndifferent distributions. We numerically validate our findings through\\nsimulations and real data analyses using the UK Biobank. Our study highlights\\nthe potential of resampling-based self-training to advance genetic risk\\nprediction and other fields that make summary data publicly available. This paper proposes new ANOVA-based approximations of functions and emulators\\nof high-dimensional models using either available derivatives or local\\nstochastic evaluations of such models. Our approach makes use of sensitivity\\nindices to design adequate structures of emulators. For high-dimensional models\\nwith available derivatives, our derivative-based emulators reach dimension-free\\nmean squared errors (MSEs) and parametric rate of convergence (i.e.,\\n$\\\\mathsf{O}(N^{-1})$). This approach is extended to cope with every model\\n(without available derivatives) by deriving global emulators that account for\\nthe local properties of models or simulators. Such generic emulators enjoy\\ndimension-free biases, parametric rates of convergence and MSEs that depend on\\nthe dimensionality. Dimension-free MSEs are obtained for high-dimensional\\nmodels with particular inputs\\' distributions. Our emulators are also\\ncompetitive in dealing with different distributions of the input variables and\\nfor selecting inputs and interactions. Simulations show the efficiency of our\\napproach. In an earlier work arXiv:2410.22038, it was shown that mixtures of\\nmultivariate Gaussian or $t$-distributions can be distinguished by projecting\\nthem onto a certain predetermined finite set of lines, the number of lines\\ndepending only on the total number of distributions involved and on the ambient\\ndimension. Using this work, we address the following two important statistical\\nproblems: that of testing and measuring the agreement between two different\\nrandom partitions, and that of estimating for mixtures of multivariate normal\\ndistributions and mixtures of $t$-distributions based of univariate\\nprojections. We also compare our proposal with robust versions of the\\nexpectation-maximization method EM. In each case, we present algorithms for\\neffecting the task, and compare them with existing methods by carrying out some\\nsimulations. We analyze the landscape and training dynamics of diagonal linear networks in\\na linear regression task, with the network parameters being perturbed by small\\nisotropic normal noise. The addition of such noise may be interpreted as a\\nstochastic form of sharpness-aware minimization (SAM) and we prove several\\nresults that relate its action on the underlying landscape and training\\ndynamics to the sharpness of the loss. In particular, the noise changes the\\nexpected gradient to force balancing of the weight matrices at a fast rate\\nalong the descent trajectory. In the diagonal linear model, we show that this\\nequates to minimizing the average sharpness, as well as the trace of the\\nHessian matrix, among all possible factorizations of the same matrix. Further,\\nthe noise forces the gradient descent iterates towards a shrinkage-thresholding\\nof the underlying true parameter, with the noise level explicitly regulating\\nboth the shrinkage factor and the threshold. Empirical Bayes estimators are based on minimizing the average risk with the\\nhyper-parameters in the weighting function being estimated from observed data.\\nThe performance of an empirical Bayes estimator is typically evaluated by its\\nmean squared error (MSE). However, the explicit expression for its MSE is\\ngenerally unavailable for finite sample sizes. To address this issue, we define\\na high-order analytical criterion: the excess MSE. It quantifies the\\nperformance difference between the maximum likelihood and empirical Bayes\\nestimators. An explicit expression for the excess MSE of an empirical Bayes\\nestimator employing a general data-dependent hyper-parameter estimator is\\nderived. As specific instances, we provide excess MSE expressions for\\nkernel-based regularized estimators using the scaled empirical Bayes, Stein\\nunbiased risk estimation, and generalized cross-validation hyper-parameter\\nestimators. Moreover, we propose a modification to the excess MSE expressions\\nfor regularized estimators for moderate sample sizes and show its improvement\\non accuracy in numerical simulations. Since polynomial regression models are generally quite reliable for data with\\na linear trend, it is important to note that, in some cases, they may encounter\\noverfitting issues during the training phase, which could result in negative\\nvalues of the coefficient of determination $R^2$ for unseen data. For this\\nreason, this work proposes the partial implementation of fractional operators\\nin polynomial regression models to generate a fractional regression model. The\\ngoal of this proposal is to attempt to mitigate overfitting, which could\\nimprove the value of the coefficient of determination for unseen data, compared\\nto the polynomial model, under the assumption that this would contribute to\\ngenerating predictive models with better performance. The methodology for\\nconstructing these fractional regression models is detailed, and examples\\napplicable to both Riemann-Liouville and Caputo fractional operators are\\npresented. Piecewise-Deterministic Markov Processes (PDMPs) hold significant promise for\\nsampling from complex probability distributions. However, their practical\\nimplementation is hindered by the need to compute model-specific bounds.\\nConversely, while Hamiltonian Monte Carlo (HMC) offers a generally efficient\\napproach to sampling, its inability to adaptively tune step sizes impedes its\\nperformance when sampling complex distributions like funnels.\\n  To address these limitations, we introduce three innovative concepts: (a) a\\nMetropolis-adjusted approximation for PDMP simulation that eliminates the need\\nfor explicit bounds without compromising the invariant measure, (b) an adaptive\\nstep size mechanism compatible with the Metropolis correction, and (c) a No\\nU-Turn Sampler (NUTS)-inspired scheme for dynamically selecting path lengths in\\nPDMPs. These three ideas can be seamlessly integrated into a single,\\n`doubly-adaptive\\' PDMP sampler with favourable robustness and efficiency\\nproperties. While measures of concordance -- such as Spearman\\'s rho, Kendall\\'s tau, and\\nBlomqvist\\'s beta -- are continuous with respect to weak convergence,\\nChatterjee\\'s rank correlation xi recently introduced in Azadkia and Chatterjee\\n[5] does not share this property, causing drawbacks in statistical inference as\\npointed out in B\\\\\"ucher and Dette [7]. As we study in this paper, xi is instead\\nweakly continuous with respect to conditionally independent copies -- the\\nMarkov products. To establish weak continuity of Markov products, we provide\\nseveral sufficient conditions, including copula-based criteria and conditions\\nrelying on the concept of conditional weak convergence in Sweeting [36]. As a\\nconsequence, we also obtain continuity results for xi and related dependence\\nmeasures and verify their continuity in the parameters of standard models such\\nas multivariate elliptical and l1-norm symmetric distributions. We examine a generalisation of the beta distribution that we call the pushed\\nbeta distribution. This is a continuous univariate distribution on the unit\\ninterval which generalises the beta distribution by \"pushing\" the density in a\\nparticular direction using an additional multiplicative term in the density\\nkernel. We examine the properties of this distribution and compare it to the\\nbeta distribution. We also examine the use of this distribution in contaminated\\nbinary sampling using Bayesian inference. We find that this distribution arises\\nas the appropriate posterior distribution for inference in certain kinds of\\ncontaminated binary models. We derive a broad range of properties of the\\ndistribution and we also establish some computational methods to compute\\nvarious functions for the distribution. Aligning large language models (LLMs) with diverse human preferences is\\ncritical for ensuring fairness and informed outcomes when deploying these\\nmodels for decision-making. In this paper, we seek to uncover fundamental\\nstatistical limits concerning aligning LLMs with human preferences, with a\\nfocus on the probabilistic representation of human preferences and the\\npreservation of diverse preferences in aligned LLMs. We first show that human\\npreferences can be represented by a reward model if and only if the preference\\namong LLM-generated responses is free of any Condorcet cycle. Moreover, we\\nprove that Condorcet cycles exist with probability converging to one\\nexponentially fast under a probabilistic preference model, thereby\\ndemonstrating the impossibility of fully aligning human preferences using\\nreward-based approaches such as reinforcement learning from human feedback.\\nNext, we explore the conditions under which LLMs would employ mixed strategies\\n-- meaning they do not collapse to a single response -- when aligned in the\\nlimit using a non-reward-based approach, such as Nash learning from human\\nfeedback (NLHF). We identify a necessary and sufficient condition for mixed\\nstrategies: the absence of a response that is preferred over all others by a\\nmajority. As a blessing, we prove that this condition holds with high\\nprobability under the probabilistic preference model, thereby highlighting the\\nstatistical possibility of preserving minority preferences without explicit\\nregularization in aligning LLMs. Finally, we leverage insights from our\\nstatistical results to design a novel, computationally efficient algorithm for\\nfinding Nash equilibria in aligning LLMs with NLHF. Our experiments show that\\nLlama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\\\\%\\nagainst the base model. This paper considers the problem of design-based inference for the average\\ntreatment effect in finely stratified experiments. Here, by \"design-based\\'\\' we\\nmean that the only source of uncertainty stems from the randomness in treatment\\nassignment; by \"finely stratified\\'\\' we mean units are first stratified into\\ngroups of size k according to baseline covariates and then, within each group,\\na fixed number l < k are assigned uniformly at random to treatment and the\\nremainder to control. In this setting, we first show under mild conditions that\\ninference using the difference-in-means estimator requires an estimator of its\\nvariance that is at least asymptotically upward-biased. We then present a novel\\nestimator of the variance and show that it is upward-biased; furthermore, the\\nmagnitude of the bias depends in a natural way on the quality of the\\nstratification. Importantly, this estimator remains well-defined even in the\\nsetting in which l = 1 or k - l = 1. We then compare our estimator with some\\nwell-known estimators that have been proposed previously for this case. We\\nfirst show that, while these estimators are also upward-biased, the magnitude\\nof their bias does not change in the natural way with the quality of\\nstratification. To further discriminate among these estimators, we introduce a\\nframework motivated by a thought experiment in which the finite population can\\nbe modeled as having been drawn once in an i.i.d. fashion from a well-behaved\\nprobability distribution. In this framework, we argue that our estimator\\ndominates the others in terms of limiting bias, and that these improvements are\\nstrict except under exceptionally strong restrictions on the treatment effects.\\nFinally, we illustrate our theoretical results through a simulation study,\\nwhich reveals that our estimator can lead to substantially more precise\\ninferences, especially when the quality of stratification is high. We study the minimax rate of estimation in nonparametric exponential family\\nregression under star-shaped constraints. Specifically, the parameter space $K$\\nis a star-shaped set contained within a bounded box $[-M, M]^n$, where $M$ is a\\nknown positive constant. Moreover, we assume that the exponential family is\\nnonsingular and that its cumulant function is twice continuously\\ndifferentiable. Our main result shows that the minimax rate for this problem is\\n$\\\\varepsilon^{*2} \\\\wedge \\\\operatorname{diam}(K)^2$, up to absolute constants,\\nwhere $\\\\varepsilon^*$ is defined as\\n  \\\\[ \\\\varepsilon^* = \\\\sup \\\\{\\\\varepsilon: \\\\varepsilon^2 \\\\kappa(M) \\\\leq \\\\log\\nN^{\\\\operatorname{loc}}(\\\\varepsilon)\\\\}, \\\\]\\n  with $N^{\\\\operatorname{loc}}(\\\\varepsilon)$ denoting the local entropy and\\n$\\\\kappa(M)$ is an absolute constant allowed to depend on $M$. We also provide\\nan example and derive its corresponding minimax optimal rate. We prove an upper bound on the expected $\\\\ell_p$ injective norm of sums of\\nsubgaussian random tensors. Our proof is simple and does not rely on any\\nexplicit geometric or chaining arguments. Instead, it follows from a simple\\napplication of the PAC-Bayesian lemma, a tool that has proven effective at\\ncontrolling the suprema of certain ``smooth\\'\\' empirical processes in recent\\nyears. Our bound strictly improves a very recent result of Bandeira, Gopi,\\nJiang, Lucca, and Rothvoss. In the Euclidean case ($p=2$), our bound sharpens a\\nresult of Lata{\\\\l}a that was central to proving his estimates on the moments of\\nGaussian chaoses. As a consequence, we obtain an elementary proof of this\\nfundamental result. Longitudinal networks are becoming increasingly relevant in the study of\\ndynamic processes characterised by known or inferred community structure.\\nGeneralised Network Autoregressive (GNAR) models provide a parsimonious\\nframework for exploiting the underlying network and multivariate time series.\\nWe introduce the community-$\\\\alpha$ GNAR model with interactions that exploits\\nprior knowledge or exogenous variables for analysing interactions within and\\nbetween communities, and can describe serial correlation in longitudinal\\nnetworks. We derive new explicit finite-sample error bounds that validate\\nanalysing high-dimensional longitudinal network data with GNAR models, and\\nprovide insights into their attractive properties. We further illustrate our\\napproach by analysing the dynamics of $\\\\textit{Red, Blue}$ and $\\\\textit{Swing}$\\nstates throughout presidential elections in the USA from 1976 to 2020, that is,\\na time series of length twelve on 51 time series (US states and Washington DC).\\nOur analysis connects network autocorrelation to eight-year long terms,\\nhighlights a possible change in the system after the 2016 election, and a\\ndifference in behaviour between $\\\\textit{Red}$ and $\\\\textit{Blue}$ states. We study the task of list-decodable linear regression using batches. A batch\\nis called clean if it consists of i.i.d. samples from an unknown linear\\nregression distribution. For a parameter $\\\\alpha \\\\in (0, 1/2)$, an unknown\\n$\\\\alpha$-fraction of the batches are clean and no assumptions are made on the\\nremaining ones. The goal is to output a small list of vectors at least one of\\nwhich is close to the true regressor vector in $\\\\ell_2$-norm. [DJKS23] gave an\\nefficient algorithm, under natural distributional assumptions, with the\\nfollowing guarantee. Assuming that the batch size $n$ satisfies $n \\\\geq\\n\\\\tilde{\\\\Omega}(\\\\alpha^{-1})$ and the number of batches is $m = \\\\mathrm{poly}(d,\\nn, 1/\\\\alpha)$, their algorithm runs in polynomial time and outputs a list of\\n$O(1/\\\\alpha^2)$ vectors at least one of which is\\n$\\\\tilde{O}(\\\\alpha^{-1/2}/\\\\sqrt{n})$ close to the target regressor. Here we\\ndesign a new polynomial time algorithm with significantly stronger guarantees\\nunder the assumption that the low-degree moments of the covariates distribution\\nare Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant\\n$\\\\delta>0$, as long as the batch size is $n \\\\geq\\n\\\\Omega_{\\\\delta}(\\\\alpha^{-\\\\delta})$ and the degree-$\\\\Theta(1/\\\\delta)$ moments of\\nthe covariates are SoS certifiably bounded, our algorithm uses $m =\\n\\\\mathrm{poly}((dn)^{1/\\\\delta}, 1/\\\\alpha)$ batches, runs in polynomial-time, and\\noutputs an $O(1/\\\\alpha)$-sized list of vectors one of which is\\n$O(\\\\alpha^{-\\\\delta/2}/\\\\sqrt{n})$ close to the target. That is, our algorithm\\nachieves substantially smaller minimum batch size and final error, while\\nachieving the optimal list size. Our approach uses higher-order moment\\ninformation by carefully combining the SoS paradigm interleaved with an\\niterative method and a novel list pruning procedure. In the process, we give an\\nSoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader\\napplicability. Score-based diffusion models have become a foundational paradigm for modern\\ngenerative modeling, demonstrating exceptional capability in generating samples\\nfrom complex high-dimensional distributions. Despite the dominant adoption of\\nprobability flow ODE-based samplers in practice due to their superior sampling\\nefficiency and precision, rigorous statistical guarantees for these methods\\nhave remained elusive in the literature. This work develops the first\\nend-to-end theoretical framework for deterministic ODE-based samplers that\\nestablishes near-minimax optimal guarantees under mild assumptions on target\\ndata distributions. Specifically, focusing on subgaussian distributions with\\n$\\\\beta$-H\\\\\"older smooth densities for $\\\\beta\\\\leq 2$, we propose a smooth\\nregularized score estimator that simultaneously controls both the $L^2$ score\\nerror and the associated mean Jacobian error. Leveraging this estimator within\\na refined convergence analysis of the ODE-based sampling process, we\\ndemonstrate that the resulting sampler achieves the minimax rate in total\\nvariation distance, modulo logarithmic factors. Notably, our theory\\ncomprehensively accounts for all sources of error in the sampling process and\\ndoes not require strong structural conditions such as density lower bounds or\\nLipschitz/smooth scores on target distributions, thereby covering a broad range\\nof practical data distributions. For one dimensional stochastic Burgers equation driven by space-time white\\nnoise we consider the problem of estimation of the diffusivity parameter in\\nfront of the second-order spatial derivative. Based on local observations in\\nspace, we study the estimator derived in [Altmeyer, Rei{\\\\ss}, Ann. Appl.\\nProbab.(2021)] for linear stochastic heat equation that has also been used in\\n[Altmeyer, Cialenco, Pasemann, Bernoulli (2023)] to cover large class of\\nsemilinear SPDEs and has been examined for the stochastic Burgers equation\\ndriven by trace class noise. We extend the achieved results by considering the\\nspace-time white noise case which has also relevant physical motivations. After\\nwe establish new regularity results for the solution, we are able to show that\\nour proposed estimator is strongly consistent and asymptotically normal. The failure of a system can result from the simultaneous effects of multiple\\ncauses, where assigning a specific cause may be inappropriate or unavailable.\\nExamples include contributing causes of death in epidemiology and the aetiology\\nof neurodegenerative diseases like Alzheimer\\'s. We propose a parametric Weibull\\naccelerated failure time model for multiple causes, incorporating a\\ndata-driven, individualized, and time-varying winning probability (relative\\nimportance) matrix. Using maximum likelihood estimation and the\\nexpectation-maximization (EM) algorithm, our approach enables simultaneous\\nestimation of regression coefficients and relative cause importance, ensuring\\nconsistency and asymptotic normality. A simulation study and an application to\\nAlzheimer\\'s disease demonstrate its effectiveness in addressing cause-mixture\\nproblems and identifying informative biomarker combinations, with comparisons\\nto Weibull and Cox proportional hazards models. This paper tackles the challenge of estimating a low-rank graphon from\\nsampled network data, employing a singular value thresholding (SVT) estimator\\nto create a piecewise-constant graphon based on the network\\'s adjacency matrix.\\nUnder certain assumptions about the graphon\\'s structural properties, we\\nestablish bounds on the operator norm distance between the true graphon and its\\nestimator, as well as on the rank of the estimated graphon. In the second part\\nof the paper, we apply our estimator to graphon games. We derive bounds on the\\nsuboptimality of interventions in the social welfare problem in graphon games\\nwhen the intervention is based on the estimated graphon. These bounds are\\nexpressed in terms of the operator norm of the difference between the true and\\nestimated graphons. We also emphasize the computational benefits of using the\\nlow-rank estimated graphon to solve these problems. Unbiased data synthesis is crucial for evaluating causal discovery algorithms\\nin the presence of unobserved confounding, given the scarcity of real-world\\ndatasets. A common approach, implicit parameterization, encodes unobserved\\nconfounding by modifying the off-diagonal entries of the idiosyncratic\\ncovariance matrix while preserving positive definiteness. Within this approach,\\nwe identify that state-of-the-art protocols have two distinct issues that\\nhinder unbiased sampling from the complete space of causal models: first, we\\ngive a detailed analysis of use of diagonally dominant constructions restricts\\nthe spectrum of partial correlation matrices; and second, the restriction of\\npossible graphical structures when sampling bidirected edges, unnecessarily\\nruling out valid causal models. To address these limitations, we propose an\\nimproved explicit modeling approach for unobserved confounding, leveraging\\nblock-hierarchical ancestral generation of ground truth causal graphs.\\nAlgorithms for converting the ground truth DAG into ancestral graph is provided\\nso that the output of causal discovery algorithms could be compared with. We\\ndraw connections between implicit and explicit parameterization, prove that our\\napproach fully covers the space of causal models, including those generated by\\nthe implicit parameterization, thus enabling more robust evaluation of methods\\nfor causal discovery and inference. This paper studies the problem of inferring a $k$-factor, specifically a\\nspanning $k$-regular graph, planted within an Erdos-Renyi random graph\\n$G(n,\\\\lambda/n)$. We uncover an interesting \"all-something-nothing\" phase\\ntransition. Specifically, we show that as the average degree $\\\\lambda$\\nsurpasses the critical threshold of $1/k$, the inference problem undergoes a\\ntransition from almost exact recovery (\"all\" phase) to partial recovery\\n(\"something\" phase). Moreover, as $\\\\lambda$ tends to infinity, the accuracy of\\nrecovery diminishes to zero, leading to the onset of the \"nothing\" phase. This\\nfinding complements the recent result by Mossel, Niles-Weed, Sohn, Sun, and\\nZadik who established that for certain sufficiently dense graphs, the problem\\nundergoes an \"all-or-nothing\" phase transition, jumping from near-perfect to\\nnear-zero recovery. In addition, we characterize the recovery accuracy of a\\nlinear-time iterative pruning algorithm and show that it achieves almost exact\\nrecovery when $\\\\lambda < 1/k$. A key component of our analysis is a two-step\\ncycle construction: we first build trees through local neighborhood exploration\\nand then connect them by sprinkling using reserved edges. Interestingly, for\\nproving impossibility of almost exact recovery, we construct $\\\\Theta(n)$ many\\nsmall trees of size $\\\\Theta(1)$, whereas for establishing the algorithmic lower\\nbound, a single large tree of size $\\\\Theta(\\\\sqrt{n\\\\log n})$ suffices. We consider two random variables $X$ and $Y$ following correlated Gamma\\ndistributions, characterized by identical scale and shape parameters and a\\nlinear correlation coefficient $\\\\rho$. Our focus is on the parameter: \\\\[\\n  D(X,Y) = \\\\frac{|X - Y|}{X + Y}, \\\\] which appears in applied contexts such as\\ndynamic speckle imaging, where it is known as the \\\\textit{Fujii index}. In this\\nwork, we derive a closed-form expression for the probability density function\\nof $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our\\nderivation starts by representing $X$ and $Y$ as two correlated exponential\\nrandom variables, obtained from the squared magnitudes of circular complex\\nGaussian variables. By considering the sum of $k$ independent exponential\\nvariables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two\\ncorrelated Gamma variables. Through appropriate varable transformations, we\\nobtain the theoretical distribution of $D(X,Y)$ and evaluate its moments\\nanalytically. These theoretical findings are validated through numerical\\nsimulations, with particular attention to two specific cases: zero correlation\\nand unit shape parameter. Early work established convergence of the principal component estimators of\\nthe factors and loadings up to a rotation for large dimensional approximate\\nfactor models with weak factors in that the factor loading $\\\\Lambda^{(0)}$\\nscales sublinearly in the number $N$ of cross-section units, i.e.,\\n$\\\\Lambda^{(0)\\\\top}\\\\Lambda^{(0)}/N^{\\\\alpha}$ is positive definite in the limit\\nfor some $\\\\alpha\\\\in (0,1)$. However, the established convergence rates for weak\\nfactors can be much slower especially for small $\\\\alpha$. This article proposes\\na Transfer Principal Component Analysis (TransPCA) method for enhancing the\\nconvergence rates for weak factors by transferring knowledge from large number\\nof available informative panel datasets, which should not be turned a blind eye\\non in this big data era. We aggregate useful information by analyzing a\\nweighted average projection matrix of the estimated loading spaces from all\\ninformative datasets which is highly flexible and computationally efficient.\\nTheoretically, we derive the convergence rates of the estimators of weak/strong\\nloading spaces and factor scores. The results indicate that as long as the\\nauxiliary datasets are similar enough to the target dataset and the auxiliary\\nsample size is sufficiently large, TransPCA estimators can achieve faster\\nconvergence rates in contrast to performing PCA solely on the target dataset.\\nTo avoid negative transfer, we also investigate the case that the informative\\ndatasets are unknown and provide a criterion for selecting useful datasets.\\nThorough simulation studies and {empirical analysis on real datasets in areas\\nof macroeconomic and finance} are conducted to illustrate the usefulness of our\\nproposed methods where large number of source panel datasets are naturally\\navailable. This work addresses the problem of estimating a vector field from a noisy\\nOrdinary Differential Equation (ODE) in a non-parametric regression setting\\nwith a random design for initial values. More specifically, given a vector\\nfield $ f:\\\\mathbb{R}^{D}\\\\rightarrow \\\\mathbb{R}^{D}$ governing a dynamical\\nsystem defined by the autonomous ODE: $y\\' = f(y)$, we assume that the\\nobservations are $\\\\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) +\\n\\\\varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time\\n$t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a\\nprobability distribution $\\\\mu$, and $\\\\varepsilon_{i,j}$ some noise. In this\\ncontext, we investigate, from a minimax perspective, the pointwise\\nreconstruction of $f$ within the envelope of trajectories originating from the\\nsupport of $\\\\mu$. We propose an estimation strategy based on preliminary flow\\nreconstruction and techniques from derivative estimation in non-parametric\\nregression. Under mild assumptions on $f$, we establish convergence rates that\\ndepend on the temporal resolution, the number of sampled initial values and the\\nmass concentration of $\\\\mu$. Importantly, we show that these rates are minimax\\noptimal. Furthermore, we discuss the implications of our results in a manifold\\nlearning setting, providing insights into how our approach can mitigate the\\ncurse of dimensionality. This paper examines the evolution of the Finnish electric energy system up to\\n2035, focusing on the likelihood of different development paths. The primary\\ncontribution of this paper is the development of an extensive Bayesian Network,\\ndesigned to model and analyse the evolution of power generation capacity mix,\\nassess the likelihood of different grid management scenarios, and understand\\nthe causal relationships underlying these scenarios. A target optimisation was\\ncarried out using the constructed Bayesian Network to explore possibilities to\\nminimise grid management complexity. The results of the optimisation reveal\\nthat the authorities and stakeholders should prioritise increasing demand\\nresponse, gas power, and battery storage capacities. These mature technologies\\nare well-suited to guarantee energy adequacy during peak consumption periods,\\nwhich in Finland typically occur during consecutive cold, dark and windless\\nwinter weeks. Although this study focuses on the evolution of the Finnish power\\ngrid, the constructed Bayesian Network approach is broadly applicable and can\\nbe utilised to explore causal relationships in other countries by employing the\\ndesigned questionnaire and engaging a panel of experts specific to the\\ncountry\\'s energy infrastructure. Graph sparsification is a well-established technique for accelerating\\ngraph-based learning algorithms, which uses edge sampling to approximate dense\\ngraphs with sparse ones. Because the sparsification error is random and\\nunknown, users must contend with uncertainty about the reliability of\\ndownstream computations. Although it is possible for users to obtain conceptual\\nguidance from theoretical error bounds in the literature, such results are\\ntypically impractical at a numerical level. Taking an alternative approach, we\\npropose to address these issues from a data-driven perspective by computing\\nempirical error estimates. The proposed error estimates are highly versatile,\\nand we demonstrate this in four use cases: Laplacian matrix approximation,\\ngraph cut queries, graph-structured regression, and spectral clustering.\\nMoreover, we provide two theoretical guarantees for the error estimates, and\\nexplain why the cost of computing them is manageable in comparison to the\\noverall cost of a typical graph sparsification workflow. COVID-19 has had a large scale negative impact on the health of opioid users\\nexacerbating the health of an already vulnerable population. Critical\\ninformation on the total impact of COVID-19 on opioid users is unknown due to a\\nlack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and\\nlack of data coverage. To assess the impact of COVID-19 on small-area opioid\\nmortality, we developed a Bayesian hierarchical excess opioid mortality\\nmodeling approach. We incorporate spatio-temporal autocorrelation structures to\\nallow for sharing of information across small areas and time to reduce\\nuncertainty in small area estimates. Excess mortality is defined as the\\ndifference between observed trends after a crisis and expected trends based on\\nobserved historical trends, which captures the total increase in observed\\nmortality rates compared to what was expected prior to the crisis. We\\nillustrate the application of our approach to assess excess opioid mortality\\nrisk estimates for 159 counties in GA. Using our proposed approach will help\\ninform interventions in opioid-related public health responses, policies, and\\nresource allocation. The application of this work also provides a general\\nframework for improving the estimation and mapping of health indicators during\\ncrisis periods for the opioid user population. It is a folklore belief that metastable wells in low-temperature statistical\\nmechanics models exhibit high-temperature behavior. We prove a rigorous version\\nof this phenomenon in the setting of the exponential random graph model (ERGM)\\nthrough the lens of concentration of measure. To do this, we first present a\\nnew general result deriving concentration inequalities in a metastable well\\nfrom the metastable mixing of a Markov chain with the appropriate stationary\\ndistribution, extending a result of Chatterjee [Cha05] which is suited for more\\ntraditional forms of global mixing. We then apply this result to the\\nsupercritical (low-temperature) ERGM which was recently proven to exhibit\\nmetastable mixing by Bresler, Nagaraj, and Nichani [BNN24], and obtain a novel\\nconcentration inequality for Lipschitz observables of the supercritical ERGM\\nconditioned on a large metastable well, answering a question posed by [BNN24].\\nThis extends a result of Ganguly and Nam [GN24] from the subcritical\\n(high-temperature) regime to a metastable well in the supercritical regime, and\\nwe are also able to extend the applications of their concentration inequality\\nto these metastable wells. Namely, we obtain an upper bound on the Wasserstein\\ndistance between the ERGM conditioned on a metastable well and an appropriate\\nErd\\\\H{o}s-R\\\\\\'enyi model, as well as derive a central limit theorem for the\\ncount of edges in certain small subcollections of possible edges. Finally, to\\nsupplement the mathematical content of the article, we also discuss the results\\nof what appears to be the first simulation study of a metastable well in the\\nsupercritical ERGM. Language model alignment (or, reinforcement learning) techniques that\\nleverage active exploration -- deliberately encouraging the model to produce\\ndiverse, informative responses -- offer the promise of super-human\\ncapabilities. However, current understanding of algorithm design primitives for\\ncomputationally efficient exploration with language models is limited. To\\nbetter understand how to leverage access to powerful pre-trained generative\\nmodels to improve the efficiency of exploration, we introduce a new\\ncomputational framework for RL with language models, in which the learner\\ninteracts with the model through a sampling oracle. Focusing on the linear\\nsoftmax model parameterization, we provide new results that reveal the\\ncomputational-statistical tradeoffs of efficient exploration:\\n  1. Necessity of coverage: Coverage refers to the extent to which the\\npre-trained model covers near-optimal responses -- a form of hidden knowledge.\\nWe show that coverage, while not necessary for data efficiency, lower bounds\\nthe runtime of any algorithm in our framework.\\n  2. Inference-time exploration: We introduce a new algorithm, SpannerSampling,\\nwhich obtains optimal data efficiency and is computationally efficient whenever\\nthe pre-trained model enjoys sufficient coverage, matching our lower bound.\\nSpannerSampling leverages inference-time computation with the pre-trained model\\nto reduce the effective search space for exploration.\\n  3. Insufficiency of training-time interventions: We contrast the result above\\nby showing that training-time interventions that produce proper policies cannot\\nachieve similar guarantees in polynomial time.\\n  4. Computational benefits of multi-turn exploration: Finally, we show that\\nunder additional representational assumptions, one can achieve improved runtime\\n(replacing sequence-level coverage with token-level coverage) through\\nmulti-turn exploration. Human physiological signals tend to exhibit both global and local structures:\\nthe former are shared across a population, while the latter reflect\\ninter-individual variability. For instance, kinetic measurements of the gait\\ncycle during locomotion present common characteristics, although idiosyncrasies\\nmay be observed due to biomechanical disposition or pathology. To better\\nrepresent datasets with local-global structure, this work extends Convolutional\\nDictionary Learning (CDL), a popular method for learning interpretable\\nrepresentations, or dictionaries, of time-series data. In particular, we\\npropose Personalized CDL (PerCDL), in which a local dictionary models local\\ninformation as a personalized spatiotemporal transformation of a global\\ndictionary. The transformation is learnable and can combine operations such as\\ntime warping and rotation. Formal computational and statistical guarantees for\\nPerCDL are provided and its effectiveness on synthetic and real human\\nlocomotion data is demonstrated. A common observation in data-driven applications is that high-dimensional\\ndata have a low intrinsic dimension, at least locally. In this work, we\\nconsider the problem of point estimation for manifold-valued data. Namely,\\ngiven a finite set of noisy samples of $\\\\mathcal{M}$, a $d$ dimensional\\nsubmanifold of $\\\\mathbb{R}^D$, and a point $r$ near the manifold we aim to\\nproject $r$ onto the manifold. Assuming that the data was sampled uniformly\\nfrom a tubular neighborhood of a $k$-times smooth boundaryless and compact\\nmanifold, we present an algorithm that takes $r$ from this neighborhood and\\noutputs $\\\\hat p_n\\\\in \\\\mathbb{R}^D$, and $\\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ an\\nelement in the Grassmannian $Gr(d, D)$. We prove that as the number of samples\\n$n\\\\to\\\\infty$, the point $\\\\hat p_n$ converges to $\\\\mathbf{p}\\\\in \\\\mathcal{M}$,\\nthe projection of $r$ onto $\\\\mathcal{M}$, and $\\\\widehat{T_{\\\\hat\\np_n}\\\\mathcal{M}}$ converges to $T_{\\\\mathbf{p}}\\\\mathcal{M}$ (the tangent space\\nat that point) with high probability. Furthermore, we show that $\\\\hat p_n$\\napproaches the manifold with an asymptotic rate of $n^{-\\\\frac{k}{2k + d}}$, and\\nthat $\\\\hat p_n, \\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ approach $\\\\mathbf{p}$ and\\n$T_{\\\\mathbf{p}}\\\\mathcal{M}$ correspondingly with asymptotic rates of\\n$n^{-\\\\frac{k-1}{2k + d}}$. We construct a family of estimators for a regression function based on a\\nsample following a qdistribution. Our approach is nonparametric, using kernel\\nmethods built from operations that leverage the properties of q-calculus.\\nFurthermore, under appropriate assumptions, we establish the weak convergence\\nand strong consistency of this family of estimators. For some discretely observed path of oscillating Brownian motion with level\\nof self-organized criticality $\\\\rho_0$, we prove in the infill asymptotics that\\nthe MLE is $n$-consistent, where $n$ denotes the sample size, and derive its\\nlimit distribution with respect to stable convergence. As the transition\\ndensity of this homogeneous Markov process is not even continuous in $\\\\rho_0$,\\nthe analysis is highly non-standard. Therefore, interesting and somewhat\\nunexpected phenomena occur: The likelihood function splits into several\\ncomponents, each of them contributing very differently depending on how close\\nthe argument $\\\\rho$ is to $\\\\rho_0$. Correspondingly, the MLE is successively\\nexcluded to lay outside a compact set, a $1/\\\\sqrt{n}$-neighborhood and finally\\na $1/n$-neigborhood of $\\\\rho_0$ asymptotically. The crucial argument to derive\\nthe stable convergence is to exploit the semimartingale structure of the\\nsequential suitably rescaled local log-likelihood function (as a process in\\ntime). Both sequentially and as a process in $\\\\rho$, it exhibits a bivariate\\nPoissonian behavior in the stable limit with its intensity being a multiple of\\nthe local time at $\\\\rho_0$. Community detection, which focuses on recovering the group structure within\\nnetworks, is a crucial and fundamental task in network analysis. However, the\\ndetection process can be quite challenging and unstable when community signals\\nare weak. Motivated by a newly collected large-scale academic network dataset\\nfrom the Web of Science, which includes multi-layer network information, we\\npropose a Bipartite Assisted Spectral-clustering approach for Identifying\\nCommunities (BASIC), which incorporates the bipartite network information into\\nthe community structure learning of the primary network. The accuracy and\\nstability enhancement of BASIC is validated theoretically on the basis of the\\ndegree-corrected stochastic block model framework, as well as numerically\\nthrough extensive simulation studies. We rigorously study the convergence rate\\nof BASIC even under weak signal scenarios and prove that BASIC yields a tighter\\nupper error bound than that based on the primary network information alone. We\\nutilize the proposed BASIC method to analyze the newly collected large-scale\\nacademic network dataset from statistical papers. During the author\\ncollaboration network structure learning, we incorporate the bipartite network\\ninformation from author-paper, author-institution, and author-region\\nrelationships. From both statistical and interpretative perspectives, these\\nbipartite networks greatly aid in identifying communities within the primary\\ncollaboration network. We present two limit theorems, a mean ergodic and a central limit theorem,\\nfor a specific class of one-dimensional diffusion processes that depend on a\\nsmall-scale parameter $\\\\varepsilon$ and converge weakly to a homogenized\\ndiffusion process in the limit $\\\\varepsilon \\\\rightarrow 0$. In these results,\\nwe allow for the time horizon to blow up such that $T_\\\\varepsilon \\\\rightarrow\\n\\\\infty$ as $\\\\varepsilon \\\\rightarrow 0$. The novelty of the results arises from\\nthe circumstance that many quantities are unbounded for $\\\\varepsilon\\n\\\\rightarrow 0$, so that formerly established theory is not directly applicable\\nhere and a careful investigation of all relevant $\\\\varepsilon$-dependent terms\\nis required. As a mathematical application, we then use these limit theorems to\\nprove asymptotic properties of a minimum distance estimator for parameters in a\\nhomogenized diffusion equation. The behavior of extreme observations is well-understood for time series or\\nspatial data, but little is known if the data generating process is a\\nstructural causal model (SCM). We study the behavior of extremes in this model\\nclass, both for the observational distribution and under extremal\\ninterventions. We show that under suitable regularity conditions on the\\nstructure functions, the extremal behavior is described by a multivariate\\nPareto distribution, which can be represented as a new SCM on an extremal\\ngraph. Importantly, the latter is a sub-graph of the graph in the original SCM,\\nwhich means that causal links can disappear in the tails. We further introduce\\na directed version of extremal graphical models and show that an extremal SCM\\nsatisfies the corresponding Markov properties. Based on a new test of extremal\\nconditional independence, we propose two algorithms for learning the extremal\\ncausal structure from data. The first is an extremal version of the\\nPC-algorithm, and the second is a pruning algorithm that removes edges from the\\noriginal graph to consistently recover the extremal graph. The methods are\\nillustrated on river data with known causal ground truth. Consider a pair of sparse correlated stochastic block models $\\\\mathcal\\nS(n,\\\\tfrac{\\\\lambda}{n},\\\\epsilon;s)$ subsampled from a common parent stochastic\\nblock model with two symmetric communities, average degree $\\\\lambda=O(1)$ and\\ndivergence parameter $\\\\epsilon \\\\in (0,1)$. For all $\\\\epsilon\\\\in(0,1)$, we\\nconstruct a statistic based on the combination of two low-degree polynomials\\nand show that there exists a sufficiently small constant\\n$\\\\delta=\\\\delta(\\\\epsilon)>0$ and a sufficiently large constant\\n$\\\\Delta=\\\\Delta(\\\\epsilon,\\\\delta)$ such that when $\\\\lambda>\\\\Delta$ and\\n$s>\\\\sqrt{\\\\alpha}-\\\\delta$ where $\\\\alpha\\\\approx 0.338$ is Otter\\'s constant, this\\nstatistic can distinguish this model and a pair of independent stochastic block\\nmodels $\\\\mathcal S(n,\\\\tfrac{\\\\lambda s}{n},\\\\epsilon)$ with probability $1-o(1)$.\\nWe also provide an efficient algorithm that approximates this statistic in\\npolynomial time. The crux of our statistic\\'s construction lies in a carefully\\ncurated family of multigraphs called \\\\emph{decorated trees}, which enables\\neffective aggregation of the community signal and graph correlation from the\\ncounts of the same decorated tree while suppressing the undesirable\\ncorrelations among counts of different decorated trees. This paper is devoted to parameter estimation for partially observed\\npolynomial state space models. This class includes discretely observed affine\\nor more generally polynomial Markov processes. The polynomial structure allows\\nfor the explicit computation of a Gaussian quasi-likelihood estimator and its\\nasymptotic covariance matrix. We show consistency and asymptotic normality of\\nthe estimating sequence and provide explicitly computable expressions for the\\ncorresponding asymptotic covariance matrix. This paper is devoted to filtering, smoothing, and prediction of polynomial\\nprocesses that are partially observed. These problems are known to allow for an\\nexplicit solution in the simpler case of linear Gaussian state space models.\\nThe key insight underlying the present piece of research is that in filtering\\napplications polynomial processes and their discrete counterpart are\\nindistinguishable from Gaussian processes sharing their first two moments. We\\ndescribe the construction of these Gaussian equivalents of polynomial processes\\nand explicitly compute optimal linear filters, predictors and smoothers for\\npolynomial processes in discrete and continuous time. The consideration of\\nGaussian equivalents also opens the door to parameter estimation and\\nlinear-quadratic optimal control in the context of polynomial processes. We consider standard gradient descent, gradient flow and conjugate gradients\\nas iterative algorithms for minimizing a penalized ridge criterion in linear\\nregression. While it is well known that conjugate gradients exhibit fast\\nnumerical convergence, the statistical properties of their iterates are more\\ndifficult to assess due to inherent nonlinearities and dependencies. On the\\nother hand, standard gradient flow is a linear method with well known\\nregularizing properties when stopped early. By an explicit non-standard error\\ndecomposition we are able to bound the prediction error for conjugate gradient\\niterates by a corresponding prediction error of gradient flow at transformed\\niteration indices. This way, the risk along the entire regularisation path of\\nconjugate gradient iterations can be compared to that for regularisation paths\\nof standard linear methods like gradient flow and ridge regression. In\\nparticular, the oracle conjugate gradient iterate shares the optimality\\nproperties of the gradient flow and ridge regression oracles up to a constant\\nfactor. Numerical examples show the similarity of the regularisation paths in\\npractice. We provide conditions for the stochastic dominance comparisons of a risk $X$\\nand an associated risk $X+Z$, where $Z$ represents the uncertainty due to the\\nenvironment and where $X$ and $Z$ can be dependent. The comparisons depend on\\nboth the copula $C$ between the distributions of $X$ and $Z$ and on the\\ndistribution of $Z$. We provide two different conditions for $C$ which\\nrepresents new positive dependence properties. Regarding $Z$, we need some\\nsymmetry or asymmetry (skew) properties. Some illustrative examples are\\nprovided. We consider the graph alignment problem, wherein the objective is to find a\\nvertex correspondence between two graphs that maximizes the edge overlap. The\\ngraph alignment problem is an instance of the quadratic assignment problem\\n(QAP), known to be NP-hard in the worst case even to approximately solve. In\\nthis paper, we analyze Birkhoff relaxation, a tight convex relaxation of QAP,\\nand present theoretical guarantees on its performance when the inputs follow\\nthe Gaussian Wigner Model. More specifically, the weighted adjacency matrices\\nare correlated Gaussian Orthogonal Ensemble with correlation\\n$1/\\\\sqrt{1+\\\\sigma^2}$. Denote the optimal solutions of the QAP and Birkhoff\\nrelaxation by $\\\\Pi^\\\\star$ and $X^\\\\star$ respectively. We show that\\n$\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = o(n)$ when $\\\\sigma = o(n^{-1.25})$ and\\n$\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = \\\\Omega(n)$ when $\\\\sigma = \\\\Omega(n^{-0.5})$. Thus,\\nthe optimal solution $X^\\\\star$ transitions from a small perturbation of\\n$\\\\Pi^\\\\star$ for small $\\\\sigma$ to being well separated from $\\\\Pi^\\\\star$ as\\n$\\\\sigma$ becomes larger than $n^{-0.5}$. This result allows us to guarantee\\nthat simple rounding procedures on $X^\\\\star$ align $1-o(1)$ fraction of\\nvertices correctly whenever $\\\\sigma = o(n^{-1.25})$. This condition on $\\\\sigma$\\nto ensure the success of the Birkhoff relaxation is state-of-the-art. We consider the problem of sequential estimation of a single change point in\\na piecewise linear regression model under a Gaussian setup. We demonstrate that\\na certain CUSUM-type statistic attains the minimax optimal rates for localizing\\nthe change point. Our minimax analysis unveils an interesting phase transition\\nfrom a jump (discontinuity in values) to a kink (change in slope).\\nSpecifically, for a jump, the minimax rate is of order $\\\\log (n) / n$, whereas\\nfor a kink it scales as $\\\\bigl(\\\\log (n) / n\\\\bigr)^{1/3}$, given that the\\nsampling rate is of order $1/n$. We further introduce an algorithm for the\\nproposed online change point detector, which requires constant computational\\nsteps and constant memory per incoming sample. Finally, the empirical\\nperformance of our method is examined on both simulated and real-world data\\nsets. An implementation is available in the R package FLOC on GitHub. We propose causal effect estimators based on empirical Fr\\\\\\'{e}chet means and\\noperator-valued kernels, tailored to functional data spaces. These methods\\naddress the challenges of high-dimensionality, sequential ordering, and model\\ncomplexity while preserving robustness to treatment misspecification. Using\\nstructural assumptions, we obtain compact representations of potential\\noutcomes, enabling scalable estimation of causal effects over time and across\\ncovariates. We provide both theoretical, regarding the consistency of\\nfunctional causal effects, as well as empirical comparison of a range of\\nproposed causal effect estimators.\\n  Applications to binary treatment settings with functional outcomes illustrate\\nthe framework\\'s utility in biomedical monitoring, where outcomes exhibit\\ncomplex temporal dynamics. Our estimators accommodate scenarios with registered\\ncovariates and outcomes, aligning them to the Fr\\\\\\'{e}chet means, as well as\\ncases requiring higher-order representations to capture intricate\\ncovariate-outcome interactions. These advancements extend causal inference to\\ndynamic and non-linear domains, offering new tools for understanding complex\\ntreatment effects in functional data settings. Given two populations from which independent binary observations are taken\\nwith parameters $p_1$ and $p_2$ respectively, estimators are proposed for the\\nrelative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their\\nlogarithms. The estimators guarantee that the relative mean-square error, or\\nthe mean-square error for the logarithmic versions, is less than a target value\\nfor any $p_1, p_2 \\\\in (0,1)$, and the ratio of average sample sizes from the\\ntwo populations is close to a prescribed value. The estimators can also be used\\nwith group sampling, whereby samples are taken in batches of fixed size from\\nthe two populations. The efficiency of the estimators with respect to the\\nCram\\\\\\'er-Rao bound is good, and in particular it is close to $1$ for small\\nvalues of the target error. We propose a novel approach for learning causal response representations. Our\\nmethod aims to extract directions in which a multidimensional outcome is most\\ndirectly caused by a treatment variable. By bridging conditional independence\\ntesting with causal representation learning, we formulate an optimisation\\nproblem that maximises the evidence against conditional independence between\\nthe treatment and outcome, given a conditioning set. This formulation employs\\nflexible regression models tailored to specific applications, creating a\\nversatile framework. The problem is addressed through a generalised eigenvalue\\ndecomposition. We show that, under mild assumptions, the distribution of the\\nlargest eigenvalue can be bounded by a known $F$-distribution, enabling\\ntestable conditional independence. We also provide theoretical guarantees for\\nthe optimality of the learned representation in terms of signal-to-noise ratio\\nand Fisher information maximisation. Finally, we demonstrate the empirical\\neffectiveness of our approach in simulation and real-world experiments. Our\\nresults underscore the utility of this framework in uncovering direct causal\\neffects within complex, multivariate settings. Wasserstein distributionally robust optimization (WDRO) optimizes against\\nworst-case distributional shifts within a specified uncertainty set, leading to\\nenhanced generalization on unseen adversarial examples, compared to standard\\nadversarial training which focuses on pointwise adversarial perturbations.\\nHowever, WDRO still suffers fundamentally from the robust overfitting problem,\\nas it does not consider statistical error. We address this gap by proposing a\\nnovel robust optimization framework under a new uncertainty set for adversarial\\nnoise via Wasserstein distance and statistical error via Kullback-Leibler\\ndivergence, called the Statistically Robust WDRO. We establish a robust\\ngeneralization bound for the new optimization framework, implying that\\nout-of-distribution adversarial performance is at least as good as the\\nstatistically robust training loss with high probability. Furthermore, we\\nderive conditions under which Stackelberg and Nash equilibria exist between the\\nlearner and the adversary, giving an optimal robust model in certain sense.\\nFinally, through extensive experiments, we demonstrate that our method\\nsignificantly mitigates robust overfitting and enhances robustness within the\\nframework of WDRO. The primary objective of learning methods is generalization. Classic uniform\\ngeneralization bounds, which rely on VC-dimension or Rademacher complexity,\\nfail to explain the significant attribute that over-parameterized models in\\ndeep learning exhibit nice generalizability. On the other hand,\\nalgorithm-dependent generalization bounds, like stability bounds, often rely on\\nstrict assumptions. To establish generalizability under less stringent\\nassumptions, this paper investigates the generalizability of neural networks\\nthat minimize or approximately minimize empirical risk. We establish a lower\\nbound for population accuracy based on the expressiveness of these networks,\\nwhich indicates that with an adequate large number of training samples and\\nnetwork sizes, these networks, including over-parameterized ones, can\\ngeneralize effectively. Additionally, we provide a necessary condition for\\ngeneralization, demonstrating that, for certain data distributions, the\\nquantity of training data required to ensure generalization exceeds the network\\nsize needed to represent the corresponding data distribution. Finally, we\\nprovide theoretical insights into several phenomena in deep learning, including\\nrobust generalization, importance of over-parameterization, and effect of loss\\nfunction on generalization. We show how to improve the discrepancy of an iid sample by moving only a few\\npoints. Specifically, modifying \\\\( O(m) \\\\) sample points on average reduces the\\nKolmogorov-Smirnov distance to the population distribution to \\\\(1/m\\\\). We propose a new statistical hypothesis testing framework which decides\\nvisually, using confidence intervals, whether the means of two samples are\\nequal or if one is larger than the other. With our method, the user can at the\\nsame time visualize the confidence region of the means and do a test to decide\\nif the means of the two populations are significantly different or not by\\nlooking whether the two confidence intervals overlap. To design this test we\\nuse confidence intervals constructed using e-variables, which provide a measure\\nof evidence in hypothesis testing. We propose both a sequential test and a\\nnon-sequential test based on the overlap of confidence intervals and for each\\nof these tests we give finite-time error bounds on the probabilities of error.\\nWe also illustrate the practicality of our method by applying it to the\\ncomparison of sequential learning algorithms. Early-stopped iterative optimization methods are widely used as alternatives\\nto explicit regularization, and direct comparisons between early-stopping and\\nexplicit regularization have been established for many optimization geometries.\\nHowever, most analyses depend heavily on the specific properties of the\\noptimization geometry or strong convexity of the empirical objective, and it\\nremains unclear whether early-stopping could ever be less statistically\\nefficient than explicit regularization for some particular shape constraint,\\nespecially in the overparameterized regime. To address this question, we study\\nthe setting of high-dimensional linear regression under additive Gaussian noise\\nwhen the ground truth is assumed to lie in a known convex body and the task is\\nto minimize the in-sample mean squared error. Our main result shows that for\\nany convex body and any design matrix, up to an absolute constant factor, the\\nworst-case risk of unconstrained early-stopped mirror descent with an\\nappropriate potential is at most that of the least squares estimator\\nconstrained to the convex body. We achieve this by constructing algorithmic\\nregularizers based on the Minkowski functional of the convex body. We use tools from random matrix theory to study the multi-spiked tensor\\nmodel, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In\\nparticular, thanks to the nature of local optimization methods used to find the\\nmaximum likelihood estimator of this model, we propose to study the phase\\ntransition phenomenon for finding critical points of the corresponding\\noptimization problem, i.e., those points defined by the Karush-Kuhn-Tucker\\n(KKT) conditions. Moreover, we characterize the limiting alignments between the\\nestimated signals corresponding to a critical point of the likelihood and the\\nground truth signals. With the help of these results, we propose a new\\nestimator of the rank-$r$ tensor weights by solving a system of polynomial\\nequations, which is asymptotically unbiased contrary the maximum likelihood\\nestimator. We consider a process $X^\\\\varepsilon$ solution of a stochastic Volterra\\nequation with an unknown parameter $\\\\theta^\\\\star$ in the drift function. The\\nVolterra kernel is singular and given by $K(u)=c u^{\\\\alpha-1/2}\\n\\\\mathbb{1}_{u>0}$ with $\\\\alpha \\\\in (0,1/2)$. It is assumed that the diffusion\\ncoefficient is proportional to $\\\\varepsilon \\\\to 0$. From an observation of the\\npath $(X^\\\\varepsilon_s)_{s\\\\in[0,T]}$, we construct a Trajectory Fitting\\nEstimator, which is shown to be consistent and asymptotically normal. We also\\nspecify identifiability conditions insuring the $L^p$ convergence of the\\nestimator. We address the problem of safety verification for nonlinear stochastic\\nsystems, specifically the task of certifying that system trajectories remain\\nwithin a safe set with high probability. To tackle this challenge, we adopt a\\nset-erosion strategy, which decouples the effects of stochastic disturbances\\nfrom deterministic dynamics. This approach converts the stochastic safety\\nverification problem on a safe set into a deterministic safety verification\\nproblem on an eroded subset of the safe set. The success of this strategy\\nhinges on the depth of erosion, which is determined by a probabilistic tube\\nthat bounds the deviation of stochastic trajectories from their corresponding\\ndeterministic trajectories. Our main contribution is the establishment of a\\ntight bound for the probabilistic tube of nonlinear stochastic systems. To\\nobtain a probabilistic bound for stochastic trajectories, we adopt a\\nmartingale-based approach. The core innovation lies in the design of a novel\\nenergy function associated with the averaged moment generating function, which\\nforms an affine martingale, a generalization of the traditional c-martingale.\\nUsing this energy function, we derive a precise bound for the probabilistic\\ntube. Furthermore, we enhance this bound by incorporating the union-bound\\ninequality for strictly contractive dynamics. By integrating the derived\\nprobabilistic tubes into the set-erosion strategy, we demonstrate that the\\nsafety verification problem for nonlinear stochastic systems can be reduced to\\na deterministic safety verification problem. Our theoretical results are\\nvalidated through applications in reachability-based safety verification and\\nsafe controller synthesis, accompanied by several numerical examples that\\nillustrate their effectiveness. In this paper, we present the asymptotic properties of the moment estimator\\nfor autoregressive (AR for short) models subject to Markovian changes in regime\\nunder the assumption that the errors are uncorrelated but not necessarily\\nindependent. We relax the standard independence assumption on the innovation\\nprocess to extend considerably the range of application of the Markov-switching\\nAR models. We provide necessary conditions to prove the consistency and\\nasymptotic normality of the moment estimator in a specific case. Particular\\nattention is paid to the estimation of the asymptotic covariance matrix.\\nFinally, some simulation studies and an application to the hourly\\nmeteorological data are presented to corroborate theoretical work. We propose a first near complete (that will make explicit sense in the main\\ntext) nonasymptotic generalization theory for multilayer neural networks with\\narbitrary Lipschitz activations and general Lipschitz loss functions (with some\\nvery mild conditions). In particular, it doens\\'t require the boundness of loss\\nfunction, as commonly assumed in the literature. Our theory goes beyond the\\nbias-variance tradeoff, aligned with phenomenon typically encountered in deep\\nlearning. It is therefore sharp different with other existing nonasymptotic\\ngeneralization error bounds for neural networks. More explicitly, we propose an\\nexplicit generalization error upper bound for multilayer neural networks with\\narbitrary Lipschitz activations $\\\\sigma$ with $\\\\sigma(0)=0$ and broad enough\\nLipschitz loss functions, without requiring either the width, depth or other\\nhyperparameters of the neural network approaching infinity, a specific neural\\nnetwork architect (e.g. sparsity, boundness of some norms), a particular\\nactivation function, a particular optimization algorithm or boundness of the\\nloss function, and with taking the approximation error into consideration.\\nGeneral Lipschitz activation can also be accommodated into our framework. A\\nfeature of our theory is that it also considers approximation errors.\\nFurthermore, we show the near minimax optimality of our theory for multilayer\\nReLU networks for regression problems. Notably, our upper bound exhibits the\\nfamous double descent phenomenon for such networks, which is the most\\ndistinguished characteristic compared with other existing results. This work\\nemphasizes a view that many classical results should be improved to embrace the\\nunintuitive characteristics of deep learning to get a better understanding of\\nit. We develop a pseudo-likelihood theory for rank one matrix estimation problems\\nin the high dimensional limit. We prove a variational principle for the\\nlimiting pseudo-maximum likelihood which also characterizes the performance of\\nthe corresponding pseudo-maximum likelihood estimator. We show that this\\nvariational principle is universal and depends only on four parameters\\ndetermined by the corresponding null model. Through this universality, we\\nintroduce a notion of equivalence for estimation problems of this type and, in\\nparticular, show that a broad class of estimation tasks, including community\\ndetection, sparse submatrix detection, and non-linear spiked matrix models, are\\nequivalent to spiked matrix models. As an application, we obtain a complete\\ndescription of the performance of the least-squares (or ``best rank one\\'\\')\\nestimator for any rank one matrix estimation problem. Multivariate spatial phenomena are ubiquitous, spanning domains such as\\nclimate, pandemics, air quality, and social economy. Cross-correlation between\\ndifferent quantities of interest at different locations is asymmetric in\\ngeneral. This paper provides the visualization, structure, and properties of\\nasymmetric cross-correlation as well as symmetric auto-correlation. It reviews\\nmainstream multivariate spatial models and analyzes their capability to\\naccommodate asymmetric cross-correlation. It also illustrates the difference in\\nmodel accuracy with and without asymmetric accommodation using a 1D simulated\\nexample. The failure of key financial institutions may accelerate risk contagion due\\nto their interconnections within the system. In this paper, we propose a robust\\nportfolio strategy to mitigate systemic risks during extreme events. We use the\\nstock returns of key financial institutions as an indicator of their\\nperformance, apply extreme value theory to assess the extremal dependence among\\nstocks of financial institutions, and construct a network model based on a\\nthreshold approach that captures extremal dependence. Our analysis reveals\\ndifferent dependence structures in the Chinese and U.S. financial systems. By\\napplying the maximum independent set (MIS) from graph theory, we identify a\\nsubset of institutions with minimal extremal dependence, facilitating the\\nconstruction of diversified portfolios resilient to risk contagion. We also\\ncompare the performance of our proposed portfolios with that of the market\\nportfolios in the two economies. Upon observing $n$-dimensional multivariate Gaussian data, when can we infer\\nthat the largest $K$ observations came from the largest $K$ means? When $K=1$\\nand the covariance is isotropic, \\\\cite{Gutmann} argue that this inference is\\njustified when the two-sided difference-of-means test comparing the largest and\\nsecond largest observation rejects. Leveraging tools from selective inference,\\nwe provide a generalization of their procedure that applies for both any $K$\\nand any covariance structure. We show that our procedure draws the desired\\ninference whenever the two-sided difference-of-means test comparing the pair of\\nobservations inside and outside the top $K$ with the smallest standardized\\ndifference rejects, and sometimes even when this test fails to reject. Using\\nthis insight, we argue that our procedure renders existing simultaneous\\ninference approaches inadmissible when $n > 2$. When the observations are\\nindependent (with possibly unequal variances) or equicorrelated, our procedure\\ncorresponds exactly to running the two-sided difference-of-means test comparing\\nthe pair of observations inside and outside the top $K$ with the smallest\\nstandardized difference. Laplacian matrices are commonly employed in many real applications, encoding\\nthe underlying latent structural information such as graphs and manifolds. The\\nuse of the normalization terms naturally gives rise to random matrices with\\ndependency. It is well-known that dependency is a major bottleneck of new\\nrandom matrix theory (RMT) developments. To this end, in this paper, we\\nformally introduce a class of generalized (and regularized) Laplacian matrices,\\nwhich contains the Laplacian matrix and the random adjacency matrix as a\\nspecific case, and suggest the new framework of the asymptotic theory of\\neigenvectors for latent embeddings with generalized Laplacian matrices\\n(ATE-GL). Our new theory is empowered by the tool of generalized quadratic\\nvector equation for dealing with RMT under dependency, and delicate high-order\\nasymptotic expansions of the empirical spiked eigenvectors and eigenvalues\\nbased on local laws. The asymptotic normalities established for both spiked\\neigenvectors and eigenvalues will enable us to conduct precise inference and\\nuncertainty quantification for applications involving the generalized Laplacian\\nmatrices with flexibility. We discuss some applications of the suggested ATE-GL\\nframework and showcase its validity through some numerical examples. The multi-armed bandits (MAB) framework is a widely used approach for\\nsequential decision-making, where a decision-maker selects an arm in each round\\nwith the goal of maximizing long-term rewards. Moreover, in many practical\\napplications, such as personalized medicine and recommendation systems,\\nfeedback is provided in batches, contextual information is available at the\\ntime of decision-making, and rewards from different arms are related rather\\nthan independent. We propose a novel semi-parametric framework for batched\\nbandits with covariates and a shared parameter across arms, leveraging the\\nsingle-index regression (SIR) model to capture relationships between arm\\nrewards while balancing interpretability and flexibility. Our algorithm,\\nBatched single-Index Dynamic binning and Successive arm elimination (BIDS),\\nemploys a batched successive arm elimination strategy with a dynamic binning\\nmechanism guided by the single-index direction. We consider two settings: one\\nwhere a pilot direction is available and another where the direction is\\nestimated from data, deriving theoretical regret bounds for both cases. When a\\npilot direction is available with sufficient accuracy, our approach achieves\\nminimax-optimal rates (with $d = 1$) for nonparametric batched bandits,\\ncircumventing the curse of dimensionality. Extensive experiments on simulated\\nand real-world datasets demonstrate the effectiveness of our algorithm compared\\nto the nonparametric batched bandit method introduced by\\n\\\\cite{jiang2024batched}. In this paper, we consider a two-stage Gibbs sampler for a normal linear\\nregression model with a horseshoe prior. Under some assumptions, we show that\\nit produces a geometrically ergodic Markov chain. In particular, we prove\\ngeometric ergodicity under some three-parameter beta global prior which does\\nnot have a finite $(p / 5)$-th negative moment, where $p$ is the number of\\nregression coefficients. This is in contrast to the case of a known general\\nresult which is applicable if the global parameter has a finite approximately\\n$(p / 2)$-th negative moment. I present a novel uniform law of large numbers (ULLN) for network-dependent\\ndata. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive\\nsuite of limit theorems and a robust variance estimator for network-dependent\\nprocesses, their analysis focuses on pointwise convergence. On the other hand,\\nuniform convergence is essential for nonlinear estimators such as M and GMM\\nestimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I\\nestablish the ULLN under network dependence and demonstrate its utility by\\nproving the consistency of both M and GMM estimators. A byproduct of this work\\nis a novel maximal inequality for network data, which may prove useful for\\nfuture research beyond the scope of this paper. We revisit the problem of constructing predictive confidence sets for which\\nwe wish to obtain some type of conditional validity. We provide new arguments\\nshowing how ``split conformal\\'\\' methods achieve near desired coverage levels\\nwith high probability, a guarantee conditional on the validation data rather\\nthan marginal over it. In addition, we directly consider (approximate)\\nconditional coverage, where, e.g., conditional on a covariate $X$ belonging to\\nsome group of interest, we would like a guarantee that a predictive set covers\\nthe true outcome $Y$. We show that the natural method of performing quantile\\nregression on a held-out (validation) dataset yields minimax optimal guarantees\\nof coverage here. Complementing these positive results, we also provide\\nexperimental evidence that interesting work remains to be done to develop\\ncomputationally efficient but valid predictive inference methods. Linear inverse problems are ubiquitous in various science and engineering\\ndisciplines. Of particular importance in the past few decades, is the\\nincorporation of sparsity based priors, in particular $\\\\ell_1$ priors, into\\nlinear inverse problems, which led to the flowering of fields of compressive\\nsensing (CS) and sparsity based signal processing. More recently, methods based\\non a Compound Gaussian (CG) prior have been investigated and demonstrate\\nimproved results over CS in practice. This paper is the first attempt to\\nidentify and elucidate the fundamental structures underlying the success of CG\\nmethods by studying CG in the context of a broader framework of\\ngeneralized-sparsity-based-inference. After defining our notion of generalized\\nsparsity we introduce a weak null space property and proceed to generalize two\\nwell-known methods in CS, basis pursuit and iteratively reweighted least\\nsquares (IRLS). We show how a subset of CG-induced regularizers fits into this\\nframework. Given a tree $T$, its path polytope is the convex hull of the edge indicator\\nvectors for the paths between any two distinct leaves in $T$. These polytopes\\narise naturally in polyhedral geometry and applications, such as phylogenetics,\\ntropical geometry, and algebraic statistics. We provide a minimal halfspace\\nrepresentation of these polytopes. The construction is made inductively using\\ntoric fiber products. We introduce a novel class of bivariate common-shock discrete phase-type\\n(CDPH) distributions to describe dependencies in loss modeling, with an\\nemphasis on those induced by common shocks. By constructing two jointly\\nevolving terminating Markov chains that share a common evolution up to a random\\ntime corresponding to the common shock component, and then proceed\\nindependently, we capture the essential features of risk events influenced by\\nshared and individual-specific factors. We derive explicit expressions for the\\njoint distribution of the termination times and prove various class and\\ndistributional properties, facilitating tractable analysis of the risks.\\nExtending this framework, we model random sums where aggregate claims are sums\\nof continuous phase-type random variables with counts determined by these\\ntermination times, and show that their joint distribution belongs to the\\nmultivariate phase-type or matrix-exponential class. We develop estimation\\nprocedures for the CDPH distributions using the expectation-maximization\\nalgorithm and demonstrate the applicability of our models through simulation\\nstudies and an application to bivariate insurance claim frequency data. We examine the location characteristics of a conditional selective confidence\\ninterval based on the polyhedral method. This interval is constructed from the\\ndistribution of a test statistic conditional upon the event of statistical\\nsignificance. In the case of a one-sided test, the behavior of the interval\\nvaries depending on whether the parameter is highly significant or only\\nmarginally significant. When the parameter is highly significant, the interval\\nis similar to the usual confidence interval derived without considering\\nselection. However, when the parameter is only marginally significant, the\\ninterval falls into an extreme range and deviates greatly from the estimated\\nvalue of the parameter. In contrast, an interval conditional on two-sided\\nsignificance does not yield extreme results, although it may exclude the\\nestimated parameter value. Reproducing Kernel Hilbert Space (RKHS) embedding of probability\\ndistributions has proved to be an effective approach, via MMD (maximum mean\\ndiscrepancy) for nonparametric hypothesis testing problems involving\\ndistributions defined over general (non-Euclidean) domains. While a substantial\\namount of work has been done on this topic, only recently, minimax optimal\\ntwo-sample tests have been constructed that incorporate, unlike MMD, both the\\nmean element and a regularized version of the covariance operator. However, as\\nwith most kernel algorithms, the computational complexity of the optimal test\\nscales cubically in the sample size, limiting its applicability. In this paper,\\nwe propose a spectral regularized two-sample test based on random Fourier\\nfeature (RFF) approximation and investigate the trade-offs between statistical\\noptimality and computational efficiency. We show the proposed test to be\\nminimax optimal if the approximation order of RFF (which depends on the\\nsmoothness of the likelihood ratio and the decay rate of the eigenvalues of the\\nintegral operator) is sufficiently large. We develop a practically\\nimplementable permutation-based version of the proposed test with a\\ndata-adaptive strategy for selecting the regularization parameter and the\\nkernel. Finally, through numerical experiments on simulated and benchmark\\ndatasets, we demonstrate that the proposed RFF-based test is computationally\\nefficient and performs almost similar (with a small drop in power) to the exact\\ntest. We study the coverage properties of full conformal regression in the\\nproportional asymptotic regime where the ratio of the dimension and the sample\\nsize converges to a constant. In this setting, existing theory tells us only\\nthat full conformal inference is unbiased, in the sense that its average\\ncoverage lies at the desired level when marginalized over both the new test\\npoint and the training data. Considerably less is known about the behaviour of\\nthese methods conditional on the training set. As a result, the exact benefits\\nof full conformal inference over much simpler alternative methods is unclear.\\nThis paper investigates the behaviour of full conformal inference and natural\\nuncorrected alternatives for a broad class of $L_2$-regularized linear\\nregression models. We show that in the proportional asymptotic regime the\\ntraining-conditional coverage of full conformal inference concentrates at the\\ntarget value. On the other hand, simple alternatives that directly compare test\\nand training residuals realize constant undercoverage bias. While these results\\ndemonstrate the necessity of full conformal in correcting for high-dimensional\\noverfitting, we also show that this same methodology is redundant for the\\nrelated task of tuning the regularization level. In particular, we show that\\nfull conformal inference still yields asymptotically valid coverage when the\\nregularization level is selected using only the training set, without\\nconsideration of the test point. Simulations show that our asymptotic\\napproximations are accurate in finite samples and can be readily extended to\\nother popular full conformal variants, such as full conformal quantile\\nregression and the LASSO, that do not directly meet our assumptions. The Kolmogorov-Smirnov (KS) statistic is a classical nonparametric test\\nwidely used for comparing an empirical distribution function with a reference\\ndistribution or for comparing two empirical distributions. Despite its broad\\napplicability in statistical hypothesis testing and model validation, certain\\naspects of the KS statistic remain under-explored among the young generation,\\nparticularly under finite sample conditions. This paper revisits the KS\\nstatistic in both one-sample and two-sample scenarios, considering one-sided\\nand two-sided variants. We derive exact probabilities for the supremum of the\\nempirical process and present a unified treatment of the KS statistic under\\ndiverse settings. Additionally, we explore the discrete nature of the hitting\\ntimes of the normalized empirical process, providing practical insights into\\nthe computation of KS test p-values. The study also discusses the\\nDvoretzky-Kiefer-Wolfowitz-Massart (DKWM) inequality, highlighting its role in\\nconstructing confidence bands for distribution functions. Using empirical\\nprocess theory, we establish the limit distribution of the KS statistic when\\nthe true distribution includes unknown parameters. Our findings extend existing\\nresults, offering improved methodologies for statistical analysis and\\nhypothesis testing using the KS statistic, particularly in finite sample\\nscenarios. A one-shot device is a unit that operates only once, after which it is either\\ndestroyed or needs to be rebuilt. For this type of device, the operational\\nstatus can only be assessed at a specific inspection time, determining whether\\nfailure occurred before or after it. Consequently, lifetimes are subject to\\nleft- or right-censoring. One-shot devices are usually highly reliables. To\\nanalyze the reliability of such products, an accelerated life test (ALT) plan\\nis typically employed by subjecting the devices to increased levels of stress\\nfactors, thus allowing life characteristics observed under high-stress\\nconditions to be extrapolated to normal operating conditions. By accelerating\\nthe degradation process, ALT significantly reduces both the time required for\\ntesting and the associated experimental costs.\\n  Recently, robust inferential methods have gained considerable interest in\\nstatistical analysis. Among them, weighted minimum density power divergence\\nestimators (WMDPDEs) are widely recognized for their robust statistical\\nproperties with small loss of efficiency. In this work, robust WMDPDE and\\nassociated statistical tests are developed under a log-logistic lifetime\\ndistribution with multiple stresses. Explicit expressions for the estimating\\nequations and asymptotic distribution of the estimators are obtained. Further,\\na Monte Carlo simulation study is presented to evaluate the performance of the\\nWMDPDE in practical applications. Learning kernels in operators from data lies at the intersection of inverse\\nproblems and statistical learning, offering a powerful framework for capturing\\nnonlocal dependency in function spaces and high-dimensional settings. In\\ncontrast to classical nonparametric regression, where the inverse problem is\\nwell-posed, kernel estimation involves a compact normal operator and an\\nill-posed deconvolution. To address these challenges, we introduce adaptive\\nspectral Sobolev spaces, unifying Sobolev spaces and reproducing kernel Hilbert\\nspaces, that automatically discard non-identifiable components and control\\nterms with small eigenvalues. Within this framework, we establish the minimax\\nconvergence rates for the mean squared error under both polynomial and\\nexponential spectral decay regimes. Methodologically, we develop a tamed least\\nsquares estimator achieving the minimax upper rates via controlling the\\nleft-tail probability for eigenvalues of the random normal matrix; and for the\\nminimax lower rates, we resolve challenges from infinite-dimensional measures\\nthrough their projections. This paper investigates conditional specifications for multivariate count\\nvariables. Recently, the spatial count data literature has proposed several\\nconditional models such that the conditional expectations are linear in the\\nconditioning variables. These models are much easier to estimate than existing\\nspatial count models based on Gaussian random field. However, whether or not\\nsuch conditional specifications are compatible have not been addressed. We\\ninvestigate two large families of conditional models, that are the compound\\nautoregressive model and the random coefficient integer autoregressive model.\\nWe characterize all the solutions to these two families of models at arbitrary\\ndimensions, and find that only a handful of them admit non-trivial solutions.\\nWe then show that if we focus on the linearity condition of the conditional\\nexpectations only, a considerable larger family of solutions can be obtained.\\nThis suggests that for spatial count data modeling, semi-parametric type\\nspecifications that impose the conditional expectation structure is preferable. In this paper we extend the classical Glivenko-Cantelli theorem to\\nreal-valued empirical functions under dependence structures characterised by\\n$\\\\alpha$-mixing and $\\\\beta$-mixing conditions. We investigate sufficient\\nconditions ensuring that families of real-valued functions exhibit the\\nGlivenko-Cantelli (GC) property in these dependence settings. Our analysis\\nfocuses on function classes satisfying uniform entropy conditions and\\nestablishes deviation bounds under mixing coefficients that decay at\\nappropriate rates. Our results refine the existing literature by relaxing the\\nindependence assumptions and highlighting the role of dependence in empirical\\nprocess convergence. We study two G-modeling strategies for estimating the signal distribution\\n(the empirical Bayesian\\'s prior) from observations corrupted with normal noise.\\nFirst, we choose the signal distribution by minimizing Stein\\'s unbiased risk\\nestimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach\\nmotivated by optimal empirical Bayesian shrinkage estimation of the signals.\\nSecond, we select the signal distribution by minimizing Hyv\\\\\"arinen\\'s score\\nmatching objective for the implied score (derivative of log-marginal density),\\ntargeting minimal Fisher divergence between estimated and true marginal\\ndensities. While these strategies appear distinct, they are known to be\\nmathematically equivalent. We provide a unified analysis of SURE and score\\nmatching under both well-specified signal distribution classes and\\nmisspecification. In the classical well-specified setting with homoscedastic\\nnoise and compactly supported signal distribution, we establish nearly\\nparametric rates of convergence of the empirical Bayes regret and the Fisher\\ndivergence. In a commonly studied misspecified model, we establish fast rates\\nof convergence to the oracle denoiser and corresponding oracle inequalities.\\nOur empirical results demonstrate competitiveness with nonparametric maximum\\nlikelihood in well-specified settings, while showing superior performance under\\nmisspecification, particularly in settings involving heteroscedasticity and\\nside information. In many random phenomena, such as life-testing experiments and environmental\\ndata (like rainfall data), there are often positive values and an excess of\\nzeros, which create modeling challenges. In life testing, immediate failures\\nresult in zero lifetimes, often due to defects or poor quality, especially in\\nelectronics and clinical trials. These failures, called zero inliers, are\\ndifficult to model using standard approaches. When studying extreme values in\\nthe above scenarios, a key issue is selecting an appropriate threshold for\\naccurate tail approximation of the population using asymptotic models. While\\nsome extreme value mixture models address threshold estimation and tail\\napproximation, conventional parametric and non-parametric bulk and generalised\\nPareto distribution (GPD) approaches often neglect inliers, leading to\\nsuboptimal results. This paper introduces a framework for modeling extreme\\nevents and inliers using the GPD, addressing threshold uncertainty and\\neffectively capturing inliers at zero. The model\\'s parameters are estimated\\nusing the maximum likelihood estimation (MLE) method, ensuring optimal\\nprecision. Through simulation studies and real-world applications, we\\ndemonstrate that the proposed model significantly outperforms the traditional\\nmethods, which typically neglect inliers at the origin. This paper continues the study of the efficiency of conformal prediction as\\ncompared with more general randomness prediction and exchangeability\\nprediction. It does not restrict itself to the case of classification, and our\\nresults will also be applicable to the case of regression. The price to pay is\\nthat efficiency will be attained only on average, albeit with respect to a wide\\nrange of probability measures on the label space. Time series in natural sciences, such as hydrology and climatology, and other\\nenvironmental applications, often consist of continuous observations\\nconstrained to the unit interval (0,1). Traditional Gaussian-based models fail\\nto capture these bounds, requiring more flexible approaches. This paper\\nintroduces the Matsuoka Autoregressive Moving Average (MARMA) model, extending\\nthe GARMA framework by assuming a Matsuoka-distributed random component taking\\nvalues in (0,1) and an ARMA-like systematic structure allowing for random\\ntime-dependent covariates. Parameter estimation is performed via partial\\nmaximum likelihood (PMLE), for which we present the asymptotic theory. It\\nenables statistical inference, including confidence intervals and model\\nselection. To construct prediction intervals, we propose a novel\\nbootstrap-based method that accounts for dependence structure uncertainty. A\\ncomprehensive Monte Carlo simulation study assesses the finite sample\\nperformance of the proposed methodologies, while an application to forecasting\\nthe useful water volume of the Guarapiranga Reservoir in Brazil showcases their\\npractical usefulness. Many scientific problems involve data exhibiting both temporal and\\ncross-sectional dependencies. While linear dependencies have been extensively\\nstudied, the theoretical analysis of regression estimators under nonlinear\\ndependencies remains scarce. This work studies a kernel-based estimation\\nprocedure for nonlinear dynamics within the reproducing kernel Hilbert space\\nframework, focusing on nonlinear vector autoregressive models. We derive\\nnonasymptotic probabilistic bounds on the deviation between a regularized\\nkernel estimator and the nonlinear regression function. A key technical\\ncontribution is a concentration bound for quadratic forms of stochastic\\nmatrices in the presence of dependent data, which is of independent interest.\\nAdditionally, we characterize conditions on multivariate kernels that guarantee\\noptimal convergence rates. In statistics, generalized linear models (GLMs) are widely used for modeling\\ndata and can expressively capture potential nonlinear dependence of the model\\'s\\noutcomes on its covariates. Within the broad family of GLMs, those with binary\\noutcomes, which include logistic and probit regressions, are motivated by\\ncommon tasks such as binary classification with (possibly) non-separable data.\\nIn addition, in modern machine learning and statistics, data is often\\nhigh-dimensional yet has a low intrinsic dimension, making sparsity constraints\\nin models another reasonable consideration. In this work, we propose to use and\\nanalyze an iterative hard thresholding (projected gradient descent on the ReLU\\nloss) algorithm, called binary iterative hard thresholding (BIHT), for\\nparameter estimation in sparse GLMs with binary outcomes. We establish that\\nBIHT is statistically efficient and converges to the correct solution for\\nparameter estimation in a general class of sparse binary GLMs. Unlike many\\nother methods for learning GLMs, including maximum likelihood estimation,\\ngeneralized approximate message passing, and GLM-tron (Kakade et al. 2011;\\nBahmani et al. 2016), BIHT does not require knowledge of the GLM\\'s link\\nfunction, offering flexibility and generality in allowing the algorithm to\\nlearn arbitrary binary GLMs. As two applications, logistic and probit\\nregression are additionally studied. In this regard, it is shown that in\\nlogistic regression, the algorithm is in fact statistically optimal in the\\nsense that the order-wise sample complexity matches (up to logarithmic factors)\\nthe lower bound obtained previously. To the best of our knowledge, this is the\\nfirst work achieving statistical optimality for logistic regression in all\\nnoise regimes with a computationally efficient algorithm. Moreover, for probit\\nregression, our sample complexity is on the same order as that obtained for\\nlogistic regression. We study high-dimensional random geometric graphs (RGGs) of edge-density $p$\\nwith vertices uniformly distributed on the $d$-dimensional torus and edges\\ninserted between sufficiently close vertices with respect to an $L_q$-norm. We\\nfocus on distinguishing an RGG from an Erd\\\\H{o}s--R\\\\\\'enyi (ER) graph if both\\nmodels have edge probability $p$. So far, most results considered either\\nspherical RGGs with $L_2$-distance or toroidal RGGs under $L_\\\\infty$-distance.\\nHowever, for general $L_q$-distances, many questions remain open, especially if\\n$p$ is allowed to depend on $n$. The main reason for this is that RGGs under\\n$L_q$-distances can not easily be represented as the logical AND of their\\n1-dimensional counterparts, as for $L_\\\\infty$ geometries. To overcome this, we\\ndevise a novel technique for quantifying the dependence between edges based on\\nmodified Edgeworth expansions.\\n  Our technique yields the first tight algorithmic upper bounds for\\ndistinguishing toroidal RGGs under general $L_q$ norms from ER-graphs for fixed\\n$p$ and $q$. We achieve this by showing that signed triangles can distinguish\\nthe two models when $d\\\\ll n^3p^3$ for the whole regime of $c/n<p<1$.\\nAdditionally, our technique yields an improved information-theoretic lower\\nbound for this task, showing that the two distributions converge whenever\\n$d=\\\\tilde{\\\\Omega}(n^3p^2)$, which is just as strong as the currently best known\\nlower bound for spherical RGGs in case of general $p$ from Liu et al.\\n[STOC\\'22]. Finally, our expansions allow us to tightly characterize the\\nspectral properties of toroidal RGGs both under $L_q$-distances for fixed $1\\\\le\\nq<\\\\infty$, and $L_\\\\infty$-distance. Our results partially resolve a conjecture\\nof Bangachev and Bresler [COLT\\'24] and prove that the distance metric, rather\\nthan the underlying space, is responsible for the observed differences in the\\nbehavior of spherical and toroidal RGGs. In this work, we present a new perspective on the origin and interpretation\\nof adaptive filters. By applying Bayesian principles of recursive inference\\nfrom the state-space model and using a series of simplifications regarding the\\nstructure of the solution, we can present, in a unified framework, derivations\\nof many adaptive filters which depend on the probabilistic model of the\\nobservational noise. In particular, under a Gaussian model, we obtain solutions\\nwell-known in the literature (such as LMS, NLMS, or Kalman filter), while using\\nnon-Gaussian noise, we obtain new families of adaptive filter. Notably, under\\nassumption of Laplacian noise, we obtain a family of robust filters of which\\nthe signed-error algorithm is a well-known member, while other algorithms,\\nderived effortlessly in the proposed framework, are entirely new. Numerical\\nexamples are shown to illustrate the properties and provide a better insight\\ninto the performance of the derived adaptive filters. A novel method for sequential outlier detection in non-stationary time series\\nis proposed. The method tests the null hypothesis of ``no outlier\\'\\' at each\\ntime point, addressing the multiple testing problem by bounding the error\\nprobability of successive tests, using extreme value theory. The asymptotic\\nproperties of the test statistic are studied under the null hypothesis and\\nalternative. The finite sample properties of the new detection scheme are\\ninvestigated by means of a simulation study, and the method is compared with\\nalternative procedures which have recently been proposed in the statistics and\\nmachine learning literature. This work deals with the generation of theoretical correlation matrices with\\nspecific sparsity patterns, associated to graph structures. We present a novel\\napproach based on convex optimization, offering greater flexibility compared to\\nexisting techniques, notably by controlling the mean of the entry distribution\\nin the generated correlation matrices. This allows for the generation of\\ncorrelation matrices that better represent realistic data and can be used to\\nbenchmark statistical methods for graph inference. Hypothesis tests and confidence intervals are ubiquitous in empirical\\nresearch, yet their connection to subsequent decision-making is often unclear.\\nWe develop a theory of certified decisions that pairs recommended decisions\\nwith inferential guarantees. Specifically, we attach P-certificates -- upper\\nbounds on loss that hold with probability at least $1-\\\\alpha$ -- to recommended\\nactions. We show that such certificates allow \"safe,\" risk-controlling adoption\\ndecisions for ambiguity-averse downstream decision-makers. We further prove\\nthat it is without loss to limit attention to P-certificates arising as minimax\\ndecisions over confidence sets, or what Manski (2021) terms \"as-if decisions\\nwith a set estimate.\" A parallel argument applies to E-certified decisions\\nobtained from e-values in settings with unbounded loss. In many real applications of statistical learning, collecting sufficiently\\nmany training data is often expensive, time-consuming, or even unrealistic. In\\nthis case, a transfer learning approach, which aims to leverage knowledge from\\na related source domain to improve the learning performance in the target\\ndomain, is more beneficial. There have been many transfer learning methods\\ndeveloped under various distributional assumptions. In this article, we study a\\nparticular type of classification problem, called conformal prediction, under a\\nnew distributional assumption for transfer learning. Classifiers under the\\nconformal prediction framework predict a set of plausible labels instead of one\\nsingle label for each data instance, affording a more cautious and safer\\ndecision. We consider a generalization of the \\\\textit{covariate shift with\\nposterior drift} setting for transfer learning. Under this setting, we propose\\na weighted conformal classifier that leverages both the source and target\\nsamples, with a coverage guarantee in the target domain. Theoretical studies\\ndemonstrate favorable asymptotic properties. Numerical studies further\\nillustrate the usefulness of the proposed method. We consider statistical inference under a semi-supervised setting where we\\nhave access to both a labeled dataset consisting of pairs $\\\\{X_i, Y_i\\n\\\\}_{i=1}^n$ and an unlabeled dataset $\\\\{ X_i \\\\}_{i=n+1}^{n+N}$. We ask the\\nquestion: under what circumstances, and by how much, can incorporating the\\nunlabeled dataset improve upon inference using the labeled data? To answer this\\nquestion, we investigate semi-supervised learning through the lens of\\nsemiparametric efficiency theory. We characterize the efficiency lower bound\\nunder the semi-supervised setting for an arbitrary inferential problem, and\\nshow that incorporating unlabeled data can potentially improve efficiency if\\nthe parameter is not well-specified. We then propose two types of\\nsemi-supervised estimators: a safe estimator that imposes minimal assumptions,\\nis simple to compute, and is guaranteed to be at least as efficient as the\\ninitial supervised estimator; and an efficient estimator, which -- under\\nstronger assumptions -- achieves the semiparametric efficiency bound. Our\\nfindings unify existing semiparametric efficiency results for particular\\nspecial cases, and extend these results to a much more general class of\\nproblems. Moreover, we show that our estimators can flexibly incorporate\\npredicted outcomes arising from ``black-box\" machine learning models, and\\nthereby achieve the same goal as prediction-powered inference (PPI), but with\\nsuperior theoretical guarantees. We also provide a complete understanding of\\nthe theoretical basis for the existing set of PPI methods. Finally, we apply\\nthe theoretical framework developed to derive and analyze efficient\\nsemi-supervised estimators in a number of settings, including M-estimation,\\nU-statistics, and average treatment effect estimation, and demonstrate the\\nperformance of the proposed estimators via simulations. Motivated by learning dynamical structures from static snapshot data, this\\npaper presents a distribution-on-scalar regression approach for estimating the\\ndensity evolution of a stochastic process from its noisy temporal point clouds.\\nWe propose an entropy-regularized nonparametric maximum likelihood estimator\\n(E-NPMLE), which leverages the entropic optimal transport as a smoothing\\nregularizer for the density flow. We show that the E-NPMLE has almost\\ndimension-free statistical rates of convergence to the ground truth\\ndistributions, which exhibit a striking phase transition phenomenon in terms of\\nthe number of snapshots and per-snapshot sample size. To efficiently compute\\nthe E-NPMLE, we design a novel particle-based and grid-free coordinate KL\\ndivergence gradient descent (CKLGD) algorithm and prove its polynomial\\niteration complexity. Moreover, we provide numerical evidence on synthetic data\\nto support our theoretical findings. This work contributes to the theoretical\\nunderstanding and practical computation of estimating density evolution from\\nnoisy observations in arbitrary dimensions. A fundamental problem in statistics and machine learning is to estimate a\\nfunction $f$ from possibly noisy observations of its point samples. The goal is\\nto design a numerical algorithm to construct an approximation $\\\\hat f$ to $f$\\nin a prescribed norm that asymptotically achieves the best possible error (as a\\nfunction of the number $m$ of observations and the variance $\\\\sigma^2$ of the\\nnoise). This problem has received considerable attention in both nonparametric\\nstatistics (noisy observations) and optimal recovery (noiseless observations).\\nQuantitative bounds require assumptions on $f$, known as model class\\nassumptions. Classical results assume that $f$ is in the unit ball of a Besov\\nspace. In nonparametric statistics, the best possible performance of an\\nalgorithm for finding $\\\\hat f$ is known as the minimax rate and has been\\nstudied in this setting under the assumption that the noise is Gaussian. In\\noptimal recovery, the best possible performance of an algorithm is known as the\\noptimal recovery rate and has also been determined in this setting. While one\\nwould expect that the minimax rate recovers the optimal recovery rate when the\\nnoise level $\\\\sigma$ tends to zero, it turns out that the current results on\\nminimax rates do not carefully determine the dependence on $\\\\sigma$ and the\\nlimit cannot be taken. This paper handles this issue and determines the\\nnoise-level-aware (NLA) minimax rates for Besov classes when error is measured\\nin an $L_q$-norm with matching upper and lower bounds. The end result is a\\nreconciliation between minimax rates and optimal recovery rates. The NLA\\nminimax rate continuously depends on the noise level and recovers the optimal\\nrecovery rate when $\\\\sigma$ tends to zero. We study the design of adaptive, sequential experiments for unbiased average\\ntreatment effect (ATE) estimation in the design-based potential outcomes\\nsetting. Our goal is to develop adaptive designs offering sublinear Neyman\\nregret, meaning their efficiency must approach that of the hindsight-optimal\\nnonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first\\nmethod achieving $\\\\widetilde{O}(\\\\sqrt{T})$ expected Neyman regret under mild\\nconditions. In this work, we propose adaptive designs with substantially\\nstronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain\\nanytime $\\\\widetilde{O}(\\\\log T)$ Neyman regret under natural boundedness\\nassumptions. Further, in the setting where experimental units have\\npre-treatment covariates, we introduce and study a class of contextual\\n\"multigroup\" Neyman regret guarantees: Given any set of possibly overlapping\\ngroups based on the covariates, the adaptive design outperforms each group\\'s\\nbest non-adaptive designs. In particular, we develop a contextual adaptive\\ndesign with $\\\\widetilde{O}(\\\\sqrt{T})$ anytime multigroup Neyman regret. We\\nempirically validate the proposed designs through an array of experiments. Gaussian multiplicative chaos (GMC) is a canonical random fractal measure\\nobtained by exponentiating log-correlated Gaussian processes, first constructed\\nin the seminal work of Kahane (1985). Since then it has served as an important\\nbuilding block in constructions of quantum field theories and Liouville quantum\\ngravity. However, in many natural settings, non-Gaussian log-correlated\\nprocesses arise. In this paper, we investigate the universality of GMC through\\nan invariance principle. We consider the model of a random Fourier series, a\\nprocess known to be log-correlated. While the Gaussian Fourier series has been\\na classical object of study, recently, the non-Gaussian counterpart was\\ninvestigated and the associated multiplicative chaos constructed by Junnila in\\n2016. We show that the Gaussian and non-Gaussian variables can be coupled so\\nthat the associated chaos measures are almost surely mutually absolutely\\ncontinuous throughout the entire sub-critical regime. This solves the main open\\nproblem from Kim and Kriechbaum (2024) who had earlier established such a\\nresult for a part of the regime. The main ingredient is a new high dimensional\\nCLT for a sum of independent (but not i.i.d.) random vectors belonging to rank\\none subspaces with error bounds involving the isotropic properties of the\\ncovariance matrix of the sum, which we expect will find other applications. The\\nproof relies on a path-wise analysis of Skorokhod embeddings as well as a\\nperturbative result about square roots of positive semi-definite matrices\\nwhich, surprisingly, appears to be new. In this paper, we introduce a unified framework, inspired by classical\\nregularization theory, for designing and analyzing a broad class of linear\\nregression approaches. Our framework encompasses traditional methods like least\\nsquares regression and Ridge regression, as well as innovative techniques,\\nincluding seven novel regression methods such as Landweber and Showalter\\nregressions. Within this framework, we further propose a class of debiased and\\nthresholded regression methods to promote feature selection, particularly in\\nterms of sparsity. These methods may offer advantages over conventional\\nregression techniques, including Lasso, due to their ease of computation via a\\nclosed-form expression. Theoretically, we establish consistency results and\\nGaussian approximation theorems for this new class of regularization methods.\\nExtensive numerical simulations further demonstrate that the debiased and\\nthresholded counterparts of linear regression methods exhibit favorable finite\\nsample performance and may be preferable in certain settings. We address the problem of producing a lower bound for the mean of a discrete\\nprobability distribution, with known support over a finite set of real numbers,\\nfrom an iid sample of that distribution. Up to a constant, this is equivalent\\nto bounding the mean of a multinomial distribution (with known support) from a\\nsample of that distribution. Our main contribution is to characterize the\\ncomplete set of admissible bound functions for any sample space, and to show\\nthat certain previously published bounds are admissible. We prove that the\\nsolution to each one of a set of simple-to-state optimization problems yields\\nsuch an admissible bound. Single examples of such bounds, such as the trinomial\\nbound by Miratrix and Stark [2009] have been previously published, but without\\nan analysis of admissibility, and without a discussion of the full set of\\nalternative admissible bounds. In addition to a variety of results about\\nadmissible bounds, we prove the non-existence of optimal bounds for sample\\nspaces with supports of size greater than 1 and samples sizes greater than 1.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4107d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268250"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e48ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = full_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "576cbdb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'consider',\n",
       " 'nonparametric',\n",
       " 'regression',\n",
       " 'with',\n",
       " 'functional',\n",
       " 'covariates,',\n",
       " 'that',\n",
       " 'is,',\n",
       " 'they',\n",
       " 'are',\n",
       " 'elements',\n",
       " 'of',\n",
       " 'an',\n",
       " 'infinite-dimensional',\n",
       " 'Hilbert',\n",
       " 'space.',\n",
       " 'A',\n",
       " 'locally',\n",
       " 'polynomial',\n",
       " 'estimator',\n",
       " 'is',\n",
       " 'constructed,',\n",
       " 'where',\n",
       " 'an',\n",
       " 'orthonormal',\n",
       " 'basis',\n",
       " 'and',\n",
       " 'various',\n",
       " 'tuning',\n",
       " 'parameters',\n",
       " 'remain',\n",
       " 'to',\n",
       " 'be',\n",
       " 'selected.',\n",
       " 'We',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'general',\n",
       " 'asymptotic',\n",
       " 'upper',\n",
       " 'bound',\n",
       " 'on',\n",
       " 'the',\n",
       " 'estimation',\n",
       " 'error',\n",
       " 'and',\n",
       " 'show',\n",
       " 'that',\n",
       " 'this',\n",
       " 'procedure',\n",
       " 'achieves',\n",
       " 'polynomial',\n",
       " 'convergence',\n",
       " 'rates',\n",
       " 'under',\n",
       " 'appropriate',\n",
       " 'tuning',\n",
       " 'and',\n",
       " 'supersmoothness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'regression',\n",
       " 'function.',\n",
       " 'Such',\n",
       " 'polynomial',\n",
       " 'convergence',\n",
       " 'rates',\n",
       " 'have',\n",
       " 'usually',\n",
       " 'been',\n",
       " 'considered',\n",
       " 'to',\n",
       " 'be',\n",
       " 'non-attainable',\n",
       " 'in',\n",
       " 'nonparametric',\n",
       " 'functional',\n",
       " 'regression',\n",
       " 'without',\n",
       " 'any',\n",
       " 'additional',\n",
       " 'strong',\n",
       " 'structural',\n",
       " 'constraints',\n",
       " 'such',\n",
       " 'as',\n",
       " 'linearity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'regression',\n",
       " 'function.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'article',\n",
       " 'we',\n",
       " 'redefine',\n",
       " 'various',\n",
       " 'poverty',\n",
       " 'measures',\n",
       " 'in',\n",
       " 'literature',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'quantile',\n",
       " 'functions',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'distribution',\n",
       " 'functions',\n",
       " 'in',\n",
       " 'the',\n",
       " 'prevailing',\n",
       " 'approach.',\n",
       " 'This',\n",
       " 'enables',\n",
       " 'provision',\n",
       " 'for',\n",
       " 'alternative',\n",
       " 'methodology',\n",
       " 'for',\n",
       " 'poverty',\n",
       " 'measurement',\n",
       " 'and',\n",
       " 'analysis',\n",
       " 'along',\n",
       " 'with',\n",
       " 'some',\n",
       " 'new',\n",
       " 'results',\n",
       " 'that',\n",
       " 'are',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'in',\n",
       " 'the',\n",
       " 'existing',\n",
       " 'framework.',\n",
       " 'Several',\n",
       " 'flexible',\n",
       " 'quantile',\n",
       " 'function',\n",
       " 'models',\n",
       " 'that',\n",
       " 'can',\n",
       " 'enrich',\n",
       " 'the',\n",
       " 'existing',\n",
       " 'ones',\n",
       " 'are',\n",
       " 'proposed',\n",
       " 'and',\n",
       " 'their',\n",
       " 'utility',\n",
       " 'is',\n",
       " 'demonstrated',\n",
       " 'for',\n",
       " 'real',\n",
       " 'data.',\n",
       " 'Online',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'an',\n",
       " 'inferential',\n",
       " 'paradigm',\n",
       " 'in',\n",
       " 'which',\n",
       " 'parameters',\n",
       " 'are',\n",
       " 'updated',\n",
       " 'incrementally',\n",
       " 'from',\n",
       " 'sequentially',\n",
       " 'available',\n",
       " 'data,',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'batch',\n",
       " 'learning,',\n",
       " 'where',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'dataset',\n",
       " 'is',\n",
       " 'processed',\n",
       " 'at',\n",
       " 'once.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'paper,',\n",
       " 'we',\n",
       " 'assume',\n",
       " 'that',\n",
       " 'mini-batches',\n",
       " 'from',\n",
       " 'the',\n",
       " 'full',\n",
       " 'dataset',\n",
       " 'become',\n",
       " 'available',\n",
       " 'sequentially.',\n",
       " 'The',\n",
       " 'Bayesian',\n",
       " 'framework,',\n",
       " 'which',\n",
       " 'updates',\n",
       " 'beliefs',\n",
       " 'about',\n",
       " 'unknown',\n",
       " 'parameters',\n",
       " 'after',\n",
       " 'observing',\n",
       " 'each',\n",
       " 'mini-batch,',\n",
       " 'is',\n",
       " 'naturally',\n",
       " 'suited',\n",
       " 'for',\n",
       " 'online',\n",
       " 'learning.',\n",
       " 'At',\n",
       " 'each',\n",
       " 'step,',\n",
       " 'we',\n",
       " 'update',\n",
       " 'the',\n",
       " 'posterior',\n",
       " 'distribution',\n",
       " 'using',\n",
       " 'the',\n",
       " 'current',\n",
       " 'prior',\n",
       " 'and',\n",
       " 'new',\n",
       " 'observations,',\n",
       " 'with',\n",
       " 'the',\n",
       " 'updated',\n",
       " 'posterior',\n",
       " 'serving',\n",
       " 'as',\n",
       " 'the',\n",
       " 'prior',\n",
       " 'for',\n",
       " 'the',\n",
       " 'next',\n",
       " 'step.',\n",
       " 'However,',\n",
       " 'this',\n",
       " 'recursive',\n",
       " 'Bayesian',\n",
       " 'updating',\n",
       " 'is',\n",
       " 'rarely',\n",
       " 'computationally',\n",
       " 'tractable',\n",
       " 'unless',\n",
       " 'the',\n",
       " 'model',\n",
       " 'and',\n",
       " 'prior',\n",
       " 'are',\n",
       " 'conjugate.',\n",
       " 'When',\n",
       " 'the',\n",
       " 'model',\n",
       " 'is',\n",
       " 'regular,',\n",
       " 'the',\n",
       " 'updated',\n",
       " 'posterior',\n",
       " 'can',\n",
       " 'be',\n",
       " 'approximated',\n",
       " 'by',\n",
       " 'a',\n",
       " 'normal',\n",
       " 'distribution,',\n",
       " 'as',\n",
       " 'justified',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Bernstein-von',\n",
       " 'Mises',\n",
       " 'theorem.',\n",
       " 'We',\n",
       " 'adopt',\n",
       " 'a',\n",
       " 'variational',\n",
       " 'approximation',\n",
       " 'at',\n",
       " 'each',\n",
       " 'step',\n",
       " 'and',\n",
       " 'investigate',\n",
       " 'the',\n",
       " 'frequentist',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'the',\n",
       " 'final',\n",
       " 'posterior',\n",
       " 'obtained',\n",
       " 'through',\n",
       " 'this',\n",
       " 'sequential',\n",
       " 'procedure.',\n",
       " 'Under',\n",
       " 'mild',\n",
       " 'assumptions,',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that',\n",
       " 'the',\n",
       " 'accumulated',\n",
       " 'approximation',\n",
       " 'error',\n",
       " 'becomes',\n",
       " 'negligible',\n",
       " 'once',\n",
       " 'the',\n",
       " 'mini-batch',\n",
       " 'size',\n",
       " 'exceeds',\n",
       " 'a',\n",
       " 'threshold',\n",
       " 'depending',\n",
       " 'on',\n",
       " 'the',\n",
       " 'parameter',\n",
       " 'dimension.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'result,',\n",
       " 'the',\n",
       " 'sequentially',\n",
       " 'updated',\n",
       " 'posterior',\n",
       " 'is',\n",
       " 'asymptotically',\n",
       " 'indistinguishable',\n",
       " 'from',\n",
       " 'the',\n",
       " 'full',\n",
       " 'posterior.',\n",
       " 'We',\n",
       " 'study',\n",
       " 'how',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'a',\n",
       " 'class',\n",
       " 'of',\n",
       " 'continuous-time',\n",
       " 'nonlinear',\n",
       " 'systems',\n",
       " 'defined',\n",
       " 'by',\n",
       " 'an',\n",
       " 'ordinary',\n",
       " 'differential',\n",
       " 'equation',\n",
       " 'affine',\n",
       " 'in',\n",
       " 'the',\n",
       " 'unknown',\n",
       " 'parameter.',\n",
       " 'We',\n",
       " 'define',\n",
       " 'a',\n",
       " 'notion',\n",
       " 'of',\n",
       " 'asymptotic',\n",
       " 'consistency',\n",
       " 'as',\n",
       " '$(n,',\n",
       " 'h)',\n",
       " '\\\\to',\n",
       " '(\\\\infty,',\n",
       " '0)$,',\n",
       " 'and',\n",
       " 'we',\n",
       " 'achieve',\n",
       " 'it',\n",
       " 'using',\n",
       " 'a',\n",
       " 'family',\n",
       " 'of',\n",
       " 'direct',\n",
       " 'methods',\n",
       " 'where',\n",
       " 'the',\n",
       " 'first',\n",
       " 'step',\n",
       " 'is',\n",
       " 'differentiating',\n",
       " 'a',\n",
       " 'noisy',\n",
       " 'time',\n",
       " 'series',\n",
       " 'and',\n",
       " 'the',\n",
       " 'second',\n",
       " 'step',\n",
       " 'is',\n",
       " 'a',\n",
       " 'plug-in',\n",
       " 'linear',\n",
       " 'estimator.',\n",
       " 'The',\n",
       " 'first',\n",
       " 'step,',\n",
       " 'differentiation,',\n",
       " 'is',\n",
       " 'a',\n",
       " 'signal',\n",
       " 'processing',\n",
       " 'adaptation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'nonparametric',\n",
       " 'statistical',\n",
       " 'technique',\n",
       " 'of',\n",
       " 'local',\n",
       " 'polynomial',\n",
       " 'regression.',\n",
       " 'The',\n",
       " 'second',\n",
       " 'step,',\n",
       " 'generalized',\n",
       " 'linear',\n",
       " 'regression,',\n",
       " 'can',\n",
       " 'be',\n",
       " 'consistent',\n",
       " 'using',\n",
       " 'a',\n",
       " 'least',\n",
       " 'squares',\n",
       " 'estimator,',\n",
       " 'but',\n",
       " 'we',\n",
       " 'demonstrate',\n",
       " 'two',\n",
       " 'novel',\n",
       " 'bias',\n",
       " 'corrections',\n",
       " 'that',\n",
       " 'improve',\n",
       " 'the',\n",
       " 'accuracy',\n",
       " 'for',\n",
       " 'finite',\n",
       " '$h$.',\n",
       " 'These',\n",
       " 'methods',\n",
       " 'significantly',\n",
       " 'broaden',\n",
       " 'the',\n",
       " 'class',\n",
       " 'of',\n",
       " 'continuous-time',\n",
       " 'systems',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'consistently',\n",
       " 'estimated',\n",
       " 'by',\n",
       " 'direct',\n",
       " 'methods.',\n",
       " 'Tangent',\n",
       " 'approximation',\n",
       " 'form',\n",
       " 'a',\n",
       " 'popular',\n",
       " 'class',\n",
       " 'of',\n",
       " 'variational',\n",
       " 'inference',\n",
       " '(VI)',\n",
       " 'techniques',\n",
       " 'for',\n",
       " 'Bayesian',\n",
       " 'analysis',\n",
       " 'in',\n",
       " 'intractable',\n",
       " 'non-conjugate',\n",
       " 'models.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'principle',\n",
       " 'of',\n",
       " 'convex',\n",
       " 'duality',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'a',\n",
       " 'minorant',\n",
       " 'of',\n",
       " 'the',\n",
       " 'marginal',\n",
       " 'likelihood,',\n",
       " 'making',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'tractable.',\n",
       " 'Despite',\n",
       " 'its',\n",
       " 'extensive',\n",
       " 'applications,',\n",
       " 'a',\n",
       " 'general',\n",
       " 'methodology',\n",
       " 'for',\n",
       " 'tangent',\n",
       " 'approximation',\n",
       " 'encompassing',\n",
       " 'a',\n",
       " 'large',\n",
       " 'class',\n",
       " 'of',\n",
       " 'likelihoods',\n",
       " 'beyond',\n",
       " 'logit',\n",
       " 'models',\n",
       " 'with',\n",
       " 'provable',\n",
       " 'optimality',\n",
       " 'guarantees',\n",
       " 'is',\n",
       " 'still',\n",
       " 'elusive.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'article,',\n",
       " 'we',\n",
       " 'propose',\n",
       " 'a',\n",
       " 'general',\n",
       " 'Tangent',\n",
       " 'Approximation',\n",
       " 'based',\n",
       " 'Variational',\n",
       " 'InferencE',\n",
       " '(TAVIE)',\n",
       " 'framework',\n",
       " 'for',\n",
       " 'strongly',\n",
       " 'super-Gaussian',\n",
       " '(SSG)',\n",
       " 'likelihood',\n",
       " 'functions',\n",
       " 'which',\n",
       " 'includes',\n",
       " 'a',\n",
       " 'broad',\n",
       " 'class',\n",
       " 'of',\n",
       " 'flexible',\n",
       " 'probability',\n",
       " 'models.',\n",
       " 'Specifically,',\n",
       " 'TAVIE',\n",
       " 'obtains',\n",
       " 'a',\n",
       " 'quadratic',\n",
       " 'lower',\n",
       " 'bound',\n",
       " 'of',\n",
       " 'the',\n",
       " 'corresponding',\n",
       " 'log-likelihood,',\n",
       " 'thus',\n",
       " 'inducing',\n",
       " 'conjugacy',\n",
       " 'with',\n",
       " 'Gaussian',\n",
       " 'priors',\n",
       " 'over',\n",
       " 'the',\n",
       " 'model',\n",
       " 'parameters.',\n",
       " 'Under',\n",
       " 'mild',\n",
       " 'assumptions',\n",
       " 'on',\n",
       " 'the',\n",
       " 'data-generating',\n",
       " 'process,',\n",
       " 'we',\n",
       " 'demonstrate',\n",
       " 'the',\n",
       " 'optimality',\n",
       " 'of',\n",
       " 'our',\n",
       " 'proposed',\n",
       " 'methodology',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fractional',\n",
       " 'likelihood',\n",
       " 'setup.',\n",
       " 'Furthermore,',\n",
       " 'we',\n",
       " 'illustrate',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'performance',\n",
       " 'of',\n",
       " 'TAVIE',\n",
       " 'through',\n",
       " 'extensive',\n",
       " 'simulations',\n",
       " 'and',\n",
       " 'an',\n",
       " 'application',\n",
       " 'on',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " '2000',\n",
       " 'Census',\n",
       " 'real',\n",
       " 'data.',\n",
       " 'Diffusion',\n",
       " 'models',\n",
       " 'are',\n",
       " 'distinguished',\n",
       " 'by',\n",
       " 'their',\n",
       " 'exceptional',\n",
       " 'generative',\n",
       " 'performance,',\n",
       " 'particularly',\n",
       " 'in',\n",
       " 'producing',\n",
       " 'high-quality',\n",
       " 'samples',\n",
       " 'through',\n",
       " 'iterative',\n",
       " 'denoising.',\n",
       " 'While',\n",
       " 'current',\n",
       " 'theory',\n",
       " 'suggests',\n",
       " 'that',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'denoising',\n",
       " 'steps',\n",
       " 'required',\n",
       " 'for',\n",
       " 'accurate',\n",
       " 'sample',\n",
       " 'generation',\n",
       " 'should',\n",
       " 'scale',\n",
       " 'linearly',\n",
       " 'with',\n",
       " 'data',\n",
       " 'dimension,',\n",
       " 'this',\n",
       " 'does',\n",
       " 'not',\n",
       " 'reflect',\n",
       " 'the',\n",
       " 'practical',\n",
       " 'efficiency',\n",
       " 'of',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'algorithms',\n",
       " 'like',\n",
       " 'Denoising',\n",
       " 'Diffusion',\n",
       " 'Probabilistic',\n",
       " 'Models',\n",
       " '(DDPMs).',\n",
       " 'This',\n",
       " 'paper',\n",
       " 'investigates',\n",
       " 'the',\n",
       " 'effectiveness',\n",
       " 'of',\n",
       " 'diffusion',\n",
       " 'models',\n",
       " 'in',\n",
       " 'sampling',\n",
       " 'from',\n",
       " 'complex',\n",
       " 'high-dimensional',\n",
       " 'distributions',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'well-approximated',\n",
       " 'by',\n",
       " 'Gaussian',\n",
       " 'Mixture',\n",
       " 'Models',\n",
       " '(GMMs).',\n",
       " 'For',\n",
       " 'these',\n",
       " 'distributions,',\n",
       " 'our',\n",
       " 'main',\n",
       " 'result',\n",
       " 'shows',\n",
       " 'that',\n",
       " 'DDPM',\n",
       " 'takes',\n",
       " 'at',\n",
       " 'most',\n",
       " '$\\\\widetilde{O}(1/\\\\varepsilon)$',\n",
       " 'iterations',\n",
       " 'to',\n",
       " 'attain',\n",
       " 'an',\n",
       " '$\\\\varepsilon$-accurate',\n",
       " 'distribution',\n",
       " 'in',\n",
       " 'total',\n",
       " 'variation',\n",
       " '(TV)',\n",
       " 'distance,',\n",
       " 'independent',\n",
       " 'of',\n",
       " 'both',\n",
       " 'the',\n",
       " 'ambient',\n",
       " 'dimension',\n",
       " '$d$',\n",
       " 'and',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'components',\n",
       " '$K$,',\n",
       " 'up',\n",
       " 'to',\n",
       " 'logarithmic',\n",
       " 'factors.',\n",
       " 'Furthermore,',\n",
       " 'this',\n",
       " 'result',\n",
       " 'remains',\n",
       " 'robust',\n",
       " 'to',\n",
       " 'score',\n",
       " 'estimation',\n",
       " 'errors.',\n",
       " 'These',\n",
       " 'findings',\n",
       " 'highlight',\n",
       " 'the',\n",
       " 'remarkable',\n",
       " 'effectiveness',\n",
       " 'of',\n",
       " 'diffusion',\n",
       " 'models',\n",
       " 'in',\n",
       " 'high-dimensional',\n",
       " 'settings',\n",
       " 'given',\n",
       " 'the',\n",
       " 'universal',\n",
       " 'approximation',\n",
       " 'capability',\n",
       " 'of',\n",
       " 'GMMs,',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'theoretical',\n",
       " 'insights',\n",
       " 'into',\n",
       " 'their',\n",
       " 'practical',\n",
       " 'success.',\n",
       " 'Score',\n",
       " 'estimation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'backbone',\n",
       " 'of',\n",
       " 'score-based',\n",
       " 'generative',\n",
       " 'models',\n",
       " '(SGMs),',\n",
       " 'especially',\n",
       " 'denoising',\n",
       " 'diffusion',\n",
       " 'probabilistic',\n",
       " 'models',\n",
       " '(DDPMs).',\n",
       " 'A',\n",
       " 'key',\n",
       " 'result',\n",
       " 'in',\n",
       " 'this',\n",
       " 'area',\n",
       " 'shows',\n",
       " 'that',\n",
       " 'with',\n",
       " 'accurate',\n",
       " 'score',\n",
       " 'estimates,',\n",
       " 'SGMs',\n",
       " 'can',\n",
       " 'efficiently',\n",
       " 'generate',\n",
       " 'samples',\n",
       " 'from',\n",
       " 'any',\n",
       " 'realistic',\n",
       " 'data',\n",
       " 'distribution',\n",
       " '(Chen',\n",
       " 'et',\n",
       " 'al.,',\n",
       " \"ICLR'23;\",\n",
       " 'Lee',\n",
       " 'et',\n",
       " 'al.,',\n",
       " \"ALT'23).\",\n",
       " 'This',\n",
       " 'distribution',\n",
       " 'learning',\n",
       " 'result,',\n",
       " 'where',\n",
       " 'the',\n",
       " 'learned',\n",
       " 'distribution',\n",
       " 'is',\n",
       " 'implicitly',\n",
       " 'that',\n",
       " 'of',\n",
       " 'the',\n",
       " \"sampler's\",\n",
       " 'output,',\n",
       " 'does',\n",
       " 'not',\n",
       " 'explain',\n",
       " 'how',\n",
       " 'score',\n",
       " 'estimation',\n",
       " 'relates',\n",
       " 'to',\n",
       " 'classical',\n",
       " 'tasks',\n",
       " 'of',\n",
       " 'parameter',\n",
       " 'and',\n",
       " 'density',\n",
       " 'estimation.',\n",
       " 'This',\n",
       " 'paper',\n",
       " 'introduces',\n",
       " 'a',\n",
       " 'framework',\n",
       " 'that',\n",
       " 'reduces',\n",
       " 'score',\n",
       " 'estimation',\n",
       " 'to',\n",
       " 'these',\n",
       " 'two',\n",
       " 'tasks,',\n",
       " 'with',\n",
       " 'various',\n",
       " 'implications',\n",
       " 'for',\n",
       " 'statistical',\n",
       " 'and',\n",
       " 'computational',\n",
       " 'learning',\n",
       " 'theory:',\n",
       " 'Parameter',\n",
       " 'Estimation:',\n",
       " 'Koehler',\n",
       " 'et',\n",
       " 'al.',\n",
       " \"(ICLR'23)\",\n",
       " 'demonstrate',\n",
       " 'that',\n",
       " 'a',\n",
       " 'score-matching',\n",
       " 'variant',\n",
       " 'is',\n",
       " 'statistically',\n",
       " 'inefficient',\n",
       " 'for',\n",
       " 'the',\n",
       " 'parametric',\n",
       " 'estimation',\n",
       " 'of',\n",
       " 'multimodal',\n",
       " 'densities',\n",
       " 'common',\n",
       " 'in',\n",
       " 'practice.',\n",
       " 'In',\n",
       " 'contrast,',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that',\n",
       " 'under',\n",
       " 'mild',\n",
       " 'conditions,',\n",
       " 'denoising',\n",
       " 'score-matching',\n",
       " 'in',\n",
       " 'DDPMs',\n",
       " 'is',\n",
       " 'asymptotically',\n",
       " 'efficient.',\n",
       " 'Density',\n",
       " 'Estimation:',\n",
       " 'By',\n",
       " 'linking',\n",
       " 'generation',\n",
       " 'to',\n",
       " 'score',\n",
       " 'estimation,',\n",
       " 'we',\n",
       " 'lift',\n",
       " 'existing',\n",
       " 'score',\n",
       " 'estimation',\n",
       " 'guarantees',\n",
       " 'to',\n",
       " '$(\\\\epsilon,\\\\delta)$-PAC',\n",
       " 'density',\n",
       " 'estimation,',\n",
       " 'i.e.,',\n",
       " 'a',\n",
       " 'function',\n",
       " 'approximating',\n",
       " 'the',\n",
       " 'target',\n",
       " 'log-density',\n",
       " 'within',\n",
       " '$\\\\epsilon$',\n",
       " 'on',\n",
       " 'all',\n",
       " 'but',\n",
       " 'a',\n",
       " '$\\\\delta$-fraction',\n",
       " 'of',\n",
       " 'the',\n",
       " 'space.',\n",
       " 'We',\n",
       " 'provide',\n",
       " '(i)',\n",
       " 'minimax',\n",
       " 'rates',\n",
       " 'for',\n",
       " 'density',\n",
       " 'estimation',\n",
       " 'over',\n",
       " 'H\\\\\"older',\n",
       " 'classes',\n",
       " 'and',\n",
       " '(ii)',\n",
       " 'a',\n",
       " 'quasi-polynomial',\n",
       " 'PAC',\n",
       " 'density',\n",
       " 'estimation',\n",
       " 'algorithm',\n",
       " 'for',\n",
       " 'the',\n",
       " 'classical',\n",
       " 'Gaussian',\n",
       " 'location',\n",
       " 'mixture',\n",
       " 'model,',\n",
       " 'building',\n",
       " 'on',\n",
       " 'and',\n",
       " 'addressing',\n",
       " 'an',\n",
       " 'open',\n",
       " 'problem',\n",
       " 'from',\n",
       " 'Gatmiry',\n",
       " 'et',\n",
       " 'al.',\n",
       " \"(arXiv'24).\",\n",
       " 'Lower',\n",
       " 'Bounds',\n",
       " 'for',\n",
       " 'Score',\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "187b64ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37878"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4662d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(words, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = words[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += chunk_size - overlap  \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21d3f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_into_chunks(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e31da9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We consider nonparametric regression with functional covariates, that is, they are elements of an infinite-dimensional Hilbert space. A locally polynomial estimator is constructed, where an orthonormal basis and various tuning parameters remain to be selected. We provide a general asymptotic upper bound on the estimation error and show that this procedure achieves polynomial convergence rates under appropriate tuning and supersmoothness of the regression function. Such polynomial convergence rates have usually been considered to be non-attainable in nonparametric functional regression without any additional strong structural constraints such as linearity of the regression function. In this article we redefine various poverty measures in literature in terms of quantile functions instead of distribution functions in the prevailing approach. This enables provision for alternative methodology for poverty measurement and analysis along with some new results that are difficult to obtain in the existing framework. Several flexible quantile function models that can enrich the existing ones are proposed and their utility is demonstrated for real data. Online learning is an inferential paradigm in which parameters are updated incrementally from sequentially available data, in contrast to batch learning, where the entire dataset is processed at once. In this paper, we assume that mini-batches from the full dataset become available sequentially. The Bayesian framework, which updates beliefs about unknown parameters after observing each mini-batch, is naturally suited for online learning. At each step, we update the posterior distribution using the current prior and new observations, with the updated posterior serving as the prior for the next step. However, this recursive Bayesian updating is rarely computationally tractable unless the model and prior are conjugate. When the model is regular, the updated posterior can be approximated by a normal distribution, as justified by the Bernstein-von Mises theorem. We adopt a variational approximation at each step and investigate the frequentist properties of the final posterior obtained through this sequential procedure. Under mild assumptions, we show that the accumulated approximation error becomes negligible once the mini-batch size exceeds a threshold depending on the parameter dimension. As a result, the sequentially updated posterior is asymptotically indistinguishable from the full posterior. We study how to identify a class of continuous-time nonlinear systems defined by an ordinary differential equation affine in the unknown parameter. We define a notion of asymptotic consistency as $(n, h) \\\\to (\\\\infty, 0)$, and we achieve it using a family of direct methods where the first step is differentiating a noisy time series and the second step is a plug-in linear estimator. The first step, differentiation, is a signal processing adaptation of the nonparametric statistical technique of local polynomial regression. The second step, generalized linear regression, can be consistent using a least squares estimator, but we demonstrate two novel bias corrections that improve the accuracy for finite $h$. These methods significantly broaden the class of continuous-time systems that can be consistently estimated by direct methods. Tangent approximation form a popular class of variational inference (VI) techniques for Bayesian analysis in intractable non-conjugate models. It is based on the principle of',\n",
       " \"corrections that improve the accuracy for finite $h$. These methods significantly broaden the class of continuous-time systems that can be consistently estimated by direct methods. Tangent approximation form a popular class of variational inference (VI) techniques for Bayesian analysis in intractable non-conjugate models. It is based on the principle of convex duality to construct a minorant of the marginal likelihood, making the problem tractable. Despite its extensive applications, a general methodology for tangent approximation encompassing a large class of likelihoods beyond logit models with provable optimality guarantees is still elusive. In this article, we propose a general Tangent Approximation based Variational InferencE (TAVIE) framework for strongly super-Gaussian (SSG) likelihood functions which includes a broad class of flexible probability models. Specifically, TAVIE obtains a quadratic lower bound of the corresponding log-likelihood, thus inducing conjugacy with Gaussian priors over the model parameters. Under mild assumptions on the data-generating process, we demonstrate the optimality of our proposed methodology in the fractional likelihood setup. Furthermore, we illustrate the empirical performance of TAVIE through extensive simulations and an application on the U.S. 2000 Census real data. Diffusion models are distinguished by their exceptional generative performance, particularly in producing high-quality samples through iterative denoising. While current theory suggests that the number of denoising steps required for accurate sample generation should scale linearly with data dimension, this does not reflect the practical efficiency of widely used algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper investigates the effectiveness of diffusion models in sampling from complex high-dimensional distributions that can be well-approximated by Gaussian Mixture Models (GMMs). For these distributions, our main result shows that DDPM takes at most $\\\\widetilde{O}(1/\\\\varepsilon)$ iterations to attain an $\\\\varepsilon$-accurate distribution in total variation (TV) distance, independent of both the ambient dimension $d$ and the number of components $K$, up to logarithmic factors. Furthermore, this result remains robust to score estimation errors. These findings highlight the remarkable effectiveness of diffusion models in high-dimensional settings given the universal approximation capability of GMMs, and provide theoretical insights into their practical success. Score estimation is the backbone of score-based generative models (SGMs), especially denoising diffusion probabilistic models (DDPMs). A key result in this area shows that with accurate score estimates, SGMs can efficiently generate samples from any realistic data distribution (Chen et al., ICLR'23; Lee et al., ALT'23). This distribution learning result, where the learned distribution is implicitly that of the sampler's output, does not explain how score estimation relates to classical tasks of parameter and density estimation. This paper introduces a framework that reduces score estimation to these two tasks, with various implications for statistical and computational learning theory: Parameter Estimation: Koehler et al. (ICLR'23) demonstrate that a score-matching variant is statistically inefficient for the parametric estimation of multimodal densities common in practice. In contrast, we show that under mild conditions, denoising score-matching in DDPMs is asymptotically efficient. Density Estimation: By linking generation to score estimation, we lift existing score estimation guarantees to $(\\\\epsilon,\\\\delta)$-PAC density estimation, i.e., a function approximating the target log-density within $\\\\epsilon$\",\n",
       " 'parametric estimation of multimodal densities common in practice. In contrast, we show that under mild conditions, denoising score-matching in DDPMs is asymptotically efficient. Density Estimation: By linking generation to score estimation, we lift existing score estimation guarantees to $(\\\\epsilon,\\\\delta)$-PAC density estimation, i.e., a function approximating the target log-density within $\\\\epsilon$ on all but a $\\\\delta$-fraction of the space. We provide (i) minimax rates for density estimation over H\\\\\"older classes and (ii) a quasi-polynomial PAC density estimation algorithm for the classical Gaussian location mixture model, building on and addressing an open problem from Gatmiry et al. (arXiv\\'24). Lower Bounds for Score Estimation: Our framework offers the first principled method to prove computational lower bounds for score estimation across general distributions. As an application, we establish cryptographic lower bounds for score estimation in general Gaussian mixture models, conceptually recovering Song\\'s (NeurIPS\\'24) result and advancing his key open problem. Exoplanets play an important role in understanding the mechanics of planetary system formation and orbital evolution. In this context the correlations of different parameters of the planets and their host star are useful guides in the search for explanatory mechanisms. Based on a reanalysis of the data set from \\\\cite{figueria14} we study the as of now still poorly understood correlation between planetary surface gravity and stellar activity of Hot Jupiters. Unfortunately, data collection often suffers from measurement errors due to complicated and indirect measurement setups, rendering standard inference techniques unreliable. We present new methods to estimate and test for correlations in a deconvolution framework and thereby improve the state of the art analysis of the data in two directions. First, we are now able to account for additive measurement errors which facilitates reliable inference. Second we test for relevant changes, i.e. we are testing for correlations exceeding a certain threshold $\\\\Delta$. This reflects the fact that small nonzero correlations are to be expected for real life data almost always and that standard statistical tests will therefore always reject the null of no correlation given sufficient data. Our theory focuses on quantities that can be estimated by U-Statistics which contain a variety of correlation measures. We propose a bootstrap test and establish its theoretical validity. As a by product we also obtain confidence intervals. Applying our methods to the Hot Jupiter data set from \\\\cite{figueria14}, we observe that taking into account the measurement errors yields smaller point estimates and the null of no relevant correlation is rejected only for very small $\\\\Delta$. This demonstrates the importance of considering the impact of measurement errors to avoid misleading conclusions from the resulting statistical analysis. We propose a neural-network based survival model (SurvSurf) specifically designed for direct and simultaneous probabilistic prediction of the first hitting time of sequential events from baseline. Unlike existing models, SurvSurf is theoretically guaranteed to never violate the monotonic relationship between the cumulative incidence functions of sequential events, while allowing nonlinear influence from predictors. It also incorporates implicit truths for unobserved intermediate events in model fitting, and supports both discrete and continuous time and',\n",
       " 'events from baseline. Unlike existing models, SurvSurf is theoretically guaranteed to never violate the monotonic relationship between the cumulative incidence functions of sequential events, while allowing nonlinear influence from predictors. It also incorporates implicit truths for unobserved intermediate events in model fitting, and supports both discrete and continuous time and events. We also identified a variant of the Integrated Brier Score (IBS) that showed robust correlation with the mean squared error (MSE) between the true and predicted probabilities by accounting for implied truths about the missing intermediate events. We demonstrated the superiority of SurvSurf compared to modern and traditional predictive survival models in two simulated datasets and two real-world datasets, using MSE, the more robust IBS and by measuring the extent of monotonicity violation. The drift sequential parameter estimation problems for the Cox-Ingersoll-Ross (CIR) processes under the limited duration of observation are studied. Truncated sequential estimation methods for both scalar and {two}-dimensional parameter cases are proposed. In the non-asymptotic setting, for the proposed truncated estimators, the properties of guaranteed mean-square estimation accuracy are established. In the asymptotic formulation, when the observation time tends to infinity, it is shown that the proposed sequential procedures are asymptotically optimal among all possible sequential and non-sequential estimates with an average estimation time less than the fixed observation duration. It also turned out that asymptotically, without degrading the estimation quality, they significantly reduce the observation duration compared to classical non-sequential maximum likelihood estimations based on a fixed observation duration. Existing research on negations primarily focuses on entropy and extropy. Recently, new functions such as varentropy and varextropy have been developed, which can be considered as extensions of entropy and extropy. However, the impact of negation on these extended measures, particularly varentropy and varextropy, has not been extensively explored. To address this gap, this paper investigates the effect of negation on Shannon entropy, varentropy, and varextropy. We explore how the negation of a probability distribution influences these measures, showing that the negated distribution consistently leads to higher values of Shannon entropy, varentropy, and varextropy compared to the original distribution. Additionally, we prove that the negation of a probability distribution maximizes these measures during the process. The paper provides theoretical proofs and a detailed analysis of the behaviour of these measures, contributing to a better understanding of the interplay between probability distributions, negation, and information-theoretic quantities. We consider the task of Gaussian mean testing, that is, of testing whether a high-dimensional vector perturbed by white noise has large magnitude, or is the zero vector. This question, originating from the signal processing community, has recently seen a surge of interest from the machine learning and theoretical computer science community, and is by now fairly well understood. What is much less understood, and the focus of our work, is how to perform this task under truncation: that is, when the observations (i.i.d.\\\\ samples from the underlying high-dimensional Gaussian) are only observed when they fall in an given subset of the domain $\\\\R^d$. This truncation model, previously studied in the',\n",
       " 'much less understood, and the focus of our work, is how to perform this task under truncation: that is, when the observations (i.i.d.\\\\ samples from the underlying high-dimensional Gaussian) are only observed when they fall in an given subset of the domain $\\\\R^d$. This truncation model, previously studied in the context of learning (instead of testing) the mean vector, has a range of applications, in particular in Economics and Social Sciences. As our work shows, sample truncations affect the complexity of the testing task in a rather subtle and surprising way. This paper analyzes the Gini coefficient estimator for zero-truncated Poisson populations, revealing the presence of bias, and provides a mathematical expression for the bias, along with a bias-corrected estimator, which is evaluated using Monte Carlo simulation methods. The challenge of clustering short text data lies in balancing informativeness with interpretability. Traditional evaluation metrics often overlook this trade-off. Inspired by linguistic principles of communicative efficiency, this paper investigates the optimal number of clusters by quantifying the trade-off between informativeness and cognitive simplicity. We use large language models (LLMs) to generate cluster names and evaluate their effectiveness through semantic density, information theory, and clustering accuracy. Our results show that Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM, increases semantic density compared to random assignment, effectively grouping similar bios. However, as clusters increase, interpretability declines, as measured by a generative LLM\\'s ability to correctly assign bios based on cluster names. A logistic regression analysis confirms that classification accuracy depends on the semantic similarity between bios and their assigned cluster names, as well as their distinction from alternatives. These findings reveal a \"Goldilocks zone\" where clusters remain distinct yet interpretable. We identify an optimal range of 16-22 clusters, paralleling linguistic efficiency in lexical categorization. These insights inform both theoretical models and practical applications, guiding future research toward optimising cluster interpretability and usefulness. This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a \"local average quantile treatment effect\", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fr\\\\\\'echet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions',\n",
       " \"asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution. We study a multivariate Hawkes process as a model for time-continuous relational event networks. The model does not assume the network to be known, it includes covariates, and it allows for both common drivers, parameters common to all the actors in the network, and also local parameters specific for each actor. We derive rates of convergence for all of the model parameters when both the number of actors and the time horizon tends to infinity. To prevent an exploding network, sparseness is assumed. We also discuss numerical aspects. Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its power, the performance of IS is often highly sensitive to the choice of the proposal distribution and frequently requires stochastic calibration techniques. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a unique challenge: the decision and the IS distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both the analysis of convergence for decision iterates and the efficiency of the IS scheme. In this paper, we propose an iterative gradient-based algorithm that jointly updates the decision variable and the IS distribution without requiring time-scale separation between the two. Our method achieves the lowest possible asymptotic variance and guarantees global convergence under convexity of the objective and mild assumptions on the IS distribution family. Furthermore, we show that these properties are preserved under linear constraints by incorporating a recent variant of Nesterov's dual averaging method. We consider a classical First-order Vector AutoRegressive (VAR(1)) model, where we interpret the autoregressive interaction matrix as influence relationships among the components of the VAR(1) process that can be encoded by a weighted directed graph. A majority of previous work studies the structural identifiability of the graph based on time series observations and therefore relies on dynamical information. In this work we assume that an equilibrium exists, and study instead the identifiability of the graph from the stationary distribution, meaning that we seek a way to reconstruct the influence graph underlying the dynamic network using only static information. We use an approach from algebraic statistics that characterizes models using the Jacobian matroids associated with the parametrization of the models, and we introduce sufficient graphical conditions under which different graphs yield distinct steady-state distributions. Additionally, we illustrate how our results could be applied to characterize networks inspired by ecological research. Given i.i.d. observations uniformly distributed on a closed submanifold\",\n",
       " 'using the Jacobian matroids associated with the parametrization of the models, and we introduce sufficient graphical conditions under which different graphs yield distinct steady-state distributions. Additionally, we illustrate how our results could be applied to characterize networks inspired by ecological research. Given i.i.d. observations uniformly distributed on a closed submanifold of the Euclidean space, we study higher-order generalizations of graph Laplacians, so-called Hodge Laplacians on graphs, as approximations of the Laplace-Beltrami operator on differential forms. Our main result is a high-probability error bound for the associated Dirichlet forms. This bound improves existing Dirichlet form error bounds for graph Laplacians in the context of Laplacian Eigenmaps, and it provides insights into the Betti numbers studied in topological data analysis and the complementing positive part of the spectrum. Nonparametric regression with random design is considered. The $L_2$ error with integration with respect to the design measure is used as the error criterion. An over-parametrized deep neural network regression estimate with logistic activation function is defined, where all weights are learned by gradient descent. It is shown that the estimate achieves a nearly optimal rate of convergence in case that the regression function is $(p,C)$--smooth. A new formula for Marchenko-Pastur inversion is derived and used for inference of population linear spectral statistics. The formula allows for estimation of the Stieltjes transform of the population spectral distribution $s_H(z)$, when $z$ is sufficiently far from the support of the population spectral distribution $H$. If the dimension $d$ and the sample size $n$ go to infinity simultaneously such that $\\\\frac{d}{n} \\\\rightarrow c>0$, the estimation error is shown to be asymptotically less than $\\\\frac{n^{\\\\varepsilon}}{n}$ for arbitrary $\\\\varepsilon > 0$. By integrating along a curve around the support of $H$, estimators for population linear spectral statistics are constructed, which benefit from this convergence speed of $\\\\frac{n^{\\\\varepsilon}}{n}$. We consider the problem of estimating the parameters of a supercritical controlled branching process consistently from a single observed trajectory of population size counts. Our goal is to establish which parameters can and cannot be consistently estimated. When a parameter can be consistently estimated, we derive an explicit expression for the estimator. We address these questions in three scenarios: when the distribution of the control function distribution is known, when it is unknown, and when progenitor numbers are observed alongside population size counts. Our results offer a theoretical justification for the common practice in population ecology of estimating demographic and environmental stochasticity using separate observation schemes. Accurate tuning of hyperparameters is crucial to ensure that models can generalise effectively across different settings. In this paper, we present theoretical guarantees for hyperparameter selection using variational Bayes in the nonparametric regression model. We construct a variational approximation to a hierarchical Bayes procedure, and derive upper bounds for the contraction rate of the variational posterior in an abstract setting. The theory is applied to various Gaussian process priors and variational classes, resulting in minimax optimal rates. Our theoretical results are accompanied with numerical analysis both on synthetic and real world data sets. In this paper,',\n",
       " \"upper bounds for the contraction rate of the variational posterior in an abstract setting. The theory is applied to various Gaussian process priors and variational classes, resulting in minimax optimal rates. Our theoretical results are accompanied with numerical analysis both on synthetic and real world data sets. In this paper, we study the problem of multivariate shuffled linear regression, where the correspondence between predictors and responses in a linear model is obfuscated by a latent permutation. Specifically, we investigate the model $Y=\\\\tfrac{1}{\\\\sqrt{1+\\\\sigma^2}}(\\\\Pi_* X Q_* + \\\\sigma Z)$, where $X$ is an $n*d$ standard Gaussian design matrix, $Z$ is an $n*m$ Gaussian noise matrix, $\\\\Pi_*$ is an unknown $n*n$ permutation matrix, and $Q_*$ is an unknown $d*m$ on the Grassmanian manifold satisfying $Q_*^{\\\\top} Q_* = \\\\mathbb I_m$. Consider the hypothesis testing problem of distinguishing this model from the case where $X$ and $Y$ are independent Gaussian random matrices of sizes $n*d$ and $n*m$, respectively. Our results reveal a phase transition phenomenon in the performance of low-degree polynomial algorithms for this task. (1) When $m=o(d)$, we show that all degree-$D$ polynomials fail to distinguish these two models even when $\\\\sigma=0$, provided with $D^4=o\\\\big( \\\\tfrac{d}{m} \\\\big)$. (2) When $m=d$ and $\\\\sigma=\\\\omega(1)$, we show that all degree-$D$ polynomials fail to distinguish these two models provided with $D=o(\\\\sigma)$. (3) When $m=d$ and $\\\\sigma=o(1)$, we show that there exists a constant-degree polynomial that strongly distinguish these two models. These results establish a smooth transition in the effectiveness of low-degree polynomial algorithms for this problem, highlighting the interplay between the dimensions $m$ and $d$, the noise level $\\\\sigma$, and the computational complexity of the testing task. We introduce a new approach for estimating the number of spikes in a general class of spiked covariance models without directly computing the eigenvalues of the sample covariance matrix. This approach is based on the Lanczos algorithm and the asymptotic properties of the associated Jacobi matrix and its Cholesky factorization. A key aspect of the analysis is interpreting the eigenvector spectral distribution as a perturbation of its asymptotic counterpart. The specific exponential-type asymptotics of the Jacobi matrix enables an efficient approximation of the Stieltjes transform of the asymptotic spectral distribution via a finite continued fraction. As a consequence, we also obtain estimates for the density of the asymptotic distribution and the location of outliers. We provide consistency guarantees for our proposed estimators, proving their convergence in the high-dimensional regime. We demonstrate that, when applied to standard spiked covariance models, our approach outperforms existing methods in computational efficiency and runtime, while still maintaining robustness to exotic population covariances. The behavior of the random feature model in the high-dimensional regression framework has become a popular issue of interest in the machine learning literature}. This model is generally considered for feature vectors $x_i = \\\\Sigma^{1/2} x_i'$, where $x_i'$ is a random vector made of independent and identically distributed (iid) entries, and $\\\\Sigma$ is a positive definite matrix representing the covariance of the features. In this paper, we move beyond {\\\\CB this standard assumption by\",\n",
       " \"This model is generally considered for feature vectors $x_i = \\\\Sigma^{1/2} x_i'$, where $x_i'$ is a random vector made of independent and identically distributed (iid) entries, and $\\\\Sigma$ is a positive definite matrix representing the covariance of the features. In this paper, we move beyond {\\\\CB this standard assumption by studying the performances of the random features model in the setting of non-iid feature vectors}. Our approach is related to the analysis of the spectrum of large random matrices through random matrix theory (RMT) {\\\\CB and free probability} results. We turn to the analysis of non-iid data by using the notion of variance profile {\\\\CB which} is {\\\\CB well studied in RMT.} Our main contribution is then the study of the limits of the training and {\\\\CB prediction} risks associated to the ridge estimator in the random features model when its dimensions grow. We provide asymptotic equivalents of these risks that capture the behavior of ridge regression with random features in a {\\\\CB high-dimensional} framework. These asymptotic equivalents, {\\\\CB which prove to be sharp in numerical experiments}, are retrieved by adapting, to our setting, established results from operator-valued free probability theory. Moreover, {\\\\CB for various classes of random feature vectors that have not been considered so far in the literature}, our approach allows to show the appearance of the double descent phenomenon when the ridge regularization parameter is small enough. An e-variable for a family of distributions $\\\\mathcal{P}$ is a nonnegative random variable whose expected value under every distribution in $\\\\mathcal{P}$ is at most one. E-variables have recently been recognized as fundamental objects in hypothesis testing, and a rapidly growing body of work has attempted to derive admissible or optimal e-variables for various families $\\\\mathcal{P}$. In this paper, we study classes $\\\\mathcal{P}$ that are specified by constraints. Simple examples include bounds on the moments, but our general theory covers arbitrary sets of measurable constraints. Our main results characterize the set of all e-variables for such classes, as well as maximal ones. Three case studies illustrate the scope of our theory: finite constraint sets, one-sided sub-$\\\\psi$ distributions, and distributions invariant under a group of symmetries. In particular, we generalize recent results of Clerico (2024a) by dropping all assumptions on the constraints. We exploit the multiplicative structure of P\\\\'olya Tree priors for density and differential entropy estimation in $p$-dimensions. We establish: (i) a representation theorem of entropy functionals and (ii) conditions on the parameters of P\\\\'olya Trees to obtain Kullback-Leibler and Total Variation consistency for vectors with compact support. Those results motivate a novel differential entropy estimator that is consistent in probability for compact supported vectors under mild conditions. In order to enable applications of both results, we also provide a theoretical motivation for the truncation of Univariate P\\\\'olya Trees at level $3 \\\\log_2 n $. We consider the problem of sequential hypothesis testing by betting. For a general class of composite testing problems -- which include bounded mean testing, equal mean testing for bounded random tuples, and some key ingredients of\",\n",
       " \"motivation for the truncation of Univariate P\\\\'olya Trees at level $3 \\\\log_2 n $. We consider the problem of sequential hypothesis testing by betting. For a general class of composite testing problems -- which include bounded mean testing, equal mean testing for bounded random tuples, and some key ingredients of two-sample and independence testing as special cases -- we show that any $e$-process satisfying a certain sublinear regret bound is adaptively, asymptotically, and almost surely log-optimal for a composite alternative. This is a strong notion of optimality that has not previously been established for the aforementioned problems and we provide explicit test supermartingales and $e$-processes satisfying this notion in the more general case. Furthermore, we derive matching lower and upper bounds on the expected rejection time for the resulting sequential tests in all of these cases. The proofs of these results make weak, algorithm-agnostic moment assumptions and rely on a general-purpose proof technique involving the aforementioned regret and a family of numeraire portfolios. Finally, we discuss how all of these theorems hold in a distribution-uniform sense, a notion of log-optimality that is stronger still and seems to be new to the literature. We study the problem of learning a high-density region of an arbitrary distribution over $\\\\mathbb{R}^d$. Given a target coverage parameter $\\\\delta$, and sample access to an arbitrary distribution $D$, we want to output a confidence set $S \\\\subset \\\\mathbb{R}^d$ such that $S$ achieves $\\\\delta$ coverage of $D$, i.e., $\\\\mathbb{P}_{y \\\\sim D} \\\\left[ y \\\\in S \\\\right] \\\\ge \\\\delta$, and the volume of $S$ is as small as possible. This is a central problem in high-dimensional statistics with applications in finding confidence sets, uncertainty quantification, and support estimation. In the most general setting, this problem is statistically intractable, so we restrict our attention to competing with sets from a concept class $C$ with bounded VC-dimension. An algorithm is competitive with class $C$ if, given samples from an arbitrary distribution $D$, it outputs in polynomial time a set that achieves $\\\\delta$ coverage of $D$, and whose volume is competitive with the smallest set in $C$ with the required coverage $\\\\delta$. This problem is computationally challenging even in the basic setting when $C$ is the set of all Euclidean balls. Existing algorithms based on coresets find in polynomial time a ball whose volume is $\\\\exp(\\\\tilde{O}( d/ \\\\log d))$-factor competitive with the volume of the best ball. Our main result is an algorithm that finds a confidence set whose volume is $\\\\exp(\\\\tilde{O}(d^{2/3}))$ factor competitive with the optimal ball having the desired coverage. The algorithm is improper (it outputs an ellipsoid). Combined with our computational intractability result for proper learning balls within an $\\\\exp(\\\\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results provide an interesting separation between proper and (improper) learning of confidence sets. This paper introduces a periodic multivariate Poisson autoregression with potentially infinite memory, with a special focus on the network setting. Using contraction techniques, we study the stability of such a process and provide upper bounds on how fast it reaches the\",\n",
       " 'separation between proper and (improper) learning of confidence sets. This paper introduces a periodic multivariate Poisson autoregression with potentially infinite memory, with a special focus on the network setting. Using contraction techniques, we study the stability of such a process and provide upper bounds on how fast it reaches the periodically stationary regime. We then propose a computationally efficient Markov approximation using the properties of the exponential function and a density result. Furthermore, we prove the strong consistency of the maximum likelihood estimator for the Markov approximation and empirically test its robustness in the case of misspecification. Our model is applied to the prediction of weekly Rotavirus cases in Berlin, demonstrating superior performance compared to the existing PNAR model. Given a probability measure with density, Fermat distances and density-driven metrics are conformal transformation of the Euclidean metric that shrink distances in high density areas and enlarge distances in low density areas. Although they have been widely studied and have shown to be useful in various machine learning tasks, they are limited to measures with density (with respect to Lebesgue measure, or volume form on manifold). In this paper, by replacing the density with the Distance-to-Measure, we introduce a new metric, the Fermat Distance-to-Measure, defined for any probability measure in R^d. We derive strong stability properties for the Fermat Distance-to-Measure with respect to the measure and propose an estimator from random sampling of the measure, featuring an explicit bound on its convergence speed. This paper presents a unified framework for understanding the methodology and theory behind several different methods in the conformal prediction literature, which includes standard conformal prediction (CP), weighted conformal prediction (WCP), nonexchangeable conformal prediction (NexCP), and randomly-localized conformal prediction (RLCP), among others. At the crux of our framework is the idea that conformal methods are based on revealing partial information about the data at hand, and positing a conditional distribution for the data given the partial information. Different methods arise from different choices of partial information, and of the corresponding (approximate) conditional distribution. In addition to recovering and unifying existing results, our framework leads to both new theoretical guarantees for existing methods, and new extensions of the conformal methodology. We propose new statistical tests, in high-dimensional settings, for testing the independence of two random vectors and their conditional independence given a third random vector. The key idea is simple, i.e., we first transform each component variable to standard normal via its marginal empirical distribution, and we then test for independence and conditional independence of the transformed random vectors using appropriate $L_\\\\infty$-type test statistics. While we are testing some necessary conditions of the independence or the conditional independence, the new tests outperform the 13 frequently used testing methods in a large scale simulation comparison. The advantage of the new tests can be summarized as follows: (i) they do not require any moment conditions, (ii) they allow arbitrary dependence structures of the components among the random vectors, and (iii) they allow the dimensions of random vectors diverge at the exponential',\n",
       " 'a large scale simulation comparison. The advantage of the new tests can be summarized as follows: (i) they do not require any moment conditions, (ii) they allow arbitrary dependence structures of the components among the random vectors, and (iii) they allow the dimensions of random vectors diverge at the exponential rates of the sample size. The critical values of the proposed tests are determined by a computationally efficient multiplier bootstrap procedure. Theoretical analysis shows that the sizes of the proposed tests can be well controlled by the nominal significance level, and the proposed tests are also consistent under certain local alternatives. The finite sample performance of the new tests is illustrated via extensive simulation studies and a real data application. We study the geometry of Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves in binary classification problems. The key finding is that many of the most commonly used binary classification metrics are merely functions of the composition function $G := F_p \\\\circ F_n^{-1}$, where $F_p(\\\\cdot)$ and $F_n(\\\\cdot)$ are the class-conditional cumulative distribution functions of the classifier scores in the positive and negative classes, respectively. This geometric perspective facilitates the selection of operating points, understanding the effect of decision thresholds, and comparison between classifiers. It also helps explain how the shapes and geometry of ROC/PR curves reflect classifier behavior, providing objective tools for building classifiers optimized for specific applications with context-specific constraints. We further explore the conditions for classifier dominance, present analytical and numerical examples demonstrating the effects of class separability and variance on ROC and PR geometries, and derive a link between the positive-to-negative class leakage function $G(\\\\cdot)$ and the Kullback--Leibler divergence. The framework highlights practical considerations, such as model calibration, cost-sensitive optimization, and operating point selection under real-world capacity constraints, enabling more informed approaches to classifier deployment and decision-making. In this work, we are interested in studying the causal effect of an endogenous binary treatment on a dependently censored duration outcome. By dependent censoring, it is meant that the duration time ($T$) and right censoring time ($C$) are not statistically independent of each other, even after conditioning on the measured covariates. The endogeneity issue is handled by making use of a binary instrumental variable for the treatment. To deal with the dependent censoring problem, it is assumed that on the stratum of compliers: (i) $T$ follows a semiparametric proportional hazards model; (ii) $C$ follows a fully parametric model; and (iii) the relation between $T$ and $C$ is modeled by a parametric copula, such that the association parameter can be left unspecified. In this framework, the treatment effect of interest is the complier causal hazard ratio (CCHR). We devise an estimation procedure that is based on a weighted maximum likelihood approach, where the weights are the probabilities of an observation coming from a complier. The weights are estimated non-parametrically in a first stage, followed by the estimation of the CCHR. Novel conditions under which the model is identifiable are given, a two-step estimation procedure is proposed and some important asymptotic',\n",
       " \"approach, where the weights are the probabilities of an observation coming from a complier. The weights are estimated non-parametrically in a first stage, followed by the estimation of the CCHR. Novel conditions under which the model is identifiable are given, a two-step estimation procedure is proposed and some important asymptotic properties are established. Simulations are used to assess the validity and finite-sample performance of the estimation procedure. Finally, we apply the approach to estimate the CCHR of both job training programs on unemployment duration and periodic screening examinations on time until death from breast cancer. The data come from the National Job Training Partnership Act study and the Health Insurance Plan of Greater New York experiment respectively. The de Bruijn identity states that Fisher information is the half of the derivative of Shannon differential entropy along heat flow. In the same spirit, in this paper we introduce a generalized version of Fisher information, named as the R\\\\'enyi--Fisher information, which is the half of the derivative of R\\\\'enyi information along heat flow. Based on this R\\\\'enyi--Fisher information, we establish sharp R\\\\'enyi-entropic isoperimetric inequalities, which generalize the classic entropic isoperimetric inequality to the R\\\\'enyi setting. Utilizing these isoperimetric inequalities, we extend the classical Cram\\\\'er--Rao inequality from Fisher information to R\\\\'enyi--Fisher information. Lastly, we use these generalized Cram\\\\'er--Rao inequalities to determine the signs of derivatives of entropy along heat flow, strengthening existing results on the complete monotonicity of entropy. This paper focuses on nonparametric statistical inference of the hazard rate function of discrete distributions based on $\\\\delta$-record data. We derive the explicit expression of the maximum likelihood estimator and determine its exact distribution, as well as some important characteristics such as its bias and mean squared error. We then discuss the construction of confidence intervals and goodness-of-fit tests. The performance of our proposals is evaluated using simulation methods. Applications to real data are given, as well. The estimation of the hazard rate function based on usual records has been studied in the literature, although many procedures require several samples of records. In contrast, our approach relies on a single sequence of $\\\\delta$-records, simplifying the experimental design and increasing the applicability of the methods. Proper scoring rules have been a subject of growing interest in recent years, not only as tools for evaluation of probabilistic forecasts but also as methods for estimating probability distributions. In this article, we review the mathematical foundations of proper scoring rules including general characterization results and important families of scoring rules. We discuss their role in statistics and machine learning for estimation and forecast evaluation. Furthermore, we comment on interesting developments of their usage in applications. The goal of this paper is to propose a new approach to asymptotic analysis of the finite predictor for stationary sequences. It produces the exact asymptotics of the relative prediction error and the partial correlation coefficients. The assumptions are analytic in nature and applicable to processes with long range dependence. The ARIMA type process driven by the fractional Gaussian noise (fGn), which previously\",\n",
       " \"analysis of the finite predictor for stationary sequences. It produces the exact asymptotics of the relative prediction error and the partial correlation coefficients. The assumptions are analytic in nature and applicable to processes with long range dependence. The ARIMA type process driven by the fractional Gaussian noise (fGn), which previously remained elusive, serves as our study case. Empirical likelihood serves as a powerful tool for constructing confidence intervals in nonparametric regression and regression discontinuity designs (RDD). The original empirical likelihood framework can be naturally extended to these settings using local linear smoothers, with Wilks' theorem holding only when an undersmoothed bandwidth is selected. However, the generalization of bias-corrected versions of empirical likelihood under more realistic conditions is non-trivial and has remained an open challenge in the literature. This paper provides a satisfactory solution by proposing a novel approach, referred to as robust empirical likelihood, designed for nonparametric regression and RDD. The core idea is to construct robust weights which simultaneously achieve bias correction and account for the additional variability introduced by the estimated bias, thereby enabling valid confidence interval construction without extra estimation steps involved. We demonstrate that the Wilks' phenomenon still holds under weaker conditions in nonparametric regression, sharp and fuzzy RDD settings. Extensive simulation studies confirm the effectiveness of our proposed approach, showing superior performance over existing methods in terms of coverage probabilities and interval lengths. Moreover, the proposed procedure exhibits robustness to bandwidth selection, making it a flexible and reliable tool for empirical analyses. The practical usefulness is further illustrated through applications to two real datasets. In this paper, we prove exponential tail bounds for canonical (or degenerate) $U$-statistics and $U$-processes under exponential-type tail assumptions on the kernels. Most of the existing results in the relevant literature often assume bounded kernels or obtain sub-optimal tail behavior under unbounded kernels. We obtain sharp rates and optimal tail behavior under sub-Weibull kernel functions. Some examples from nonparametric and semiparametric statistics literature are considered. Multiple works regarding convergence analysis of Markov chains have led to spectral gap decomposition formulas of the form \\\\[ \\\\mathrm{Gap}(S) \\\\geq c_0 \\\\left[\\\\inf_z \\\\mathrm{Gap}(Q_z)\\\\right] \\\\mathrm{Gap}(\\\\bar{S}), \\\\] where $c_0$ is a constant, $\\\\mathrm{Gap}$ denotes the right spectral gap of a reversible Markov operator, $S$ is the Markov transition kernel (Mtk) of interest, $\\\\bar{S}$ is an idealized or simplified version of $S$, and $\\\\{Q_z\\\\}$ is a collection of Mtks characterizing the differences between $S$ and $\\\\bar{S}$. This type of relationship has been established in various contexts, including: 1. decomposition of Markov chains based on a finite cover of the state space, 2. hybrid Gibbs samplers, and 3. spectral independence and localization schemes. We show that multiple key decomposition results across these domains can be connected within a unified framework, rooted in a simple sandwich structure of $S$. Within the general framework, we establish new instances of spectral gap decomposition for hybrid hit-and-run samplers and hybrid data augmentation algorithms with two intractable conditional distributions. Additionally, we explore several other properties of the sandwich structure, and derive extensions of the\",\n",
       " 'framework, rooted in a simple sandwich structure of $S$. Within the general framework, we establish new instances of spectral gap decomposition for hybrid hit-and-run samplers and hybrid data augmentation algorithms with two intractable conditional distributions. Additionally, we explore several other properties of the sandwich structure, and derive extensions of the spectral gap decomposition formula. Multiparameter persistent homology is a generalization of classical persistent homology, a central and widely-used methodology from topological data analysis, which takes into account density estimation and is an effective tool for data analysis in the presence of noise. Similar to its classical single-parameter counterpart, however, it is challenging to compute and use in practice due to its complex algebraic construction. In this paper, we study a popular and tractable invariant for multiparameter persistent homology in a statistical setting: the multiparameter persistence landscape. We derive a functional central limit theorem for multiparameter persistence landscapes, from which we compute confidence bands, giving rise to one of the first statistical inference methodologies for multiparameter persistence landscapes. We provide an implementation of confidence bands and demonstrate their application in a machine learning task on synthetic data. Real-world networks grow over time; statistical models based on node exchangeability are not appropriate. Instead of constraining the structure of the \\\\textit{distribution} of edges, we propose that the relevant symmetries refer to the \\\\textit{causal structure} between them. We first enumerate the 96 causal directed acyclic graph (DAG) models over pairs of nodes (dyad variables) in a growing network with finite ancestral sets that are invariant to node deletion. We then partition them into 21 classes with ancestral sets that are closed under node marginalization. Several of these classes are remarkably amenable to distributed and asynchronous evaluation. As an example, we highlight a simple model that exhibits flexible power-law degree distributions and emergent phase transitions in sparsity, which we characterize analytically. With few parameters and much conditional independence, our proposed framework provides natural baseline models for causal inference in relational data. We address the problem of nonparametric estimation of the spectral density for a centered stationary Gaussian time series under local differential privacy constraints. Specifically, we propose new interactive privacy mechanisms for three tasks: estimating a single covariance coefficient, estimating the spectral density at a fixed frequency, and estimating the entire spectral density function. Our approach achieves faster rates through a two-stage process: we apply first the Laplace mechanism to the truncated value and then use the former privatized sample to gain knowledge on the dependence mechanism in the time series. For spectral densities belonging to H\\\\\"older and Sobolev smoothness classes, we demonstrate that our estimators improve upon the non-interactive mechanism of Kroll (2024) for small privacy parameter $\\\\alpha$, since the pointwise rates depend on $n\\\\alpha^2$ instead of $n\\\\alpha^4$. Moreover, we show that the rate $(n\\\\alpha^4)^{-1}$ is optimal for estimating a covariance coefficient with non-interactive mechanisms. However, the $L_2$ rate of our interactive estimator is slower than the pointwise rate. We show how to use these estimators to provide a bona-fide locally differentially private covariance',\n",
       " \"$n\\\\alpha^2$ instead of $n\\\\alpha^4$. Moreover, we show that the rate $(n\\\\alpha^4)^{-1}$ is optimal for estimating a covariance coefficient with non-interactive mechanisms. However, the $L_2$ rate of our interactive estimator is slower than the pointwise rate. We show how to use these estimators to provide a bona-fide locally differentially private covariance matrix estimator. In this paper, we derive power guarantees of some sequential tests for bounded mean under general alternatives. We focus on testing procedures using nonnegative supermartingales which are anytime valid and consider alternatives which coincide asymptotically with the null (e.g. vanishing mean) while still allowing to reject in finite time. Introducing variance constraints, we show that the alternative can be broaden while keeping power guarantees for certain second-order testing procedures. We also compare different test procedures in multidimensional setting using characteristics of the rejection times. Finally, we extend our analysis to other functionals as well as testing and comparing forecasters. Our results are illustrated with numerical simulations including bounded mean testing and comparison of forecasters. Multivariate phase relationships are important to characterize and understand numerous physical, biological, and chemical systems, from electromagnetic waves to neural oscillations. These systems exhibit complex spatiotemporal dynamics and intricate interdependencies among their constituent elements. While classical models of multivariate phase relationships, such as the wave equation and Kuramoto model, give theoretical models to describe phenomena, the development of statistical tools for hypothesis testing and inference for multivariate phase relationships in complex systems remains limited. This paper introduces a novel probabilistic modeling framework to characterize multivariate phase relationships, with wave-like phenomena serving as a key example. This approach describes spatial patterns and interactions between oscillators through a pairwise exponential family distribution. Building upon the literature of graphical model inference, including methods like Ising models, graphical lasso, and interaction screening, this work bridges the gap between classical wave dynamics and modern statistical approaches. Efficient inference methods are introduced, leveraging the Chow-Liu algorithm for directed tree approximations and interaction screening for general graphical models. Simulated experiments demonstrate the utility of these methods for uncovering wave properties and sparse interaction structures, highlighting their applicability to diverse scientific domains. This framework establishes a new paradigm for statistical modeling of multivariate phase relationships, providing a powerful toolset for exploring the complexity of these systems. In survival analysis, the estimation of the proportion of subjects who will never experience the event of interest, termed the cure rate, has received considerable attention recently. Its estimation can be a particularly difficult task when follow-up is not sufficient, that is when the censoring mechanism has a smaller support than the distribution of the target data. In the latter case, non-parametric estimators were recently proposed using extreme value methodology, assuming that the distribution of the susceptible population is in the Fr\\\\'echet or Gumbel max-domains of attraction. In this paper, we take the extreme value techniques one step further, to jointly estimate the cure rate and the extreme value index, using probability plotting methodology, and in particular using the full information contained in the top order\",\n",
       " \"susceptible population is in the Fr\\\\'echet or Gumbel max-domains of attraction. In this paper, we take the extreme value techniques one step further, to jointly estimate the cure rate and the extreme value index, using probability plotting methodology, and in particular using the full information contained in the top order statistics. In other words, under sufficient or insufficient follow-up, we reconstruct the immune proportion. To this end, a Peaks-over-Threshold approach is proposed under the Gumbel max-domain assumption. Next, the approach is also transferred to more specific models such as Pareto, log-normal and Weibull tail models, allowing to recognize the most important tail characteristics of the susceptible population. We establish the asymptotic behavior of our estimators under regularization. Though simulation studies, our estimators are show to rival and often outperform established models, even when purely considering cure rate estimation. Finally, we provide an application of our method to Norwegian birth registry data. Spectral estimation is an important tool in time series analysis, with applications including economics, astronomy, and climatology. The asymptotic theory for non-parametric estimation is well-known but the development of non-asymptotic theory is still ongoing. Our recent work obtained the first non-asymptotic error bounds on the Bartlett and Welch methods for $L$-mixing stochastic processes. The class of $L$-mixing processes contains common models in time series analysis, including autoregressive processes and measurements of geometrically ergodic Markov chains. Our prior analysis assumes that the process has zero mean. While zero-mean assumptions are common, real-world time-series data often has unknown, non-zero mean. In this work, we derive non-asymptotic error bounds for both Bartlett and Welch estimators for $L$-mixing time-series data with unknown means. The obtained error bounds are of $O(\\\\frac{1}{\\\\sqrt{k}})$, where $k$ is the number of data segments used in the algorithm, which are tighter than our previous results under the zero-mean assumption. Exact eigendecomposition of large matrices is very expensive, and it is practically impossible to compute exact eigenvalues. Instead, one may set a more modest goal of approaching the empirical distribution of the eigenvalues, recovering the overall shape of the eigenspectrum. Current approaches to spectral estimation typically work with \\\\emph{moments} of the spectral distribution. These moments are first estimated using Monte Carlo trace estimators, then the estimates are combined to approximate the spectral density. In this article we show how \\\\emph{Kirchhoff forests}, which are random forests on graphs, can be used to estimate certain non-linear moments of very large graph Laplacians. We show how to combine these moments into an estimate of the spectral density. If the estimate's desired precision isn't too high, our approach paves the way to the estimation of a graph's spectrum in time sublinear in the number of links. In this work, we construct optimal low-rank approximations for the Gaussian posterior distribution in linear Gaussian inverse problems. The parameter space is a separable Hilbert space of possibly infinite dimension, and the data space is assumed to be finite-dimensional. We consider various types of approximation families for the posterior. We first consider approximate posteriors in which the means\",\n",
       " 'for the Gaussian posterior distribution in linear Gaussian inverse problems. The parameter space is a separable Hilbert space of possibly infinite dimension, and the data space is assumed to be finite-dimensional. We consider various types of approximation families for the posterior. We first consider approximate posteriors in which the means vary among a class of either structure-preserving or structure-ignoring low-rank transformations of the data, and in which the posterior covariance is kept fixed. We give necessary and sufficient conditions for these approximating posteriors to be equivalent to the exact posterior, for all possible realisations of the data simultaneously. For such approximations, we measure approximation error with the Kullback-Leibler, R\\\\\\'enyi and Amari $\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, and with the Hellinger distance, all averaged over the data distribution. With these losses, we find the optimal approximations and formulate an equivalent condition for their uniqueness, extending the work in finite dimensions of Spantini et al. (SIAM J. Sci. Comput. 2015). We then consider joint approximation of the mean and covariance, by also varying the posterior covariance over the low-rank updates considered in Part I of this work. For the reverse Kullback-Leibler divergence, we show that the separate optimal approximations of the mean and of the covariance can be combined to yield an optimal joint approximation of the mean and covariance. In addition, we interpret the joint approximation with the optimal structure-ignoring approximate mean in terms of an optimal projector in parameter space. Consider an observation of a multivariate temporal point process $N$ with law $\\\\mathcal P$ on the time interval $[0,T]$. To test the null hypothesis that $\\\\mathcal P$ belongs to a given parametric family, we construct a convergent compensated counting process to which we apply an innovation martingale transformation. We prove that the resulting process converges weakly to a standard Wiener process. Consequently, taking a suitable functional of this process yields an asymptotically distribution-free goodness-of-fit test for point processes. For several standard tests based on the increments of this transformed process, we establish consistency under alternative hypotheses. Finally, we assess the performance of the proposed testing procedure through a Monte Carlo simulation study and illustrate its practical utility with two real-data examples. In this paper, in a multivariate setting we derive near optimal rates of convergence in the minimax sense for estimating partial derivatives of the mean function for functional data observed under a fixed synchronous design over H\\\\\"older smoothness classes. We focus on the supremum norm since it corresponds to the visualisation of the estimation error, and is closely related to the construction of uniform confidence bands. In contrast to mean function estimation, for derivative estimation the smoothness of the paths of the processes is crucial for the rates of convergence. On the one hand, if the paths have higher-order smoothness than the order of the partial derivative to be estimated, the parametric $\\\\sqrt n$ rate can be achieved under sufficiently dense design. On the other hand, for processes with rough paths of lower-order smoothness, we show that the rates of convergence',\n",
       " \"the one hand, if the paths have higher-order smoothness than the order of the partial derivative to be estimated, the parametric $\\\\sqrt n$ rate can be achieved under sufficiently dense design. On the other hand, for processes with rough paths of lower-order smoothness, we show that the rates of convergence are necessarily slower than the parametric rate, and determine a near-optimal rate at which estimation is still possible. We implement a multivariate local polynomial derivative estimator and illustrate its finite-sample performance in a simulation as well as for two real-data sets. To assess the smoothness of the sample paths in the applications we further discuss a method based on comparing restricted estimates of the partial derivatives of the covariance kernel. We introduce a new version of the KL-divergence for Gaussian distributions which is based on Wasserstein geometry and referred to as WKL-divergence. We show that this version is consistent with the geometry of the sample space ${\\\\Bbb R}^n$. In particular, we can evaluate the WKL-divergence of the Dirac measures concentrated in two points which turns out to be proportional to the squared distance between these points. For linear inverse problems with Gaussian priors and Gaussian observation noise, the posterior is Gaussian, with mean and covariance determined by the conditioning formula. Using the Feldman-Hajek theorem, we analyse the prior-to-posterior update and its low-rank approximation for infinite-dimensional Hilbert parameter spaces and finite-dimensional observations. We show that the posterior distribution differs from the prior on a finite-dimensional subspace, and construct low-rank approximations to the posterior covariance, while keeping the mean fixed. Since in infinite dimensions, not all low-rank covariance approximations yield approximate posterior distributions which are equivalent to the posterior and prior distribution, we characterise the low-rank covariance approximations which do yield this equivalence, and their respective inverses, or `precisions'. For such approximations, a family of measure approximation problems is solved by identifying the low-rank approximations which are optimal for various losses simultaneously. These loss functions include the family of R\\\\'enyi divergences, the Amari $\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, the Hellinger metric and the Kullback-Leibler divergence. Our results extend those of Spantini et al. (SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide theoretical underpinning for the construction of low-rank approximations of discretised versions of the infinite-dimensional inverse problem, by formulating discretization independent results. Species sampling processes have long served as the framework for studying random discrete distributions. However, their statistical applicability is limited when partial exchangeability is assumed as probabilistic invariance for the observables. Despite numerous discrete models for partially exchangeable observations, a unifying framework is currently missing, leaving many questions about the induced learning mechanisms unanswered in this setting. To fill this gap, we consider the natural extension of species sampling models to a multivariate framework, obtaining a general class of models characterized by their partially exchangeable partition probability function. A notable subclass, named regular multivariate species sampling models, exists among these models. In the subclass, dependence across processes is accurately captured by the correlation among them: a correlation of one\",\n",
       " 'models to a multivariate framework, obtaining a general class of models characterized by their partially exchangeable partition probability function. A notable subclass, named regular multivariate species sampling models, exists among these models. In the subclass, dependence across processes is accurately captured by the correlation among them: a correlation of one equals full exchangeability and a null correlation corresponds to independence. Regular multivariate species sampling models encompass discrete processes for partial exchangeable data used in Bayesian models, thereby highlighting their core distributional properties and providing a means for developing new models. We consider the Wigner matrix $W_{n}$ of dimension $n \\\\times n$ as $n \\\\to \\\\infty$. The objective of this paper is two folds: first we construct an operator $\\\\mathcal{W}$ on a suitable Hilbert space $\\\\mathcal{H}$ and then define a suitable notion of convergence such that the matrices $W_{n}$ converge in that notion of convergence to $\\\\mathcal{W}$. We further investigate some properties of $\\\\mathcal{W}$ and $\\\\mathcal{H}$. We show that $\\\\mathcal{H}$ is a nontrivial extension of $L^{2}[0,1]$ with respect to the Lebesgue measure and the spectral measure of $\\\\mathcal{W}$ at any function $f \\\\in L^{2}[0,1]$ is almost surely the semicircular law. Selective prediction, where a model has the option to abstain from making a decision, is crucial for machine learning applications in which mistakes are costly. In this work, we focus on distributional regression and introduce a framework that enables the model to abstain from estimation in situations of high uncertainty. We refer to this approach as distributional regression with reject option, inspired by similar concepts in classification and regression with reject option. We study the scenario where the rejection rate is fixed. We derive a closed-form expression for the optimal rule, which relies on thresholding the entropy function of the Continuous Ranked Probability Score (CRPS). We propose a semi-supervised estimation procedure for the optimal rule, using two datasets: the first, labeled, is used to estimate both the conditional distribution function and the entropy function of the CRPS, while the second, unlabeled, is employed to calibrate the desired rejection rate. Notably, the control of the rejection rate is distribution-free. Under mild conditions, we show that our procedure is asymptotically as effective as the optimal rule, both in terms of error rate and rejection rate. Additionally, we establish rates of convergence for our approach based on distributional k-nearest neighbor. A numerical analysis on real-world datasets demonstrates the strong performance of our procedure Estimating the mode of a unimodal distribution is a classical problem in statistics. Although there are several approaches for point-estimation of mode in the literature, very little has been explored about the interval-estimation of mode. Our work proposes a collection of novel methods of obtaining finite sample valid confidence set of the mode of a unimodal distribution. We analyze the behaviour of the width of the proposed confidence sets under some regularity assumptions of the density about the mode and show that the width of these confidence sets shrink to zero near optimally. Simply put, we show that it is possible',\n",
       " \"the mode of a unimodal distribution. We analyze the behaviour of the width of the proposed confidence sets under some regularity assumptions of the density about the mode and show that the width of these confidence sets shrink to zero near optimally. Simply put, we show that it is possible to build finite sample valid confidence sets for the mode that shrink to a singleton as sample size increases. We support the theoretical results by showing the performance of the proposed methods on some synthetic data-sets. We believe that our confidence sets can be improved both in construction and in terms of rate. We study the bias and the mean-squared error of the maximum likelihood estimators (MLE) of parameters associated with a two-parameter mean-reverting process for a finite time $T$. Using the likelihood ratio process, we derive the expressions for MLEs, then compute the bias and the MSE via the change of measure and Ito's formula. We apply the derived expressions to the general Ornstein-Uhlenbeck process, where the bias and the MSE are numerically computed through a joint moment-generating function of key functionals of the O-U process. A numerical study is provided to illustrate the behaviour of bias and the MSE for the MLE of the mean-reverting speed parameter. In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``projection-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis. We propose to model the records of the maximum Drawdown in capital markets by means a Piecewise Deterministic Markov Process (PDMP). We derive statistical results such as the mean and variance that describes the sequence of maximum Drawdown records. In addition, we developed a simulation study and techniques for estimating the parameters governing the stochastic process, using a practical example in the capital market to illustrate the procedure. This paper addresses the problem of detecting change points in the spectral density of time series, motivated by EEG analysis of seizure patients. Seizures disrupt coherence and functional connectivity, necessitating precise detection. Departing from traditional parametric approaches, we utilize the Wold decomposition, representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy,\",\n",
       " \"representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy, we introduce an optimal rate method with an asymptotic distribution, facilitating the construction of confidence intervals. The proposed method effectively identifies seizure onset in EEG data and extends to event detection in video data. Comprehensive numerical experiments demonstrate its superior performance compared to existing techniques. Under certain conditions, the largest eigenvalue of a sample covariance matrix undergoes a well-known phase transition when the sample size $n$ and data dimension $p$ diverge proportionally. In the subcritical regime, this eigenvalue has fluctuations of order $n^{-2/3}$ that can be approximated by a Tracy-Widom distribution, while in the supercritical regime, it has fluctuations of order $n^{-1/2}$ that can be approximated with a Gaussian distribution. However, the statistical problem of determining which regime underlies a given dataset is far from resolved. We develop a new testing framework and procedure to address this problem. In particular, we demonstrate that the procedure has an asymptotically controlled level, and that it is power consistent for certain alternatives. Also, this testing procedure enables the design a new bootstrap method for approximating the distributions of functionals of the leading sample eigenvalues within the subcritical regime -- which is the first such method that is supported by theoretical guarantees. We developed a mathematical setup inspired by Buyse's generalized pairwise comparisons to define a notion of optimal individualized treatment rule (ITR) in the presence of prioritized outcomes in a randomized controlled trial, terming such an ITR pairwise optimal. We present two approaches to estimate pairwise optimal ITRs. The first is a variant of the k-nearest neighbors algorithm. The second is a meta-learner based on a randomized bagging scheme, allowing the use of any classification algorithm for constructing an ITR. We study the behavior of these estimation schemes from a theoretical standpoint and through Monte Carlo simulations and illustrate their use on trial data. Significant treatment effects are often emphasized when interpreting and summarizing empirical findings in studies that estimate multiple, possibly many, treatment effects. Under this kind of selective reporting, conventional treatment effect estimates may be biased and their corresponding confidence intervals may undercover the true effect sizes. We propose new estimators and confidence intervals that provide valid inferences on the effect sizes of the significant effects after multiple hypothesis testing. Our methods are based on the principle of selective conditional inference and complement a wide range of tests, including step-up tests and bootstrap-based step-down tests. Our approach is scalable, allowing us to study an application with over 370 estimated effects. We justify our procedure for asymptotically normal treatment effect estimators. We provide two empirical examples that demonstrate bias correction and confidence interval adjustments for significant effects. The magnitude and direction of the bias correction depend on the correlation structure of the\",\n",
       " 'to study an application with over 370 estimated effects. We justify our procedure for asymptotically normal treatment effect estimators. We provide two empirical examples that demonstrate bias correction and confidence interval adjustments for significant effects. The magnitude and direction of the bias correction depend on the correlation structure of the estimated effects and whether the interpretation of the significant effects depends on the (in)significance of other effects. We study the consistency and weak convergence of the conditional tail function and conditional Hill estimators under broad dependence assumptions for a heavy-tailed response sequence and a covariate sequence. Consistency is established under $\\\\alpha$-mixing, while asymptotic normality follows from $\\\\beta$-mixing and second-order conditions. A key aspect of our approach is its versatile functional formulation in terms of the conditional tail process. Simulations demonstrate its performance across dependence scenarios. We apply our method to extreme event modeling in the oil industry, revealing distinct tail behaviors under varying conditioning values. The extremal dependence structure of a regularly varying $d$-dimensional random vector can be described by its angular measure. The standard nonparametric estimator of this measure is the empirical measure of the observed angles of the $k$ random vectors with largest norm, for a suitably chosen number $k$. Due to the curse of dimensionality, for moderate or large $d$, this estimator is often inaccurate. If the angular measure is concentrated on a vicinity of a lower dimensional subspace, then first projecting the data on a lower dimensional subspace obtained by a principal component analysis of the angles of extreme observations can substantially improve the performance of the estimator. We derive the asymptotic behavior of such PCA projections and the resulting excess risk. In particular, it is shown that, under mild conditions, the excess risk (as a function of $k$) decreases much faster than it was suggested by empirical risk bounds obtained in \\\\cite{DS21}. Moreover, functional limit theorems for local empirical processes of the (empirical) reconstruction error of projections uniformly over neighborhoods of the true optimal projection are established. Based on these asymptotic results, we propose a data-driven method to select the dimension of the projection space. Finally, the finite sample performance of resulting estimators is examined in a simulation study. This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025) we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings including Rubins (1987) total variance estimation in multiple imputations where weighted',\n",
       " \"new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings including Rubins (1987) total variance estimation in multiple imputations where weighted variance combinations are common. The proposed estimator generalizes von Daviers (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights. Given a planar curve, imagine rolling a sphere along that curve without slipping or twisting, and by this means tracing out a curve on the sphere. It is well known that such a rolling operation induces a local isometry between the sphere and the plane so that the two curves uniquely determine each other, and moreover, the operation extends to a general class of manifolds in any dimension. We use rolling to construct an analogue of a Gaussian process on a manifold starting from a Euclidean Gaussian process. The resulting model is generative, and is amenable to statistical inference given data as curves on a manifold. We illustrate with examples on the unit sphere, symmetric positive-definite matrices, and with a robotics application involving 3D orientations. Most Kalman filters for non-linear systems, such as the unscented Kalman filter, are based on Gaussian approximations. We use Poincar\\\\'e inequalities to bound the Wasserstein distance between the true joint distribution of the prediction and measurement and its Gaussian approximation. The bounds can be used to assess the performance of non-linear Gaussian filters and determine those filtering approximations that are most likely to induce error. We revisit the discrete argmin inference problem in high-dimensional settings. Given $n$ observations from a $d$ dimensional vector, the goal is to test whether the $r$th component of the mean vector is the smallest among all components. We propose dimension-agnostic tests that maintain validity regardless of how $d$ scales with $n$, and regardless of arbitrary ties in the mean vector. Notably, our validity holds under mild moment conditions, requiring little more than finiteness of a second moment, and permitting possibly strong dependence between coordinates. In addition, we establish the local minimax separation rate for this problem, which adapts to the cardinality of a confusion set, and show that the proposed tests attain this rate. Our method uses the sample splitting and self-normalization approach of Kim and Ramdas (2024). Our tests can be easily inverted to yield confidence sets for the argmin index. Empirical results illustrate the strong performance of our approach in terms of type I error control and power compared to existing methods. The Glivenko-Cantelli theorem is a uniform version of the strong law of large numbers. It states that for every IID sequence of random variables, the empirical measure converges to the underlying distribution (in the sense of uniform convergence of the CDF). In this work, we provide tools to study such limits of empirical measures in categorical probability. We propose two axioms, permutation invariance and empirical adequacy, that a morphism of type $X^\\\\mathbb{N} \\\\to\",\n",
       " \"variables, the empirical measure converges to the underlying distribution (in the sense of uniform convergence of the CDF). In this work, we provide tools to study such limits of empirical measures in categorical probability. We propose two axioms, permutation invariance and empirical adequacy, that a morphism of type $X^\\\\mathbb{N} \\\\to X$ should satisfy to be interpretable as taking an infinite sequence as input and producing a sample from its empirical measure as output. Since not all sequences have a well-defined empirical measure, ``such empirical sampling morphisms'' live in quasi-Markov categories, which, unlike Markov categories, allow partial morphisms. Given an empirical sampling morphism and a few other properties, we prove representability as well as abstract versions of the de Finetti theorem, the Glivenko-Cantelli theorem and the strong law of large numbers. We provide several concrete constructions of empirical sampling morphisms as partially defined Markov kernels on standard Borel spaces. Instantiating our abstract results then recovers the standard Glivenko-Cantelli theorem and the strong law of large numbers for random variables with finite first moment. Our work thus provides a joint proof of these two theorems in conjunction with the de Finetti theorem from first principles. In this paper we consider the use of tiered background knowledge within constraint based causal discovery. Our focus is on settings relaxing causal sufficiency, i.e. allowing for latent variables which may arise because relevant information could not be measured at all, or not jointly, as in the case of multiple overlapping datasets. We first present novel insights into the properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce a new extension of the IOD (integrating overlapping datasets) algorithm incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm. We show that under full usage of the tiered background knowledge tFCI and tIOD are sound, while simple versions of the tIOD and tFCI are sound and complete. We further show that the tIOD algorithm can often be expected to be considerably more efficient and informative than the IOD algorithm even beyond the obvious restriction of the Markov equivalence classes. We provide a formal result on the conditions for this gain in efficiency and informativeness. Our results are accompanied by a series of examples illustrating the exact role and usefulness of tiered background knowledge. Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology that images the heart at up to 50 frames per second, offering insight into the respiratory effects on the heartbeat. However, this method significantly increases the number of images that must be segmented to derive critical health indicators. Although neural networks perform well on inner slices, predictions on outer slices are often unreliable. This work proposes sparse Bayesian learning (SBL) to predict the ventricular volume on outer slices with minimal manual labeling to address this challenge. The ventricular volume over time is assumed to be dominated by sparse frequencies corresponding to the heart and respiratory rates. Moreover, SBL identifies these sparse frequencies on well-segmented inner slices by optimizing hyperparameters via type -II likelihood,\",\n",
       " 'ventricular volume on outer slices with minimal manual labeling to address this challenge. The ventricular volume over time is assumed to be dominated by sparse frequencies corresponding to the heart and respiratory rates. Moreover, SBL identifies these sparse frequencies on well-segmented inner slices by optimizing hyperparameters via type -II likelihood, automatically pruning irrelevant components. The identified sparse frequencies guide the selection of outer slice images for labeling, minimizing posterior variance. This work provides performance guarantees for the greedy algorithm. Testing on patient data demonstrates that only a few labeled images are necessary for accurate volume prediction. The labeling procedure effectively avoids selecting inefficient images. Furthermore, the Bayesian approach provides uncertainty estimates, highlighting unreliable predictions (e.g., when choosing suboptimal labels). We consider the problem of constructing a least conservative estimator of the expected value $\\\\mu$ of a non-negative heavy-tailed random variable. We require that the probability of overestimating the expected value $\\\\mu$ is kept appropriately small; a natural requirement if its subsequent use in a decision process is anticipated. In this setting, we show it is optimal to estimate $\\\\mu$ by solving a distributionally robust optimization (DRO) problem using the Kullback-Leibler (KL) divergence. We further show that the statistical properties of KL-DRO compare favorably with other estimators based on truncation, variance regularization, or Wasserstein DRO. Particle filters (PFs) is a class of Monte Carlo algorithms that propagate over time a set of $N\\\\in\\\\mathbb{N}$ particles which can be used to estimate, in an online fashion, the sequence of filtering distributions $(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ defined by a state-space model. Despite the popularity of PFs, the study of the time evolution of their estimates has only received very little attention in the literature. Denoting by $(\\\\hat{\\\\eta}_t^N)_{t\\\\geq 1}$ the PF estimate of $(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ and letting $\\\\kappa\\\\in (0,1)$, in this work we first show that for any number of particles $N$ it holds that, with probability one, we have $\\\\|\\\\hat{\\\\eta}_t^N- \\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa$ for infinitely many $t\\\\geq 1$, with $\\\\|\\\\cdot\\\\|$ a measure of distance between probability distributions. Considering a simple filtering problem we then provide reassuring results concerning the ability of PFs to estimate jointly a finite set $\\\\{\\\\hat{\\\\eta}_t\\\\}_{t=1}^T$ of filtering distributions by studying $\\\\P(\\\\sup_{t\\\\in\\\\{1,\\\\dots,T\\\\}}\\\\|\\\\hat{\\\\eta}_t^{N}-\\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa)$. Finally, on the same toy filtering problem, we prove that sequential quasi-Monte Carlo, a randomized quasi-Monte Carlo version of PF algorithms, offers greater safety guarantees than PFs in the sense that, for this algorithm, it holds that $\\\\lim_{N\\\\rightarrow\\\\infty}\\\\sup_{t\\\\geq 1}\\\\|\\\\hat{\\\\eta}_t^N-\\\\hat{\\\\eta}_t\\\\|=0$ with probability one. This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference. In this paper, we analyze the relative errors in various reliability measures due to the tacit assumption that the components associated with a $n$-component series system or',\n",
       " \"allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference. In this paper, we analyze the relative errors in various reliability measures due to the tacit assumption that the components associated with a $n$-component series system or a parallel system are independently working where the components are dependent. We use Copula functions in said error analysis. This technique generalizes the existing work on error assessment for many wide class of distributions. In this paper, we analyze the relative errors that crop up in the various reliability measures due to the tacit assumption that the components are independently working associated with a $n$-component series system or a parallel system where the components are dependent and follow a well-defined multivariate Weibull or exponential distribution. We also list some important observations which the previous authors have not noted in their earlier works. In this paper, we focus on the incurred error in multi-component series and parallel systems having multivariate Weibull distributions. In the upcoming sections, we establish that the present study has relevance with stochastic orders and statistical dependence which were not previously pointed out by previous authors. Evaluation is critical to advance decision making across domains, yet existing methodologies often struggle to balance theoretical rigor and practical scalability. In order to reduce the cost of experimental evaluation, we introduce a computational theory of evaluation for parameterisable subjects. We prove upper bounds of generalized evaluation error and generalized causal effect error of evaluation metric on subject. We also prove efficiency, and consistency to estimated causal effect of subject on metric by prediction. To optimize evaluation models, we propose a meta-learner to handle heterogeneous evaluation subjects space. Comparing with other computational approaches, our (conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12 scenes, including individual medicine, scientific simulation, business activities, and quantum trade. The evaluation time is reduced 3-7 order of magnitude comparing with experiments or simulations. Variable selection comprises an important step in many modern statistical inference procedures. In the regression setting, when estimators cannot shrink irrelevant signals to zero, covariates without relationships to the response often manifest small but non-zero regression coefficients. The ad hoc procedure of discarding variables whose coefficients are smaller than some threshold is often employed in practice. We formally analyze a version of such thresholding procedures and develop a simple thresholding method that consistently estimates the set of relevant variables under mild regularity assumptions. Using this thresholding procedure, we propose a sparse, $\\\\sqrt{n}$-consistent and asymptotically normal estimator whose non-zero elements do not exhibit shrinkage. The performance and applicability of our approach are examined via numerical studies of simulated and real data. This document is an extended version of an abstract for a talk, with approximately the same title, to be held at the 7th Joint Statistical Meeting of the Deutsche Arbeitsgemeinschaft Statistik, from 24 to 28 March 2025 in Berlin. Here ``teachable'' is meant to apply to people ranging from sufficiently advanced high school pupils\",\n",
       " \"extended version of an abstract for a talk, with approximately the same title, to be held at the 7th Joint Statistical Meeting of the Deutsche Arbeitsgemeinschaft Statistik, from 24 to 28 March 2025 in Berlin. Here ``teachable'' is meant to apply to people ranging from sufficiently advanced high school pupils to university students in mathematics or statistics: For understanding most of the proposed approximation results, it should suffice to know binomial laws, their means and variances, and the standard normal distribution function (but not necessarily the concept of a corresponding normal random variable). Of the proposed approximations, some are well-known (at least to experts), and some are based on teaching experience and research at Trier University. This paper introduces a new kind of periodic fractional autoregressive process (PFAR) driven by fractional Gaussian noise (fGn). The new model is a specialized varying coefficient fractional autoregressive model, where the coefficients adhere to a periodic structure. In this working, Generalized least squares estimation and GPH method are employed to construct an initial estimator to estimate the joint estimation of the parameters of these models. Then one-step procedure is used to obtain a more asymptotically-efficient estimator. The paper proves that both estimators are consistent and asymptotically normal, and their performance is demonstrated through a simulation study using finite-size samples via Monte Carlo simulations. Simulation studies suggests that, while both estimation methods can accurately estimate the model, the one-step estimator outperforms the initial estimator. In this work, we present a theoretical and computational framework for constructing stochastic transport maps between probability distributions using diffusion processes. We begin by proving that the time-marginal distribution of the sum of two independent diffusion processes satisfies a Fokker-Planck equation. Building on this result and applying Ambrosio-Figalli-Trevisan's superposition principle, we establish the existence and uniqueness of solutions to the associated stochastic differential equation (SDE). Leveraging these theoretical foundations, we develop a method to construct (stochastic) transport maps between arbitrary probability distributions using dynamical ordinary differential equations (ODEs) and SDEs. Furthermore, we introduce a unified framework that generalizes and extends a broad class of diffusion-based generative models and sampling techniques. Finally, we analyze the convergence properties of particle approximations for the SDEs underlying our framework, providing theoretical guarantees for their practical implementation. This work bridges theoretical insights with practical applications, offering new tools for generative modeling and sampling in high-dimensional spaces. In Learning Theory, the smoothness assumption on the target function (known as source condition) is a key factor in establishing theoretical convergence rates for an estimator. The existing general form of the source condition, as discussed in learning theory literature, has traditionally been restricted to a class of functions that can be expressed as a product of an operator monotone function and a Lipschitz continuous function. In this note, we remove these restrictions on the index function and establish optimal convergence rates for least-square regression over a Hilbert space with general regularization under a general source condition, thereby significantly broadening the scope of existing theoretical results. Let $P=(x_1,\\\\ldots,x_n)$ be a\",\n",
       " 'function and a Lipschitz continuous function. In this note, we remove these restrictions on the index function and establish optimal convergence rates for least-square regression over a Hilbert space with general regularization under a general source condition, thereby significantly broadening the scope of existing theoretical results. Let $P=(x_1,\\\\ldots,x_n)$ be a population consisting of $n\\\\ge 2$ real numbers whose sum is zero, and let $k <n$ be a positive integer. We sample $k$ elements from $P$ without replacement and denote by $X_P$ the sum of the elements in our sample. In this article, using ideas from the theory of majorization, we deduce non-asymptotic lower and upper bounds on the probability that $X_P$ exceeds its expected value. We consider nonlinear mixed effects models including high-dimensional covariates to model individual parameters. The objective is to identify relevant covariates and estimate model parameters. We combine a penalized LASSO-type estimator with an eBIC model choice criterion to select the covariates of interest. Then we estimate the parameters by maximum likelihood in the reduced model. We calculate the LASSO-type penalized estimator by a weighted proximal gradient descent algorithm with an adaptive learning rate. This choice allows us in particular to consider models that do not necessarily belong to the curved exponential family. We compare first the performance of the proposed methodology with those of the glmmLasso procedure in a linear mixed effects model in a simulation study. We then illustrate its performance in a nonlinear mixed-effects logistic growth model through simulation. We study the nonparametric maximum likelihood estimator $\\\\widehat{\\\\pi}$ for Gaussian location mixtures in one dimension. It has been known since (Lindsay, 1983) that given an $n$-point dataset, this estimator always returns a mixture with at most $n$ components, and more recently (Wu-Polyanskiy, 2020) gave a sharp $O(\\\\log n)$ bound for subgaussian data. In this work we study computational aspects of $\\\\widehat{\\\\pi}$. We provide an algorithm which for small enough $\\\\varepsilon>0$ computes an $\\\\varepsilon$-approximation of $\\\\widehat\\\\pi$ in Wasserstein distance in time $K+Cnk^2\\\\log\\\\log(1/\\\\varepsilon)$. Here $K$ is data-dependent but independent of $\\\\varepsilon$, while $C$ is an absolute constant and $k=|supp(\\\\widehat{\\\\pi})|\\\\leq n$ is the number of atoms in $\\\\widehat\\\\pi$. We also certifiably compute the exact value of $|supp(\\\\widehat\\\\pi)|$ in finite time. These guarantees hold almost surely whenever the dataset $(x_1,\\\\dots,x_n)\\\\in [-cn^{1/4},cn^{1/4}]$ consists of independent points from a probability distribution with a density (relative to Lebesgue measure). We also show the distribution of $\\\\widehat\\\\pi$ conditioned to be $k$-atomic admits a density on the associated $2k-1$ dimensional parameter space for all $k\\\\leq \\\\sqrt{n}/3$, and almost sure locally linear convergence of the EM algorithm. One key tool is a classical Fourier analytic estimate for non-degenerate curves. Statistical learning methods typically assume that the training and test data originate from the same distribution, enabling effective risk minimization. However, real-world applications frequently involve distributional shifts, leading to poor model generalization. To address this, recent advances in causal inference and robust learning have introduced strategies such as invariant causal prediction and anchor regression. While these approaches have been explored for traditional structural equation models (SEMs),',\n",
       " \"enabling effective risk minimization. However, real-world applications frequently involve distributional shifts, leading to poor model generalization. To address this, recent advances in causal inference and robust learning have introduced strategies such as invariant causal prediction and anchor regression. While these approaches have been explored for traditional structural equation models (SEMs), their extension to functional systems remains limited. This paper develops a risk minimization framework for functional SEMs using linear, potentially unbounded operators. We introduce a functional worst-risk minimization approach, ensuring robust predictive performance across shifted environments. Our key contribution is a novel worst-risk decomposition theorem, which expresses the maximum out-of-sample risk in terms of observed environments. We establish conditions for the existence and uniqueness of the worst-risk minimizer and provide consistent estimation procedures. Empirical results on functional systems illustrate the advantages of our method in mitigating distributional shifts. These findings contribute to the growing literature on robust functional regression and causal learning, offering practical guarantees for out-of-sample generalization in dynamic environments. We investigate the asymptotic behavior of the number of parts $K_n$ in the Ewens--Pitman partition model under the regime where the diversity parameter is scaled linearly with the sample size, that is, $\\\\theta = \\\\lambda n$ for some~$\\\\lambda > 0$. While recent work has established a law of large numbers (LLN) and a central limit theorem (CLT) for $K_n$ in this regime, we revisit these results through a martingale-based approach. Our method yields significantly shorter proofs, and leads to sharper convergence rates in the CLT, including improved Berry--Esseen bounds in the case $\\\\alpha = 0$, and a new result for the regime $\\\\alpha \\\\in (0,1)$, filling a gap in the literature. At present, in the theory of stochastic process modeling a problem of assessment of reliability and accuracy of stochastic process model in $C(T)$ space wasn't studied for the case of implicit decomposition of process in the form of a series with independent terms. The goal is to study reliability and accuracy in $C(T)$ of models of processes from $Sub_\\\\varphi(\\\\Omega)$ that cannot be decomposed in a series with independent elements explicitly. Using previous research in the field of modeling of stochastic processes, assumption is considered about possibility of decomposition of a stochastic process in the series with independent elements that can be found using approximations. Impact of approximation error of process decomposition in series with independent elements on reliability and accuracy of modeling of stochastic process in $C(T)$ is studied. Theorems are proved that allow estimation of reliability and accuracy of a model in $C(T)$ of a stochastic process from $Sub_\\\\varphi(\\\\Omega)$ in the case when decomposition of this process in a series with independent elements can be found only with some error, for example, using numerical approximations. Deep neural networks (DNNs) have become powerful tools for modeling complex data structures through sequentially integrating simple functions in each hidden layer. In survival analysis, recent advances of DNNs primarily focus on enhancing model capabilities, especially in exploring nonlinear covariate effects under right censoring. However, deep learning methods for interval-censored data, where\",\n",
       " 'networks (DNNs) have become powerful tools for modeling complex data structures through sequentially integrating simple functions in each hidden layer. In survival analysis, recent advances of DNNs primarily focus on enhancing model capabilities, especially in exploring nonlinear covariate effects under right censoring. However, deep learning methods for interval-censored data, where the unobservable failure time is only known to lie in an interval, remain underexplored and limited to specific data type or model. This work proposes a general regression framework for interval-censored data with a broad class of partially linear transformation models, where key covariate effects are modeled parametrically while nonlinear effects of nuisance multi-modal covariates are approximated via DNNs, balancing interpretability and flexibility. We employ sieve maximum likelihood estimation by leveraging monotone splines to approximate the cumulative baseline hazard function. To ensure reliable and tractable estimation, we develop an EM algorithm incorporating stochastic gradient descent. We establish the asymptotic properties of parameter estimators and show that the DNN estimator achieves minimax-optimal convergence. Extensive simulations demonstrate superior estimation and prediction accuracy over state-of-the-art methods. Applying our method to the Alzheimer\\'s Disease Neuroimaging Initiative dataset yields novel insights and improved predictive performance compared to traditional approaches. When prior information is lacking, the go-to strategy for probabilistic inference is to combine a \"default prior\" and the likelihood via Bayes\\'s theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach offering posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM\\'s desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution obtained in applications where the model has a group transformation structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations. We formalize the generalization error bound using Rademacher complexity in the Lean 4 theorem prover. Generalization error quantifies the gap between a learning machine\\'s performance on given training data versus unseen test data, and Rademacher complexity serves as an estimate of this error based on the complexity of learning machines, or hypothesis class. Unlike traditional methods such as PAC learning and VC dimension, Rademacher complexity is applicable across diverse machine learning scenarios including deep learning and kernel methods. We formalize key concepts and theorems, including the empirical and population Rademacher complexities, and establish generalization error bounds through formal proofs of McDiarmid\\'s inequality, Hoeffding\\'s lemma, and symmetrization arguments. The problems of detecting and recovering planted structures/subgraphs in Erd\\\\H{o}s-R\\\\\\'{e}nyi random graphs, have received significant attention over the past three decades, leading to many exciting results and mathematical techniques. However, prior work has largely focused on specific ad hoc planted structures and inferential settings,',\n",
       " \"inequality, Hoeffding's lemma, and symmetrization arguments. The problems of detecting and recovering planted structures/subgraphs in Erd\\\\H{o}s-R\\\\'{e}nyi random graphs, have received significant attention over the past three decades, leading to many exciting results and mathematical techniques. However, prior work has largely focused on specific ad hoc planted structures and inferential settings, while a general theory has remained elusive. In this paper, we bridge this gap by investigating the detection of an \\\\emph{arbitrary} planted subgraph $\\\\Gamma = \\\\Gamma_n$ in an Erd\\\\H{o}s-R\\\\'{e}nyi random graph $\\\\mathcal{G}(n, q_n)$, where the edge probability within $\\\\Gamma$ is $p_n$. We examine both the statistical and computational aspects of this problem and establish the following results. In the dense regime, where the edge probabilities $p_n$ and $q_n$ are fixed, we tightly characterize the information-theoretic and computational thresholds for detecting $\\\\Gamma$, and provide conditions under which a computational-statistical gap arises. Most notably, these thresholds depend on $\\\\Gamma$ only through its number of edges, maximum degree, and maximum subgraph density. Our lower and upper bounds are general and apply to any value of $p_n$ and $q_n$ as functions of $n$. Accordingly, we also analyze the sparse regime where $q_n = \\\\Theta(n^{-\\\\alpha})$ and $p_n-q_n =\\\\Theta(q_n)$, with $\\\\alpha\\\\in[0,2]$, as well as the critical regime where $p_n=1-o(1)$ and $q_n = \\\\Theta(n^{-\\\\alpha})$, both of which have been widely studied, for specific choices of $\\\\Gamma$. For these regimes, we show that our bounds are tight for all planted subgraphs investigated in the literature thus far\\\\textemdash{}and many more. Finally, we identify conditions under which detection undergoes sharp phase transition, where the boundaries at which algorithms succeed or fail shift abruptly as a function of $q_n$. A statistical model is said to be calibrated if the resulting mean estimates perfectly match the true means of the underlying responses. Aiming for calibration is often not achievable in practice as one has to deal with finite samples of noisy observations. A weaker notion of calibration is auto-calibration. An auto-calibrated model satisfies that the expected value of the responses being given the same mean estimate matches this estimate. Testing for auto-calibration has only been considered recently in the literature and we propose a new approach based on calibration bands. Calibration bands denote a set of lower and upper bounds such that the probability that the true means lie simultaneously inside those bounds exceeds some given confidence level. Such bands were constructed by Yang-Barber (2019) for sub-Gaussian distributions. Dimitriadis et al. (2023) then introduced narrower bands for the Bernoulli distribution and we use the same idea in order to extend the construction to the entire exponential dispersion family that contains for example the binomial, Poisson, negative binomial, gamma and normal distributions. Moreover, we show that the obtained calibration bands allow us to construct various tests for calibration and auto-calibration, respectively. Optimal transport theory has become a fundamental tool for handling diverse types of data, with growing applications across various fields. However, the Wasserstein distance presents significant computational and statistical challenges in high-dimensional settings. To address these issues, alternative distances such as the\",\n",
       " \"construct various tests for calibration and auto-calibration, respectively. Optimal transport theory has become a fundamental tool for handling diverse types of data, with growing applications across various fields. However, the Wasserstein distance presents significant computational and statistical challenges in high-dimensional settings. To address these issues, alternative distances such as the sliced Wasserstein distance, which leverages one-dimensional projections, have been introduced. In this work, we establish a novel central limit theorem for the p-sliced Wasserstein distance, for p>1, using the Efron-Stein inequality-a technique that has proven effective in related problems. This approach yields a central limit theorem centered at the expected value of the empirical cost, under mild regularity conditions. Notably, unlike the general Wasserstein distance in arbitrary dimensions, we demonstrate that, under specific assumptions, the centering constants can be replaced by the population cost, which is essential for statistical inference. This generalizes and significantly refines existing results for the one-dimensional case. Consequently, we present the first asymptotically valid inference framework for the sliced Wasserstein distance applicable to measures that are not necessarily compactly supported, for p>1. Finally, we address key practical aspects for inference, including Monte Carlo estimation of the integral and estimation of the asymptotic variance, ensuring applicability in real-world scenarios. We study the nonconvex optimization landscapes of synchronization problems on spheres. First, we present new results for the statistical problem of synchronization over the two-element group $\\\\mathbf{Z}_2$. We consider the nonconvex least-squares problem with $\\\\mathbf{Z}_2 = \\\\{\\\\pm 1\\\\}$ relaxed to the unit sphere in $\\\\mathbf{R}^r$ for $r \\\\geq 2$; for several popular models, including graph clustering under the binary stochastic block model, we show that, for any $r \\\\geq 2$, every second-order critical point recovers the ground truth in the asymptotic regimes where exact recovery is information-theoretically possible. Such statistical optimality via spherical relaxations had previously only been shown for (potentially arbitrarily) larger relaxation dimension $r$. Second, we consider the global synchronization of networks of coupled oscillators under the (homogeneous) Kuramoto model. We prove new and optimal asymptotic results for random signed networks on an Erd\\\\H{o}s--R\\\\'enyi graph, and we give new and simple proofs for several existing state-of-the-art results. Our key tool is a deterministic landscape condition that extends a recent result of Rakoto Endor and Waldspurger. This result says that, if a certain problem-dependent Laplacian matrix has small enough condition number, the nonconvex landscape is benign. Our extension allows the condition number to include an arbitrary diagonal preconditioner, which gives tighter results for many problems. We show that, for the synchronization of Kuramoto oscillator networks on nearest-neighbor circulant graphs as studied by Wiley, Strogatz, and Girvan, this condition is optimal. We also prove a natural complex extension that may be of interest for synchronization on the special orthogonal group $\\\\operatorname{SO}(2)$. A fundamental challenge in the application of finite mixture models is selecting the number of mixture components, also known as order. Traditional approaches rely on selecting a single best model using information criteria. However, in the presence of noisy data, and when models with different orders yield\",\n",
       " 'orthogonal group $\\\\operatorname{SO}(2)$. A fundamental challenge in the application of finite mixture models is selecting the number of mixture components, also known as order. Traditional approaches rely on selecting a single best model using information criteria. However, in the presence of noisy data, and when models with different orders yield similar fits, model selection uncertainty can be substantial, making it challenging to confidently identify the true number of components. In this paper, we introduce the Model Selection Confidence Set (MSCS) for order selection - a set-valued estimator that, with a predefined confidence level, includes the true mixture order across repeated samples. Rather than selecting a single model, our MSCS identifies all plausible orders by determining whether each candidate model is at least as plausible as the best-selected one, using a screening test based on a penalized likelihood ratio statistic. We provide theoretical guarantees for the asymptotic coverage of our confidence set and demonstrate its practical advantages through simulations and real data analysis. Identification of joint dependence among more than two random vectors plays an important role in many statistical applications, where the data may contain sensitive or confidential information. In this paper, we consider the the d-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of differential privacy. Given the limiting distribution of the empirical estimate of dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy regime is typically based on permutation and bootstrap. To detect joint dependence in privacy, we propose a dHSIC-based testing procedure by employing a differentially private permutation methodology. Our method enjoys privacy guarantee, valid level and pointwise consistency, while the bootstrap counterpart suffers inconsistent power. We further investigate the uniform power of the proposed test in dHSIC metric and $L_2$ metric, indicating that the proposed test attains the minimax optimal power across different privacy regimes. As a byproduct, our results also contain the pointwise and uniform power of the non-private permutation dHSIC, addressing an unsolved question remained in Pfister et al. (2018). We introduce a new compositional framework for generalized variational inference, clarifying the different parts of a model, how they interact, and how they compose. We explain that both exact Bayesian inference and the loss functions typical of variational inference (such as variational free energy and its generalizations) satisfy chain rules akin to that of reverse-mode automatic differentiation, and we advocate for exploiting this to build and optimize models accordingly. To this end, we construct a series of compositional tools: for building models; for constructing their inversions; for attaching local loss functions; and for exposing parameters. Finally, we explain how the resulting parameterized statistical games may be optimized locally, too. We illustrate our framework with a number of classic examples, pointing to new areas of extensibility that are revealed. The quantification of treatment effects plays an important role in a wide range of applications, including policy making and bio-pharmaceutical research. In this article, we study the quantile treatment effect (QTE) while addressing two specific types of heterogeneities: (a) personalized heterogeneity, which captures the varying',\n",
       " \"of extensibility that are revealed. The quantification of treatment effects plays an important role in a wide range of applications, including policy making and bio-pharmaceutical research. In this article, we study the quantile treatment effect (QTE) while addressing two specific types of heterogeneities: (a) personalized heterogeneity, which captures the varying treatment effects for different individuals, and (b) quantile heterogeneity, which accounts for how the impact of covariates varies across different quantile levels. A well-designed debiased estimator for the individualized quantile treatment effect (IQTE) is proposed to capture such heterogeneities effectively. We show that this estimator converges weakly to a Gaussian process as a function of the quantile levels and propose valid statistical inference methods, including the construction of confidence intervals and the development of hypothesis testing decision rules. In addition, the minimax optimality frameworks for these inference procedures are established. Specifically, we derive the minimax optimal rates for the expected length of confidence intervals and the magnitude of the detection boundary for hypothesis testing procedures, illustrating the superiority of the proposed estimator. The effectiveness of our methods is demonstrated through extensive simulations and an analysis of the National Health and Nutrition Examination Survey (NHANES) datasets. We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave equation on $\\\\mathbb R_+$ with zero initial conditions is excited with a Neumann boundary data modelled as a white noise process. Given also the Dirichlet data at the same point, determine the unknown first order coefficient function of the system. We first establish that direct problem is well-posed. The inverse problem is then solved by showing that correlations of the boundary data determine the Neumann-to-Dirichlet operator in the sense of distributions, which is known to uniquely identify the coefficient. This approach has applications in acoustic measurements of internal cross-sections of fluid pipes such as pressurised water supply pipes and vocal tract shape determination. This paper introduces a quasi-likelihood ratio testing procedure for diffusion processes observed under nonsynchronous sampling schemes. High-frequency data, particularly in financial econometrics, are often recorded at irregular time points, challenging conventional synchronous methods for parameter estimation and hypothesis testing. To address these challenges, we develop a quasi-likelihood framework that accommodates irregular sampling while integrating adaptive estimation techniques for both drift and diffusion coefficients, thereby enhancing optimization stability and reducing computational burden. We rigorously derive the asymptotic properties of the proposed test statistic, showing that it converges to a chi-squared distribution under the null hypothesis and exhibits consistency under alternatives. Moreover, we establish that the resulting tests are asymptotically uniformly most powerful. Extensive numerical experiments corroborate the theoretical findings and demonstrate that our method outperforms existing nonparametric approaches. Deviations from Bayesian updating are traditionally categorized as biases, errors, or fallacies, thus implying their inherent ``sub-optimality.'' We offer a more nuanced view. We demonstrate that, in learning problems with misspecified models, non-Bayesian updating can outperform Bayesian updating. We consider a quotient of a complete Riemannian manifold modulo an isometrically and properly acting Lie group and lifts of the quotient to the manifolds in optimal\",\n",
       " 'inherent ``sub-optimality.\\'\\' We offer a more nuanced view. We demonstrate that, in learning problems with misspecified models, non-Bayesian updating can outperform Bayesian updating. We consider a quotient of a complete Riemannian manifold modulo an isometrically and properly acting Lie group and lifts of the quotient to the manifolds in optimal position to a reference point on the manifold. With respect to the pushed forward Riemannian volume onto the quotient we derive continuity and uniqueness a.e. and smoothness to large extents also with respect to the reference point. In consequence we derive a general manifold stability theorem: the Fr\\\\\\'echet mean lies in the highest dimensional stratum assumed with positive probability, and a strong law for optimal lifts. This allows to define new two-sample tests utilizing individual optimal lifts which outperform existing two-sample tests on simulated data. They also outperform existing tests on a newly derived reverse labeling reflection shape space, that is used to model filament data of microtubules within cells in a biological application. Topic modeling is traditionally applied to word counts without accounting for the context in which words appear. Recent advancements in large language models (LLMs) offer contextualized word embeddings, which capture deeper meaning and relationships between words. We aim to leverage such embeddings to improve topic modeling. We use a pre-trained LLM to convert each document into a sequence of word embeddings. This sequence is then modeled as a Poisson point process, with its intensity measure expressed as a convex combination of $K$ base measures, each corresponding to a topic. To estimate these topics, we propose a flexible algorithm that integrates traditional topic modeling methods, enhanced by net-rounding applied before and kernel smoothing applied after. One advantage of this framework is that it treats the LLM as a black box, requiring no fine-tuning of its parameters. Another advantage is its ability to seamlessly integrate any traditional topic modeling approach as a plug-in module, without the need for modifications Assuming each topic is a $\\\\beta$-H\\\\\"{o}lder smooth intensity measure on the embedded space, we establish the rate of convergence of our method. We also provide a minimax lower bound and show that the rate of our method matches with the lower bound when $\\\\beta\\\\leq 1$. Additionally, we apply our method to several datasets, providing evidence that it offers an advantage over traditional topic modeling approaches. We construct a new tail bound for the sum of independent random variables for situations in which the expected value of the sum is known and each random variable lies within a specified interval, which may be different for each variable. This new bound can be computed by solving a two-dimensional convex optimization problem. Simulations demonstrate that the new bound is often substantially tighter than Hoeffding\\'s inequality for cases in which both bounds are applicable. Contrastive learning -- a modern approach to extract useful representations from unlabeled data by training models to distinguish similar samples from dissimilar ones -- has driven significant progress in foundation models. In this work, we develop a new theoretical',\n",
       " \"than Hoeffding's inequality for cases in which both bounds are applicable. Contrastive learning -- a modern approach to extract useful representations from unlabeled data by training models to distinguish similar samples from dissimilar ones -- has driven significant progress in foundation models. In this work, we develop a new theoretical framework for analyzing data augmentation-based contrastive learning, with a focus on SimCLR as a representative example. Our approach is based on the concept of \\\\emph{approximate sufficient statistics}, which we extend beyond its original definition in \\\\cite{oko2025statistical} for contrastive language-image pretraining (CLIP) using KL-divergence. We generalize it to equivalent forms and general f-divergences, and show that minimizing SimCLR and other contrastive losses yields encoders that are approximately sufficient. Furthermore, we demonstrate that these near-sufficient encoders can be effectively adapted to downstream regression and classification tasks, with performance depending on their sufficiency and the error induced by data augmentation in contrastive learning. Concrete examples in linear regression and topic classification are provided to illustrate the broad applicability of our results. In this paper, models that approximate stochastic processes from the space $Sub_\\\\varphi(\\\\Omega)$ with given reliability and accuracy in $L_p(T)$ are considered for some specific functions $\\\\varphi(t)$. For processes that are decomposited in series using orthonormal bases, such models are constructed in the case where elements of such decomposition cannot be found explicitly. The Generalized Mallows Model (GMM) is a well known family of models for ranking data. A GMM is a distribution over $\\\\mathbb{S}_n$, the set of permutations of n objects, characterized by a location parameter $\\\\sigma \\\\in \\\\mathbb{S}_n$, known as central permutation and a set of dispersion parameters $\\\\theta_{1:n-1}\\\\in(0,1]$. The GMM shares many properties, such as having sufficient statistics, with exponential models, thus it can be seen as an exponential family with a discrete parameter $\\\\sigma$. This paper shows that computing entropy, crossentropy and Kullback-Leibler divergence in the the class of GMM is tractable, paving the way for a better understanding of this exponential family. Knowledge distillation is a technique used to train a small student network using the output generated by a large teacher network, and has many empirical advantages~\\\\citep{Hinton2015DistillingTK}. While the standard one-shot approach to distillation only uses the output of the final teacher network, recent work~\\\\citep{panigrahi2024progressive} has shown that using intermediate checkpoints from the teacher's training process as an implicit ``curriculum'' for progressive distillation can significantly speed up training. However, such schemes require storing these checkpoints, and often require careful selection of the intermediate checkpoints to train on, which can be impractical for large-scale training. In this paper, we show that a curriculum can be \\\\emph{extracted} from just the fully trained teacher network, and that this extracted curriculum can give similar efficiency benefits to those of progressive distillation. Our extraction scheme is natural; we use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. We show that our scheme significantly outperforms one-shot distillation and achieves a performance similar to that of\",\n",
       " 'Our extraction scheme is natural; we use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. We show that our scheme significantly outperforms one-shot distillation and achieves a performance similar to that of progressive distillation for learning sparse parities with two-layer networks, and provide theoretical guarantees for this setting. Additionally, we show that our method outperforms one-shot distillation even when using transformer-based architectures, both for sparse-parity learning, and language modeling tasks. We extend the celebrated Glivenko-Cantelli theorem, sometimes called the fundamental theorem of statistics, from its standard setting of total variation distance to all $f$-divergences. A key obstacle in this endeavor is to define $f$-divergence on a subcollection of a $\\\\sigma$-algebra that forms a $\\\\pi$-system but not a $\\\\sigma$-subalgebra. This is a side contribution of our work. We will show that this notion of $f$-divergence on the $\\\\pi$-system of rays preserves nearly all known properties of standard $f$-divergence, yields a novel integral representation of the Kolmogorov-Smirnov distance, and has a Glivenko-Cantelli theorem. We will also discuss the prospects of a Vapnik-Chervonenkis theory for $f$-divergence. We investigate differentially private estimators for individual parameters within larger parametric models. While generic private estimators exist, the estimators we provide repose on new local notions of estimand stability, and these notions allow procedures that provide private certificates of their own stability. By leveraging these private certificates, we provide computationally and statistical efficient mechanisms that release private statistics that are, at least asymptotically in the sample size, essentially unimprovable: they achieve instance optimal bounds. Additionally, we investigate the practicality of the algorithms both in simulated data and in real-world data from the American Community Survey and US Census, highlighting scenarios in which the new procedures are successful and identifying areas for future work. The Friedman test has been extensively applied as a nonparametric alternative to the conventional F procedure for comparing treatment effects in randomized complete block designs. A chi-square distribution provides a convenient approximation to determining the critical values for the Friedman procedure in hypothesis testing. However, the chi-square approximation is generally conservative and the accuracy declines with increasing number of treatments. This paper describes an alternative transformation of the Friedman statistic along with an approximate F distribution that has the same numerator degrees of freedom as the ANOVA F test. Moreover, two approximate noncentral F distributions are presented for the proposed F-transformation under the alternative hypothesis of heterogeneous location shifts. Explicit power functions are derived when the underlying populations have the uniform, normal, Laplace, and exponential distributions. Theoretical examination and empirical assessment are presented to validate the advantages of the proposed approaches over the existing methods of the Friedman test. The developed test and power procedures are recommended due to their consistently acceptable Type I error rates and accurate power calculations for the location shift structures and population distributions considered here. Nearly all identifiability results in unsupervised representation learning inspired by, e.g., independent component analysis, factor analysis,',\n",
       " \"the Friedman test. The developed test and power procedures are recommended due to their consistently acceptable Type I error rates and accurate power calculations for the location shift structures and population distributions considered here. Nearly all identifiability results in unsupervised representation learning inspired by, e.g., independent component analysis, factor analysis, and causal representation learning, rely on assumptions of additive independent noise or noiseless regimes. In contrast, we study the more general case where noise can take arbitrary forms, depend on latent variables, and be non-invertibly entangled within a nonlinear function. We propose a general framework for identifying latent variables in the nonparametric noisy settings. We first show that, under suitable conditions, the generative model is identifiable up to certain submanifold indeterminacies even in the presence of non-negligible noise. Furthermore, under the structural or distributional variability conditions, we prove that latent variables of the general nonlinear models are identifiable up to trivial indeterminacies. Based on the proposed theoretical framework, we have also developed corresponding estimation methods and validated them in various synthetic and real-world settings. Interestingly, our estimate of the true GDP growth from alternative measurements suggests more insightful information on the economies than official reports. We expect our framework to provide new insight into how both researchers and practitioners deal with latent variables in real-world scenarios. We consider price competition among multiple sellers over a selling horizon of $T$ periods. In each period, sellers simultaneously offer their prices and subsequently observe their respective demand that is unobservable to competitors. The demand function for each seller depends on all sellers' prices through a private, unknown, and nonlinear relationship. To address this challenge, we propose a semi-parametric least-squares estimation of the nonlinear mean function, which does not require sellers to communicate demand information. We show that when all sellers employ our policy, their prices converge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers would reach if they were fully informed. Each seller incurs a regret of $O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution of our work is proving the existence of equilibrium under shape-constrained demand functions via the concept of $s$-concavity and establishing regret bounds of our proposed policy. Technically, we also establish new concentration results for the least squares estimator under shape constraints. Our findings offer significant insights into dynamic competition-aware pricing and contribute to the broader study of non-parametric learning in strategic decision-making. We consider estimating the proportion of random variables for two types of composite null hypotheses: (i) the means or medians of the random variables belonging to a non-empty, bounded interval; (ii) the means or medians of the random variables belonging to an unbounded interval that is not the whole real line. For each type of composite null hypotheses, uniformly consistent estimators of the proportion of false null hypotheses are constructed for random variables whose distributions are members of a Type I location-shift family. Further, uniformly consistent estimators of certain functions of a bounded null on the means or medians are\",\n",
       " \"line. For each type of composite null hypotheses, uniformly consistent estimators of the proportion of false null hypotheses are constructed for random variables whose distributions are members of a Type I location-shift family. Further, uniformly consistent estimators of certain functions of a bounded null on the means or medians are provided for the random variables mentioned earlier; these functions are continuous and of bounded variation. The estimators are constructed via solutions to Lebesgue-Stieltjes integral equations and harmonic analysis, do not rely on a concept of p-value, and have various applications. A key trait of stochastic optimizers is that multiple runs of the same optimizer in attempting to solve the same problem can produce different results. As a result, their performance is evaluated over several repeats, or runs, on the problem. However, the accuracy of the estimated performance metrics depends on the number of runs and should be studied using statistical tools. We present a statistical analysis of the common metrics, and develop guidelines for experiment design to measure the optimizer's performance using these metrics to a high level of confidence and accuracy. To this end, we first discuss the confidence interval of the metrics and how they are related to the number of runs of an experiment. We then derive a lower bound on the number of repeats in order to guarantee achieving a given accuracy in the metrics. Using this bound, we propose an algorithm to adaptively adjust the number of repeats needed to ensure the accuracy of the evaluated metric. Our simulation results demonstrate the utility of our analysis and how it allows us to conduct reliable benchmarking as well as hyperparameter tuning and prevent us from drawing premature conclusions regarding the performance of stochastic optimizers. Estimating the state of a dynamical system from partial and noisy observations is a ubiquitous problem in a large number of applications, such as probabilistic weather forecasting and prediction of epidemics. Particle filters are a widely adopted approach to the problem and provide provably accurate approximations of the statistics of the state, but they perform poorly in high dimensions because of weight collapse. The ensemble Kalman filter does not suffer from this issue, as it relies on an interacting particle system with equal weights. Despite its wide adoption in the geophysical sciences, mathematical analysis of the accuracy of this filter is predominantly confined to the setting of linear dynamical models and linear observations operators, and analysis beyond the linear Gaussian setting is still in its infancy. In this short note, we provide an accessible overview of recent work in which the authors take first steps to analyze the accuracy of the filter beyond the linear Gaussian setting. By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to\",\n",
       " 'of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems). In this paper, we consider the reproducing property in Reproducing Kernel Hilbert Spaces (RKHS). We establish a reproducing property for the closure of the class of combinations of composition operators under minimal conditions. This allows to revisit the sufficient conditions for the reproducing property to hold for the derivative operator, as well as for the existence of the mean embedding function. These results provide a framework of application of the representer theorem for regularized learning algorithms that involve data for function values, gradients, or any other operator from the considered class. We consider a borderline case: the central limit theorem for a strictly stationary time series with infinite variance but a Gaussian limit. In the iid case a well-known sufficient condition for this central limit theorem is regular variation of the marginal distribution with tail index $\\\\alpha=2$. In the dependent case we assume the stronger condition of sequential regular variation of the time series with tail index $\\\\alpha=2$. We assume that a sample of size $n$ from this time series can be split into $k_n$ blocks of size $r_n\\\\to\\\\infty$ such that $r_n/n\\\\to 0$ as $n\\\\to\\\\infty$ and that the block sums are asymptotically independent. Then we apply classical central limit theory for row-wise iid triangular arrays. The necessary and sufficient conditions for such independent block sums will be verified by using large deviation results for the time series. We derive the central limit theorem for $m$-dependent sequences, linear processes, stochastic volatility processes and solutions to affine stochastic recurrence equations whose marginal distributions have infinite variance and are regularly varying with tail index $\\\\alpha=2$. Given an arbitrary subgraph $H=H_n$ and $p=p_n \\\\in (0,1)$, the planted subgraph model is defined as follows. A statistician observes the union a random copy $H^*$ of $H$, together with random noise in the form of an instance of an Erdos-Renyi graph $G(n,p)$. Their goal is to recover',\n",
       " 'index $\\\\alpha=2$. Given an arbitrary subgraph $H=H_n$ and $p=p_n \\\\in (0,1)$, the planted subgraph model is defined as follows. A statistician observes the union a random copy $H^*$ of $H$, together with random noise in the form of an instance of an Erdos-Renyi graph $G(n,p)$. Their goal is to recover the planted $H^*$ from the observed graph. Our focus in this work is to understand the minimum mean squared error (MMSE) for sufficiently large $n$. A recent paper [MNSSZ23] characterizes the graphs for which the limiting MMSE curve undergoes a sharp phase transition from $0$ to $1$ as $p$ increases, a behavior known as the all-or-nothing phenomenon, up to a mild density assumption on $H$. In this paper, we provide a formula for the limiting MMSE curve for any graph $H=H_n$, up to the same mild density assumption. This curve is expressed in terms of a variational formula over pairs of subgraphs of $H$, and is inspired by the celebrated subgraph expectation thresholds from the probabilistic combinatorics literature [KK07]. Furthermore, we give a polynomial-time description of the optimizers of this variational problem. This allows one to efficiently approximately compute the MMSE curve for any dense graph $H$ when $n$ is large enough. The proof relies on a novel graph decomposition of $H$ as well as a new minimax theorem which may be of independent interest. Our results generalize to the setting of minimax rates of recovering arbitrary monotone boolean properties planted in random noise, where the statistician observes the union of a planted minimal element $A \\\\subseteq [N]$ of a monotone property and a random $Ber(p)^{\\\\otimes N}$ vector. In this setting, we provide a variational formula inspired by the so-called \"fractional\" expectation threshold [Tal10], again describing the MMSE curve (in this case up to a multiplicative constant) for large enough $n$. Undirected graphical models are a widely used class of probabilistic models in machine learning that capture prior knowledge or putative pairwise interactions between variables. Those interactions are encoded in a graph for pairwise interactions; however, generalizations such as factor graphs account for higher-degree interactions using hypergraphs. Inference on such models, which is performed by conditioning on some observed variables, is typically done approximately by optimizing a free energy, which is an instance of variational inference. The Belief Propagation algorithm is a dynamic programming algorithm that finds critical points of that free energy. Recent efforts have been made to unify and extend inference on graphical models and factor graphs to more expressive probabilistic models. A synthesis of these works shows that inference on graphical models, factor graphs, and their generalizations relies on the introduction of presheaves and associated invariants (homology and cohomology groups).We propose to study the impact of the transformation of the presheaves onto the associated message passing algorithms. We show that natural transformations between presheaves associated with graphical models and their generalizations, which can be understood as coherent binning of the set of values of the variables, induce morphisms between associated message-passing algorithms. It is, to our knowledge,',\n",
       " \"transformation of the presheaves onto the associated message passing algorithms. We show that natural transformations between presheaves associated with graphical models and their generalizations, which can be understood as coherent binning of the set of values of the variables, induce morphisms between associated message-passing algorithms. It is, to our knowledge, the first result on functoriality of the Loopy Belief Propagation. Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications, the theoretical reasons behind it remain largely intuitive, with formal proofs currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. Interestingly, in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension. Nonlinear Bayesian update for a prior ensemble is proposed to extend traditional ensemble Kalman filtering to settings characterized by non-Gaussian priors and nonlinear measurement operators. In this framework, the observed component is first denoised via a standard Kalman update, while the unobserved component is estimated using a nonlinear regression approach based on kernel density estimation. The method incorporates a subsampling strategy to ensure stability and, when necessary, employs unsupervised clustering to refine the conditional estimate. Numerical experiments on Lorenz systems and a PDE-constrained inverse problem illustrate that the proposed nonlinear update can reduce estimation errors compared to standard linear updates, especially in highly nonlinear scenarios. This study introduces a dynamic investment framework to enhance portfolio management in volatile markets, offering clear advantages over traditional static strategies. Evaluates four conventional approaches : equal weighted, minimum variance, maximum diversification, and equal risk contribution under dynamic conditions. Using K means clustering, the market is segmented into ten volatility-based states, with transitions forecasted by a Bayesian Markov switching model employing Dirichlet priors and Gibbs sampling. This enables real-time asset allocation adjustments. Tested across two asset sets, the dynamic portfolio consistently achieves significantly higher risk-adjusted returns and substantially higher total returns, outperforming most static methods. By integrating classical optimization with machine learning and Bayesian techniques, this research provides a robust strategy for optimizing investment outcomes in unpredictable market environments. Although the specification of bivariate probability models using a collection of assumed conditional distributions is not a novel concept, it has received considerable attention in the last decade. In this study, a bivariate distribution-the bivariate Poisson-Gamma conditional distribution-is introduced, combining both univariate continuous and discrete distributions. This work explores aspects of this model's structure and statistical inference that have not been studied before. This paper contributes to the field of statistical modeling and distribution\",\n",
       " \"in the last decade. In this study, a bivariate distribution-the bivariate Poisson-Gamma conditional distribution-is introduced, combining both univariate continuous and discrete distributions. This work explores aspects of this model's structure and statistical inference that have not been studied before. This paper contributes to the field of statistical modeling and distribution theory through the use of maximum likelihood estimation, along with simulations and analyses of real data. The recent paper \\\\cite{GSZ2023} on estimation and inference for top-ranking problem in Bradley-Terry-Lice (BTL) model presented a surprising result: componentwise estimation and inference can be done under much weaker conditions on the number of comparison then it is required for the full dimensional estimation. The present paper revisits this finding from completely different viewpoint. Namely, we show how a theoretical study of estimation in sup-norm can be reduced to the analysis of plug-in semiparametric estimation. For the latter, we adopt and extend the general approach from \\\\cite{Sp2024} for high-dimensional estimation. The main tool of the analysis is a theory of perturbed marginal optimization when an objective function depends on a low-dimensional target parameter along with a high-dimensional nuisance parameter. A particular focus of the study is the critical dimension condition. Full-dimensional estimation requires in general the condition \\\\( \\\\mathbbmsl{N} \\\\gg \\\\mathbb{p} \\\\) between the effective parameter dimension \\\\( \\\\mathbb{p} \\\\) and the effective sample size \\\\( \\\\mathbbmsl{N} \\\\) corresponding to the smallest eigenvalue of the Fisher information matrix \\\\( \\\\mathbbmsl{F} \\\\). Inference on the estimated parameter is even more demanding: the condition \\\\( \\\\mathbbmsl{N} \\\\gg \\\\mathbb{p}^{2} \\\\) cannot be generally avoided; see \\\\cite{Sp2024}. However, for the sup-norm estimation, the critical dimension condition can be reduced to \\\\( \\\\mathbbmsl{N} \\\\geq \\\\CONST \\\\log(\\\\dimp) \\\\). Compared to \\\\cite{GSZ2023}, the proposed approach works for the classical MLE and does not require any resampling procedure, applies to more general structure of the comparison graph, and yields more accurate expansions for each component of the parameter vector. This paper investigates the asymptotic properties of least absolute deviation (LAD) regression for linear models with polynomial regressors, highlighting its robustness against heavy-tailed noise and outliers. Assuming independent and identically distributed (i.i.d.) errors, we establish the multiscale asymptotic normality of LAD estimators. A central result is the derivation of the asymptotic precision matrix, shown to be proportional to Hilbert matrices, with the proportionality coefficient depending on the asymptotic variance of the sample median of the noise distribution. We further explore the estimator's convergence properties, both in probability and almost surely, under varying model specifications. Through comprehensive simulations, we evaluate the speed of convergence of the LAD estimator and the empirical coverage probabilities of confidence intervals constructed under different scaling factors (T 1/2 and T $\\\\alpha$ ). These experiments incorporate a range of noise distributions, including Laplace, Gaussian, and Cauchy, to demonstrate the estimator's robustness and efficiency. The findings underscore the versatility and practical relevance of LAD regression in handling non-standard data environments. By connecting the statistical properties of LAD estimators to classical mathematical structures, such as Hilbert matrices, this study offers both theoretical\",\n",
       " 'distributions, including Laplace, Gaussian, and Cauchy, to demonstrate the estimator\\'s robustness and efficiency. The findings underscore the versatility and practical relevance of LAD regression in handling non-standard data environments. By connecting the statistical properties of LAD estimators to classical mathematical structures, such as Hilbert matrices, this study offers both theoretical insights and practical tools for robust statistical modeling. This work extends local linear regression to Banach space-valued time series for estimating smoothly varying means and their derivatives in non-stationary data. The asymptotic properties of both the standard and bias-reduced Jackknife estimators are analyzed under mild moment conditions, establishing their convergence rates. Simulation studies assess the finite sample performance of these estimators and compare them with the Nadaraya-Watson estimator. Additionally, the proposed methods are applied to smooth EEG recordings for reconstructing eye movements and to video analysis for detecting pedestrians and abandoned objects. We consider diffusion of independent molecules in an insulated Euclidean domain with unknown diffusivity parameter. At a random time and position, the molecules may bind and stop diffusing in dependence of a given `binding potential\\'. The binding process can be modeled by an additive random functional corresponding to the canonical construction of a `killed\\' diffusion Markov process. We study the problem of conducting inference on the infinite-dimensional diffusion parameter from a histogram plot of the `killing\\' positions of the process. We show first that these positions follow a Poisson point process whose intensity measure is determined by the solution of a certain Schr\\\\\"odinger equation. The inference problem can then be re-cast as a non-linear inverse problem for this PDE, which we show to be consistently solvable in a Bayesian way under natural conditions on the initial state of the diffusion, provided the binding potential is not too `aggressive\\'. In the course of our proofs we obtain novel posterior contraction rate results for high-dimensional Poisson count data that are of independent interest. A numerical illustration of the algorithm by standard MCMC methods is also provided. We view penalized risks through the lens of the calculus of variations. We consider risks comprised of a fitness-term (e.g. MSE) and a gradient-based penalty. After establishing the Euler-Lagrange field equations as a systematic approach to finding minimizers of risks involving only first derivatives, we proceed to exemplify this approach to the MSE penalized by the integral over the squared l2-norm of the gradient of the regression function. The minimizer of this risk is given as the solution to a second order inhomogeneous PDE, where the inhomogeneity is given as the conditional expectation of the target variable conditioned on the features. We discuss properties of the field equations and practical implications thereof, which also apply to the classical Ridge penalty for linear models, and embed our findings into the existing literature. In particular, we find that we can recover the Rudin-Osher-Fatemi model for image-denoising, if we consider the features as deterministic and evenly distributed. Last, we outline several directions for future research. In reliability theory and survival analysis, observed data are often weakly dependent',\n",
       " \"embed our findings into the existing literature. In particular, we find that we can recover the Rudin-Osher-Fatemi model for image-denoising, if we consider the features as deterministic and evenly distributed. Last, we outline several directions for future research. In reliability theory and survival analysis, observed data are often weakly dependent and subject to additive measurement errors. Such contamination arises when the underlying data are neither independent nor strongly mixed but instead exhibit association. This paper focuses on estimating the hazard rate by deconvolving the density function and constructing an estimator of the distribution function. We assume that the data originate from a strictly stationary sequence satisfying association conditions. Under appropriate smoothness assumptions on the error distribution, we establish the quadratic-mean convergence and asymptotic normality of the proposed estimators. The finite-sample performance of both the hazard rate and distribution function estimators is evaluated through a simulation study. We conclude with a discussion of open problems and potential future research directions. This paper introduces a novel test for conditional stochastic dominance (CSD) at specific values of the conditioning covariates, referred to as target points. The test is relevant for analyzing income inequality, evaluating treatment effects, and studying discrimination. We propose a Kolmogorov-Smirnov-type test statistic that utilizes induced order statistics from independent samples. Notably, the test features a data-independent critical value, eliminating the need for resampling techniques such as the bootstrap. Our approach avoids kernel smoothing and parametric assumptions, instead relying on a tuning parameter to select relevant observations. We establish the asymptotic properties of our test, showing that the induced order statistics converge to independent draws from the true conditional distributions and that the test controls asymptotic size under weak regularity conditions. While our results apply to both continuous and discrete data, in the discrete case, the critical value only provides a valid upper bound. To address this, we propose a refined critical value that significantly enhances power, requiring only knowledge of the support size of the distributions. Additionally, we analyze the test's behavior in the limit experiment, demonstrating that it reduces to a problem analogous to testing unconditional stochastic dominance in finite samples. This framework allows us to prove the validity of permutation-based tests for stochastic dominance when the random variables are continuous. Monte Carlo simulations confirm the strong finite-sample performance of our method. In statistical inference, confidence set procedures are typically evaluated based on their validity and width properties. Even when procedures achieve rate-optimal widths, confidence sets can still be excessively wide in practice due to elusive constants, leading to extreme conservativeness, where the empirical coverage probability of nominal $1-\\\\alpha$ level confidence sets approaches one. This manuscript studies this gap between validity and conservativeness, using universal inference (Wasserman et al., 2020) with a regular parametric model under model misspecification as a running example. We identify the source of asymptotic conservativeness and propose a general remedy based on studentization and bias correction. The resulting method attains exact asymptotic coverage at the nominal $1-\\\\alpha$ level, even under model misspecification, provided that the\",\n",
       " \"2020) with a regular parametric model under model misspecification as a running example. We identify the source of asymptotic conservativeness and propose a general remedy based on studentization and bias correction. The resulting method attains exact asymptotic coverage at the nominal $1-\\\\alpha$ level, even under model misspecification, provided that the product of the estimation errors of two unknowns is negligible, exhibiting an intriguing resemblance to double robustness in semiparametric theory. We revisit the classical broken sample problem: Two samples of i.i.d. data points $\\\\mathbf{X}=\\\\{X_1,\\\\cdots, X_n\\\\}$ and $\\\\mathbf{Y}=\\\\{Y_1,\\\\cdots,Y_m\\\\}$ are observed without correspondence with $m\\\\leq n$. Under the null hypothesis, $\\\\mathbf{X}$ and $\\\\mathbf{Y}$ are independent. Under the alternative hypothesis, $\\\\mathbf{Y}$ is correlated with a random subsample of $\\\\mathbf{X}$, in the sense that $(X_{\\\\pi(i)},Y_i)$'s are drawn independently from some bivariate distribution for some latent injection $\\\\pi:[m] \\\\to [n]$. Originally introduced by DeGroot, Feder, and Goel (1971) to model matching records in census data, this problem has recently gained renewed interest due to its applications in data de-anonymization, data integration, and target tracking. Despite extensive research over the past decades, determining the precise detection threshold has remained an open problem even for equal sample sizes ($m=n$). Assuming $m$ and $n$ grow proportionally, we show that the sharp threshold is given by a spectral and an $L_2$ condition of the likelihood ratio operator, resolving a conjecture of Bai and Hsing (2005) in the positive. These results are extended to high dimensions and settle the sharp detection thresholds for Gaussian and Bernoulli models. In this paper, we study the minimizers of U-processes and their domains of attraction. U-processes arise in various statistical contexts, particularly in M-estimation, where estimators are defined as minimizers of certain objective functions. Our main results establish necessary and sufficient conditions for the distributional convergence of these minimizers, identifying a broad class of normalizing sequences that go beyond the standard square-root asymptotics with normal limits. We show that the limit distribution belongs to exactly one of the four classes introduced by Smirnov. These results do not only extend Smirnov's theory but also generalize existing asymptotic theories for M-estimators, including classical results by Huber and extensions to higher-degree U-statistics. Furthermore, we analyze the domain of attraction for each class, providing alternative characterizations that determine which types of statistical estimators fall into a given asymptotic regime. The log-logistic distribution is a versatile parametric family widely used across various applied fields, including survival analysis, reliability engineering, and econometrics. When estimating parameters of the log-logistic distribution, hypothesis testing is necessary to verify assumptions about these parameters. The Wald test and Rao test provide formal methods for testing hypotheses about these parameters. However, these test statistics are not robust, and their rejection decisions may be affected by data contamination. In this paper we develop new families of Wald-type test statistics and Rao-type test statistics based on minimum density power divergence estimators (MDPDEs) for the parameters of the log-logistic distribution. These new families generalize the Wald and Rao test statistics, inheriting the robustness properties from the MDPDEs and thus\",\n",
       " \"contamination. In this paper we develop new families of Wald-type test statistics and Rao-type test statistics based on minimum density power divergence estimators (MDPDEs) for the parameters of the log-logistic distribution. These new families generalize the Wald and Rao test statistics, inheriting the robustness properties from the MDPDEs and thus addressing the lack of robustness of the classical tests. Explicit expressions for the test statistics under the log-logistic model for both simple and composite null hypotheses are derived, and their properties are analyzed in detail. An extensive simulation study empirically demonstrates the robustness of these families and compares their performance with the classical methods. Orthogonal-split trees perform well, but evidence suggests oblique splits can enhance their performance. This paper explores optimizing high-dimensional $s$-sparse oblique splits from $\\\\{(\\\\vec{w}, \\\\vec{w}^{\\\\top}\\\\boldsymbol{X}_{i}) : i\\\\in \\\\{1,\\\\dots, n\\\\}, \\\\vec{w} \\\\in \\\\mathbb{R}^p, \\\\| \\\\vec{w} \\\\|_{2} = 1, \\\\| \\\\vec{w} \\\\|_{0} \\\\leq s \\\\}$ for growing oblique trees, where $ s $ is a user-defined sparsity parameter. We establish a connection between SID convergence and $s_0$-sparse oblique splits with $s_0\\\\ge 1$, showing that the SID function class expands as $s_0$ increases, enabling the capture of more complex data-generating functions such as the $s_0$-dimensional XOR function. Thus, $s_0$ represents the unknown potential complexity of the underlying data-generating function. Learning these complex functions requires an $s$-sparse oblique tree with $s \\\\geq s_0$ and greater computational resources. This highlights a trade-off between statistical accuracy, governed by the SID function class size depending on $s_0$, and computational cost. In contrast, previous studies have explored the problem of SID convergence using orthogonal splits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we introduce a practical framework for oblique trees that integrates optimized oblique splits alongside orthogonal splits into random forests. The proposed approach is assessed through simulations and real-data experiments, comparing its performance against various oblique tree models. We present a new proof of the sub-Gaussian norm concentration inequality. Our proof is based on an averaged version of the moment generating function termed the averaged moment generating function. Compared with the widely adopted $\\\\varepsilon$-net technique-based proof of the sub-Gaussian norm concentration inequality, our method does not rely on the union bound and promises a tighter concentration bound. Turing's estimator allows one to estimate the probabilities of outcomes that either do not appear or only rarely appear in a given random sample. We perform a simulation study to understand the finite sample performance of several related confidence intervals (CIs) and introduce an approach for selecting the appropriate CI for a given sample. We give an application to the problem of authorship attribution and apply it to a dataset comprised of tweets from users on X (Twitter). Further, we derive several theoretical results about asymptotic normality and asymptotic Poissonity of Turing's estimator for two important discrete distributions. The aim of distributional regression is to find the best candidate in a given parametric family of conditional distributions to model a given dataset. As each candidate in the distribution family can\",\n",
       " \"derive several theoretical results about asymptotic normality and asymptotic Poissonity of Turing's estimator for two important discrete distributions. The aim of distributional regression is to find the best candidate in a given parametric family of conditional distributions to model a given dataset. As each candidate in the distribution family can be identified by the corresponding distribution parameters, a common approach for this task is using the maximum likelihood estimator (MLE) for the parameters. In this paper, we establish theoretical results for this estimator in case the response variable is subject to random right censoring. In particular, we provide proofs of almost sure consistency and asymptotic normality of the MLE under censoring. Further, the finite-sample behavior is exemplarily demonstrated in a simulation study. Previously [Journal of Causal Inference, 10, 90-105 (2022)], we computed the variance of two estimators of causal effects for a v-structure of binary variables. Here we show that a linear combination of these estimators has lower variance than either. Furthermore, we show that this holds also when the treatment variable is block randomised with a predefined number receiving treatment, with analogous results to when it is sampled randomly. The stratified linear permutation statistic arises in various statistics problems, including stratified and post-stratified survey sampling, stratified and post-stratified experiments, conditional permutation tests, etc. Although we can derive the Berry--Esseen bounds for the stratified linear permutation statistic based on existing bounds for the non-stratified statistics, those bounds are not sharp, and moreover, this strategy does not work in general settings with heterogeneous strata with varying sizes. We first use Stein's method to obtain a unified stratified permutational Berry--Esseen bound that can accommodate heterogeneous strata. We then apply the bound to various statistics problems, leading to stronger theoretical quantifications and thereby facilitating statistical inference in those problems. Quadratic discriminant analysis (QDA) is a widely used method for classification problems, particularly preferable over Linear Discriminant Analysis (LDA) for heterogeneous data. However, QDA loses its effectiveness in high-dimensional settings, where the data dimension and sample size tend to infinity. To address this issue, we propose a novel QDA method utilizing spectral correction and regularization techniques, termed SR-QDA. The regularization parameters in our method are selected by maximizing the Fisher-discriminant ratio. We compare SR-QDA with QDA, regularized quadratic discriminant analysis (R-QDA), and several other competitors. The results indicate that SR-QDA performs exceptionally well, especially in moderate and high-dimensional situations. Empirical experiments across diverse datasets further support this conclusion. Providing theoretical guarantees for parameter estimation in exponential random graph models is a largely open problem. While maximum likelihood estimation has theoretical guarantees in principle, verifying the assumptions for these guarantees to hold can be very difficult. Moreover, in complex networks, numerical maximum likelihood estimation is computer-intensive and may not converge in reasonable time. To ameliorate this issue, local dependency exponential random graph models have been introduced, which assume that the network consists of many independent exponential random graphs. In this setting, progress towards maximum likelihood estimation has been made. However the estimation is still computer-intensive.\",\n",
       " \"computer-intensive and may not converge in reasonable time. To ameliorate this issue, local dependency exponential random graph models have been introduced, which assume that the network consists of many independent exponential random graphs. In this setting, progress towards maximum likelihood estimation has been made. However the estimation is still computer-intensive. Instead, we propose to use so-called Stein estimators: we use the Stein characterizations to obtain new estimators for local dependency exponential random graph models. Quantifying the association between two random variables is crucial in applications. Traditional estimation techniques for common association measures, such as Spearman's rank correlation coefficient, $\\\\rho_S$, often fail when data contain ties. This is particularly problematic in zero-inflated contexts and fields like insurance, healthcare, and weather forecasting, where zeros are more frequent and require an extra probability mass. In this paper, we provide a new formulation of Spearman's rho specifically designed for zero-inflated data and propose a novel estimator of Spearman's rho based on our derived expression. Besides, we make our proposed estimator useful in practice by deriving its achievable bounds and suggest how to estimate them. We analyze our method in a comprehensive simulation study and show that our approach overcomes state-of-the-art methods in all the simulated scenarios. Additionally, we illustrate how the proposed theory can be used in practice for a more accurate quantification of association by considering two real-life applications. The evaluation of G-Wishart normalising constants is a core component for Bayesian analyses for Gaussian graphical models, but remains a computationally intensive task in general. Based on empirical evidence, Roverato [Scandinavian Journal of Statistics, 29:391--411 (2002)] observed and conjectured that such constants can be simplified and rewritten in terms of constants with an identity scale matrix. In this note, we disprove this conjecture for general graphs by showing that the conjecture instead implies an independently-derived approximation for certain ratios of normalising constants. We study the properties of a stochastic heat equation with a generalized mixed fractional Brownian noise. We obtain the covariance structure, stationarity and obtain bounds for the asymptotic behaviour of the solution. We suggest estimators for the unknown parameters based on discrete time observations and study their asymptotic properties. Suppose we observe a trajectory of length $n$ from an $\\\\alpha$-mixing stochastic process over a finite but potentially large state space. We consider the problem of estimating the probability mass placed by the stationary distribution of any such process on elements that occur with a certain frequency in the observed sequence. We estimate this vector of probabilities in total variation distance, showing universal consistency in $n$ and recovering known results for i.i.d. sequences as special cases. Our proposed methodology carefully combines the plug-in (or empirical) estimator with a recently-proposed modification of the Good--Turing estimator called WingIt, which was originally developed for Markovian sequences. En route to controlling the error of our estimator, we develop new performance bounds on WingIt and the plug-in estimator for $\\\\alpha$-mixing stochastic processes. Importantly, the extensively used method of Poissonization can no longer be applied in our non i.i.d.\",\n",
       " 'estimator called WingIt, which was originally developed for Markovian sequences. En route to controlling the error of our estimator, we develop new performance bounds on WingIt and the plug-in estimator for $\\\\alpha$-mixing stochastic processes. Importantly, the extensively used method of Poissonization can no longer be applied in our non i.i.d. setting, and so we develop complementary tools -- including concentration inequalities for a natural self-normalized statistic of mixing sequences -- that may prove independently useful in the design and analysis of estimators for related problems. We continue our work [arXiv:2403.07628] on asymptotic expansions at the soft edge for the classical $n$-dimensional Gaussian and Laguerre random matrix ensembles. By revisiting the construction of the associated skew-orthogonal polynomials in terms of wave functions, we obtain concise expressions for the level densities that are well suited for proving asymptotic expansions in powers of a certain parameter $h \\\\asymp n^{-2/3}$. In the unitary case, the expansion for the level density can be used to reconstruct the first correction term in an established asymptotic expansion of the associated generating function. In the orthogonal and symplectic cases, we can even reconstruct the conjectured first and second correction terms. Prediction model training is often hindered by limited access to individual-level data due to privacy concerns and logistical challenges, particularly in biomedical research. Resampling-based self-training presents a promising approach for building prediction models using only summary-level data. These methods leverage summary statistics to sample pseudo datasets for model training and parameter optimization, allowing for model development without individual-level data. Although increasingly used in precision medicine, the general behaviors of self-training remain unexplored. In this paper, we leverage a random matrix theory framework to establish the statistical properties of self-training algorithms for high-dimensional sparsity-free summary data. We demonstrate that, within a class of linear estimators, resampling-based self-training achieves the same asymptotic predictive accuracy as conventional training methods that require individual-level datasets. These results suggest that self-training with only summary data incurs no additional cost in prediction accuracy, while offering significant practical convenience. Our analysis provides several valuable insights and counterintuitive findings. For example, while pseudo-training and validation datasets are inherently dependent, their interdependence unexpectedly cancels out when calculating prediction accuracy measures, preventing overfitting in self-training algorithms. Furthermore, we extend our analysis to show that the self-training framework maintains this no-cost advantage when combining multiple methods or when jointly training on data from different distributions. We numerically validate our findings through simulations and real data analyses using the UK Biobank. Our study highlights the potential of resampling-based self-training to advance genetic risk prediction and other fields that make summary data publicly available. This paper proposes new ANOVA-based approximations of functions and emulators of high-dimensional models using either available derivatives or local stochastic evaluations of such models. Our approach makes use of sensitivity indices to design adequate structures of emulators. For high-dimensional models with available derivatives, our derivative-based emulators reach dimension-free mean squared errors (MSEs) and parametric rate of convergence (i.e., $\\\\mathsf{O}(N^{-1})$). This approach is extended to cope with every model',\n",
       " \"local stochastic evaluations of such models. Our approach makes use of sensitivity indices to design adequate structures of emulators. For high-dimensional models with available derivatives, our derivative-based emulators reach dimension-free mean squared errors (MSEs) and parametric rate of convergence (i.e., $\\\\mathsf{O}(N^{-1})$). This approach is extended to cope with every model (without available derivatives) by deriving global emulators that account for the local properties of models or simulators. Such generic emulators enjoy dimension-free biases, parametric rates of convergence and MSEs that depend on the dimensionality. Dimension-free MSEs are obtained for high-dimensional models with particular inputs' distributions. Our emulators are also competitive in dealing with different distributions of the input variables and for selecting inputs and interactions. Simulations show the efficiency of our approach. In an earlier work arXiv:2410.22038, it was shown that mixtures of multivariate Gaussian or $t$-distributions can be distinguished by projecting them onto a certain predetermined finite set of lines, the number of lines depending only on the total number of distributions involved and on the ambient dimension. Using this work, we address the following two important statistical problems: that of testing and measuring the agreement between two different random partitions, and that of estimating for mixtures of multivariate normal distributions and mixtures of $t$-distributions based of univariate projections. We also compare our proposal with robust versions of the expectation-maximization method EM. In each case, we present algorithms for effecting the task, and compare them with existing methods by carrying out some simulations. We analyze the landscape and training dynamics of diagonal linear networks in a linear regression task, with the network parameters being perturbed by small isotropic normal noise. The addition of such noise may be interpreted as a stochastic form of sharpness-aware minimization (SAM) and we prove several results that relate its action on the underlying landscape and training dynamics to the sharpness of the loss. In particular, the noise changes the expected gradient to force balancing of the weight matrices at a fast rate along the descent trajectory. In the diagonal linear model, we show that this equates to minimizing the average sharpness, as well as the trace of the Hessian matrix, among all possible factorizations of the same matrix. Further, the noise forces the gradient descent iterates towards a shrinkage-thresholding of the underlying true parameter, with the noise level explicitly regulating both the shrinkage factor and the threshold. Empirical Bayes estimators are based on minimizing the average risk with the hyper-parameters in the weighting function being estimated from observed data. The performance of an empirical Bayes estimator is typically evaluated by its mean squared error (MSE). However, the explicit expression for its MSE is generally unavailable for finite sample sizes. To address this issue, we define a high-order analytical criterion: the excess MSE. It quantifies the performance difference between the maximum likelihood and empirical Bayes estimators. An explicit expression for the excess MSE of an empirical Bayes estimator employing a general data-dependent hyper-parameter estimator is derived. As specific instances, we provide excess MSE expressions for\",\n",
       " 'define a high-order analytical criterion: the excess MSE. It quantifies the performance difference between the maximum likelihood and empirical Bayes estimators. An explicit expression for the excess MSE of an empirical Bayes estimator employing a general data-dependent hyper-parameter estimator is derived. As specific instances, we provide excess MSE expressions for kernel-based regularized estimators using the scaled empirical Bayes, Stein unbiased risk estimation, and generalized cross-validation hyper-parameter estimators. Moreover, we propose a modification to the excess MSE expressions for regularized estimators for moderate sample sizes and show its improvement on accuracy in numerical simulations. Since polynomial regression models are generally quite reliable for data with a linear trend, it is important to note that, in some cases, they may encounter overfitting issues during the training phase, which could result in negative values of the coefficient of determination $R^2$ for unseen data. For this reason, this work proposes the partial implementation of fractional operators in polynomial regression models to generate a fractional regression model. The goal of this proposal is to attempt to mitigate overfitting, which could improve the value of the coefficient of determination for unseen data, compared to the polynomial model, under the assumption that this would contribute to generating predictive models with better performance. The methodology for constructing these fractional regression models is detailed, and examples applicable to both Riemann-Liouville and Caputo fractional operators are presented. Piecewise-Deterministic Markov Processes (PDMPs) hold significant promise for sampling from complex probability distributions. However, their practical implementation is hindered by the need to compute model-specific bounds. Conversely, while Hamiltonian Monte Carlo (HMC) offers a generally efficient approach to sampling, its inability to adaptively tune step sizes impedes its performance when sampling complex distributions like funnels. To address these limitations, we introduce three innovative concepts: (a) a Metropolis-adjusted approximation for PDMP simulation that eliminates the need for explicit bounds without compromising the invariant measure, (b) an adaptive step size mechanism compatible with the Metropolis correction, and (c) a No U-Turn Sampler (NUTS)-inspired scheme for dynamically selecting path lengths in PDMPs. These three ideas can be seamlessly integrated into a single, `doubly-adaptive\\' PDMP sampler with favourable robustness and efficiency properties. While measures of concordance -- such as Spearman\\'s rho, Kendall\\'s tau, and Blomqvist\\'s beta -- are continuous with respect to weak convergence, Chatterjee\\'s rank correlation xi recently introduced in Azadkia and Chatterjee [5] does not share this property, causing drawbacks in statistical inference as pointed out in B\\\\\"ucher and Dette [7]. As we study in this paper, xi is instead weakly continuous with respect to conditionally independent copies -- the Markov products. To establish weak continuity of Markov products, we provide several sufficient conditions, including copula-based criteria and conditions relying on the concept of conditional weak convergence in Sweeting [36]. As a consequence, we also obtain continuity results for xi and related dependence measures and verify their continuity in the parameters of standard models such as multivariate elliptical and l1-norm symmetric distributions. We examine a generalisation of the beta distribution that we call the pushed',\n",
       " 'weak convergence in Sweeting [36]. As a consequence, we also obtain continuity results for xi and related dependence measures and verify their continuity in the parameters of standard models such as multivariate elliptical and l1-norm symmetric distributions. We examine a generalisation of the beta distribution that we call the pushed beta distribution. This is a continuous univariate distribution on the unit interval which generalises the beta distribution by \"pushing\" the density in a particular direction using an additional multiplicative term in the density kernel. We examine the properties of this distribution and compare it to the beta distribution. We also examine the use of this distribution in contaminated binary sampling using Bayesian inference. We find that this distribution arises as the appropriate posterior distribution for inference in certain kinds of contaminated binary models. We derive a broad range of properties of the distribution and we also establish some computational methods to compute various functions for the distribution. Aligning large language models (LLMs) with diverse human preferences is critical for ensuring fairness and informed outcomes when deploying these models for decision-making. In this paper, we seek to uncover fundamental statistical limits concerning aligning LLMs with human preferences, with a focus on the probabilistic representation of human preferences and the preservation of diverse preferences in aligned LLMs. We first show that human preferences can be represented by a reward model if and only if the preference among LLM-generated responses is free of any Condorcet cycle. Moreover, we prove that Condorcet cycles exist with probability converging to one exponentially fast under a probabilistic preference model, thereby demonstrating the impossibility of fully aligning human preferences using reward-based approaches such as reinforcement learning from human feedback. Next, we explore the conditions under which LLMs would employ mixed strategies -- meaning they do not collapse to a single response -- when aligned in the limit using a non-reward-based approach, such as Nash learning from human feedback (NLHF). We identify a necessary and sufficient condition for mixed strategies: the absence of a response that is preferred over all others by a majority. As a blessing, we prove that this condition holds with high probability under the probabilistic preference model, thereby highlighting the statistical possibility of preserving minority preferences without explicit regularization in aligning LLMs. Finally, we leverage insights from our statistical results to design a novel, computationally efficient algorithm for finding Nash equilibria in aligning LLMs with NLHF. Our experiments show that Llama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\\\\% against the base model. This paper considers the problem of design-based inference for the average treatment effect in finely stratified experiments. Here, by \"design-based\\'\\' we mean that the only source of uncertainty stems from the randomness in treatment assignment; by \"finely stratified\\'\\' we mean units are first stratified into groups of size k according to baseline covariates and then, within each group, a fixed number l < k are assigned uniformly at random to treatment and the remainder to control. In this setting, we',\n",
       " 'the randomness in treatment assignment; by \"finely stratified\\'\\' we mean units are first stratified into groups of size k according to baseline covariates and then, within each group, a fixed number l < k are assigned uniformly at random to treatment and the remainder to control. In this setting, we first show under mild conditions that inference using the difference-in-means estimator requires an estimator of its variance that is at least asymptotically upward-biased. We then present a novel estimator of the variance and show that it is upward-biased; furthermore, the magnitude of the bias depends in a natural way on the quality of the stratification. Importantly, this estimator remains well-defined even in the setting in which l = 1 or k - l = 1. We then compare our estimator with some well-known estimators that have been proposed previously for this case. We first show that, while these estimators are also upward-biased, the magnitude of their bias does not change in the natural way with the quality of stratification. To further discriminate among these estimators, we introduce a framework motivated by a thought experiment in which the finite population can be modeled as having been drawn once in an i.i.d. fashion from a well-behaved probability distribution. In this framework, we argue that our estimator dominates the others in terms of limiting bias, and that these improvements are strict except under exceptionally strong restrictions on the treatment effects. Finally, we illustrate our theoretical results through a simulation study, which reveals that our estimator can lead to substantially more precise inferences, especially when the quality of stratification is high. We study the minimax rate of estimation in nonparametric exponential family regression under star-shaped constraints. Specifically, the parameter space $K$ is a star-shaped set contained within a bounded box $[-M, M]^n$, where $M$ is a known positive constant. Moreover, we assume that the exponential family is nonsingular and that its cumulant function is twice continuously differentiable. Our main result shows that the minimax rate for this problem is $\\\\varepsilon^{*2} \\\\wedge \\\\operatorname{diam}(K)^2$, up to absolute constants, where $\\\\varepsilon^*$ is defined as \\\\[ \\\\varepsilon^* = \\\\sup \\\\{\\\\varepsilon: \\\\varepsilon^2 \\\\kappa(M) \\\\leq \\\\log N^{\\\\operatorname{loc}}(\\\\varepsilon)\\\\}, \\\\] with $N^{\\\\operatorname{loc}}(\\\\varepsilon)$ denoting the local entropy and $\\\\kappa(M)$ is an absolute constant allowed to depend on $M$. We also provide an example and derive its corresponding minimax optimal rate. We prove an upper bound on the expected $\\\\ell_p$ injective norm of sums of subgaussian random tensors. Our proof is simple and does not rely on any explicit geometric or chaining arguments. Instead, it follows from a simple application of the PAC-Bayesian lemma, a tool that has proven effective at controlling the suprema of certain ``smooth\\'\\' empirical processes in recent years. Our bound strictly improves a very recent result of Bandeira, Gopi, Jiang, Lucca, and Rothvoss. In the Euclidean case ($p=2$), our bound sharpens a result of Lata{\\\\l}a that was central to proving his estimates on the moments of Gaussian chaoses. As a consequence, we obtain an elementary proof of this fundamental result.',\n",
       " 'strictly improves a very recent result of Bandeira, Gopi, Jiang, Lucca, and Rothvoss. In the Euclidean case ($p=2$), our bound sharpens a result of Lata{\\\\l}a that was central to proving his estimates on the moments of Gaussian chaoses. As a consequence, we obtain an elementary proof of this fundamental result. Longitudinal networks are becoming increasingly relevant in the study of dynamic processes characterised by known or inferred community structure. Generalised Network Autoregressive (GNAR) models provide a parsimonious framework for exploiting the underlying network and multivariate time series. We introduce the community-$\\\\alpha$ GNAR model with interactions that exploits prior knowledge or exogenous variables for analysing interactions within and between communities, and can describe serial correlation in longitudinal networks. We derive new explicit finite-sample error bounds that validate analysing high-dimensional longitudinal network data with GNAR models, and provide insights into their attractive properties. We further illustrate our approach by analysing the dynamics of $\\\\textit{Red, Blue}$ and $\\\\textit{Swing}$ states throughout presidential elections in the USA from 1976 to 2020, that is, a time series of length twelve on 51 time series (US states and Washington DC). Our analysis connects network autocorrelation to eight-year long terms, highlights a possible change in the system after the 2016 election, and a difference in behaviour between $\\\\textit{Red}$ and $\\\\textit{Blue}$ states. We study the task of list-decodable linear regression using batches. A batch is called clean if it consists of i.i.d. samples from an unknown linear regression distribution. For a parameter $\\\\alpha \\\\in (0, 1/2)$, an unknown $\\\\alpha$-fraction of the batches are clean and no assumptions are made on the remaining ones. The goal is to output a small list of vectors at least one of which is close to the true regressor vector in $\\\\ell_2$-norm. [DJKS23] gave an efficient algorithm, under natural distributional assumptions, with the following guarantee. Assuming that the batch size $n$ satisfies $n \\\\geq \\\\tilde{\\\\Omega}(\\\\alpha^{-1})$ and the number of batches is $m = \\\\mathrm{poly}(d, n, 1/\\\\alpha)$, their algorithm runs in polynomial time and outputs a list of $O(1/\\\\alpha^2)$ vectors at least one of which is $\\\\tilde{O}(\\\\alpha^{-1/2}/\\\\sqrt{n})$ close to the target regressor. Here we design a new polynomial time algorithm with significantly stronger guarantees under the assumption that the low-degree moments of the covariates distribution are Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant $\\\\delta>0$, as long as the batch size is $n \\\\geq \\\\Omega_{\\\\delta}(\\\\alpha^{-\\\\delta})$ and the degree-$\\\\Theta(1/\\\\delta)$ moments of the covariates are SoS certifiably bounded, our algorithm uses $m = \\\\mathrm{poly}((dn)^{1/\\\\delta}, 1/\\\\alpha)$ batches, runs in polynomial-time, and outputs an $O(1/\\\\alpha)$-sized list of vectors one of which is $O(\\\\alpha^{-\\\\delta/2}/\\\\sqrt{n})$ close to the target. That is, our algorithm achieves substantially smaller minimum batch size and final error, while achieving the optimal list size. Our approach uses higher-order moment information by carefully combining the SoS paradigm interleaved with an iterative method and a novel list pruning procedure. In the process, we give an SoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader applicability. Score-based diffusion models have become a foundational paradigm for modern generative modeling,',\n",
       " 'higher-order moment information by carefully combining the SoS paradigm interleaved with an iterative method and a novel list pruning procedure. In the process, we give an SoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader applicability. Score-based diffusion models have become a foundational paradigm for modern generative modeling, demonstrating exceptional capability in generating samples from complex high-dimensional distributions. Despite the dominant adoption of probability flow ODE-based samplers in practice due to their superior sampling efficiency and precision, rigorous statistical guarantees for these methods have remained elusive in the literature. This work develops the first end-to-end theoretical framework for deterministic ODE-based samplers that establishes near-minimax optimal guarantees under mild assumptions on target data distributions. Specifically, focusing on subgaussian distributions with $\\\\beta$-H\\\\\"older smooth densities for $\\\\beta\\\\leq 2$, we propose a smooth regularized score estimator that simultaneously controls both the $L^2$ score error and the associated mean Jacobian error. Leveraging this estimator within a refined convergence analysis of the ODE-based sampling process, we demonstrate that the resulting sampler achieves the minimax rate in total variation distance, modulo logarithmic factors. Notably, our theory comprehensively accounts for all sources of error in the sampling process and does not require strong structural conditions such as density lower bounds or Lipschitz/smooth scores on target distributions, thereby covering a broad range of practical data distributions. For one dimensional stochastic Burgers equation driven by space-time white noise we consider the problem of estimation of the diffusivity parameter in front of the second-order spatial derivative. Based on local observations in space, we study the estimator derived in [Altmeyer, Rei{\\\\ss}, Ann. Appl. Probab.(2021)] for linear stochastic heat equation that has also been used in [Altmeyer, Cialenco, Pasemann, Bernoulli (2023)] to cover large class of semilinear SPDEs and has been examined for the stochastic Burgers equation driven by trace class noise. We extend the achieved results by considering the space-time white noise case which has also relevant physical motivations. After we establish new regularity results for the solution, we are able to show that our proposed estimator is strongly consistent and asymptotically normal. The failure of a system can result from the simultaneous effects of multiple causes, where assigning a specific cause may be inappropriate or unavailable. Examples include contributing causes of death in epidemiology and the aetiology of neurodegenerative diseases like Alzheimer\\'s. We propose a parametric Weibull accelerated failure time model for multiple causes, incorporating a data-driven, individualized, and time-varying winning probability (relative importance) matrix. Using maximum likelihood estimation and the expectation-maximization (EM) algorithm, our approach enables simultaneous estimation of regression coefficients and relative cause importance, ensuring consistency and asymptotic normality. A simulation study and an application to Alzheimer\\'s disease demonstrate its effectiveness in addressing cause-mixture problems and identifying informative biomarker combinations, with comparisons to Weibull and Cox proportional hazards models. This paper tackles the challenge of estimating a low-rank graphon from sampled network data, employing a singular value thresholding (SVT) estimator to create a piecewise-constant graphon based on the network\\'s adjacency matrix. Under certain assumptions about the graphon\\'s',\n",
       " 'informative biomarker combinations, with comparisons to Weibull and Cox proportional hazards models. This paper tackles the challenge of estimating a low-rank graphon from sampled network data, employing a singular value thresholding (SVT) estimator to create a piecewise-constant graphon based on the network\\'s adjacency matrix. Under certain assumptions about the graphon\\'s structural properties, we establish bounds on the operator norm distance between the true graphon and its estimator, as well as on the rank of the estimated graphon. In the second part of the paper, we apply our estimator to graphon games. We derive bounds on the suboptimality of interventions in the social welfare problem in graphon games when the intervention is based on the estimated graphon. These bounds are expressed in terms of the operator norm of the difference between the true and estimated graphons. We also emphasize the computational benefits of using the low-rank estimated graphon to solve these problems. Unbiased data synthesis is crucial for evaluating causal discovery algorithms in the presence of unobserved confounding, given the scarcity of real-world datasets. A common approach, implicit parameterization, encodes unobserved confounding by modifying the off-diagonal entries of the idiosyncratic covariance matrix while preserving positive definiteness. Within this approach, we identify that state-of-the-art protocols have two distinct issues that hinder unbiased sampling from the complete space of causal models: first, we give a detailed analysis of use of diagonally dominant constructions restricts the spectrum of partial correlation matrices; and second, the restriction of possible graphical structures when sampling bidirected edges, unnecessarily ruling out valid causal models. To address these limitations, we propose an improved explicit modeling approach for unobserved confounding, leveraging block-hierarchical ancestral generation of ground truth causal graphs. Algorithms for converting the ground truth DAG into ancestral graph is provided so that the output of causal discovery algorithms could be compared with. We draw connections between implicit and explicit parameterization, prove that our approach fully covers the space of causal models, including those generated by the implicit parameterization, thus enabling more robust evaluation of methods for causal discovery and inference. This paper studies the problem of inferring a $k$-factor, specifically a spanning $k$-regular graph, planted within an Erdos-Renyi random graph $G(n,\\\\lambda/n)$. We uncover an interesting \"all-something-nothing\" phase transition. Specifically, we show that as the average degree $\\\\lambda$ surpasses the critical threshold of $1/k$, the inference problem undergoes a transition from almost exact recovery (\"all\" phase) to partial recovery (\"something\" phase). Moreover, as $\\\\lambda$ tends to infinity, the accuracy of recovery diminishes to zero, leading to the onset of the \"nothing\" phase. This finding complements the recent result by Mossel, Niles-Weed, Sohn, Sun, and Zadik who established that for certain sufficiently dense graphs, the problem undergoes an \"all-or-nothing\" phase transition, jumping from near-perfect to near-zero recovery. In addition, we characterize the recovery accuracy of a linear-time iterative pruning algorithm and show that it achieves almost exact recovery when $\\\\lambda < 1/k$. A key component of our analysis is a two-step cycle construction: we first build trees through local neighborhood exploration and',\n",
       " 'from near-perfect to near-zero recovery. In addition, we characterize the recovery accuracy of a linear-time iterative pruning algorithm and show that it achieves almost exact recovery when $\\\\lambda < 1/k$. A key component of our analysis is a two-step cycle construction: we first build trees through local neighborhood exploration and then connect them by sprinkling using reserved edges. Interestingly, for proving impossibility of almost exact recovery, we construct $\\\\Theta(n)$ many small trees of size $\\\\Theta(1)$, whereas for establishing the algorithmic lower bound, a single large tree of size $\\\\Theta(\\\\sqrt{n\\\\log n})$ suffices. We consider two random variables $X$ and $Y$ following correlated Gamma distributions, characterized by identical scale and shape parameters and a linear correlation coefficient $\\\\rho$. Our focus is on the parameter: \\\\[ D(X,Y) = \\\\frac{|X - Y|}{X + Y}, \\\\] which appears in applied contexts such as dynamic speckle imaging, where it is known as the \\\\textit{Fujii index}. In this work, we derive a closed-form expression for the probability density function of $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our derivation starts by representing $X$ and $Y$ as two correlated exponential random variables, obtained from the squared magnitudes of circular complex Gaussian variables. By considering the sum of $k$ independent exponential variables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two correlated Gamma variables. Through appropriate varable transformations, we obtain the theoretical distribution of $D(X,Y)$ and evaluate its moments analytically. These theoretical findings are validated through numerical simulations, with particular attention to two specific cases: zero correlation and unit shape parameter. Early work established convergence of the principal component estimators of the factors and loadings up to a rotation for large dimensional approximate factor models with weak factors in that the factor loading $\\\\Lambda^{(0)}$ scales sublinearly in the number $N$ of cross-section units, i.e., $\\\\Lambda^{(0)\\\\top}\\\\Lambda^{(0)}/N^{\\\\alpha}$ is positive definite in the limit for some $\\\\alpha\\\\in (0,1)$. However, the established convergence rates for weak factors can be much slower especially for small $\\\\alpha$. This article proposes a Transfer Principal Component Analysis (TransPCA) method for enhancing the convergence rates for weak factors by transferring knowledge from large number of available informative panel datasets, which should not be turned a blind eye on in this big data era. We aggregate useful information by analyzing a weighted average projection matrix of the estimated loading spaces from all informative datasets which is highly flexible and computationally efficient. Theoretically, we derive the convergence rates of the estimators of weak/strong loading spaces and factor scores. The results indicate that as long as the auxiliary datasets are similar enough to the target dataset and the auxiliary sample size is sufficiently large, TransPCA estimators can achieve faster convergence rates in contrast to performing PCA solely on the target dataset. To avoid negative transfer, we also investigate the case that the informative datasets are unknown and provide a criterion for selecting useful datasets. Thorough simulation studies and {empirical analysis on real datasets in areas of macroeconomic and finance} are conducted',\n",
       " \"in contrast to performing PCA solely on the target dataset. To avoid negative transfer, we also investigate the case that the informative datasets are unknown and provide a criterion for selecting useful datasets. Thorough simulation studies and {empirical analysis on real datasets in areas of macroeconomic and finance} are conducted to illustrate the usefulness of our proposed methods where large number of source panel datasets are naturally available. This work addresses the problem of estimating a vector field from a noisy Ordinary Differential Equation (ODE) in a non-parametric regression setting with a random design for initial values. More specifically, given a vector field $ f:\\\\mathbb{R}^{D}\\\\rightarrow \\\\mathbb{R}^{D}$ governing a dynamical system defined by the autonomous ODE: $y' = f(y)$, we assume that the observations are $\\\\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) + \\\\varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time $t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a probability distribution $\\\\mu$, and $\\\\varepsilon_{i,j}$ some noise. In this context, we investigate, from a minimax perspective, the pointwise reconstruction of $f$ within the envelope of trajectories originating from the support of $\\\\mu$. We propose an estimation strategy based on preliminary flow reconstruction and techniques from derivative estimation in non-parametric regression. Under mild assumptions on $f$, we establish convergence rates that depend on the temporal resolution, the number of sampled initial values and the mass concentration of $\\\\mu$. Importantly, we show that these rates are minimax optimal. Furthermore, we discuss the implications of our results in a manifold learning setting, providing insights into how our approach can mitigate the curse of dimensionality. This paper examines the evolution of the Finnish electric energy system up to 2035, focusing on the likelihood of different development paths. The primary contribution of this paper is the development of an extensive Bayesian Network, designed to model and analyse the evolution of power generation capacity mix, assess the likelihood of different grid management scenarios, and understand the causal relationships underlying these scenarios. A target optimisation was carried out using the constructed Bayesian Network to explore possibilities to minimise grid management complexity. The results of the optimisation reveal that the authorities and stakeholders should prioritise increasing demand response, gas power, and battery storage capacities. These mature technologies are well-suited to guarantee energy adequacy during peak consumption periods, which in Finland typically occur during consecutive cold, dark and windless winter weeks. Although this study focuses on the evolution of the Finnish power grid, the constructed Bayesian Network approach is broadly applicable and can be utilised to explore causal relationships in other countries by employing the designed questionnaire and engaging a panel of experts specific to the country's energy infrastructure. Graph sparsification is a well-established technique for accelerating graph-based learning algorithms, which uses edge sampling to approximate dense graphs with sparse ones. Because the sparsification error is random and unknown, users must contend with uncertainty about the reliability of downstream computations. Although it is possible for users to obtain conceptual guidance from theoretical error bounds in the literature, such\",\n",
       " 'learning algorithms, which uses edge sampling to approximate dense graphs with sparse ones. Because the sparsification error is random and unknown, users must contend with uncertainty about the reliability of downstream computations. Although it is possible for users to obtain conceptual guidance from theoretical error bounds in the literature, such results are typically impractical at a numerical level. Taking an alternative approach, we propose to address these issues from a data-driven perspective by computing empirical error estimates. The proposed error estimates are highly versatile, and we demonstrate this in four use cases: Laplacian matrix approximation, graph cut queries, graph-structured regression, and spectral clustering. Moreover, we provide two theoretical guarantees for the error estimates, and explain why the cost of computing them is manageable in comparison to the overall cost of a typical graph sparsification workflow. COVID-19 has had a large scale negative impact on the health of opioid users exacerbating the health of an already vulnerable population. Critical information on the total impact of COVID-19 on opioid users is unknown due to a lack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and lack of data coverage. To assess the impact of COVID-19 on small-area opioid mortality, we developed a Bayesian hierarchical excess opioid mortality modeling approach. We incorporate spatio-temporal autocorrelation structures to allow for sharing of information across small areas and time to reduce uncertainty in small area estimates. Excess mortality is defined as the difference between observed trends after a crisis and expected trends based on observed historical trends, which captures the total increase in observed mortality rates compared to what was expected prior to the crisis. We illustrate the application of our approach to assess excess opioid mortality risk estimates for 159 counties in GA. Using our proposed approach will help inform interventions in opioid-related public health responses, policies, and resource allocation. The application of this work also provides a general framework for improving the estimation and mapping of health indicators during crisis periods for the opioid user population. It is a folklore belief that metastable wells in low-temperature statistical mechanics models exhibit high-temperature behavior. We prove a rigorous version of this phenomenon in the setting of the exponential random graph model (ERGM) through the lens of concentration of measure. To do this, we first present a new general result deriving concentration inequalities in a metastable well from the metastable mixing of a Markov chain with the appropriate stationary distribution, extending a result of Chatterjee [Cha05] which is suited for more traditional forms of global mixing. We then apply this result to the supercritical (low-temperature) ERGM which was recently proven to exhibit metastable mixing by Bresler, Nagaraj, and Nichani [BNN24], and obtain a novel concentration inequality for Lipschitz observables of the supercritical ERGM conditioned on a large metastable well, answering a question posed by [BNN24]. This extends a result of Ganguly and Nam [GN24] from the subcritical (high-temperature) regime to a metastable well in the supercritical regime, and we are also able to extend the applications',\n",
       " \"for Lipschitz observables of the supercritical ERGM conditioned on a large metastable well, answering a question posed by [BNN24]. This extends a result of Ganguly and Nam [GN24] from the subcritical (high-temperature) regime to a metastable well in the supercritical regime, and we are also able to extend the applications of their concentration inequality to these metastable wells. Namely, we obtain an upper bound on the Wasserstein distance between the ERGM conditioned on a metastable well and an appropriate Erd\\\\H{o}s-R\\\\'enyi model, as well as derive a central limit theorem for the count of edges in certain small subcollections of possible edges. Finally, to supplement the mathematical content of the article, we also discuss the results of what appears to be the first simulation study of a metastable well in the supercritical ERGM. Language model alignment (or, reinforcement learning) techniques that leverage active exploration -- deliberately encouraging the model to produce diverse, informative responses -- offer the promise of super-human capabilities. However, current understanding of algorithm design primitives for computationally efficient exploration with language models is limited. To better understand how to leverage access to powerful pre-trained generative models to improve the efficiency of exploration, we introduce a new computational framework for RL with language models, in which the learner interacts with the model through a sampling oracle. Focusing on the linear softmax model parameterization, we provide new results that reveal the computational-statistical tradeoffs of efficient exploration: 1. Necessity of coverage: Coverage refers to the extent to which the pre-trained model covers near-optimal responses -- a form of hidden knowledge. We show that coverage, while not necessary for data efficiency, lower bounds the runtime of any algorithm in our framework. 2. Inference-time exploration: We introduce a new algorithm, SpannerSampling, which obtains optimal data efficiency and is computationally efficient whenever the pre-trained model enjoys sufficient coverage, matching our lower bound. SpannerSampling leverages inference-time computation with the pre-trained model to reduce the effective search space for exploration. 3. Insufficiency of training-time interventions: We contrast the result above by showing that training-time interventions that produce proper policies cannot achieve similar guarantees in polynomial time. 4. Computational benefits of multi-turn exploration: Finally, we show that under additional representational assumptions, one can achieve improved runtime (replacing sequence-level coverage with token-level coverage) through multi-turn exploration. Human physiological signals tend to exhibit both global and local structures: the former are shared across a population, while the latter reflect inter-individual variability. For instance, kinetic measurements of the gait cycle during locomotion present common characteristics, although idiosyncrasies may be observed due to biomechanical disposition or pathology. To better represent datasets with local-global structure, this work extends Convolutional Dictionary Learning (CDL), a popular method for learning interpretable representations, or dictionaries, of time-series data. In particular, we propose Personalized CDL (PerCDL), in which a local dictionary models local information as a personalized spatiotemporal transformation of a global dictionary. The transformation is learnable and can combine operations such as time warping and rotation. Formal computational and statistical guarantees for PerCDL are provided and\",\n",
       " 'time-series data. In particular, we propose Personalized CDL (PerCDL), in which a local dictionary models local information as a personalized spatiotemporal transformation of a global dictionary. The transformation is learnable and can combine operations such as time warping and rotation. Formal computational and statistical guarantees for PerCDL are provided and its effectiveness on synthetic and real human locomotion data is demonstrated. A common observation in data-driven applications is that high-dimensional data have a low intrinsic dimension, at least locally. In this work, we consider the problem of point estimation for manifold-valued data. Namely, given a finite set of noisy samples of $\\\\mathcal{M}$, a $d$ dimensional submanifold of $\\\\mathbb{R}^D$, and a point $r$ near the manifold we aim to project $r$ onto the manifold. Assuming that the data was sampled uniformly from a tubular neighborhood of a $k$-times smooth boundaryless and compact manifold, we present an algorithm that takes $r$ from this neighborhood and outputs $\\\\hat p_n\\\\in \\\\mathbb{R}^D$, and $\\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ an element in the Grassmannian $Gr(d, D)$. We prove that as the number of samples $n\\\\to\\\\infty$, the point $\\\\hat p_n$ converges to $\\\\mathbf{p}\\\\in \\\\mathcal{M}$, the projection of $r$ onto $\\\\mathcal{M}$, and $\\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ converges to $T_{\\\\mathbf{p}}\\\\mathcal{M}$ (the tangent space at that point) with high probability. Furthermore, we show that $\\\\hat p_n$ approaches the manifold with an asymptotic rate of $n^{-\\\\frac{k}{2k + d}}$, and that $\\\\hat p_n, \\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ approach $\\\\mathbf{p}$ and $T_{\\\\mathbf{p}}\\\\mathcal{M}$ correspondingly with asymptotic rates of $n^{-\\\\frac{k-1}{2k + d}}$. We construct a family of estimators for a regression function based on a sample following a qdistribution. Our approach is nonparametric, using kernel methods built from operations that leverage the properties of q-calculus. Furthermore, under appropriate assumptions, we establish the weak convergence and strong consistency of this family of estimators. For some discretely observed path of oscillating Brownian motion with level of self-organized criticality $\\\\rho_0$, we prove in the infill asymptotics that the MLE is $n$-consistent, where $n$ denotes the sample size, and derive its limit distribution with respect to stable convergence. As the transition density of this homogeneous Markov process is not even continuous in $\\\\rho_0$, the analysis is highly non-standard. Therefore, interesting and somewhat unexpected phenomena occur: The likelihood function splits into several components, each of them contributing very differently depending on how close the argument $\\\\rho$ is to $\\\\rho_0$. Correspondingly, the MLE is successively excluded to lay outside a compact set, a $1/\\\\sqrt{n}$-neighborhood and finally a $1/n$-neigborhood of $\\\\rho_0$ asymptotically. The crucial argument to derive the stable convergence is to exploit the semimartingale structure of the sequential suitably rescaled local log-likelihood function (as a process in time). Both sequentially and as a process in $\\\\rho$, it exhibits a bivariate Poissonian behavior in the stable limit with its intensity being a multiple of the local time at $\\\\rho_0$. Community detection, which focuses on recovering the group structure within networks, is a crucial and fundamental task in network analysis. However, the detection process can be quite challenging and unstable when community signals are weak. Motivated by a newly collected',\n",
       " 'intensity being a multiple of the local time at $\\\\rho_0$. Community detection, which focuses on recovering the group structure within networks, is a crucial and fundamental task in network analysis. However, the detection process can be quite challenging and unstable when community signals are weak. Motivated by a newly collected large-scale academic network dataset from the Web of Science, which includes multi-layer network information, we propose a Bipartite Assisted Spectral-clustering approach for Identifying Communities (BASIC), which incorporates the bipartite network information into the community structure learning of the primary network. The accuracy and stability enhancement of BASIC is validated theoretically on the basis of the degree-corrected stochastic block model framework, as well as numerically through extensive simulation studies. We rigorously study the convergence rate of BASIC even under weak signal scenarios and prove that BASIC yields a tighter upper error bound than that based on the primary network information alone. We utilize the proposed BASIC method to analyze the newly collected large-scale academic network dataset from statistical papers. During the author collaboration network structure learning, we incorporate the bipartite network information from author-paper, author-institution, and author-region relationships. From both statistical and interpretative perspectives, these bipartite networks greatly aid in identifying communities within the primary collaboration network. We present two limit theorems, a mean ergodic and a central limit theorem, for a specific class of one-dimensional diffusion processes that depend on a small-scale parameter $\\\\varepsilon$ and converge weakly to a homogenized diffusion process in the limit $\\\\varepsilon \\\\rightarrow 0$. In these results, we allow for the time horizon to blow up such that $T_\\\\varepsilon \\\\rightarrow \\\\infty$ as $\\\\varepsilon \\\\rightarrow 0$. The novelty of the results arises from the circumstance that many quantities are unbounded for $\\\\varepsilon \\\\rightarrow 0$, so that formerly established theory is not directly applicable here and a careful investigation of all relevant $\\\\varepsilon$-dependent terms is required. As a mathematical application, we then use these limit theorems to prove asymptotic properties of a minimum distance estimator for parameters in a homogenized diffusion equation. The behavior of extreme observations is well-understood for time series or spatial data, but little is known if the data generating process is a structural causal model (SCM). We study the behavior of extremes in this model class, both for the observational distribution and under extremal interventions. We show that under suitable regularity conditions on the structure functions, the extremal behavior is described by a multivariate Pareto distribution, which can be represented as a new SCM on an extremal graph. Importantly, the latter is a sub-graph of the graph in the original SCM, which means that causal links can disappear in the tails. We further introduce a directed version of extremal graphical models and show that an extremal SCM satisfies the corresponding Markov properties. Based on a new test of extremal conditional independence, we propose two algorithms for learning the extremal causal structure from data. The first is an extremal version of the PC-algorithm, and the second is a pruning algorithm that removes edges from the',\n",
       " \"an extremal SCM satisfies the corresponding Markov properties. Based on a new test of extremal conditional independence, we propose two algorithms for learning the extremal causal structure from data. The first is an extremal version of the PC-algorithm, and the second is a pruning algorithm that removes edges from the original graph to consistently recover the extremal graph. The methods are illustrated on river data with known causal ground truth. Consider a pair of sparse correlated stochastic block models $\\\\mathcal S(n,\\\\tfrac{\\\\lambda}{n},\\\\epsilon;s)$ subsampled from a common parent stochastic block model with two symmetric communities, average degree $\\\\lambda=O(1)$ and divergence parameter $\\\\epsilon \\\\in (0,1)$. For all $\\\\epsilon\\\\in(0,1)$, we construct a statistic based on the combination of two low-degree polynomials and show that there exists a sufficiently small constant $\\\\delta=\\\\delta(\\\\epsilon)>0$ and a sufficiently large constant $\\\\Delta=\\\\Delta(\\\\epsilon,\\\\delta)$ such that when $\\\\lambda>\\\\Delta$ and $s>\\\\sqrt{\\\\alpha}-\\\\delta$ where $\\\\alpha\\\\approx 0.338$ is Otter's constant, this statistic can distinguish this model and a pair of independent stochastic block models $\\\\mathcal S(n,\\\\tfrac{\\\\lambda s}{n},\\\\epsilon)$ with probability $1-o(1)$. We also provide an efficient algorithm that approximates this statistic in polynomial time. The crux of our statistic's construction lies in a carefully curated family of multigraphs called \\\\emph{decorated trees}, which enables effective aggregation of the community signal and graph correlation from the counts of the same decorated tree while suppressing the undesirable correlations among counts of different decorated trees. This paper is devoted to parameter estimation for partially observed polynomial state space models. This class includes discretely observed affine or more generally polynomial Markov processes. The polynomial structure allows for the explicit computation of a Gaussian quasi-likelihood estimator and its asymptotic covariance matrix. We show consistency and asymptotic normality of the estimating sequence and provide explicitly computable expressions for the corresponding asymptotic covariance matrix. This paper is devoted to filtering, smoothing, and prediction of polynomial processes that are partially observed. These problems are known to allow for an explicit solution in the simpler case of linear Gaussian state space models. The key insight underlying the present piece of research is that in filtering applications polynomial processes and their discrete counterpart are indistinguishable from Gaussian processes sharing their first two moments. We describe the construction of these Gaussian equivalents of polynomial processes and explicitly compute optimal linear filters, predictors and smoothers for polynomial processes in discrete and continuous time. The consideration of Gaussian equivalents also opens the door to parameter estimation and linear-quadratic optimal control in the context of polynomial processes. We consider standard gradient descent, gradient flow and conjugate gradients as iterative algorithms for minimizing a penalized ridge criterion in linear regression. While it is well known that conjugate gradients exhibit fast numerical convergence, the statistical properties of their iterates are more difficult to assess due to inherent nonlinearities and dependencies. On the other hand, standard gradient flow is a linear method with well known regularizing properties when stopped early. By an explicit non-standard error decomposition we are able to bound the prediction error for conjugate gradient iterates by a corresponding prediction error of\",\n",
       " 'assess due to inherent nonlinearities and dependencies. On the other hand, standard gradient flow is a linear method with well known regularizing properties when stopped early. By an explicit non-standard error decomposition we are able to bound the prediction error for conjugate gradient iterates by a corresponding prediction error of gradient flow at transformed iteration indices. This way, the risk along the entire regularisation path of conjugate gradient iterations can be compared to that for regularisation paths of standard linear methods like gradient flow and ridge regression. In particular, the oracle conjugate gradient iterate shares the optimality properties of the gradient flow and ridge regression oracles up to a constant factor. Numerical examples show the similarity of the regularisation paths in practice. We provide conditions for the stochastic dominance comparisons of a risk $X$ and an associated risk $X+Z$, where $Z$ represents the uncertainty due to the environment and where $X$ and $Z$ can be dependent. The comparisons depend on both the copula $C$ between the distributions of $X$ and $Z$ and on the distribution of $Z$. We provide two different conditions for $C$ which represents new positive dependence properties. Regarding $Z$, we need some symmetry or asymmetry (skew) properties. Some illustrative examples are provided. We consider the graph alignment problem, wherein the objective is to find a vertex correspondence between two graphs that maximizes the edge overlap. The graph alignment problem is an instance of the quadratic assignment problem (QAP), known to be NP-hard in the worst case even to approximately solve. In this paper, we analyze Birkhoff relaxation, a tight convex relaxation of QAP, and present theoretical guarantees on its performance when the inputs follow the Gaussian Wigner Model. More specifically, the weighted adjacency matrices are correlated Gaussian Orthogonal Ensemble with correlation $1/\\\\sqrt{1+\\\\sigma^2}$. Denote the optimal solutions of the QAP and Birkhoff relaxation by $\\\\Pi^\\\\star$ and $X^\\\\star$ respectively. We show that $\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = o(n)$ when $\\\\sigma = o(n^{-1.25})$ and $\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = \\\\Omega(n)$ when $\\\\sigma = \\\\Omega(n^{-0.5})$. Thus, the optimal solution $X^\\\\star$ transitions from a small perturbation of $\\\\Pi^\\\\star$ for small $\\\\sigma$ to being well separated from $\\\\Pi^\\\\star$ as $\\\\sigma$ becomes larger than $n^{-0.5}$. This result allows us to guarantee that simple rounding procedures on $X^\\\\star$ align $1-o(1)$ fraction of vertices correctly whenever $\\\\sigma = o(n^{-1.25})$. This condition on $\\\\sigma$ to ensure the success of the Birkhoff relaxation is state-of-the-art. We consider the problem of sequential estimation of a single change point in a piecewise linear regression model under a Gaussian setup. We demonstrate that a certain CUSUM-type statistic attains the minimax optimal rates for localizing the change point. Our minimax analysis unveils an interesting phase transition from a jump (discontinuity in values) to a kink (change in slope). Specifically, for a jump, the minimax rate is of order $\\\\log (n) / n$, whereas for a kink it scales as $\\\\bigl(\\\\log (n) / n\\\\bigr)^{1/3}$, given that the sampling rate is of order $1/n$. We further introduce an algorithm for the proposed online change point detector, which requires constant computational',\n",
       " \"Specifically, for a jump, the minimax rate is of order $\\\\log (n) / n$, whereas for a kink it scales as $\\\\bigl(\\\\log (n) / n\\\\bigr)^{1/3}$, given that the sampling rate is of order $1/n$. We further introduce an algorithm for the proposed online change point detector, which requires constant computational steps and constant memory per incoming sample. Finally, the empirical performance of our method is examined on both simulated and real-world data sets. An implementation is available in the R package FLOC on GitHub. We propose causal effect estimators based on empirical Fr\\\\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators. Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\\\\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings. Given two populations from which independent binary observations are taken with parameters $p_1$ and $p_2$ respectively, estimators are proposed for the relative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their logarithms. The estimators guarantee that the relative mean-square error, or the mean-square error for the logarithmic versions, is less than a target value for any $p_1, p_2 \\\\in (0,1)$, and the ratio of average sample sizes from the two populations is close to a prescribed value. The estimators can also be used with group sampling, whereby samples are taken in batches of fixed size from the two populations. The efficiency of the estimators with respect to the Cram\\\\'er-Rao bound is good, and in particular it is close to $1$ for small values of the target error. We propose a novel approach for learning causal response representations. Our method aims to extract directions in which a multidimensional outcome is most directly caused by a treatment variable. By bridging conditional independence testing with causal representation learning, we formulate an optimisation problem that maximises the evidence against conditional independence between the treatment and outcome, given a conditioning set. This formulation employs flexible regression models tailored to specific applications, creating a versatile framework. The problem is addressed through a generalised eigenvalue decomposition. We show that, under mild assumptions, the distribution of the largest eigenvalue can be bounded by a known $F$-distribution, enabling testable conditional independence. We also provide theoretical guarantees for the optimality of the learned representation in terms of signal-to-noise ratio and Fisher information maximisation. Finally, we demonstrate the empirical effectiveness of our approach in simulation and real-world\",\n",
       " 'distribution of the largest eigenvalue can be bounded by a known $F$-distribution, enabling testable conditional independence. We also provide theoretical guarantees for the optimality of the learned representation in terms of signal-to-noise ratio and Fisher information maximisation. Finally, we demonstrate the empirical effectiveness of our approach in simulation and real-world experiments. Our results underscore the utility of this framework in uncovering direct causal effects within complex, multivariate settings. Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense. Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO. The primary objective of learning methods is generalization. Classic uniform generalization bounds, which rely on VC-dimension or Rademacher complexity, fail to explain the significant attribute that over-parameterized models in deep learning exhibit nice generalizability. On the other hand, algorithm-dependent generalization bounds, like stability bounds, often rely on strict assumptions. To establish generalizability under less stringent assumptions, this paper investigates the generalizability of neural networks that minimize or approximately minimize empirical risk. We establish a lower bound for population accuracy based on the expressiveness of these networks, which indicates that with an adequate large number of training samples and network sizes, these networks, including over-parameterized ones, can generalize effectively. Additionally, we provide a necessary condition for generalization, demonstrating that, for certain data distributions, the quantity of training data required to ensure generalization exceeds the network size needed to represent the corresponding data distribution. Finally, we provide theoretical insights into several phenomena in deep learning, including robust generalization, importance of over-parameterization, and effect of loss function on generalization. We show how to improve the discrepancy of an iid sample by moving only a few points. Specifically, modifying \\\\( O(m) \\\\) sample points on average reduces the Kolmogorov-Smirnov distance to the population distribution to \\\\(1/m\\\\). We propose a new statistical hypothesis testing framework which decides visually, using confidence intervals, whether the means of two samples are equal or if one is larger than the other. With our method, the user can at the same time visualize the confidence region of the means and do a test to decide if the means of the two populations are significantly different or not by looking whether the two',\n",
       " 'are equal or if one is larger than the other. With our method, the user can at the same time visualize the confidence region of the means and do a test to decide if the means of the two populations are significantly different or not by looking whether the two confidence intervals overlap. To design this test we use confidence intervals constructed using e-variables, which provide a measure of evidence in hypothesis testing. We propose both a sequential test and a non-sequential test based on the overlap of confidence intervals and for each of these tests we give finite-time error bounds on the probabilities of error. We also illustrate the practicality of our method by applying it to the comparison of sequential learning algorithms. Early-stopped iterative optimization methods are widely used as alternatives to explicit regularization, and direct comparisons between early-stopping and explicit regularization have been established for many optimization geometries. However, most analyses depend heavily on the specific properties of the optimization geometry or strong convexity of the empirical objective, and it remains unclear whether early-stopping could ever be less statistically efficient than explicit regularization for some particular shape constraint, especially in the overparameterized regime. To address this question, we study the setting of high-dimensional linear regression under additive Gaussian noise when the ground truth is assumed to lie in a known convex body and the task is to minimize the in-sample mean squared error. Our main result shows that for any convex body and any design matrix, up to an absolute constant factor, the worst-case risk of unconstrained early-stopped mirror descent with an appropriate potential is at most that of the least squares estimator constrained to the convex body. We achieve this by constructing algorithmic regularizers based on the Minkowski functional of the convex body. We use tools from random matrix theory to study the multi-spiked tensor model, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In particular, thanks to the nature of local optimization methods used to find the maximum likelihood estimator of this model, we propose to study the phase transition phenomenon for finding critical points of the corresponding optimization problem, i.e., those points defined by the Karush-Kuhn-Tucker (KKT) conditions. Moreover, we characterize the limiting alignments between the estimated signals corresponding to a critical point of the likelihood and the ground truth signals. With the help of these results, we propose a new estimator of the rank-$r$ tensor weights by solving a system of polynomial equations, which is asymptotically unbiased contrary the maximum likelihood estimator. We consider a process $X^\\\\varepsilon$ solution of a stochastic Volterra equation with an unknown parameter $\\\\theta^\\\\star$ in the drift function. The Volterra kernel is singular and given by $K(u)=c u^{\\\\alpha-1/2} \\\\mathbb{1}_{u>0}$ with $\\\\alpha \\\\in (0,1/2)$. It is assumed that the diffusion coefficient is proportional to $\\\\varepsilon \\\\to 0$. From an observation of the path $(X^\\\\varepsilon_s)_{s\\\\in[0,T]}$, we construct a Trajectory Fitting Estimator, which is shown to be consistent and asymptotically normal. We also specify identifiability conditions insuring the $L^p$ convergence',\n",
       " \"u^{\\\\alpha-1/2} \\\\mathbb{1}_{u>0}$ with $\\\\alpha \\\\in (0,1/2)$. It is assumed that the diffusion coefficient is proportional to $\\\\varepsilon \\\\to 0$. From an observation of the path $(X^\\\\varepsilon_s)_{s\\\\in[0,T]}$, we construct a Trajectory Fitting Estimator, which is shown to be consistent and asymptotically normal. We also specify identifiability conditions insuring the $L^p$ convergence of the estimator. We address the problem of safety verification for nonlinear stochastic systems, specifically the task of certifying that system trajectories remain within a safe set with high probability. To tackle this challenge, we adopt a set-erosion strategy, which decouples the effects of stochastic disturbances from deterministic dynamics. This approach converts the stochastic safety verification problem on a safe set into a deterministic safety verification problem on an eroded subset of the safe set. The success of this strategy hinges on the depth of erosion, which is determined by a probabilistic tube that bounds the deviation of stochastic trajectories from their corresponding deterministic trajectories. Our main contribution is the establishment of a tight bound for the probabilistic tube of nonlinear stochastic systems. To obtain a probabilistic bound for stochastic trajectories, we adopt a martingale-based approach. The core innovation lies in the design of a novel energy function associated with the averaged moment generating function, which forms an affine martingale, a generalization of the traditional c-martingale. Using this energy function, we derive a precise bound for the probabilistic tube. Furthermore, we enhance this bound by incorporating the union-bound inequality for strictly contractive dynamics. By integrating the derived probabilistic tubes into the set-erosion strategy, we demonstrate that the safety verification problem for nonlinear stochastic systems can be reduced to a deterministic safety verification problem. Our theoretical results are validated through applications in reachability-based safety verification and safe controller synthesis, accompanied by several numerical examples that illustrate their effectiveness. In this paper, we present the asymptotic properties of the moment estimator for autoregressive (AR for short) models subject to Markovian changes in regime under the assumption that the errors are uncorrelated but not necessarily independent. We relax the standard independence assumption on the innovation process to extend considerably the range of application of the Markov-switching AR models. We provide necessary conditions to prove the consistency and asymptotic normality of the moment estimator in a specific case. Particular attention is paid to the estimation of the asymptotic covariance matrix. Finally, some simulation studies and an application to the hourly meteorological data are presented to corroborate theoretical work. We propose a first near complete (that will make explicit sense in the main text) nonasymptotic generalization theory for multilayer neural networks with arbitrary Lipschitz activations and general Lipschitz loss functions (with some very mild conditions). In particular, it doens't require the boundness of loss function, as commonly assumed in the literature. Our theory goes beyond the bias-variance tradeoff, aligned with phenomenon typically encountered in deep learning. It is therefore sharp different with other existing nonasymptotic generalization error bounds for neural networks. More explicitly, we propose an explicit generalization error upper bound for multilayer neural networks\",\n",
       " \"as commonly assumed in the literature. Our theory goes beyond the bias-variance tradeoff, aligned with phenomenon typically encountered in deep learning. It is therefore sharp different with other existing nonasymptotic generalization error bounds for neural networks. More explicitly, we propose an explicit generalization error upper bound for multilayer neural networks with arbitrary Lipschitz activations $\\\\sigma$ with $\\\\sigma(0)=0$ and broad enough Lipschitz loss functions, without requiring either the width, depth or other hyperparameters of the neural network approaching infinity, a specific neural network architect (e.g. sparsity, boundness of some norms), a particular activation function, a particular optimization algorithm or boundness of the loss function, and with taking the approximation error into consideration. General Lipschitz activation can also be accommodated into our framework. A feature of our theory is that it also considers approximation errors. Furthermore, we show the near minimax optimality of our theory for multilayer ReLU networks for regression problems. Notably, our upper bound exhibits the famous double descent phenomenon for such networks, which is the most distinguished characteristic compared with other existing results. This work emphasizes a view that many classical results should be improved to embrace the unintuitive characteristics of deep learning to get a better understanding of it. We develop a pseudo-likelihood theory for rank one matrix estimation problems in the high dimensional limit. We prove a variational principle for the limiting pseudo-maximum likelihood which also characterizes the performance of the corresponding pseudo-maximum likelihood estimator. We show that this variational principle is universal and depends only on four parameters determined by the corresponding null model. Through this universality, we introduce a notion of equivalence for estimation problems of this type and, in particular, show that a broad class of estimation tasks, including community detection, sparse submatrix detection, and non-linear spiked matrix models, are equivalent to spiked matrix models. As an application, we obtain a complete description of the performance of the least-squares (or ``best rank one'') estimator for any rank one matrix estimation problem. Multivariate spatial phenomena are ubiquitous, spanning domains such as climate, pandemics, air quality, and social economy. Cross-correlation between different quantities of interest at different locations is asymmetric in general. This paper provides the visualization, structure, and properties of asymmetric cross-correlation as well as symmetric auto-correlation. It reviews mainstream multivariate spatial models and analyzes their capability to accommodate asymmetric cross-correlation. It also illustrates the difference in model accuracy with and without asymmetric accommodation using a 1D simulated example. The failure of key financial institutions may accelerate risk contagion due to their interconnections within the system. In this paper, we propose a robust portfolio strategy to mitigate systemic risks during extreme events. We use the stock returns of key financial institutions as an indicator of their performance, apply extreme value theory to assess the extremal dependence among stocks of financial institutions, and construct a network model based on a threshold approach that captures extremal dependence. Our analysis reveals different dependence structures in the Chinese and U.S. financial systems. By applying the maximum independent set (MIS)\",\n",
       " 'their performance, apply extreme value theory to assess the extremal dependence among stocks of financial institutions, and construct a network model based on a threshold approach that captures extremal dependence. Our analysis reveals different dependence structures in the Chinese and U.S. financial systems. By applying the maximum independent set (MIS) from graph theory, we identify a subset of institutions with minimal extremal dependence, facilitating the construction of diversified portfolios resilient to risk contagion. We also compare the performance of our proposed portfolios with that of the market portfolios in the two economies. Upon observing $n$-dimensional multivariate Gaussian data, when can we infer that the largest $K$ observations came from the largest $K$ means? When $K=1$ and the covariance is isotropic, \\\\cite{Gutmann} argue that this inference is justified when the two-sided difference-of-means test comparing the largest and second largest observation rejects. Leveraging tools from selective inference, we provide a generalization of their procedure that applies for both any $K$ and any covariance structure. We show that our procedure draws the desired inference whenever the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference rejects, and sometimes even when this test fails to reject. Using this insight, we argue that our procedure renders existing simultaneous inference approaches inadmissible when $n > 2$. When the observations are independent (with possibly unequal variances) or equicorrelated, our procedure corresponds exactly to running the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference. Laplacian matrices are commonly employed in many real applications, encoding the underlying latent structural information such as graphs and manifolds. The use of the normalization terms naturally gives rise to random matrices with dependency. It is well-known that dependency is a major bottleneck of new random matrix theory (RMT) developments. To this end, in this paper, we formally introduce a class of generalized (and regularized) Laplacian matrices, which contains the Laplacian matrix and the random adjacency matrix as a specific case, and suggest the new framework of the asymptotic theory of eigenvectors for latent embeddings with generalized Laplacian matrices (ATE-GL). Our new theory is empowered by the tool of generalized quadratic vector equation for dealing with RMT under dependency, and delicate high-order asymptotic expansions of the empirical spiked eigenvectors and eigenvalues based on local laws. The asymptotic normalities established for both spiked eigenvectors and eigenvalues will enable us to conduct precise inference and uncertainty quantification for applications involving the generalized Laplacian matrices with flexibility. We discuss some applications of the suggested ATE-GL framework and showcase its validity through some numerical examples. The multi-armed bandits (MAB) framework is a widely used approach for sequential decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, and rewards from different arms are related',\n",
       " \"decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, and rewards from different arms are related rather than independent. We propose a novel semi-parametric framework for batched bandits with covariates and a shared parameter across arms, leveraging the single-index regression (SIR) model to capture relationships between arm rewards while balancing interpretability and flexibility. Our algorithm, Batched single-Index Dynamic binning and Successive arm elimination (BIDS), employs a batched successive arm elimination strategy with a dynamic binning mechanism guided by the single-index direction. We consider two settings: one where a pilot direction is available and another where the direction is estimated from data, deriving theoretical regret bounds for both cases. When a pilot direction is available with sufficient accuracy, our approach achieves minimax-optimal rates (with $d = 1$) for nonparametric batched bandits, circumventing the curse of dimensionality. Extensive experiments on simulated and real-world datasets demonstrate the effectiveness of our algorithm compared to the nonparametric batched bandit method introduced by \\\\cite{jiang2024batched}. In this paper, we consider a two-stage Gibbs sampler for a normal linear regression model with a horseshoe prior. Under some assumptions, we show that it produces a geometrically ergodic Markov chain. In particular, we prove geometric ergodicity under some three-parameter beta global prior which does not have a finite $(p / 5)$-th negative moment, where $p$ is the number of regression coefficients. This is in contrast to the case of a known general result which is applicable if the global parameter has a finite approximately $(p / 2)$-th negative moment. I present a novel uniform law of large numbers (ULLN) for network-dependent data. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive suite of limit theorems and a robust variance estimator for network-dependent processes, their analysis focuses on pointwise convergence. On the other hand, uniform convergence is essential for nonlinear estimators such as M and GMM estimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I establish the ULLN under network dependence and demonstrate its utility by proving the consistency of both M and GMM estimators. A byproduct of this work is a novel maximal inequality for network data, which may prove useful for future research beyond the scope of this paper. We revisit the problem of constructing predictive confidence sets for which we wish to obtain some type of conditional validity. We provide new arguments showing how ``split conformal'' methods achieve near desired coverage levels with high probability, a guarantee conditional on the validation data rather than marginal over it. In addition, we directly consider (approximate) conditional coverage, where, e.g., conditional on a covariate $X$ belonging to some group of interest, we would like a guarantee that a predictive set covers the true outcome $Y$. We show that the natural method of performing quantile regression on a held-out (validation) dataset yields minimax optimal\",\n",
       " 'directly consider (approximate) conditional coverage, where, e.g., conditional on a covariate $X$ belonging to some group of interest, we would like a guarantee that a predictive set covers the true outcome $Y$. We show that the natural method of performing quantile regression on a held-out (validation) dataset yields minimax optimal guarantees of coverage here. Complementing these positive results, we also provide experimental evidence that interesting work remains to be done to develop computationally efficient but valid predictive inference methods. Linear inverse problems are ubiquitous in various science and engineering disciplines. Of particular importance in the past few decades, is the incorporation of sparsity based priors, in particular $\\\\ell_1$ priors, into linear inverse problems, which led to the flowering of fields of compressive sensing (CS) and sparsity based signal processing. More recently, methods based on a Compound Gaussian (CG) prior have been investigated and demonstrate improved results over CS in practice. This paper is the first attempt to identify and elucidate the fundamental structures underlying the success of CG methods by studying CG in the context of a broader framework of generalized-sparsity-based-inference. After defining our notion of generalized sparsity we introduce a weak null space property and proceed to generalize two well-known methods in CS, basis pursuit and iteratively reweighted least squares (IRLS). We show how a subset of CG-induced regularizers fits into this framework. Given a tree $T$, its path polytope is the convex hull of the edge indicator vectors for the paths between any two distinct leaves in $T$. These polytopes arise naturally in polyhedral geometry and applications, such as phylogenetics, tropical geometry, and algebraic statistics. We provide a minimal halfspace representation of these polytopes. The construction is made inductively using toric fiber products. We introduce a novel class of bivariate common-shock discrete phase-type (CDPH) distributions to describe dependencies in loss modeling, with an emphasis on those induced by common shocks. By constructing two jointly evolving terminating Markov chains that share a common evolution up to a random time corresponding to the common shock component, and then proceed independently, we capture the essential features of risk events influenced by shared and individual-specific factors. We derive explicit expressions for the joint distribution of the termination times and prove various class and distributional properties, facilitating tractable analysis of the risks. Extending this framework, we model random sums where aggregate claims are sums of continuous phase-type random variables with counts determined by these termination times, and show that their joint distribution belongs to the multivariate phase-type or matrix-exponential class. We develop estimation procedures for the CDPH distributions using the expectation-maximization algorithm and demonstrate the applicability of our models through simulation studies and an application to bivariate insurance claim frequency data. We examine the location characteristics of a conditional selective confidence interval based on the polyhedral method. This interval is constructed from the distribution of a test statistic conditional upon the event of statistical significance. In the case of a one-sided test, the behavior of the interval varies depending on whether the parameter',\n",
       " 'the location characteristics of a conditional selective confidence interval based on the polyhedral method. This interval is constructed from the distribution of a test statistic conditional upon the event of statistical significance. In the case of a one-sided test, the behavior of the interval varies depending on whether the parameter is highly significant or only marginally significant. When the parameter is highly significant, the interval is similar to the usual confidence interval derived without considering selection. However, when the parameter is only marginally significant, the interval falls into an extreme range and deviates greatly from the estimated value of the parameter. In contrast, an interval conditional on two-sided significance does not yield extreme results, although it may exclude the estimated parameter value. Reproducing Kernel Hilbert Space (RKHS) embedding of probability distributions has proved to be an effective approach, via MMD (maximum mean discrepancy) for nonparametric hypothesis testing problems involving distributions defined over general (non-Euclidean) domains. While a substantial amount of work has been done on this topic, only recently, minimax optimal two-sample tests have been constructed that incorporate, unlike MMD, both the mean element and a regularized version of the covariance operator. However, as with most kernel algorithms, the computational complexity of the optimal test scales cubically in the sample size, limiting its applicability. In this paper, we propose a spectral regularized two-sample test based on random Fourier feature (RFF) approximation and investigate the trade-offs between statistical optimality and computational efficiency. We show the proposed test to be minimax optimal if the approximation order of RFF (which depends on the smoothness of the likelihood ratio and the decay rate of the eigenvalues of the integral operator) is sufficiently large. We develop a practically implementable permutation-based version of the proposed test with a data-adaptive strategy for selecting the regularization parameter and the kernel. Finally, through numerical experiments on simulated and benchmark datasets, we demonstrate that the proposed RFF-based test is computationally efficient and performs almost similar (with a small drop in power) to the exact test. We study the coverage properties of full conformal regression in the proportional asymptotic regime where the ratio of the dimension and the sample size converges to a constant. In this setting, existing theory tells us only that full conformal inference is unbiased, in the sense that its average coverage lies at the desired level when marginalized over both the new test point and the training data. Considerably less is known about the behaviour of these methods conditional on the training set. As a result, the exact benefits of full conformal inference over much simpler alternative methods is unclear. This paper investigates the behaviour of full conformal inference and natural uncorrected alternatives for a broad class of $L_2$-regularized linear regression models. We show that in the proportional asymptotic regime the training-conditional coverage of full conformal inference concentrates at the target value. On the other hand, simple alternatives that directly compare test and training residuals realize constant undercoverage bias. While these results demonstrate the necessity of full',\n",
       " 'of $L_2$-regularized linear regression models. We show that in the proportional asymptotic regime the training-conditional coverage of full conformal inference concentrates at the target value. On the other hand, simple alternatives that directly compare test and training residuals realize constant undercoverage bias. While these results demonstrate the necessity of full conformal in correcting for high-dimensional overfitting, we also show that this same methodology is redundant for the related task of tuning the regularization level. In particular, we show that full conformal inference still yields asymptotically valid coverage when the regularization level is selected using only the training set, without consideration of the test point. Simulations show that our asymptotic approximations are accurate in finite samples and can be readily extended to other popular full conformal variants, such as full conformal quantile regression and the LASSO, that do not directly meet our assumptions. The Kolmogorov-Smirnov (KS) statistic is a classical nonparametric test widely used for comparing an empirical distribution function with a reference distribution or for comparing two empirical distributions. Despite its broad applicability in statistical hypothesis testing and model validation, certain aspects of the KS statistic remain under-explored among the young generation, particularly under finite sample conditions. This paper revisits the KS statistic in both one-sample and two-sample scenarios, considering one-sided and two-sided variants. We derive exact probabilities for the supremum of the empirical process and present a unified treatment of the KS statistic under diverse settings. Additionally, we explore the discrete nature of the hitting times of the normalized empirical process, providing practical insights into the computation of KS test p-values. The study also discusses the Dvoretzky-Kiefer-Wolfowitz-Massart (DKWM) inequality, highlighting its role in constructing confidence bands for distribution functions. Using empirical process theory, we establish the limit distribution of the KS statistic when the true distribution includes unknown parameters. Our findings extend existing results, offering improved methodologies for statistical analysis and hypothesis testing using the KS statistic, particularly in finite sample scenarios. A one-shot device is a unit that operates only once, after which it is either destroyed or needs to be rebuilt. For this type of device, the operational status can only be assessed at a specific inspection time, determining whether failure occurred before or after it. Consequently, lifetimes are subject to left- or right-censoring. One-shot devices are usually highly reliables. To analyze the reliability of such products, an accelerated life test (ALT) plan is typically employed by subjecting the devices to increased levels of stress factors, thus allowing life characteristics observed under high-stress conditions to be extrapolated to normal operating conditions. By accelerating the degradation process, ALT significantly reduces both the time required for testing and the associated experimental costs. Recently, robust inferential methods have gained considerable interest in statistical analysis. Among them, weighted minimum density power divergence estimators (WMDPDEs) are widely recognized for their robust statistical properties with small loss of efficiency. In this work, robust WMDPDE and associated statistical tests are developed under a log-logistic lifetime distribution with multiple stresses. Explicit expressions for the estimating',\n",
       " \"interest in statistical analysis. Among them, weighted minimum density power divergence estimators (WMDPDEs) are widely recognized for their robust statistical properties with small loss of efficiency. In this work, robust WMDPDE and associated statistical tests are developed under a log-logistic lifetime distribution with multiple stresses. Explicit expressions for the estimating equations and asymptotic distribution of the estimators are obtained. Further, a Monte Carlo simulation study is presented to evaluate the performance of the WMDPDE in practical applications. Learning kernels in operators from data lies at the intersection of inverse problems and statistical learning, offering a powerful framework for capturing nonlocal dependency in function spaces and high-dimensional settings. In contrast to classical nonparametric regression, where the inverse problem is well-posed, kernel estimation involves a compact normal operator and an ill-posed deconvolution. To address these challenges, we introduce adaptive spectral Sobolev spaces, unifying Sobolev spaces and reproducing kernel Hilbert spaces, that automatically discard non-identifiable components and control terms with small eigenvalues. Within this framework, we establish the minimax convergence rates for the mean squared error under both polynomial and exponential spectral decay regimes. Methodologically, we develop a tamed least squares estimator achieving the minimax upper rates via controlling the left-tail probability for eigenvalues of the random normal matrix; and for the minimax lower rates, we resolve challenges from infinite-dimensional measures through their projections. This paper investigates conditional specifications for multivariate count variables. Recently, the spatial count data literature has proposed several conditional models such that the conditional expectations are linear in the conditioning variables. These models are much easier to estimate than existing spatial count models based on Gaussian random field. However, whether or not such conditional specifications are compatible have not been addressed. We investigate two large families of conditional models, that are the compound autoregressive model and the random coefficient integer autoregressive model. We characterize all the solutions to these two families of models at arbitrary dimensions, and find that only a handful of them admit non-trivial solutions. We then show that if we focus on the linearity condition of the conditional expectations only, a considerable larger family of solutions can be obtained. This suggests that for spatial count data modeling, semi-parametric type specifications that impose the conditional expectation structure is preferable. In this paper we extend the classical Glivenko-Cantelli theorem to real-valued empirical functions under dependence structures characterised by $\\\\alpha$-mixing and $\\\\beta$-mixing conditions. We investigate sufficient conditions ensuring that families of real-valued functions exhibit the Glivenko-Cantelli (GC) property in these dependence settings. Our analysis focuses on function classes satisfying uniform entropy conditions and establishes deviation bounds under mixing coefficients that decay at appropriate rates. Our results refine the existing literature by relaxing the independence assumptions and highlighting the role of dependence in empirical process convergence. We study two G-modeling strategies for estimating the signal distribution (the empirical Bayesian's prior) from observations corrupted with normal noise. First, we choose the signal distribution by minimizing Stein's unbiased risk estimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach motivated by optimal\",\n",
       " 'of dependence in empirical process convergence. We study two G-modeling strategies for estimating the signal distribution (the empirical Bayesian\\'s prior) from observations corrupted with normal noise. First, we choose the signal distribution by minimizing Stein\\'s unbiased risk estimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach motivated by optimal empirical Bayesian shrinkage estimation of the signals. Second, we select the signal distribution by minimizing Hyv\\\\\"arinen\\'s score matching objective for the implied score (derivative of log-marginal density), targeting minimal Fisher divergence between estimated and true marginal densities. While these strategies appear distinct, they are known to be mathematically equivalent. We provide a unified analysis of SURE and score matching under both well-specified signal distribution classes and misspecification. In the classical well-specified setting with homoscedastic noise and compactly supported signal distribution, we establish nearly parametric rates of convergence of the empirical Bayes regret and the Fisher divergence. In a commonly studied misspecified model, we establish fast rates of convergence to the oracle denoiser and corresponding oracle inequalities. Our empirical results demonstrate competitiveness with nonparametric maximum likelihood in well-specified settings, while showing superior performance under misspecification, particularly in settings involving heteroscedasticity and side information. In many random phenomena, such as life-testing experiments and environmental data (like rainfall data), there are often positive values and an excess of zeros, which create modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called zero inliers, are difficult to model using standard approaches. When studying extreme values in the above scenarios, a key issue is selecting an appropriate threshold for accurate tail approximation of the population using asymptotic models. While some extreme value mixture models address threshold estimation and tail approximation, conventional parametric and non-parametric bulk and generalised Pareto distribution (GPD) approaches often neglect inliers, leading to suboptimal results. This paper introduces a framework for modeling extreme events and inliers using the GPD, addressing threshold uncertainty and effectively capturing inliers at zero. The model\\'s parameters are estimated using the maximum likelihood estimation (MLE) method, ensuring optimal precision. Through simulation studies and real-world applications, we demonstrate that the proposed model significantly outperforms the traditional methods, which typically neglect inliers at the origin. This paper continues the study of the efficiency of conformal prediction as compared with more general randomness prediction and exchangeability prediction. It does not restrict itself to the case of classification, and our results will also be applicable to the case of regression. The price to pay is that efficiency will be attained only on average, albeit with respect to a wide range of probability measures on the label space. Time series in natural sciences, such as hydrology and climatology, and other environmental applications, often consist of continuous observations constrained to the unit interval (0,1). Traditional Gaussian-based models fail to capture these bounds, requiring more flexible approaches. This paper introduces the Matsuoka Autoregressive Moving Average (MARMA) model, extending the GARMA framework by assuming a Matsuoka-distributed random component taking',\n",
       " \"and climatology, and other environmental applications, often consist of continuous observations constrained to the unit interval (0,1). Traditional Gaussian-based models fail to capture these bounds, requiring more flexible approaches. This paper introduces the Matsuoka Autoregressive Moving Average (MARMA) model, extending the GARMA framework by assuming a Matsuoka-distributed random component taking values in (0,1) and an ARMA-like systematic structure allowing for random time-dependent covariates. Parameter estimation is performed via partial maximum likelihood (PMLE), for which we present the asymptotic theory. It enables statistical inference, including confidence intervals and model selection. To construct prediction intervals, we propose a novel bootstrap-based method that accounts for dependence structure uncertainty. A comprehensive Monte Carlo simulation study assesses the finite sample performance of the proposed methodologies, while an application to forecasting the useful water volume of the Guarapiranga Reservoir in Brazil showcases their practical usefulness. Many scientific problems involve data exhibiting both temporal and cross-sectional dependencies. While linear dependencies have been extensively studied, the theoretical analysis of regression estimators under nonlinear dependencies remains scarce. This work studies a kernel-based estimation procedure for nonlinear dynamics within the reproducing kernel Hilbert space framework, focusing on nonlinear vector autoregressive models. We derive nonasymptotic probabilistic bounds on the deviation between a regularized kernel estimator and the nonlinear regression function. A key technical contribution is a concentration bound for quadratic forms of stochastic matrices in the presence of dependent data, which is of independent interest. Additionally, we characterize conditions on multivariate kernels that guarantee optimal convergence rates. In statistics, generalized linear models (GLMs) are widely used for modeling data and can expressively capture potential nonlinear dependence of the model's outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in a general class of sparse binary GLMs. Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron (Kakade et al. 2011; Bahmani et al. 2016), BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications, logistic and probit regression are additionally studied. In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise\",\n",
       " \"shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression. We study high-dimensional random geometric graphs (RGGs) of edge-density $p$ with vertices uniformly distributed on the $d$-dimensional torus and edges inserted between sufficiently close vertices with respect to an $L_q$-norm. We focus on distinguishing an RGG from an Erd\\\\H{o}s--R\\\\'enyi (ER) graph if both models have edge probability $p$. So far, most results considered either spherical RGGs with $L_2$-distance or toroidal RGGs under $L_\\\\infty$-distance. However, for general $L_q$-distances, many questions remain open, especially if $p$ is allowed to depend on $n$. The main reason for this is that RGGs under $L_q$-distances can not easily be represented as the logical AND of their 1-dimensional counterparts, as for $L_\\\\infty$ geometries. To overcome this, we devise a novel technique for quantifying the dependence between edges based on modified Edgeworth expansions. Our technique yields the first tight algorithmic upper bounds for distinguishing toroidal RGGs under general $L_q$ norms from ER-graphs for fixed $p$ and $q$. We achieve this by showing that signed triangles can distinguish the two models when $d\\\\ll n^3p^3$ for the whole regime of $c/n<p<1$. Additionally, our technique yields an improved information-theoretic lower bound for this task, showing that the two distributions converge whenever $d=\\\\tilde{\\\\Omega}(n^3p^2)$, which is just as strong as the currently best known lower bound for spherical RGGs in case of general $p$ from Liu et al. [STOC'22]. Finally, our expansions allow us to tightly characterize the spectral properties of toroidal RGGs both under $L_q$-distances for fixed $1\\\\le q<\\\\infty$, and $L_\\\\infty$-distance. Our results partially resolve a conjecture of Bangachev and Bresler [COLT'24] and prove that the distance metric, rather than the underlying space, is responsible for the observed differences in the behavior of spherical and toroidal RGGs. In this work, we present a new perspective on the origin and interpretation of adaptive filters. By applying Bayesian principles of recursive inference from the state-space model and using a series of simplifications regarding the structure of the solution, we can present, in a unified framework, derivations of many adaptive filters which depend on the probabilistic model of the observational noise. In particular, under a Gaussian model, we obtain solutions well-known in the literature (such as LMS, NLMS, or Kalman filter), while using non-Gaussian noise, we obtain new families of adaptive filter. Notably, under assumption of Laplacian noise, we obtain a family of robust filters of which the signed-error algorithm is a well-known member, while other algorithms, derived effortlessly in the proposed framework, are entirely new. Numerical examples are shown to illustrate the properties and provide a better insight into the performance of the derived adaptive filters. A novel method\",\n",
       " 'a family of robust filters of which the signed-error algorithm is a well-known member, while other algorithms, derived effortlessly in the proposed framework, are entirely new. Numerical examples are shown to illustrate the properties and provide a better insight into the performance of the derived adaptive filters. A novel method for sequential outlier detection in non-stationary time series is proposed. The method tests the null hypothesis of ``no outlier\\'\\' at each time point, addressing the multiple testing problem by bounding the error probability of successive tests, using extreme value theory. The asymptotic properties of the test statistic are studied under the null hypothesis and alternative. The finite sample properties of the new detection scheme are investigated by means of a simulation study, and the method is compared with alternative procedures which have recently been proposed in the statistics and machine learning literature. This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference. Hypothesis tests and confidence intervals are ubiquitous in empirical research, yet their connection to subsequent decision-making is often unclear. We develop a theory of certified decisions that pairs recommended decisions with inferential guarantees. Specifically, we attach P-certificates -- upper bounds on loss that hold with probability at least $1-\\\\alpha$ -- to recommended actions. We show that such certificates allow \"safe,\" risk-controlling adoption decisions for ambiguity-averse downstream decision-makers. We further prove that it is without loss to limit attention to P-certificates arising as minimax decisions over confidence sets, or what Manski (2021) terms \"as-if decisions with a set estimate.\" A parallel argument applies to E-certified decisions obtained from e-values in settings with unbounded loss. In many real applications of statistical learning, collecting sufficiently many training data is often expensive, time-consuming, or even unrealistic. In this case, a transfer learning approach, which aims to leverage knowledge from a related source domain to improve the learning performance in the target domain, is more beneficial. There have been many transfer learning methods developed under various distributional assumptions. In this article, we study a particular type of classification problem, called conformal prediction, under a new distributional assumption for transfer learning. Classifiers under the conformal prediction framework predict a set of plausible labels instead of one single label for each data instance, affording a more cautious and safer decision. We consider a generalization of the \\\\textit{covariate shift with posterior drift} setting for transfer learning. Under this setting, we propose a weighted conformal classifier that leverages both the source and target samples, with a coverage guarantee in the target domain. Theoretical studies demonstrate favorable asymptotic properties. Numerical studies further illustrate the usefulness of the proposed method. We consider statistical inference under a semi-supervised',\n",
       " 'transfer learning. Under this setting, we propose a weighted conformal classifier that leverages both the source and target samples, with a coverage guarantee in the target domain. Theoretical studies demonstrate favorable asymptotic properties. Numerical studies further illustrate the usefulness of the proposed method. We consider statistical inference under a semi-supervised setting where we have access to both a labeled dataset consisting of pairs $\\\\{X_i, Y_i \\\\}_{i=1}^n$ and an unlabeled dataset $\\\\{ X_i \\\\}_{i=n+1}^{n+N}$. We ask the question: under what circumstances, and by how much, can incorporating the unlabeled dataset improve upon inference using the labeled data? To answer this question, we investigate semi-supervised learning through the lens of semiparametric efficiency theory. We characterize the efficiency lower bound under the semi-supervised setting for an arbitrary inferential problem, and show that incorporating unlabeled data can potentially improve efficiency if the parameter is not well-specified. We then propose two types of semi-supervised estimators: a safe estimator that imposes minimal assumptions, is simple to compute, and is guaranteed to be at least as efficient as the initial supervised estimator; and an efficient estimator, which -- under stronger assumptions -- achieves the semiparametric efficiency bound. Our findings unify existing semiparametric efficiency results for particular special cases, and extend these results to a much more general class of problems. Moreover, we show that our estimators can flexibly incorporate predicted outcomes arising from ``black-box\" machine learning models, and thereby achieve the same goal as prediction-powered inference (PPI), but with superior theoretical guarantees. We also provide a complete understanding of the theoretical basis for the existing set of PPI methods. Finally, we apply the theoretical framework developed to derive and analyze efficient semi-supervised estimators in a number of settings, including M-estimation, U-statistics, and average treatment effect estimation, and demonstrate the performance of the proposed estimators via simulations. Motivated by learning dynamical structures from static snapshot data, this paper presents a distribution-on-scalar regression approach for estimating the density evolution of a stochastic process from its noisy temporal point clouds. We propose an entropy-regularized nonparametric maximum likelihood estimator (E-NPMLE), which leverages the entropic optimal transport as a smoothing regularizer for the density flow. We show that the E-NPMLE has almost dimension-free statistical rates of convergence to the ground truth distributions, which exhibit a striking phase transition phenomenon in terms of the number of snapshots and per-snapshot sample size. To efficiently compute the E-NPMLE, we design a novel particle-based and grid-free coordinate KL divergence gradient descent (CKLGD) algorithm and prove its polynomial iteration complexity. Moreover, we provide numerical evidence on synthetic data to support our theoretical findings. This work contributes to the theoretical understanding and practical computation of estimating density evolution from noisy observations in arbitrary dimensions. A fundamental problem in statistics and machine learning is to estimate a function $f$ from possibly noisy observations of its point samples. The goal is to design a numerical algorithm to construct an approximation $\\\\hat f$ to $f$ in a prescribed norm that asymptotically achieves the best possible error (as a function of the',\n",
       " 'statistics and machine learning is to estimate a function $f$ from possibly noisy observations of its point samples. The goal is to design a numerical algorithm to construct an approximation $\\\\hat f$ to $f$ in a prescribed norm that asymptotically achieves the best possible error (as a function of the number $m$ of observations and the variance $\\\\sigma^2$ of the noise). This problem has received considerable attention in both nonparametric statistics (noisy observations) and optimal recovery (noiseless observations). Quantitative bounds require assumptions on $f$, known as model class assumptions. Classical results assume that $f$ is in the unit ball of a Besov space. In nonparametric statistics, the best possible performance of an algorithm for finding $\\\\hat f$ is known as the minimax rate and has been studied in this setting under the assumption that the noise is Gaussian. In optimal recovery, the best possible performance of an algorithm is known as the optimal recovery rate and has also been determined in this setting. While one would expect that the minimax rate recovers the optimal recovery rate when the noise level $\\\\sigma$ tends to zero, it turns out that the current results on minimax rates do not carefully determine the dependence on $\\\\sigma$ and the limit cannot be taken. This paper handles this issue and determines the noise-level-aware (NLA) minimax rates for Besov classes when error is measured in an $L_q$-norm with matching upper and lower bounds. The end result is a reconciliation between minimax rates and optimal recovery rates. The NLA minimax rate continuously depends on the noise level and recovers the optimal recovery rate when $\\\\sigma$ tends to zero. We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering sublinear Neyman regret, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\\\\widetilde{O}(\\\\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\\\\widetilde{O}(\\\\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual \"multigroup\" Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group\\'s best non-adaptive designs. In particular, we develop a contextual adaptive design with $\\\\widetilde{O}(\\\\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments. Gaussian multiplicative chaos (GMC) is a canonical random fractal measure obtained by exponentiating log-correlated Gaussian processes, first constructed in the seminal work of Kahane (1985). Since then it has served as an important building block in constructions of quantum field theories and Liouville quantum gravity. However, in many natural settings, non-Gaussian log-correlated processes arise. In this paper, we investigate the universality of GMC through an invariance principle. We consider the model of',\n",
       " 'of Kahane (1985). Since then it has served as an important building block in constructions of quantum field theories and Liouville quantum gravity. However, in many natural settings, non-Gaussian log-correlated processes arise. In this paper, we investigate the universality of GMC through an invariance principle. We consider the model of a random Fourier series, a process known to be log-correlated. While the Gaussian Fourier series has been a classical object of study, recently, the non-Gaussian counterpart was investigated and the associated multiplicative chaos constructed by Junnila in 2016. We show that the Gaussian and non-Gaussian variables can be coupled so that the associated chaos measures are almost surely mutually absolutely continuous throughout the entire sub-critical regime. This solves the main open problem from Kim and Kriechbaum (2024) who had earlier established such a result for a part of the regime. The main ingredient is a new high dimensional CLT for a sum of independent (but not i.i.d.) random vectors belonging to rank one subspaces with error bounds involving the isotropic properties of the covariance matrix of the sum, which we expect will find other applications. The proof relies on a path-wise analysis of Skorokhod embeddings as well as a perturbative result about square roots of positive semi-definite matrices which, surprisingly, appears to be new. In this paper, we introduce a unified framework, inspired by classical regularization theory, for designing and analyzing a broad class of linear regression approaches. Our framework encompasses traditional methods like least squares regression and Ridge regression, as well as innovative techniques, including seven novel regression methods such as Landweber and Showalter regressions. Within this framework, we further propose a class of debiased and thresholded regression methods to promote feature selection, particularly in terms of sparsity. These methods may offer advantages over conventional regression techniques, including Lasso, due to their ease of computation via a closed-form expression. Theoretically, we establish consistency results and Gaussian approximation theorems for this new class of regularization methods. Extensive numerical simulations further demonstrate that the debiased and thresholded counterparts of linear regression methods exhibit favorable finite sample performance and may be preferable in certain settings. We address the problem of producing a lower bound for the mean of a discrete probability distribution, with known support over a finite set of real numbers, from an iid sample of that distribution. Up to a constant, this is equivalent to bounding the mean of a multinomial distribution (with known support) from a sample of that distribution. Our main contribution is to characterize the complete set of admissible bound functions for any sample space, and to show that certain previously published bounds are admissible. We prove that the solution to each one of a set of simple-to-state optimization problems yields such an admissible bound. Single examples of such bounds, such as the trinomial bound by Miratrix and Stark [2009] have been previously published, but without an analysis of admissibility, and without a discussion of the full set of alternative admissible bounds. In addition to a variety',\n",
       " 'simple-to-state optimization problems yields such an admissible bound. Single examples of such bounds, such as the trinomial bound by Miratrix and Stark [2009] have been previously published, but without an analysis of admissibility, and without a discussion of the full set of alternative admissible bounds. In addition to a variety of results about admissible bounds, we prove the non-existence of optimal bounds for sample spaces with supports of size greater than 1 and samples sizes greater than 1.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b258b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.DataFrame({\"chunk\": chunks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "835955a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider nonparametric regression with func...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>corrections that improve the accuracy for fini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parametric estimation of multimodal densities ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>events from baseline. Unlike existing models, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much less understood, and the focus of our wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>a family of robust filters of which the signed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>transfer learning. Under this setting, we prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>statistics and machine learning is to estimate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>of Kahane (1985). Since then it has served as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>simple-to-state optimization problems yields s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                chunk\n",
       "0   We consider nonparametric regression with func...\n",
       "1   corrections that improve the accuracy for fini...\n",
       "2   parametric estimation of multimodal densities ...\n",
       "3   events from baseline. Unlike existing models, ...\n",
       "4   much less understood, and the focus of our wor...\n",
       "..                                                ...\n",
       "80  a family of robust filters of which the signed...\n",
       "81  transfer learning. Under this setting, we prop...\n",
       "82  statistics and machine learning is to estimate...\n",
       "83  of Kahane (1985). Since then it has served as ...\n",
       "84  simple-to-state optimization problems yields s...\n",
       "\n",
       "[85 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f07824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_enriched_chunks(df, chunk_size=500, overlap=50):\n",
    "    enriched_rows = []\n",
    "\n",
    "    for article_id, row in df.iterrows():\n",
    "        title = row[\"title\"]\n",
    "        authors = \", \".join(row[\"authors\"])\n",
    "        keywords = \", \".join(row[\"cleaned_keywords\"]) if isinstance(row[\"cleaned_keywords\"], list) else \"\"\n",
    "        summary_words = row[\"summary\"].split()\n",
    "        chunks = split_into_chunks(summary_words, chunk_size, overlap)\n",
    "\n",
    "        for chunk_idx, chunk_text in enumerate(chunks):\n",
    "            enriched_text = f\"Title: {title}\\nAuthors: {authors}\\nKeywords: {keywords}\\nText: {chunk_text}\"\n",
    "            enriched_rows.append({\n",
    "                \"article_id\": article_id,\n",
    "                \"chunk_id\": chunk_idx,\n",
    "                \"title\": title,\n",
    "                \"authors\": authors,\n",
    "                \"keywords\": keywords,\n",
    "                \"chunk\": chunk_text,\n",
    "                \"enriched_text\": enriched_text\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(enriched_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6a33d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_chunks = make_enriched_chunks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43c2f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>chunk</th>\n",
       "      <th>enriched_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonparametric local polynomial regression for ...</td>\n",
       "      <td>Moritz Jirak, Alois Kneip, Alexander Meister, ...</td>\n",
       "      <td>space, polynomial, nonparametric local, hilber...</td>\n",
       "      <td>We consider nonparametric regression with func...</td>\n",
       "      <td>Title: Nonparametric local polynomial regressi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Revisiting poverty measures using quantile fun...</td>\n",
       "      <td>N. Unnikrishnan Nair, S. M. Sunoj</td>\n",
       "      <td>flexible quantile function model, measure lite...</td>\n",
       "      <td>In this article we redefine various poverty me...</td>\n",
       "      <td>Title: Revisiting poverty measures using quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Online Bernstein-von Mises theorem</td>\n",
       "      <td>Jeyong Lee, Junhyeok Choi, Minwoo Chae</td>\n",
       "      <td>sequentially update posterior, update posterio...</td>\n",
       "      <td>Online learning is an inferential paradigm in ...</td>\n",
       "      <td>Title: Online Bernstein-von Mises theorem\\nAut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Debiasing Continuous-time Nonlinear Autoregres...</td>\n",
       "      <td>Simon Kuang, Xinfan Lin</td>\n",
       "      <td>debiase continuous - time nonlinear, ordinary ...</td>\n",
       "      <td>We study how to identify a class of continuous...</td>\n",
       "      <td>Title: Debiasing Continuous-time Nonlinear Aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>A Generalized Tangent Approximation Framework ...</td>\n",
       "      <td>Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. ...</td>\n",
       "      <td>framework strongly super - gaussian, analysis ...</td>\n",
       "      <td>Tangent approximation form a popular class of ...</td>\n",
       "      <td>Title: A Generalized Tangent Approximation Fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>Optimal Recovery Meets Minimax Estimation</td>\n",
       "      <td>Ronald DeVore, Robert D. Nowak, Rahul Parhi, G...</td>\n",
       "      <td>recovery, estimation fundamental problem, rate...</td>\n",
       "      <td>A fundamental problem in statistics and machin...</td>\n",
       "      <td>Title: Optimal Recovery Meets Minimax Estimati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>0</td>\n",
       "      <td>Stronger Neyman Regret Guarantees for Adaptive...</td>\n",
       "      <td>Georgy Noarov, Riccardo Fogliato, Martin Bertr...</td>\n",
       "      <td>potential outcome, treatment effect, potential...</td>\n",
       "      <td>We study the design of adaptive, sequential ex...</td>\n",
       "      <td>Title: Stronger Neyman Regret Guarantees for A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>Invariance principle for the Gaussian Multipli...</td>\n",
       "      <td>Mriganka Basu Roy Chowdhury, Shirshendu Ganguly</td>\n",
       "      <td>random fractal measure obtain, non-gaussian lo...</td>\n",
       "      <td>Gaussian multiplicative chaos (GMC) is a canon...</td>\n",
       "      <td>Title: Invariance principle for the Gaussian M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>0</td>\n",
       "      <td>On a class of high dimensional linear regressi...</td>\n",
       "      <td>Ying-Ao Wang, Yunyi Zhang, Ye Zhang</td>\n",
       "      <td>inspire classical, broad class linear, method,...</td>\n",
       "      <td>In this paper, we introduce a unified framewor...</td>\n",
       "      <td>Title: On a class of high dimensional linear r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>On the admissibility of bounds on the mean of ...</td>\n",
       "      <td>Erik Learned-Miller</td>\n",
       "      <td>distribution, bound, finite set real, scalar p...</td>\n",
       "      <td>We address the problem of producing a lower bo...</td>\n",
       "      <td>Title: On the admissibility of bounds on the m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id  chunk_id                                              title  \\\n",
       "0             0         0  Nonparametric local polynomial regression for ...   \n",
       "1             1         0  Revisiting poverty measures using quantile fun...   \n",
       "2             2         0                 Online Bernstein-von Mises theorem   \n",
       "3             3         0  Debiasing Continuous-time Nonlinear Autoregres...   \n",
       "4             4         0  A Generalized Tangent Approximation Framework ...   \n",
       "..          ...       ...                                                ...   \n",
       "245         245         0          Optimal Recovery Meets Minimax Estimation   \n",
       "246         246         0  Stronger Neyman Regret Guarantees for Adaptive...   \n",
       "247         247         0  Invariance principle for the Gaussian Multipli...   \n",
       "248         248         0  On a class of high dimensional linear regressi...   \n",
       "249         249         0  On the admissibility of bounds on the mean of ...   \n",
       "\n",
       "                                               authors  \\\n",
       "0    Moritz Jirak, Alois Kneip, Alexander Meister, ...   \n",
       "1                    N. Unnikrishnan Nair, S. M. Sunoj   \n",
       "2               Jeyong Lee, Junhyeok Choi, Minwoo Chae   \n",
       "3                              Simon Kuang, Xinfan Lin   \n",
       "4    Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. ...   \n",
       "..                                                 ...   \n",
       "245  Ronald DeVore, Robert D. Nowak, Rahul Parhi, G...   \n",
       "246  Georgy Noarov, Riccardo Fogliato, Martin Bertr...   \n",
       "247    Mriganka Basu Roy Chowdhury, Shirshendu Ganguly   \n",
       "248                Ying-Ao Wang, Yunyi Zhang, Ye Zhang   \n",
       "249                                Erik Learned-Miller   \n",
       "\n",
       "                                              keywords  \\\n",
       "0    space, polynomial, nonparametric local, hilber...   \n",
       "1    flexible quantile function model, measure lite...   \n",
       "2    sequentially update posterior, update posterio...   \n",
       "3    debiase continuous - time nonlinear, ordinary ...   \n",
       "4    framework strongly super - gaussian, analysis ...   \n",
       "..                                                 ...   \n",
       "245  recovery, estimation fundamental problem, rate...   \n",
       "246  potential outcome, treatment effect, potential...   \n",
       "247  random fractal measure obtain, non-gaussian lo...   \n",
       "248  inspire classical, broad class linear, method,...   \n",
       "249  distribution, bound, finite set real, scalar p...   \n",
       "\n",
       "                                                 chunk  \\\n",
       "0    We consider nonparametric regression with func...   \n",
       "1    In this article we redefine various poverty me...   \n",
       "2    Online learning is an inferential paradigm in ...   \n",
       "3    We study how to identify a class of continuous...   \n",
       "4    Tangent approximation form a popular class of ...   \n",
       "..                                                 ...   \n",
       "245  A fundamental problem in statistics and machin...   \n",
       "246  We study the design of adaptive, sequential ex...   \n",
       "247  Gaussian multiplicative chaos (GMC) is a canon...   \n",
       "248  In this paper, we introduce a unified framewor...   \n",
       "249  We address the problem of producing a lower bo...   \n",
       "\n",
       "                                         enriched_text  \n",
       "0    Title: Nonparametric local polynomial regressi...  \n",
       "1    Title: Revisiting poverty measures using quant...  \n",
       "2    Title: Online Bernstein-von Mises theorem\\nAut...  \n",
       "3    Title: Debiasing Continuous-time Nonlinear Aut...  \n",
       "4    Title: A Generalized Tangent Approximation Fra...  \n",
       "..                                                 ...  \n",
       "245  Title: Optimal Recovery Meets Minimax Estimati...  \n",
       "246  Title: Stronger Neyman Regret Guarantees for A...  \n",
       "247  Title: Invariance principle for the Gaussian M...  \n",
       "248  Title: On a class of high dimensional linear r...  \n",
       "249  Title: On the admissibility of bounds on the m...  \n",
       "\n",
       "[250 rows x 7 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enriched_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1724f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40a8c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_chunks = df_enriched_chunks.groupby(\"article_id\")[\"chunk\"].apply(lambda chunks: \" \".join(chunks)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a88b3e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'We consider nonparametric regression with functional covariates, that is, they are elements of an infinite-dimensional Hilbert space. A locally polynomial estimator is constructed, where an orthonormal basis and various tuning parameters remain to be selected. We provide a general asymptotic upper bound on the estimation error and show that this procedure achieves polynomial convergence rates under appropriate tuning and supersmoothness of the regression function. Such polynomial convergence rates have usually been considered to be non-attainable in nonparametric functional regression without any additional strong structural constraints such as linearity of the regression function.',\n",
       " 1: 'In this article we redefine various poverty measures in literature in terms of quantile functions instead of distribution functions in the prevailing approach. This enables provision for alternative methodology for poverty measurement and analysis along with some new results that are difficult to obtain in the existing framework. Several flexible quantile function models that can enrich the existing ones are proposed and their utility is demonstrated for real data.',\n",
       " 2: 'Online learning is an inferential paradigm in which parameters are updated incrementally from sequentially available data, in contrast to batch learning, where the entire dataset is processed at once. In this paper, we assume that mini-batches from the full dataset become available sequentially. The Bayesian framework, which updates beliefs about unknown parameters after observing each mini-batch, is naturally suited for online learning. At each step, we update the posterior distribution using the current prior and new observations, with the updated posterior serving as the prior for the next step. However, this recursive Bayesian updating is rarely computationally tractable unless the model and prior are conjugate. When the model is regular, the updated posterior can be approximated by a normal distribution, as justified by the Bernstein-von Mises theorem. We adopt a variational approximation at each step and investigate the frequentist properties of the final posterior obtained through this sequential procedure. Under mild assumptions, we show that the accumulated approximation error becomes negligible once the mini-batch size exceeds a threshold depending on the parameter dimension. As a result, the sequentially updated posterior is asymptotically indistinguishable from the full posterior.',\n",
       " 3: 'We study how to identify a class of continuous-time nonlinear systems defined by an ordinary differential equation affine in the unknown parameter. We define a notion of asymptotic consistency as $(n, h) \\\\to (\\\\infty, 0)$, and we achieve it using a family of direct methods where the first step is differentiating a noisy time series and the second step is a plug-in linear estimator. The first step, differentiation, is a signal processing adaptation of the nonparametric statistical technique of local polynomial regression. The second step, generalized linear regression, can be consistent using a least squares estimator, but we demonstrate two novel bias corrections that improve the accuracy for finite $h$. These methods significantly broaden the class of continuous-time systems that can be consistently estimated by direct methods.',\n",
       " 4: 'Tangent approximation form a popular class of variational inference (VI) techniques for Bayesian analysis in intractable non-conjugate models. It is based on the principle of convex duality to construct a minorant of the marginal likelihood, making the problem tractable. Despite its extensive applications, a general methodology for tangent approximation encompassing a large class of likelihoods beyond logit models with provable optimality guarantees is still elusive. In this article, we propose a general Tangent Approximation based Variational InferencE (TAVIE) framework for strongly super-Gaussian (SSG) likelihood functions which includes a broad class of flexible probability models. Specifically, TAVIE obtains a quadratic lower bound of the corresponding log-likelihood, thus inducing conjugacy with Gaussian priors over the model parameters. Under mild assumptions on the data-generating process, we demonstrate the optimality of our proposed methodology in the fractional likelihood setup. Furthermore, we illustrate the empirical performance of TAVIE through extensive simulations and an application on the U.S. 2000 Census real data.',\n",
       " 5: 'Diffusion models are distinguished by their exceptional generative performance, particularly in producing high-quality samples through iterative denoising. While current theory suggests that the number of denoising steps required for accurate sample generation should scale linearly with data dimension, this does not reflect the practical efficiency of widely used algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper investigates the effectiveness of diffusion models in sampling from complex high-dimensional distributions that can be well-approximated by Gaussian Mixture Models (GMMs). For these distributions, our main result shows that DDPM takes at most $\\\\widetilde{O}(1/\\\\varepsilon)$ iterations to attain an $\\\\varepsilon$-accurate distribution in total variation (TV) distance, independent of both the ambient dimension $d$ and the number of components $K$, up to logarithmic factors. Furthermore, this result remains robust to score estimation errors. These findings highlight the remarkable effectiveness of diffusion models in high-dimensional settings given the universal approximation capability of GMMs, and provide theoretical insights into their practical success.',\n",
       " 6: 'Score estimation is the backbone of score-based generative models (SGMs), especially denoising diffusion probabilistic models (DDPMs). A key result in this area shows that with accurate score estimates, SGMs can efficiently generate samples from any realistic data distribution (Chen et al., ICLR\\'23; Lee et al., ALT\\'23). This distribution learning result, where the learned distribution is implicitly that of the sampler\\'s output, does not explain how score estimation relates to classical tasks of parameter and density estimation. This paper introduces a framework that reduces score estimation to these two tasks, with various implications for statistical and computational learning theory: Parameter Estimation: Koehler et al. (ICLR\\'23) demonstrate that a score-matching variant is statistically inefficient for the parametric estimation of multimodal densities common in practice. In contrast, we show that under mild conditions, denoising score-matching in DDPMs is asymptotically efficient. Density Estimation: By linking generation to score estimation, we lift existing score estimation guarantees to $(\\\\epsilon,\\\\delta)$-PAC density estimation, i.e., a function approximating the target log-density within $\\\\epsilon$ on all but a $\\\\delta$-fraction of the space. We provide (i) minimax rates for density estimation over H\\\\\"older classes and (ii) a quasi-polynomial PAC density estimation algorithm for the classical Gaussian location mixture model, building on and addressing an open problem from Gatmiry et al. (arXiv\\'24). Lower Bounds for Score Estimation: Our framework offers the first principled method to prove computational lower bounds for score estimation across general distributions. As an application, we establish cryptographic lower bounds for score estimation in general Gaussian mixture models, conceptually recovering Song\\'s (NeurIPS\\'24) result and advancing his key open problem.',\n",
       " 7: 'Exoplanets play an important role in understanding the mechanics of planetary system formation and orbital evolution. In this context the correlations of different parameters of the planets and their host star are useful guides in the search for explanatory mechanisms. Based on a reanalysis of the data set from \\\\cite{figueria14} we study the as of now still poorly understood correlation between planetary surface gravity and stellar activity of Hot Jupiters. Unfortunately, data collection often suffers from measurement errors due to complicated and indirect measurement setups, rendering standard inference techniques unreliable. We present new methods to estimate and test for correlations in a deconvolution framework and thereby improve the state of the art analysis of the data in two directions. First, we are now able to account for additive measurement errors which facilitates reliable inference. Second we test for relevant changes, i.e. we are testing for correlations exceeding a certain threshold $\\\\Delta$. This reflects the fact that small nonzero correlations are to be expected for real life data almost always and that standard statistical tests will therefore always reject the null of no correlation given sufficient data. Our theory focuses on quantities that can be estimated by U-Statistics which contain a variety of correlation measures. We propose a bootstrap test and establish its theoretical validity. As a by product we also obtain confidence intervals. Applying our methods to the Hot Jupiter data set from \\\\cite{figueria14}, we observe that taking into account the measurement errors yields smaller point estimates and the null of no relevant correlation is rejected only for very small $\\\\Delta$. This demonstrates the importance of considering the impact of measurement errors to avoid misleading conclusions from the resulting statistical analysis.',\n",
       " 8: 'We propose a neural-network based survival model (SurvSurf) specifically designed for direct and simultaneous probabilistic prediction of the first hitting time of sequential events from baseline. Unlike existing models, SurvSurf is theoretically guaranteed to never violate the monotonic relationship between the cumulative incidence functions of sequential events, while allowing nonlinear influence from predictors. It also incorporates implicit truths for unobserved intermediate events in model fitting, and supports both discrete and continuous time and events. We also identified a variant of the Integrated Brier Score (IBS) that showed robust correlation with the mean squared error (MSE) between the true and predicted probabilities by accounting for implied truths about the missing intermediate events. We demonstrated the superiority of SurvSurf compared to modern and traditional predictive survival models in two simulated datasets and two real-world datasets, using MSE, the more robust IBS and by measuring the extent of monotonicity violation.',\n",
       " 9: 'The drift sequential parameter estimation problems for the Cox-Ingersoll-Ross (CIR) processes under the limited duration of observation are studied. Truncated sequential estimation methods for both scalar and {two}-dimensional parameter cases are proposed. In the non-asymptotic setting, for the proposed truncated estimators, the properties of guaranteed mean-square estimation accuracy are established. In the asymptotic formulation, when the observation time tends to infinity, it is shown that the proposed sequential procedures are asymptotically optimal among all possible sequential and non-sequential estimates with an average estimation time less than the fixed observation duration. It also turned out that asymptotically, without degrading the estimation quality, they significantly reduce the observation duration compared to classical non-sequential maximum likelihood estimations based on a fixed observation duration.',\n",
       " 10: 'Existing research on negations primarily focuses on entropy and extropy. Recently, new functions such as varentropy and varextropy have been developed, which can be considered as extensions of entropy and extropy. However, the impact of negation on these extended measures, particularly varentropy and varextropy, has not been extensively explored. To address this gap, this paper investigates the effect of negation on Shannon entropy, varentropy, and varextropy. We explore how the negation of a probability distribution influences these measures, showing that the negated distribution consistently leads to higher values of Shannon entropy, varentropy, and varextropy compared to the original distribution. Additionally, we prove that the negation of a probability distribution maximizes these measures during the process. The paper provides theoretical proofs and a detailed analysis of the behaviour of these measures, contributing to a better understanding of the interplay between probability distributions, negation, and information-theoretic quantities.',\n",
       " 11: 'We consider the task of Gaussian mean testing, that is, of testing whether a high-dimensional vector perturbed by white noise has large magnitude, or is the zero vector. This question, originating from the signal processing community, has recently seen a surge of interest from the machine learning and theoretical computer science community, and is by now fairly well understood. What is much less understood, and the focus of our work, is how to perform this task under truncation: that is, when the observations (i.i.d.\\\\ samples from the underlying high-dimensional Gaussian) are only observed when they fall in an given subset of the domain $\\\\R^d$. This truncation model, previously studied in the context of learning (instead of testing) the mean vector, has a range of applications, in particular in Economics and Social Sciences. As our work shows, sample truncations affect the complexity of the testing task in a rather subtle and surprising way.',\n",
       " 12: 'This paper analyzes the Gini coefficient estimator for zero-truncated Poisson populations, revealing the presence of bias, and provides a mathematical expression for the bias, along with a bias-corrected estimator, which is evaluated using Monte Carlo simulation methods.',\n",
       " 13: 'The challenge of clustering short text data lies in balancing informativeness with interpretability. Traditional evaluation metrics often overlook this trade-off. Inspired by linguistic principles of communicative efficiency, this paper investigates the optimal number of clusters by quantifying the trade-off between informativeness and cognitive simplicity. We use large language models (LLMs) to generate cluster names and evaluate their effectiveness through semantic density, information theory, and clustering accuracy. Our results show that Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM, increases semantic density compared to random assignment, effectively grouping similar bios. However, as clusters increase, interpretability declines, as measured by a generative LLM\\'s ability to correctly assign bios based on cluster names. A logistic regression analysis confirms that classification accuracy depends on the semantic similarity between bios and their assigned cluster names, as well as their distinction from alternatives. These findings reveal a \"Goldilocks zone\" where clusters remain distinct yet interpretable. We identify an optimal range of 16-22 clusters, paralleling linguistic efficiency in lexical categorization. These insights inform both theoretical models and practical applications, guiding future research toward optimising cluster interpretability and usefulness.',\n",
       " 14: 'This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a \"local average quantile treatment effect\", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fr\\\\\\'echet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution.',\n",
       " 15: 'We study a multivariate Hawkes process as a model for time-continuous relational event networks. The model does not assume the network to be known, it includes covariates, and it allows for both common drivers, parameters common to all the actors in the network, and also local parameters specific for each actor. We derive rates of convergence for all of the model parameters when both the number of actors and the time horizon tends to infinity. To prevent an exploding network, sparseness is assumed. We also discuss numerical aspects.',\n",
       " 16: \"Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its power, the performance of IS is often highly sensitive to the choice of the proposal distribution and frequently requires stochastic calibration techniques. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a unique challenge: the decision and the IS distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both the analysis of convergence for decision iterates and the efficiency of the IS scheme. In this paper, we propose an iterative gradient-based algorithm that jointly updates the decision variable and the IS distribution without requiring time-scale separation between the two. Our method achieves the lowest possible asymptotic variance and guarantees global convergence under convexity of the objective and mild assumptions on the IS distribution family. Furthermore, we show that these properties are preserved under linear constraints by incorporating a recent variant of Nesterov's dual averaging method.\",\n",
       " 17: 'We consider a classical First-order Vector AutoRegressive (VAR(1)) model, where we interpret the autoregressive interaction matrix as influence relationships among the components of the VAR(1) process that can be encoded by a weighted directed graph. A majority of previous work studies the structural identifiability of the graph based on time series observations and therefore relies on dynamical information. In this work we assume that an equilibrium exists, and study instead the identifiability of the graph from the stationary distribution, meaning that we seek a way to reconstruct the influence graph underlying the dynamic network using only static information. We use an approach from algebraic statistics that characterizes models using the Jacobian matroids associated with the parametrization of the models, and we introduce sufficient graphical conditions under which different graphs yield distinct steady-state distributions. Additionally, we illustrate how our results could be applied to characterize networks inspired by ecological research.',\n",
       " 18: 'Given i.i.d. observations uniformly distributed on a closed submanifold of the Euclidean space, we study higher-order generalizations of graph Laplacians, so-called Hodge Laplacians on graphs, as approximations of the Laplace-Beltrami operator on differential forms. Our main result is a high-probability error bound for the associated Dirichlet forms. This bound improves existing Dirichlet form error bounds for graph Laplacians in the context of Laplacian Eigenmaps, and it provides insights into the Betti numbers studied in topological data analysis and the complementing positive part of the spectrum.',\n",
       " 19: 'Nonparametric regression with random design is considered. The $L_2$ error with integration with respect to the design measure is used as the error criterion. An over-parametrized deep neural network regression estimate with logistic activation function is defined, where all weights are learned by gradient descent. It is shown that the estimate achieves a nearly optimal rate of convergence in case that the regression function is $(p,C)$--smooth.',\n",
       " 20: 'A new formula for Marchenko-Pastur inversion is derived and used for inference of population linear spectral statistics. The formula allows for estimation of the Stieltjes transform of the population spectral distribution $s_H(z)$, when $z$ is sufficiently far from the support of the population spectral distribution $H$. If the dimension $d$ and the sample size $n$ go to infinity simultaneously such that $\\\\frac{d}{n} \\\\rightarrow c>0$, the estimation error is shown to be asymptotically less than $\\\\frac{n^{\\\\varepsilon}}{n}$ for arbitrary $\\\\varepsilon > 0$. By integrating along a curve around the support of $H$, estimators for population linear spectral statistics are constructed, which benefit from this convergence speed of $\\\\frac{n^{\\\\varepsilon}}{n}$.',\n",
       " 21: 'We consider the problem of estimating the parameters of a supercritical controlled branching process consistently from a single observed trajectory of population size counts. Our goal is to establish which parameters can and cannot be consistently estimated. When a parameter can be consistently estimated, we derive an explicit expression for the estimator. We address these questions in three scenarios: when the distribution of the control function distribution is known, when it is unknown, and when progenitor numbers are observed alongside population size counts. Our results offer a theoretical justification for the common practice in population ecology of estimating demographic and environmental stochasticity using separate observation schemes.',\n",
       " 22: 'Accurate tuning of hyperparameters is crucial to ensure that models can generalise effectively across different settings. In this paper, we present theoretical guarantees for hyperparameter selection using variational Bayes in the nonparametric regression model. We construct a variational approximation to a hierarchical Bayes procedure, and derive upper bounds for the contraction rate of the variational posterior in an abstract setting. The theory is applied to various Gaussian process priors and variational classes, resulting in minimax optimal rates. Our theoretical results are accompanied with numerical analysis both on synthetic and real world data sets.',\n",
       " 23: 'In this paper, we study the problem of multivariate shuffled linear regression, where the correspondence between predictors and responses in a linear model is obfuscated by a latent permutation. Specifically, we investigate the model $Y=\\\\tfrac{1}{\\\\sqrt{1+\\\\sigma^2}}(\\\\Pi_* X Q_* + \\\\sigma Z)$, where $X$ is an $n*d$ standard Gaussian design matrix, $Z$ is an $n*m$ Gaussian noise matrix, $\\\\Pi_*$ is an unknown $n*n$ permutation matrix, and $Q_*$ is an unknown $d*m$ on the Grassmanian manifold satisfying $Q_*^{\\\\top} Q_* = \\\\mathbb I_m$. Consider the hypothesis testing problem of distinguishing this model from the case where $X$ and $Y$ are independent Gaussian random matrices of sizes $n*d$ and $n*m$, respectively. Our results reveal a phase transition phenomenon in the performance of low-degree polynomial algorithms for this task. (1) When $m=o(d)$, we show that all degree-$D$ polynomials fail to distinguish these two models even when $\\\\sigma=0$, provided with $D^4=o\\\\big( \\\\tfrac{d}{m} \\\\big)$. (2) When $m=d$ and $\\\\sigma=\\\\omega(1)$, we show that all degree-$D$ polynomials fail to distinguish these two models provided with $D=o(\\\\sigma)$. (3) When $m=d$ and $\\\\sigma=o(1)$, we show that there exists a constant-degree polynomial that strongly distinguish these two models. These results establish a smooth transition in the effectiveness of low-degree polynomial algorithms for this problem, highlighting the interplay between the dimensions $m$ and $d$, the noise level $\\\\sigma$, and the computational complexity of the testing task.',\n",
       " 24: 'We introduce a new approach for estimating the number of spikes in a general class of spiked covariance models without directly computing the eigenvalues of the sample covariance matrix. This approach is based on the Lanczos algorithm and the asymptotic properties of the associated Jacobi matrix and its Cholesky factorization. A key aspect of the analysis is interpreting the eigenvector spectral distribution as a perturbation of its asymptotic counterpart. The specific exponential-type asymptotics of the Jacobi matrix enables an efficient approximation of the Stieltjes transform of the asymptotic spectral distribution via a finite continued fraction. As a consequence, we also obtain estimates for the density of the asymptotic distribution and the location of outliers. We provide consistency guarantees for our proposed estimators, proving their convergence in the high-dimensional regime. We demonstrate that, when applied to standard spiked covariance models, our approach outperforms existing methods in computational efficiency and runtime, while still maintaining robustness to exotic population covariances.',\n",
       " 25: \"The behavior of the random feature model in the high-dimensional regression framework has become a popular issue of interest in the machine learning literature}. This model is generally considered for feature vectors $x_i = \\\\Sigma^{1/2} x_i'$, where $x_i'$ is a random vector made of independent and identically distributed (iid) entries, and $\\\\Sigma$ is a positive definite matrix representing the covariance of the features. In this paper, we move beyond {\\\\CB this standard assumption by studying the performances of the random features model in the setting of non-iid feature vectors}. Our approach is related to the analysis of the spectrum of large random matrices through random matrix theory (RMT) {\\\\CB and free probability} results. We turn to the analysis of non-iid data by using the notion of variance profile {\\\\CB which} is {\\\\CB well studied in RMT.} Our main contribution is then the study of the limits of the training and {\\\\CB prediction} risks associated to the ridge estimator in the random features model when its dimensions grow. We provide asymptotic equivalents of these risks that capture the behavior of ridge regression with random features in a {\\\\CB high-dimensional} framework. These asymptotic equivalents, {\\\\CB which prove to be sharp in numerical experiments}, are retrieved by adapting, to our setting, established results from operator-valued free probability theory. Moreover, {\\\\CB for various classes of random feature vectors that have not been considered so far in the literature}, our approach allows to show the appearance of the double descent phenomenon when the ridge regularization parameter is small enough.\",\n",
       " 26: 'An e-variable for a family of distributions $\\\\mathcal{P}$ is a nonnegative random variable whose expected value under every distribution in $\\\\mathcal{P}$ is at most one. E-variables have recently been recognized as fundamental objects in hypothesis testing, and a rapidly growing body of work has attempted to derive admissible or optimal e-variables for various families $\\\\mathcal{P}$. In this paper, we study classes $\\\\mathcal{P}$ that are specified by constraints. Simple examples include bounds on the moments, but our general theory covers arbitrary sets of measurable constraints. Our main results characterize the set of all e-variables for such classes, as well as maximal ones. Three case studies illustrate the scope of our theory: finite constraint sets, one-sided sub-$\\\\psi$ distributions, and distributions invariant under a group of symmetries. In particular, we generalize recent results of Clerico (2024a) by dropping all assumptions on the constraints.',\n",
       " 27: \"We exploit the multiplicative structure of P\\\\'olya Tree priors for density and differential entropy estimation in $p$-dimensions. We establish: (i) a representation theorem of entropy functionals and (ii) conditions on the parameters of P\\\\'olya Trees to obtain Kullback-Leibler and Total Variation consistency for vectors with compact support. Those results motivate a novel differential entropy estimator that is consistent in probability for compact supported vectors under mild conditions. In order to enable applications of both results, we also provide a theoretical motivation for the truncation of Univariate P\\\\'olya Trees at level $3 \\\\log_2 n $.\",\n",
       " 28: 'We consider the problem of sequential hypothesis testing by betting. For a general class of composite testing problems -- which include bounded mean testing, equal mean testing for bounded random tuples, and some key ingredients of two-sample and independence testing as special cases -- we show that any $e$-process satisfying a certain sublinear regret bound is adaptively, asymptotically, and almost surely log-optimal for a composite alternative. This is a strong notion of optimality that has not previously been established for the aforementioned problems and we provide explicit test supermartingales and $e$-processes satisfying this notion in the more general case. Furthermore, we derive matching lower and upper bounds on the expected rejection time for the resulting sequential tests in all of these cases. The proofs of these results make weak, algorithm-agnostic moment assumptions and rely on a general-purpose proof technique involving the aforementioned regret and a family of numeraire portfolios. Finally, we discuss how all of these theorems hold in a distribution-uniform sense, a notion of log-optimality that is stronger still and seems to be new to the literature.',\n",
       " 29: 'We study the problem of learning a high-density region of an arbitrary distribution over $\\\\mathbb{R}^d$. Given a target coverage parameter $\\\\delta$, and sample access to an arbitrary distribution $D$, we want to output a confidence set $S \\\\subset \\\\mathbb{R}^d$ such that $S$ achieves $\\\\delta$ coverage of $D$, i.e., $\\\\mathbb{P}_{y \\\\sim D} \\\\left[ y \\\\in S \\\\right] \\\\ge \\\\delta$, and the volume of $S$ is as small as possible. This is a central problem in high-dimensional statistics with applications in finding confidence sets, uncertainty quantification, and support estimation. In the most general setting, this problem is statistically intractable, so we restrict our attention to competing with sets from a concept class $C$ with bounded VC-dimension. An algorithm is competitive with class $C$ if, given samples from an arbitrary distribution $D$, it outputs in polynomial time a set that achieves $\\\\delta$ coverage of $D$, and whose volume is competitive with the smallest set in $C$ with the required coverage $\\\\delta$. This problem is computationally challenging even in the basic setting when $C$ is the set of all Euclidean balls. Existing algorithms based on coresets find in polynomial time a ball whose volume is $\\\\exp(\\\\tilde{O}( d/ \\\\log d))$-factor competitive with the volume of the best ball. Our main result is an algorithm that finds a confidence set whose volume is $\\\\exp(\\\\tilde{O}(d^{2/3}))$ factor competitive with the optimal ball having the desired coverage. The algorithm is improper (it outputs an ellipsoid). Combined with our computational intractability result for proper learning balls within an $\\\\exp(\\\\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results provide an interesting separation between proper and (improper) learning of confidence sets.',\n",
       " 30: 'This paper introduces a periodic multivariate Poisson autoregression with potentially infinite memory, with a special focus on the network setting. Using contraction techniques, we study the stability of such a process and provide upper bounds on how fast it reaches the periodically stationary regime. We then propose a computationally efficient Markov approximation using the properties of the exponential function and a density result. Furthermore, we prove the strong consistency of the maximum likelihood estimator for the Markov approximation and empirically test its robustness in the case of misspecification. Our model is applied to the prediction of weekly Rotavirus cases in Berlin, demonstrating superior performance compared to the existing PNAR model.',\n",
       " 31: 'Given a probability measure with density, Fermat distances and density-driven metrics are conformal transformation of the Euclidean metric that shrink distances in high density areas and enlarge distances in low density areas. Although they have been widely studied and have shown to be useful in various machine learning tasks, they are limited to measures with density (with respect to Lebesgue measure, or volume form on manifold). In this paper, by replacing the density with the Distance-to-Measure, we introduce a new metric, the Fermat Distance-to-Measure, defined for any probability measure in R^d. We derive strong stability properties for the Fermat Distance-to-Measure with respect to the measure and propose an estimator from random sampling of the measure, featuring an explicit bound on its convergence speed.',\n",
       " 32: 'This paper presents a unified framework for understanding the methodology and theory behind several different methods in the conformal prediction literature, which includes standard conformal prediction (CP), weighted conformal prediction (WCP), nonexchangeable conformal prediction (NexCP), and randomly-localized conformal prediction (RLCP), among others. At the crux of our framework is the idea that conformal methods are based on revealing partial information about the data at hand, and positing a conditional distribution for the data given the partial information. Different methods arise from different choices of partial information, and of the corresponding (approximate) conditional distribution. In addition to recovering and unifying existing results, our framework leads to both new theoretical guarantees for existing methods, and new extensions of the conformal methodology.',\n",
       " 33: 'We propose new statistical tests, in high-dimensional settings, for testing the independence of two random vectors and their conditional independence given a third random vector. The key idea is simple, i.e., we first transform each component variable to standard normal via its marginal empirical distribution, and we then test for independence and conditional independence of the transformed random vectors using appropriate $L_\\\\infty$-type test statistics. While we are testing some necessary conditions of the independence or the conditional independence, the new tests outperform the 13 frequently used testing methods in a large scale simulation comparison. The advantage of the new tests can be summarized as follows: (i) they do not require any moment conditions, (ii) they allow arbitrary dependence structures of the components among the random vectors, and (iii) they allow the dimensions of random vectors diverge at the exponential rates of the sample size. The critical values of the proposed tests are determined by a computationally efficient multiplier bootstrap procedure. Theoretical analysis shows that the sizes of the proposed tests can be well controlled by the nominal significance level, and the proposed tests are also consistent under certain local alternatives. The finite sample performance of the new tests is illustrated via extensive simulation studies and a real data application.',\n",
       " 34: 'We study the geometry of Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves in binary classification problems. The key finding is that many of the most commonly used binary classification metrics are merely functions of the composition function $G := F_p \\\\circ F_n^{-1}$, where $F_p(\\\\cdot)$ and $F_n(\\\\cdot)$ are the class-conditional cumulative distribution functions of the classifier scores in the positive and negative classes, respectively. This geometric perspective facilitates the selection of operating points, understanding the effect of decision thresholds, and comparison between classifiers. It also helps explain how the shapes and geometry of ROC/PR curves reflect classifier behavior, providing objective tools for building classifiers optimized for specific applications with context-specific constraints. We further explore the conditions for classifier dominance, present analytical and numerical examples demonstrating the effects of class separability and variance on ROC and PR geometries, and derive a link between the positive-to-negative class leakage function $G(\\\\cdot)$ and the Kullback--Leibler divergence. The framework highlights practical considerations, such as model calibration, cost-sensitive optimization, and operating point selection under real-world capacity constraints, enabling more informed approaches to classifier deployment and decision-making.',\n",
       " 35: 'In this work, we are interested in studying the causal effect of an endogenous binary treatment on a dependently censored duration outcome. By dependent censoring, it is meant that the duration time ($T$) and right censoring time ($C$) are not statistically independent of each other, even after conditioning on the measured covariates. The endogeneity issue is handled by making use of a binary instrumental variable for the treatment. To deal with the dependent censoring problem, it is assumed that on the stratum of compliers: (i) $T$ follows a semiparametric proportional hazards model; (ii) $C$ follows a fully parametric model; and (iii) the relation between $T$ and $C$ is modeled by a parametric copula, such that the association parameter can be left unspecified. In this framework, the treatment effect of interest is the complier causal hazard ratio (CCHR). We devise an estimation procedure that is based on a weighted maximum likelihood approach, where the weights are the probabilities of an observation coming from a complier. The weights are estimated non-parametrically in a first stage, followed by the estimation of the CCHR. Novel conditions under which the model is identifiable are given, a two-step estimation procedure is proposed and some important asymptotic properties are established. Simulations are used to assess the validity and finite-sample performance of the estimation procedure. Finally, we apply the approach to estimate the CCHR of both job training programs on unemployment duration and periodic screening examinations on time until death from breast cancer. The data come from the National Job Training Partnership Act study and the Health Insurance Plan of Greater New York experiment respectively.',\n",
       " 36: \"The de Bruijn identity states that Fisher information is the half of the derivative of Shannon differential entropy along heat flow. In the same spirit, in this paper we introduce a generalized version of Fisher information, named as the R\\\\'enyi--Fisher information, which is the half of the derivative of R\\\\'enyi information along heat flow. Based on this R\\\\'enyi--Fisher information, we establish sharp R\\\\'enyi-entropic isoperimetric inequalities, which generalize the classic entropic isoperimetric inequality to the R\\\\'enyi setting. Utilizing these isoperimetric inequalities, we extend the classical Cram\\\\'er--Rao inequality from Fisher information to R\\\\'enyi--Fisher information. Lastly, we use these generalized Cram\\\\'er--Rao inequalities to determine the signs of derivatives of entropy along heat flow, strengthening existing results on the complete monotonicity of entropy.\",\n",
       " 37: 'This paper focuses on nonparametric statistical inference of the hazard rate function of discrete distributions based on $\\\\delta$-record data. We derive the explicit expression of the maximum likelihood estimator and determine its exact distribution, as well as some important characteristics such as its bias and mean squared error. We then discuss the construction of confidence intervals and goodness-of-fit tests. The performance of our proposals is evaluated using simulation methods. Applications to real data are given, as well. The estimation of the hazard rate function based on usual records has been studied in the literature, although many procedures require several samples of records. In contrast, our approach relies on a single sequence of $\\\\delta$-records, simplifying the experimental design and increasing the applicability of the methods.',\n",
       " 38: 'Proper scoring rules have been a subject of growing interest in recent years, not only as tools for evaluation of probabilistic forecasts but also as methods for estimating probability distributions. In this article, we review the mathematical foundations of proper scoring rules including general characterization results and important families of scoring rules. We discuss their role in statistics and machine learning for estimation and forecast evaluation. Furthermore, we comment on interesting developments of their usage in applications.',\n",
       " 39: 'The goal of this paper is to propose a new approach to asymptotic analysis of the finite predictor for stationary sequences. It produces the exact asymptotics of the relative prediction error and the partial correlation coefficients. The assumptions are analytic in nature and applicable to processes with long range dependence. The ARIMA type process driven by the fractional Gaussian noise (fGn), which previously remained elusive, serves as our study case.',\n",
       " 40: \"Empirical likelihood serves as a powerful tool for constructing confidence intervals in nonparametric regression and regression discontinuity designs (RDD). The original empirical likelihood framework can be naturally extended to these settings using local linear smoothers, with Wilks' theorem holding only when an undersmoothed bandwidth is selected. However, the generalization of bias-corrected versions of empirical likelihood under more realistic conditions is non-trivial and has remained an open challenge in the literature. This paper provides a satisfactory solution by proposing a novel approach, referred to as robust empirical likelihood, designed for nonparametric regression and RDD. The core idea is to construct robust weights which simultaneously achieve bias correction and account for the additional variability introduced by the estimated bias, thereby enabling valid confidence interval construction without extra estimation steps involved. We demonstrate that the Wilks' phenomenon still holds under weaker conditions in nonparametric regression, sharp and fuzzy RDD settings. Extensive simulation studies confirm the effectiveness of our proposed approach, showing superior performance over existing methods in terms of coverage probabilities and interval lengths. Moreover, the proposed procedure exhibits robustness to bandwidth selection, making it a flexible and reliable tool for empirical analyses. The practical usefulness is further illustrated through applications to two real datasets.\",\n",
       " 41: 'In this paper, we prove exponential tail bounds for canonical (or degenerate) $U$-statistics and $U$-processes under exponential-type tail assumptions on the kernels. Most of the existing results in the relevant literature often assume bounded kernels or obtain sub-optimal tail behavior under unbounded kernels. We obtain sharp rates and optimal tail behavior under sub-Weibull kernel functions. Some examples from nonparametric and semiparametric statistics literature are considered.',\n",
       " 42: 'Multiple works regarding convergence analysis of Markov chains have led to spectral gap decomposition formulas of the form \\\\[ \\\\mathrm{Gap}(S) \\\\geq c_0 \\\\left[\\\\inf_z \\\\mathrm{Gap}(Q_z)\\\\right] \\\\mathrm{Gap}(\\\\bar{S}), \\\\] where $c_0$ is a constant, $\\\\mathrm{Gap}$ denotes the right spectral gap of a reversible Markov operator, $S$ is the Markov transition kernel (Mtk) of interest, $\\\\bar{S}$ is an idealized or simplified version of $S$, and $\\\\{Q_z\\\\}$ is a collection of Mtks characterizing the differences between $S$ and $\\\\bar{S}$. This type of relationship has been established in various contexts, including: 1. decomposition of Markov chains based on a finite cover of the state space, 2. hybrid Gibbs samplers, and 3. spectral independence and localization schemes. We show that multiple key decomposition results across these domains can be connected within a unified framework, rooted in a simple sandwich structure of $S$. Within the general framework, we establish new instances of spectral gap decomposition for hybrid hit-and-run samplers and hybrid data augmentation algorithms with two intractable conditional distributions. Additionally, we explore several other properties of the sandwich structure, and derive extensions of the spectral gap decomposition formula.',\n",
       " 43: 'Multiparameter persistent homology is a generalization of classical persistent homology, a central and widely-used methodology from topological data analysis, which takes into account density estimation and is an effective tool for data analysis in the presence of noise. Similar to its classical single-parameter counterpart, however, it is challenging to compute and use in practice due to its complex algebraic construction. In this paper, we study a popular and tractable invariant for multiparameter persistent homology in a statistical setting: the multiparameter persistence landscape. We derive a functional central limit theorem for multiparameter persistence landscapes, from which we compute confidence bands, giving rise to one of the first statistical inference methodologies for multiparameter persistence landscapes. We provide an implementation of confidence bands and demonstrate their application in a machine learning task on synthetic data.',\n",
       " 44: 'Real-world networks grow over time; statistical models based on node exchangeability are not appropriate. Instead of constraining the structure of the \\\\textit{distribution} of edges, we propose that the relevant symmetries refer to the \\\\textit{causal structure} between them. We first enumerate the 96 causal directed acyclic graph (DAG) models over pairs of nodes (dyad variables) in a growing network with finite ancestral sets that are invariant to node deletion. We then partition them into 21 classes with ancestral sets that are closed under node marginalization. Several of these classes are remarkably amenable to distributed and asynchronous evaluation. As an example, we highlight a simple model that exhibits flexible power-law degree distributions and emergent phase transitions in sparsity, which we characterize analytically. With few parameters and much conditional independence, our proposed framework provides natural baseline models for causal inference in relational data.',\n",
       " 45: 'We address the problem of nonparametric estimation of the spectral density for a centered stationary Gaussian time series under local differential privacy constraints. Specifically, we propose new interactive privacy mechanisms for three tasks: estimating a single covariance coefficient, estimating the spectral density at a fixed frequency, and estimating the entire spectral density function. Our approach achieves faster rates through a two-stage process: we apply first the Laplace mechanism to the truncated value and then use the former privatized sample to gain knowledge on the dependence mechanism in the time series. For spectral densities belonging to H\\\\\"older and Sobolev smoothness classes, we demonstrate that our estimators improve upon the non-interactive mechanism of Kroll (2024) for small privacy parameter $\\\\alpha$, since the pointwise rates depend on $n\\\\alpha^2$ instead of $n\\\\alpha^4$. Moreover, we show that the rate $(n\\\\alpha^4)^{-1}$ is optimal for estimating a covariance coefficient with non-interactive mechanisms. However, the $L_2$ rate of our interactive estimator is slower than the pointwise rate. We show how to use these estimators to provide a bona-fide locally differentially private covariance matrix estimator.',\n",
       " 46: 'In this paper, we derive power guarantees of some sequential tests for bounded mean under general alternatives. We focus on testing procedures using nonnegative supermartingales which are anytime valid and consider alternatives which coincide asymptotically with the null (e.g. vanishing mean) while still allowing to reject in finite time. Introducing variance constraints, we show that the alternative can be broaden while keeping power guarantees for certain second-order testing procedures. We also compare different test procedures in multidimensional setting using characteristics of the rejection times. Finally, we extend our analysis to other functionals as well as testing and comparing forecasters. Our results are illustrated with numerical simulations including bounded mean testing and comparison of forecasters.',\n",
       " 47: 'Multivariate phase relationships are important to characterize and understand numerous physical, biological, and chemical systems, from electromagnetic waves to neural oscillations. These systems exhibit complex spatiotemporal dynamics and intricate interdependencies among their constituent elements. While classical models of multivariate phase relationships, such as the wave equation and Kuramoto model, give theoretical models to describe phenomena, the development of statistical tools for hypothesis testing and inference for multivariate phase relationships in complex systems remains limited. This paper introduces a novel probabilistic modeling framework to characterize multivariate phase relationships, with wave-like phenomena serving as a key example. This approach describes spatial patterns and interactions between oscillators through a pairwise exponential family distribution. Building upon the literature of graphical model inference, including methods like Ising models, graphical lasso, and interaction screening, this work bridges the gap between classical wave dynamics and modern statistical approaches. Efficient inference methods are introduced, leveraging the Chow-Liu algorithm for directed tree approximations and interaction screening for general graphical models. Simulated experiments demonstrate the utility of these methods for uncovering wave properties and sparse interaction structures, highlighting their applicability to diverse scientific domains. This framework establishes a new paradigm for statistical modeling of multivariate phase relationships, providing a powerful toolset for exploring the complexity of these systems.',\n",
       " 48: \"In survival analysis, the estimation of the proportion of subjects who will never experience the event of interest, termed the cure rate, has received considerable attention recently. Its estimation can be a particularly difficult task when follow-up is not sufficient, that is when the censoring mechanism has a smaller support than the distribution of the target data. In the latter case, non-parametric estimators were recently proposed using extreme value methodology, assuming that the distribution of the susceptible population is in the Fr\\\\'echet or Gumbel max-domains of attraction. In this paper, we take the extreme value techniques one step further, to jointly estimate the cure rate and the extreme value index, using probability plotting methodology, and in particular using the full information contained in the top order statistics. In other words, under sufficient or insufficient follow-up, we reconstruct the immune proportion. To this end, a Peaks-over-Threshold approach is proposed under the Gumbel max-domain assumption. Next, the approach is also transferred to more specific models such as Pareto, log-normal and Weibull tail models, allowing to recognize the most important tail characteristics of the susceptible population. We establish the asymptotic behavior of our estimators under regularization. Though simulation studies, our estimators are show to rival and often outperform established models, even when purely considering cure rate estimation. Finally, we provide an application of our method to Norwegian birth registry data.\",\n",
       " 49: 'Spectral estimation is an important tool in time series analysis, with applications including economics, astronomy, and climatology. The asymptotic theory for non-parametric estimation is well-known but the development of non-asymptotic theory is still ongoing. Our recent work obtained the first non-asymptotic error bounds on the Bartlett and Welch methods for $L$-mixing stochastic processes. The class of $L$-mixing processes contains common models in time series analysis, including autoregressive processes and measurements of geometrically ergodic Markov chains. Our prior analysis assumes that the process has zero mean. While zero-mean assumptions are common, real-world time-series data often has unknown, non-zero mean. In this work, we derive non-asymptotic error bounds for both Bartlett and Welch estimators for $L$-mixing time-series data with unknown means. The obtained error bounds are of $O(\\\\frac{1}{\\\\sqrt{k}})$, where $k$ is the number of data segments used in the algorithm, which are tighter than our previous results under the zero-mean assumption.',\n",
       " 50: \"Exact eigendecomposition of large matrices is very expensive, and it is practically impossible to compute exact eigenvalues. Instead, one may set a more modest goal of approaching the empirical distribution of the eigenvalues, recovering the overall shape of the eigenspectrum. Current approaches to spectral estimation typically work with \\\\emph{moments} of the spectral distribution. These moments are first estimated using Monte Carlo trace estimators, then the estimates are combined to approximate the spectral density. In this article we show how \\\\emph{Kirchhoff forests}, which are random forests on graphs, can be used to estimate certain non-linear moments of very large graph Laplacians. We show how to combine these moments into an estimate of the spectral density. If the estimate's desired precision isn't too high, our approach paves the way to the estimation of a graph's spectrum in time sublinear in the number of links.\",\n",
       " 51: \"In this work, we construct optimal low-rank approximations for the Gaussian posterior distribution in linear Gaussian inverse problems. The parameter space is a separable Hilbert space of possibly infinite dimension, and the data space is assumed to be finite-dimensional. We consider various types of approximation families for the posterior. We first consider approximate posteriors in which the means vary among a class of either structure-preserving or structure-ignoring low-rank transformations of the data, and in which the posterior covariance is kept fixed. We give necessary and sufficient conditions for these approximating posteriors to be equivalent to the exact posterior, for all possible realisations of the data simultaneously. For such approximations, we measure approximation error with the Kullback-Leibler, R\\\\'enyi and Amari $\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, and with the Hellinger distance, all averaged over the data distribution. With these losses, we find the optimal approximations and formulate an equivalent condition for their uniqueness, extending the work in finite dimensions of Spantini et al. (SIAM J. Sci. Comput. 2015). We then consider joint approximation of the mean and covariance, by also varying the posterior covariance over the low-rank updates considered in Part I of this work. For the reverse Kullback-Leibler divergence, we show that the separate optimal approximations of the mean and of the covariance can be combined to yield an optimal joint approximation of the mean and covariance. In addition, we interpret the joint approximation with the optimal structure-ignoring approximate mean in terms of an optimal projector in parameter space.\",\n",
       " 52: 'Consider an observation of a multivariate temporal point process $N$ with law $\\\\mathcal P$ on the time interval $[0,T]$. To test the null hypothesis that $\\\\mathcal P$ belongs to a given parametric family, we construct a convergent compensated counting process to which we apply an innovation martingale transformation. We prove that the resulting process converges weakly to a standard Wiener process. Consequently, taking a suitable functional of this process yields an asymptotically distribution-free goodness-of-fit test for point processes. For several standard tests based on the increments of this transformed process, we establish consistency under alternative hypotheses. Finally, we assess the performance of the proposed testing procedure through a Monte Carlo simulation study and illustrate its practical utility with two real-data examples.',\n",
       " 53: 'In this paper, in a multivariate setting we derive near optimal rates of convergence in the minimax sense for estimating partial derivatives of the mean function for functional data observed under a fixed synchronous design over H\\\\\"older smoothness classes. We focus on the supremum norm since it corresponds to the visualisation of the estimation error, and is closely related to the construction of uniform confidence bands. In contrast to mean function estimation, for derivative estimation the smoothness of the paths of the processes is crucial for the rates of convergence. On the one hand, if the paths have higher-order smoothness than the order of the partial derivative to be estimated, the parametric $\\\\sqrt n$ rate can be achieved under sufficiently dense design. On the other hand, for processes with rough paths of lower-order smoothness, we show that the rates of convergence are necessarily slower than the parametric rate, and determine a near-optimal rate at which estimation is still possible. We implement a multivariate local polynomial derivative estimator and illustrate its finite-sample performance in a simulation as well as for two real-data sets. To assess the smoothness of the sample paths in the applications we further discuss a method based on comparing restricted estimates of the partial derivatives of the covariance kernel.',\n",
       " 54: 'We introduce a new version of the KL-divergence for Gaussian distributions which is based on Wasserstein geometry and referred to as WKL-divergence. We show that this version is consistent with the geometry of the sample space ${\\\\Bbb R}^n$. In particular, we can evaluate the WKL-divergence of the Dirac measures concentrated in two points which turns out to be proportional to the squared distance between these points.',\n",
       " 55: \"For linear inverse problems with Gaussian priors and Gaussian observation noise, the posterior is Gaussian, with mean and covariance determined by the conditioning formula. Using the Feldman-Hajek theorem, we analyse the prior-to-posterior update and its low-rank approximation for infinite-dimensional Hilbert parameter spaces and finite-dimensional observations. We show that the posterior distribution differs from the prior on a finite-dimensional subspace, and construct low-rank approximations to the posterior covariance, while keeping the mean fixed. Since in infinite dimensions, not all low-rank covariance approximations yield approximate posterior distributions which are equivalent to the posterior and prior distribution, we characterise the low-rank covariance approximations which do yield this equivalence, and their respective inverses, or `precisions'. For such approximations, a family of measure approximation problems is solved by identifying the low-rank approximations which are optimal for various losses simultaneously. These loss functions include the family of R\\\\'enyi divergences, the Amari $\\\\alpha$-divergences for $\\\\alpha\\\\in(0,1)$, the Hellinger metric and the Kullback-Leibler divergence. Our results extend those of Spantini et al. (SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide theoretical underpinning for the construction of low-rank approximations of discretised versions of the infinite-dimensional inverse problem, by formulating discretization independent results.\",\n",
       " 56: 'Species sampling processes have long served as the framework for studying random discrete distributions. However, their statistical applicability is limited when partial exchangeability is assumed as probabilistic invariance for the observables. Despite numerous discrete models for partially exchangeable observations, a unifying framework is currently missing, leaving many questions about the induced learning mechanisms unanswered in this setting. To fill this gap, we consider the natural extension of species sampling models to a multivariate framework, obtaining a general class of models characterized by their partially exchangeable partition probability function. A notable subclass, named regular multivariate species sampling models, exists among these models. In the subclass, dependence across processes is accurately captured by the correlation among them: a correlation of one equals full exchangeability and a null correlation corresponds to independence. Regular multivariate species sampling models encompass discrete processes for partial exchangeable data used in Bayesian models, thereby highlighting their core distributional properties and providing a means for developing new models.',\n",
       " 57: 'We consider the Wigner matrix $W_{n}$ of dimension $n \\\\times n$ as $n \\\\to \\\\infty$. The objective of this paper is two folds: first we construct an operator $\\\\mathcal{W}$ on a suitable Hilbert space $\\\\mathcal{H}$ and then define a suitable notion of convergence such that the matrices $W_{n}$ converge in that notion of convergence to $\\\\mathcal{W}$. We further investigate some properties of $\\\\mathcal{W}$ and $\\\\mathcal{H}$. We show that $\\\\mathcal{H}$ is a nontrivial extension of $L^{2}[0,1]$ with respect to the Lebesgue measure and the spectral measure of $\\\\mathcal{W}$ at any function $f \\\\in L^{2}[0,1]$ is almost surely the semicircular law.',\n",
       " 58: 'Selective prediction, where a model has the option to abstain from making a decision, is crucial for machine learning applications in which mistakes are costly. In this work, we focus on distributional regression and introduce a framework that enables the model to abstain from estimation in situations of high uncertainty. We refer to this approach as distributional regression with reject option, inspired by similar concepts in classification and regression with reject option. We study the scenario where the rejection rate is fixed. We derive a closed-form expression for the optimal rule, which relies on thresholding the entropy function of the Continuous Ranked Probability Score (CRPS). We propose a semi-supervised estimation procedure for the optimal rule, using two datasets: the first, labeled, is used to estimate both the conditional distribution function and the entropy function of the CRPS, while the second, unlabeled, is employed to calibrate the desired rejection rate. Notably, the control of the rejection rate is distribution-free. Under mild conditions, we show that our procedure is asymptotically as effective as the optimal rule, both in terms of error rate and rejection rate. Additionally, we establish rates of convergence for our approach based on distributional k-nearest neighbor. A numerical analysis on real-world datasets demonstrates the strong performance of our procedure',\n",
       " 59: 'Estimating the mode of a unimodal distribution is a classical problem in statistics. Although there are several approaches for point-estimation of mode in the literature, very little has been explored about the interval-estimation of mode. Our work proposes a collection of novel methods of obtaining finite sample valid confidence set of the mode of a unimodal distribution. We analyze the behaviour of the width of the proposed confidence sets under some regularity assumptions of the density about the mode and show that the width of these confidence sets shrink to zero near optimally. Simply put, we show that it is possible to build finite sample valid confidence sets for the mode that shrink to a singleton as sample size increases. We support the theoretical results by showing the performance of the proposed methods on some synthetic data-sets. We believe that our confidence sets can be improved both in construction and in terms of rate.',\n",
       " 60: \"We study the bias and the mean-squared error of the maximum likelihood estimators (MLE) of parameters associated with a two-parameter mean-reverting process for a finite time $T$. Using the likelihood ratio process, we derive the expressions for MLEs, then compute the bias and the MSE via the change of measure and Ito's formula. We apply the derived expressions to the general Ornstein-Uhlenbeck process, where the bias and the MSE are numerically computed through a joint moment-generating function of key functionals of the O-U process. A numerical study is provided to illustrate the behaviour of bias and the MSE for the MLE of the mean-reverting speed parameter.\",\n",
       " 61: \"In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``projection-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis.\",\n",
       " 62: 'We propose to model the records of the maximum Drawdown in capital markets by means a Piecewise Deterministic Markov Process (PDMP). We derive statistical results such as the mean and variance that describes the sequence of maximum Drawdown records. In addition, we developed a simulation study and techniques for estimating the parameters governing the stochastic process, using a practical example in the capital market to illustrate the procedure.',\n",
       " 63: 'This paper addresses the problem of detecting change points in the spectral density of time series, motivated by EEG analysis of seizure patients. Seizures disrupt coherence and functional connectivity, necessitating precise detection. Departing from traditional parametric approaches, we utilize the Wold decomposition, representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy, we introduce an optimal rate method with an asymptotic distribution, facilitating the construction of confidence intervals. The proposed method effectively identifies seizure onset in EEG data and extends to event detection in video data. Comprehensive numerical experiments demonstrate its superior performance compared to existing techniques.',\n",
       " 64: 'Under certain conditions, the largest eigenvalue of a sample covariance matrix undergoes a well-known phase transition when the sample size $n$ and data dimension $p$ diverge proportionally. In the subcritical regime, this eigenvalue has fluctuations of order $n^{-2/3}$ that can be approximated by a Tracy-Widom distribution, while in the supercritical regime, it has fluctuations of order $n^{-1/2}$ that can be approximated with a Gaussian distribution. However, the statistical problem of determining which regime underlies a given dataset is far from resolved. We develop a new testing framework and procedure to address this problem. In particular, we demonstrate that the procedure has an asymptotically controlled level, and that it is power consistent for certain alternatives. Also, this testing procedure enables the design a new bootstrap method for approximating the distributions of functionals of the leading sample eigenvalues within the subcritical regime -- which is the first such method that is supported by theoretical guarantees.',\n",
       " 65: \"We developed a mathematical setup inspired by Buyse's generalized pairwise comparisons to define a notion of optimal individualized treatment rule (ITR) in the presence of prioritized outcomes in a randomized controlled trial, terming such an ITR pairwise optimal. We present two approaches to estimate pairwise optimal ITRs. The first is a variant of the k-nearest neighbors algorithm. The second is a meta-learner based on a randomized bagging scheme, allowing the use of any classification algorithm for constructing an ITR. We study the behavior of these estimation schemes from a theoretical standpoint and through Monte Carlo simulations and illustrate their use on trial data.\",\n",
       " 66: 'Significant treatment effects are often emphasized when interpreting and summarizing empirical findings in studies that estimate multiple, possibly many, treatment effects. Under this kind of selective reporting, conventional treatment effect estimates may be biased and their corresponding confidence intervals may undercover the true effect sizes. We propose new estimators and confidence intervals that provide valid inferences on the effect sizes of the significant effects after multiple hypothesis testing. Our methods are based on the principle of selective conditional inference and complement a wide range of tests, including step-up tests and bootstrap-based step-down tests. Our approach is scalable, allowing us to study an application with over 370 estimated effects. We justify our procedure for asymptotically normal treatment effect estimators. We provide two empirical examples that demonstrate bias correction and confidence interval adjustments for significant effects. The magnitude and direction of the bias correction depend on the correlation structure of the estimated effects and whether the interpretation of the significant effects depends on the (in)significance of other effects.',\n",
       " 67: 'We study the consistency and weak convergence of the conditional tail function and conditional Hill estimators under broad dependence assumptions for a heavy-tailed response sequence and a covariate sequence. Consistency is established under $\\\\alpha$-mixing, while asymptotic normality follows from $\\\\beta$-mixing and second-order conditions. A key aspect of our approach is its versatile functional formulation in terms of the conditional tail process. Simulations demonstrate its performance across dependence scenarios. We apply our method to extreme event modeling in the oil industry, revealing distinct tail behaviors under varying conditioning values.',\n",
       " 68: 'The extremal dependence structure of a regularly varying $d$-dimensional random vector can be described by its angular measure. The standard nonparametric estimator of this measure is the empirical measure of the observed angles of the $k$ random vectors with largest norm, for a suitably chosen number $k$. Due to the curse of dimensionality, for moderate or large $d$, this estimator is often inaccurate. If the angular measure is concentrated on a vicinity of a lower dimensional subspace, then first projecting the data on a lower dimensional subspace obtained by a principal component analysis of the angles of extreme observations can substantially improve the performance of the estimator. We derive the asymptotic behavior of such PCA projections and the resulting excess risk. In particular, it is shown that, under mild conditions, the excess risk (as a function of $k$) decreases much faster than it was suggested by empirical risk bounds obtained in \\\\cite{DS21}. Moreover, functional limit theorems for local empirical processes of the (empirical) reconstruction error of projections uniformly over neighborhoods of the true optimal projection are established. Based on these asymptotic results, we propose a data-driven method to select the dimension of the projection space. Finally, the finite sample performance of resulting estimators is examined in a simulation study.',\n",
       " 69: 'This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025) we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings including Rubins (1987) total variance estimation in multiple imputations where weighted variance combinations are common. The proposed estimator generalizes von Daviers (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights.',\n",
       " 70: 'Given a planar curve, imagine rolling a sphere along that curve without slipping or twisting, and by this means tracing out a curve on the sphere. It is well known that such a rolling operation induces a local isometry between the sphere and the plane so that the two curves uniquely determine each other, and moreover, the operation extends to a general class of manifolds in any dimension. We use rolling to construct an analogue of a Gaussian process on a manifold starting from a Euclidean Gaussian process. The resulting model is generative, and is amenable to statistical inference given data as curves on a manifold. We illustrate with examples on the unit sphere, symmetric positive-definite matrices, and with a robotics application involving 3D orientations.',\n",
       " 71: \"Most Kalman filters for non-linear systems, such as the unscented Kalman filter, are based on Gaussian approximations. We use Poincar\\\\'e inequalities to bound the Wasserstein distance between the true joint distribution of the prediction and measurement and its Gaussian approximation. The bounds can be used to assess the performance of non-linear Gaussian filters and determine those filtering approximations that are most likely to induce error.\",\n",
       " 72: 'We revisit the discrete argmin inference problem in high-dimensional settings. Given $n$ observations from a $d$ dimensional vector, the goal is to test whether the $r$th component of the mean vector is the smallest among all components. We propose dimension-agnostic tests that maintain validity regardless of how $d$ scales with $n$, and regardless of arbitrary ties in the mean vector. Notably, our validity holds under mild moment conditions, requiring little more than finiteness of a second moment, and permitting possibly strong dependence between coordinates. In addition, we establish the local minimax separation rate for this problem, which adapts to the cardinality of a confusion set, and show that the proposed tests attain this rate. Our method uses the sample splitting and self-normalization approach of Kim and Ramdas (2024). Our tests can be easily inverted to yield confidence sets for the argmin index. Empirical results illustrate the strong performance of our approach in terms of type I error control and power compared to existing methods.',\n",
       " 73: \"The Glivenko-Cantelli theorem is a uniform version of the strong law of large numbers. It states that for every IID sequence of random variables, the empirical measure converges to the underlying distribution (in the sense of uniform convergence of the CDF). In this work, we provide tools to study such limits of empirical measures in categorical probability. We propose two axioms, permutation invariance and empirical adequacy, that a morphism of type $X^\\\\mathbb{N} \\\\to X$ should satisfy to be interpretable as taking an infinite sequence as input and producing a sample from its empirical measure as output. Since not all sequences have a well-defined empirical measure, ``such empirical sampling morphisms'' live in quasi-Markov categories, which, unlike Markov categories, allow partial morphisms. Given an empirical sampling morphism and a few other properties, we prove representability as well as abstract versions of the de Finetti theorem, the Glivenko-Cantelli theorem and the strong law of large numbers. We provide several concrete constructions of empirical sampling morphisms as partially defined Markov kernels on standard Borel spaces. Instantiating our abstract results then recovers the standard Glivenko-Cantelli theorem and the strong law of large numbers for random variables with finite first moment. Our work thus provides a joint proof of these two theorems in conjunction with the de Finetti theorem from first principles.\",\n",
       " 74: \"In this paper we consider the use of tiered background knowledge within constraint based causal discovery. Our focus is on settings relaxing causal sufficiency, i.e. allowing for latent variables which may arise because relevant information could not be measured at all, or not jointly, as in the case of multiple overlapping datasets. We first present novel insights into the properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce a new extension of the IOD (integrating overlapping datasets) algorithm incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm. We show that under full usage of the tiered background knowledge tFCI and tIOD are sound, while simple versions of the tIOD and tFCI are sound and complete. We further show that the tIOD algorithm can often be expected to be considerably more efficient and informative than the IOD algorithm even beyond the obvious restriction of the Markov equivalence classes. We provide a formal result on the conditions for this gain in efficiency and informativeness. Our results are accompanied by a series of examples illustrating the exact role and usefulness of tiered background knowledge.\",\n",
       " 75: 'Cardiac real-time magnetic resonance imaging (MRI) is an emerging technology that images the heart at up to 50 frames per second, offering insight into the respiratory effects on the heartbeat. However, this method significantly increases the number of images that must be segmented to derive critical health indicators. Although neural networks perform well on inner slices, predictions on outer slices are often unreliable. This work proposes sparse Bayesian learning (SBL) to predict the ventricular volume on outer slices with minimal manual labeling to address this challenge. The ventricular volume over time is assumed to be dominated by sparse frequencies corresponding to the heart and respiratory rates. Moreover, SBL identifies these sparse frequencies on well-segmented inner slices by optimizing hyperparameters via type -II likelihood, automatically pruning irrelevant components. The identified sparse frequencies guide the selection of outer slice images for labeling, minimizing posterior variance. This work provides performance guarantees for the greedy algorithm. Testing on patient data demonstrates that only a few labeled images are necessary for accurate volume prediction. The labeling procedure effectively avoids selecting inefficient images. Furthermore, the Bayesian approach provides uncertainty estimates, highlighting unreliable predictions (e.g., when choosing suboptimal labels).',\n",
       " 76: 'We consider the problem of constructing a least conservative estimator of the expected value $\\\\mu$ of a non-negative heavy-tailed random variable. We require that the probability of overestimating the expected value $\\\\mu$ is kept appropriately small; a natural requirement if its subsequent use in a decision process is anticipated. In this setting, we show it is optimal to estimate $\\\\mu$ by solving a distributionally robust optimization (DRO) problem using the Kullback-Leibler (KL) divergence. We further show that the statistical properties of KL-DRO compare favorably with other estimators based on truncation, variance regularization, or Wasserstein DRO.',\n",
       " 77: 'Particle filters (PFs) is a class of Monte Carlo algorithms that propagate over time a set of $N\\\\in\\\\mathbb{N}$ particles which can be used to estimate, in an online fashion, the sequence of filtering distributions $(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ defined by a state-space model. Despite the popularity of PFs, the study of the time evolution of their estimates has only received very little attention in the literature. Denoting by $(\\\\hat{\\\\eta}_t^N)_{t\\\\geq 1}$ the PF estimate of $(\\\\hat{\\\\eta}_t)_{t\\\\geq 1}$ and letting $\\\\kappa\\\\in (0,1)$, in this work we first show that for any number of particles $N$ it holds that, with probability one, we have $\\\\|\\\\hat{\\\\eta}_t^N- \\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa$ for infinitely many $t\\\\geq 1$, with $\\\\|\\\\cdot\\\\|$ a measure of distance between probability distributions. Considering a simple filtering problem we then provide reassuring results concerning the ability of PFs to estimate jointly a finite set $\\\\{\\\\hat{\\\\eta}_t\\\\}_{t=1}^T$ of filtering distributions by studying $\\\\P(\\\\sup_{t\\\\in\\\\{1,\\\\dots,T\\\\}}\\\\|\\\\hat{\\\\eta}_t^{N}-\\\\hat{\\\\eta}_t\\\\|\\\\geq \\\\kappa)$. Finally, on the same toy filtering problem, we prove that sequential quasi-Monte Carlo, a randomized quasi-Monte Carlo version of PF algorithms, offers greater safety guarantees than PFs in the sense that, for this algorithm, it holds that $\\\\lim_{N\\\\rightarrow\\\\infty}\\\\sup_{t\\\\geq 1}\\\\|\\\\hat{\\\\eta}_t^N-\\\\hat{\\\\eta}_t\\\\|=0$ with probability one.',\n",
       " 78: 'This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference.',\n",
       " 79: 'In this paper, we analyze the relative errors in various reliability measures due to the tacit assumption that the components associated with a $n$-component series system or a parallel system are independently working where the components are dependent. We use Copula functions in said error analysis. This technique generalizes the existing work on error assessment for many wide class of distributions.',\n",
       " 80: 'In this paper, we analyze the relative errors that crop up in the various reliability measures due to the tacit assumption that the components are independently working associated with a $n$-component series system or a parallel system where the components are dependent and follow a well-defined multivariate Weibull or exponential distribution. We also list some important observations which the previous authors have not noted in their earlier works. In this paper, we focus on the incurred error in multi-component series and parallel systems having multivariate Weibull distributions. In the upcoming sections, we establish that the present study has relevance with stochastic orders and statistical dependence which were not previously pointed out by previous authors.',\n",
       " 81: 'Evaluation is critical to advance decision making across domains, yet existing methodologies often struggle to balance theoretical rigor and practical scalability. In order to reduce the cost of experimental evaluation, we introduce a computational theory of evaluation for parameterisable subjects. We prove upper bounds of generalized evaluation error and generalized causal effect error of evaluation metric on subject. We also prove efficiency, and consistency to estimated causal effect of subject on metric by prediction. To optimize evaluation models, we propose a meta-learner to handle heterogeneous evaluation subjects space. Comparing with other computational approaches, our (conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12 scenes, including individual medicine, scientific simulation, business activities, and quantum trade. The evaluation time is reduced 3-7 order of magnitude comparing with experiments or simulations.',\n",
       " 82: 'Variable selection comprises an important step in many modern statistical inference procedures. In the regression setting, when estimators cannot shrink irrelevant signals to zero, covariates without relationships to the response often manifest small but non-zero regression coefficients. The ad hoc procedure of discarding variables whose coefficients are smaller than some threshold is often employed in practice. We formally analyze a version of such thresholding procedures and develop a simple thresholding method that consistently estimates the set of relevant variables under mild regularity assumptions. Using this thresholding procedure, we propose a sparse, $\\\\sqrt{n}$-consistent and asymptotically normal estimator whose non-zero elements do not exhibit shrinkage. The performance and applicability of our approach are examined via numerical studies of simulated and real data.',\n",
       " 83: \"This document is an extended version of an abstract for a talk, with approximately the same title, to be held at the 7th Joint Statistical Meeting of the Deutsche Arbeitsgemeinschaft Statistik, from 24 to 28 March 2025 in Berlin. Here ``teachable'' is meant to apply to people ranging from sufficiently advanced high school pupils to university students in mathematics or statistics: For understanding most of the proposed approximation results, it should suffice to know binomial laws, their means and variances, and the standard normal distribution function (but not necessarily the concept of a corresponding normal random variable). Of the proposed approximations, some are well-known (at least to experts), and some are based on teaching experience and research at Trier University.\",\n",
       " 84: 'This paper introduces a new kind of periodic fractional autoregressive process (PFAR) driven by fractional Gaussian noise (fGn). The new model is a specialized varying coefficient fractional autoregressive model, where the coefficients adhere to a periodic structure. In this working, Generalized least squares estimation and GPH method are employed to construct an initial estimator to estimate the joint estimation of the parameters of these models. Then one-step procedure is used to obtain a more asymptotically-efficient estimator. The paper proves that both estimators are consistent and asymptotically normal, and their performance is demonstrated through a simulation study using finite-size samples via Monte Carlo simulations. Simulation studies suggests that, while both estimation methods can accurately estimate the model, the one-step estimator outperforms the initial estimator.',\n",
       " 85: \"In this work, we present a theoretical and computational framework for constructing stochastic transport maps between probability distributions using diffusion processes. We begin by proving that the time-marginal distribution of the sum of two independent diffusion processes satisfies a Fokker-Planck equation. Building on this result and applying Ambrosio-Figalli-Trevisan's superposition principle, we establish the existence and uniqueness of solutions to the associated stochastic differential equation (SDE). Leveraging these theoretical foundations, we develop a method to construct (stochastic) transport maps between arbitrary probability distributions using dynamical ordinary differential equations (ODEs) and SDEs. Furthermore, we introduce a unified framework that generalizes and extends a broad class of diffusion-based generative models and sampling techniques. Finally, we analyze the convergence properties of particle approximations for the SDEs underlying our framework, providing theoretical guarantees for their practical implementation. This work bridges theoretical insights with practical applications, offering new tools for generative modeling and sampling in high-dimensional spaces.\",\n",
       " 86: 'In Learning Theory, the smoothness assumption on the target function (known as source condition) is a key factor in establishing theoretical convergence rates for an estimator. The existing general form of the source condition, as discussed in learning theory literature, has traditionally been restricted to a class of functions that can be expressed as a product of an operator monotone function and a Lipschitz continuous function. In this note, we remove these restrictions on the index function and establish optimal convergence rates for least-square regression over a Hilbert space with general regularization under a general source condition, thereby significantly broadening the scope of existing theoretical results.',\n",
       " 87: 'Let $P=(x_1,\\\\ldots,x_n)$ be a population consisting of $n\\\\ge 2$ real numbers whose sum is zero, and let $k <n$ be a positive integer. We sample $k$ elements from $P$ without replacement and denote by $X_P$ the sum of the elements in our sample. In this article, using ideas from the theory of majorization, we deduce non-asymptotic lower and upper bounds on the probability that $X_P$ exceeds its expected value.',\n",
       " 88: 'We consider nonlinear mixed effects models including high-dimensional covariates to model individual parameters. The objective is to identify relevant covariates and estimate model parameters. We combine a penalized LASSO-type estimator with an eBIC model choice criterion to select the covariates of interest. Then we estimate the parameters by maximum likelihood in the reduced model. We calculate the LASSO-type penalized estimator by a weighted proximal gradient descent algorithm with an adaptive learning rate. This choice allows us in particular to consider models that do not necessarily belong to the curved exponential family. We compare first the performance of the proposed methodology with those of the glmmLasso procedure in a linear mixed effects model in a simulation study. We then illustrate its performance in a nonlinear mixed-effects logistic growth model through simulation.',\n",
       " 89: 'We study the nonparametric maximum likelihood estimator $\\\\widehat{\\\\pi}$ for Gaussian location mixtures in one dimension. It has been known since (Lindsay, 1983) that given an $n$-point dataset, this estimator always returns a mixture with at most $n$ components, and more recently (Wu-Polyanskiy, 2020) gave a sharp $O(\\\\log n)$ bound for subgaussian data. In this work we study computational aspects of $\\\\widehat{\\\\pi}$. We provide an algorithm which for small enough $\\\\varepsilon>0$ computes an $\\\\varepsilon$-approximation of $\\\\widehat\\\\pi$ in Wasserstein distance in time $K+Cnk^2\\\\log\\\\log(1/\\\\varepsilon)$. Here $K$ is data-dependent but independent of $\\\\varepsilon$, while $C$ is an absolute constant and $k=|supp(\\\\widehat{\\\\pi})|\\\\leq n$ is the number of atoms in $\\\\widehat\\\\pi$. We also certifiably compute the exact value of $|supp(\\\\widehat\\\\pi)|$ in finite time. These guarantees hold almost surely whenever the dataset $(x_1,\\\\dots,x_n)\\\\in [-cn^{1/4},cn^{1/4}]$ consists of independent points from a probability distribution with a density (relative to Lebesgue measure). We also show the distribution of $\\\\widehat\\\\pi$ conditioned to be $k$-atomic admits a density on the associated $2k-1$ dimensional parameter space for all $k\\\\leq \\\\sqrt{n}/3$, and almost sure locally linear convergence of the EM algorithm. One key tool is a classical Fourier analytic estimate for non-degenerate curves.',\n",
       " 90: 'Statistical learning methods typically assume that the training and test data originate from the same distribution, enabling effective risk minimization. However, real-world applications frequently involve distributional shifts, leading to poor model generalization. To address this, recent advances in causal inference and robust learning have introduced strategies such as invariant causal prediction and anchor regression. While these approaches have been explored for traditional structural equation models (SEMs), their extension to functional systems remains limited. This paper develops a risk minimization framework for functional SEMs using linear, potentially unbounded operators. We introduce a functional worst-risk minimization approach, ensuring robust predictive performance across shifted environments. Our key contribution is a novel worst-risk decomposition theorem, which expresses the maximum out-of-sample risk in terms of observed environments. We establish conditions for the existence and uniqueness of the worst-risk minimizer and provide consistent estimation procedures. Empirical results on functional systems illustrate the advantages of our method in mitigating distributional shifts. These findings contribute to the growing literature on robust functional regression and causal learning, offering practical guarantees for out-of-sample generalization in dynamic environments.',\n",
       " 91: 'We investigate the asymptotic behavior of the number of parts $K_n$ in the Ewens--Pitman partition model under the regime where the diversity parameter is scaled linearly with the sample size, that is, $\\\\theta = \\\\lambda n$ for some~$\\\\lambda > 0$. While recent work has established a law of large numbers (LLN) and a central limit theorem (CLT) for $K_n$ in this regime, we revisit these results through a martingale-based approach. Our method yields significantly shorter proofs, and leads to sharper convergence rates in the CLT, including improved Berry--Esseen bounds in the case $\\\\alpha = 0$, and a new result for the regime $\\\\alpha \\\\in (0,1)$, filling a gap in the literature.',\n",
       " 92: \"At present, in the theory of stochastic process modeling a problem of assessment of reliability and accuracy of stochastic process model in $C(T)$ space wasn't studied for the case of implicit decomposition of process in the form of a series with independent terms. The goal is to study reliability and accuracy in $C(T)$ of models of processes from $Sub_\\\\varphi(\\\\Omega)$ that cannot be decomposed in a series with independent elements explicitly. Using previous research in the field of modeling of stochastic processes, assumption is considered about possibility of decomposition of a stochastic process in the series with independent elements that can be found using approximations. Impact of approximation error of process decomposition in series with independent elements on reliability and accuracy of modeling of stochastic process in $C(T)$ is studied. Theorems are proved that allow estimation of reliability and accuracy of a model in $C(T)$ of a stochastic process from $Sub_\\\\varphi(\\\\Omega)$ in the case when decomposition of this process in a series with independent elements can be found only with some error, for example, using numerical approximations.\",\n",
       " 93: \"Deep neural networks (DNNs) have become powerful tools for modeling complex data structures through sequentially integrating simple functions in each hidden layer. In survival analysis, recent advances of DNNs primarily focus on enhancing model capabilities, especially in exploring nonlinear covariate effects under right censoring. However, deep learning methods for interval-censored data, where the unobservable failure time is only known to lie in an interval, remain underexplored and limited to specific data type or model. This work proposes a general regression framework for interval-censored data with a broad class of partially linear transformation models, where key covariate effects are modeled parametrically while nonlinear effects of nuisance multi-modal covariates are approximated via DNNs, balancing interpretability and flexibility. We employ sieve maximum likelihood estimation by leveraging monotone splines to approximate the cumulative baseline hazard function. To ensure reliable and tractable estimation, we develop an EM algorithm incorporating stochastic gradient descent. We establish the asymptotic properties of parameter estimators and show that the DNN estimator achieves minimax-optimal convergence. Extensive simulations demonstrate superior estimation and prediction accuracy over state-of-the-art methods. Applying our method to the Alzheimer's Disease Neuroimaging Initiative dataset yields novel insights and improved predictive performance compared to traditional approaches.\",\n",
       " 94: 'When prior information is lacking, the go-to strategy for probabilistic inference is to combine a \"default prior\" and the likelihood via Bayes\\'s theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach offering posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM\\'s desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution obtained in applications where the model has a group transformation structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations.',\n",
       " 95: \"We formalize the generalization error bound using Rademacher complexity in the Lean 4 theorem prover. Generalization error quantifies the gap between a learning machine's performance on given training data versus unseen test data, and Rademacher complexity serves as an estimate of this error based on the complexity of learning machines, or hypothesis class. Unlike traditional methods such as PAC learning and VC dimension, Rademacher complexity is applicable across diverse machine learning scenarios including deep learning and kernel methods. We formalize key concepts and theorems, including the empirical and population Rademacher complexities, and establish generalization error bounds through formal proofs of McDiarmid's inequality, Hoeffding's lemma, and symmetrization arguments.\",\n",
       " 96: \"The problems of detecting and recovering planted structures/subgraphs in Erd\\\\H{o}s-R\\\\'{e}nyi random graphs, have received significant attention over the past three decades, leading to many exciting results and mathematical techniques. However, prior work has largely focused on specific ad hoc planted structures and inferential settings, while a general theory has remained elusive. In this paper, we bridge this gap by investigating the detection of an \\\\emph{arbitrary} planted subgraph $\\\\Gamma = \\\\Gamma_n$ in an Erd\\\\H{o}s-R\\\\'{e}nyi random graph $\\\\mathcal{G}(n, q_n)$, where the edge probability within $\\\\Gamma$ is $p_n$. We examine both the statistical and computational aspects of this problem and establish the following results. In the dense regime, where the edge probabilities $p_n$ and $q_n$ are fixed, we tightly characterize the information-theoretic and computational thresholds for detecting $\\\\Gamma$, and provide conditions under which a computational-statistical gap arises. Most notably, these thresholds depend on $\\\\Gamma$ only through its number of edges, maximum degree, and maximum subgraph density. Our lower and upper bounds are general and apply to any value of $p_n$ and $q_n$ as functions of $n$. Accordingly, we also analyze the sparse regime where $q_n = \\\\Theta(n^{-\\\\alpha})$ and $p_n-q_n =\\\\Theta(q_n)$, with $\\\\alpha\\\\in[0,2]$, as well as the critical regime where $p_n=1-o(1)$ and $q_n = \\\\Theta(n^{-\\\\alpha})$, both of which have been widely studied, for specific choices of $\\\\Gamma$. For these regimes, we show that our bounds are tight for all planted subgraphs investigated in the literature thus far\\\\textemdash{}and many more. Finally, we identify conditions under which detection undergoes sharp phase transition, where the boundaries at which algorithms succeed or fail shift abruptly as a function of $q_n$.\",\n",
       " 97: 'A statistical model is said to be calibrated if the resulting mean estimates perfectly match the true means of the underlying responses. Aiming for calibration is often not achievable in practice as one has to deal with finite samples of noisy observations. A weaker notion of calibration is auto-calibration. An auto-calibrated model satisfies that the expected value of the responses being given the same mean estimate matches this estimate. Testing for auto-calibration has only been considered recently in the literature and we propose a new approach based on calibration bands. Calibration bands denote a set of lower and upper bounds such that the probability that the true means lie simultaneously inside those bounds exceeds some given confidence level. Such bands were constructed by Yang-Barber (2019) for sub-Gaussian distributions. Dimitriadis et al. (2023) then introduced narrower bands for the Bernoulli distribution and we use the same idea in order to extend the construction to the entire exponential dispersion family that contains for example the binomial, Poisson, negative binomial, gamma and normal distributions. Moreover, we show that the obtained calibration bands allow us to construct various tests for calibration and auto-calibration, respectively.',\n",
       " 98: 'Optimal transport theory has become a fundamental tool for handling diverse types of data, with growing applications across various fields. However, the Wasserstein distance presents significant computational and statistical challenges in high-dimensional settings. To address these issues, alternative distances such as the sliced Wasserstein distance, which leverages one-dimensional projections, have been introduced. In this work, we establish a novel central limit theorem for the p-sliced Wasserstein distance, for p>1, using the Efron-Stein inequality-a technique that has proven effective in related problems. This approach yields a central limit theorem centered at the expected value of the empirical cost, under mild regularity conditions. Notably, unlike the general Wasserstein distance in arbitrary dimensions, we demonstrate that, under specific assumptions, the centering constants can be replaced by the population cost, which is essential for statistical inference. This generalizes and significantly refines existing results for the one-dimensional case. Consequently, we present the first asymptotically valid inference framework for the sliced Wasserstein distance applicable to measures that are not necessarily compactly supported, for p>1. Finally, we address key practical aspects for inference, including Monte Carlo estimation of the integral and estimation of the asymptotic variance, ensuring applicability in real-world scenarios.',\n",
       " 99: \"We study the nonconvex optimization landscapes of synchronization problems on spheres. First, we present new results for the statistical problem of synchronization over the two-element group $\\\\mathbf{Z}_2$. We consider the nonconvex least-squares problem with $\\\\mathbf{Z}_2 = \\\\{\\\\pm 1\\\\}$ relaxed to the unit sphere in $\\\\mathbf{R}^r$ for $r \\\\geq 2$; for several popular models, including graph clustering under the binary stochastic block model, we show that, for any $r \\\\geq 2$, every second-order critical point recovers the ground truth in the asymptotic regimes where exact recovery is information-theoretically possible. Such statistical optimality via spherical relaxations had previously only been shown for (potentially arbitrarily) larger relaxation dimension $r$. Second, we consider the global synchronization of networks of coupled oscillators under the (homogeneous) Kuramoto model. We prove new and optimal asymptotic results for random signed networks on an Erd\\\\H{o}s--R\\\\'enyi graph, and we give new and simple proofs for several existing state-of-the-art results. Our key tool is a deterministic landscape condition that extends a recent result of Rakoto Endor and Waldspurger. This result says that, if a certain problem-dependent Laplacian matrix has small enough condition number, the nonconvex landscape is benign. Our extension allows the condition number to include an arbitrary diagonal preconditioner, which gives tighter results for many problems. We show that, for the synchronization of Kuramoto oscillator networks on nearest-neighbor circulant graphs as studied by Wiley, Strogatz, and Girvan, this condition is optimal. We also prove a natural complex extension that may be of interest for synchronization on the special orthogonal group $\\\\operatorname{SO}(2)$.\",\n",
       " 100: 'A fundamental challenge in the application of finite mixture models is selecting the number of mixture components, also known as order. Traditional approaches rely on selecting a single best model using information criteria. However, in the presence of noisy data, and when models with different orders yield similar fits, model selection uncertainty can be substantial, making it challenging to confidently identify the true number of components. In this paper, we introduce the Model Selection Confidence Set (MSCS) for order selection - a set-valued estimator that, with a predefined confidence level, includes the true mixture order across repeated samples. Rather than selecting a single model, our MSCS identifies all plausible orders by determining whether each candidate model is at least as plausible as the best-selected one, using a screening test based on a penalized likelihood ratio statistic. We provide theoretical guarantees for the asymptotic coverage of our confidence set and demonstrate its practical advantages through simulations and real data analysis.',\n",
       " 101: 'Identification of joint dependence among more than two random vectors plays an important role in many statistical applications, where the data may contain sensitive or confidential information. In this paper, we consider the the d-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of differential privacy. Given the limiting distribution of the empirical estimate of dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy regime is typically based on permutation and bootstrap. To detect joint dependence in privacy, we propose a dHSIC-based testing procedure by employing a differentially private permutation methodology. Our method enjoys privacy guarantee, valid level and pointwise consistency, while the bootstrap counterpart suffers inconsistent power. We further investigate the uniform power of the proposed test in dHSIC metric and $L_2$ metric, indicating that the proposed test attains the minimax optimal power across different privacy regimes. As a byproduct, our results also contain the pointwise and uniform power of the non-private permutation dHSIC, addressing an unsolved question remained in Pfister et al. (2018).',\n",
       " 102: 'We introduce a new compositional framework for generalized variational inference, clarifying the different parts of a model, how they interact, and how they compose. We explain that both exact Bayesian inference and the loss functions typical of variational inference (such as variational free energy and its generalizations) satisfy chain rules akin to that of reverse-mode automatic differentiation, and we advocate for exploiting this to build and optimize models accordingly. To this end, we construct a series of compositional tools: for building models; for constructing their inversions; for attaching local loss functions; and for exposing parameters. Finally, we explain how the resulting parameterized statistical games may be optimized locally, too. We illustrate our framework with a number of classic examples, pointing to new areas of extensibility that are revealed.',\n",
       " 103: 'The quantification of treatment effects plays an important role in a wide range of applications, including policy making and bio-pharmaceutical research. In this article, we study the quantile treatment effect (QTE) while addressing two specific types of heterogeneities: (a) personalized heterogeneity, which captures the varying treatment effects for different individuals, and (b) quantile heterogeneity, which accounts for how the impact of covariates varies across different quantile levels. A well-designed debiased estimator for the individualized quantile treatment effect (IQTE) is proposed to capture such heterogeneities effectively. We show that this estimator converges weakly to a Gaussian process as a function of the quantile levels and propose valid statistical inference methods, including the construction of confidence intervals and the development of hypothesis testing decision rules. In addition, the minimax optimality frameworks for these inference procedures are established. Specifically, we derive the minimax optimal rates for the expected length of confidence intervals and the magnitude of the detection boundary for hypothesis testing procedures, illustrating the superiority of the proposed estimator. The effectiveness of our methods is demonstrated through extensive simulations and an analysis of the National Health and Nutrition Examination Survey (NHANES) datasets.',\n",
       " 104: 'We consider the following inverse problem: Suppose a $(1+1)$-dimensional wave equation on $\\\\mathbb R_+$ with zero initial conditions is excited with a Neumann boundary data modelled as a white noise process. Given also the Dirichlet data at the same point, determine the unknown first order coefficient function of the system. We first establish that direct problem is well-posed. The inverse problem is then solved by showing that correlations of the boundary data determine the Neumann-to-Dirichlet operator in the sense of distributions, which is known to uniquely identify the coefficient. This approach has applications in acoustic measurements of internal cross-sections of fluid pipes such as pressurised water supply pipes and vocal tract shape determination.',\n",
       " 105: 'This paper introduces a quasi-likelihood ratio testing procedure for diffusion processes observed under nonsynchronous sampling schemes. High-frequency data, particularly in financial econometrics, are often recorded at irregular time points, challenging conventional synchronous methods for parameter estimation and hypothesis testing. To address these challenges, we develop a quasi-likelihood framework that accommodates irregular sampling while integrating adaptive estimation techniques for both drift and diffusion coefficients, thereby enhancing optimization stability and reducing computational burden. We rigorously derive the asymptotic properties of the proposed test statistic, showing that it converges to a chi-squared distribution under the null hypothesis and exhibits consistency under alternatives. Moreover, we establish that the resulting tests are asymptotically uniformly most powerful. Extensive numerical experiments corroborate the theoretical findings and demonstrate that our method outperforms existing nonparametric approaches.',\n",
       " 106: \"Deviations from Bayesian updating are traditionally categorized as biases, errors, or fallacies, thus implying their inherent ``sub-optimality.'' We offer a more nuanced view. We demonstrate that, in learning problems with misspecified models, non-Bayesian updating can outperform Bayesian updating.\",\n",
       " 107: \"We consider a quotient of a complete Riemannian manifold modulo an isometrically and properly acting Lie group and lifts of the quotient to the manifolds in optimal position to a reference point on the manifold. With respect to the pushed forward Riemannian volume onto the quotient we derive continuity and uniqueness a.e. and smoothness to large extents also with respect to the reference point. In consequence we derive a general manifold stability theorem: the Fr\\\\'echet mean lies in the highest dimensional stratum assumed with positive probability, and a strong law for optimal lifts. This allows to define new two-sample tests utilizing individual optimal lifts which outperform existing two-sample tests on simulated data. They also outperform existing tests on a newly derived reverse labeling reflection shape space, that is used to model filament data of microtubules within cells in a biological application.\",\n",
       " 108: 'Topic modeling is traditionally applied to word counts without accounting for the context in which words appear. Recent advancements in large language models (LLMs) offer contextualized word embeddings, which capture deeper meaning and relationships between words. We aim to leverage such embeddings to improve topic modeling. We use a pre-trained LLM to convert each document into a sequence of word embeddings. This sequence is then modeled as a Poisson point process, with its intensity measure expressed as a convex combination of $K$ base measures, each corresponding to a topic. To estimate these topics, we propose a flexible algorithm that integrates traditional topic modeling methods, enhanced by net-rounding applied before and kernel smoothing applied after. One advantage of this framework is that it treats the LLM as a black box, requiring no fine-tuning of its parameters. Another advantage is its ability to seamlessly integrate any traditional topic modeling approach as a plug-in module, without the need for modifications Assuming each topic is a $\\\\beta$-H\\\\\"{o}lder smooth intensity measure on the embedded space, we establish the rate of convergence of our method. We also provide a minimax lower bound and show that the rate of our method matches with the lower bound when $\\\\beta\\\\leq 1$. Additionally, we apply our method to several datasets, providing evidence that it offers an advantage over traditional topic modeling approaches.',\n",
       " 109: \"We construct a new tail bound for the sum of independent random variables for situations in which the expected value of the sum is known and each random variable lies within a specified interval, which may be different for each variable. This new bound can be computed by solving a two-dimensional convex optimization problem. Simulations demonstrate that the new bound is often substantially tighter than Hoeffding's inequality for cases in which both bounds are applicable.\",\n",
       " 110: 'Contrastive learning -- a modern approach to extract useful representations from unlabeled data by training models to distinguish similar samples from dissimilar ones -- has driven significant progress in foundation models. In this work, we develop a new theoretical framework for analyzing data augmentation-based contrastive learning, with a focus on SimCLR as a representative example. Our approach is based on the concept of \\\\emph{approximate sufficient statistics}, which we extend beyond its original definition in \\\\cite{oko2025statistical} for contrastive language-image pretraining (CLIP) using KL-divergence. We generalize it to equivalent forms and general f-divergences, and show that minimizing SimCLR and other contrastive losses yields encoders that are approximately sufficient. Furthermore, we demonstrate that these near-sufficient encoders can be effectively adapted to downstream regression and classification tasks, with performance depending on their sufficiency and the error induced by data augmentation in contrastive learning. Concrete examples in linear regression and topic classification are provided to illustrate the broad applicability of our results.',\n",
       " 111: 'In this paper, models that approximate stochastic processes from the space $Sub_\\\\varphi(\\\\Omega)$ with given reliability and accuracy in $L_p(T)$ are considered for some specific functions $\\\\varphi(t)$. For processes that are decomposited in series using orthonormal bases, such models are constructed in the case where elements of such decomposition cannot be found explicitly.',\n",
       " 112: 'The Generalized Mallows Model (GMM) is a well known family of models for ranking data. A GMM is a distribution over $\\\\mathbb{S}_n$, the set of permutations of n objects, characterized by a location parameter $\\\\sigma \\\\in \\\\mathbb{S}_n$, known as central permutation and a set of dispersion parameters $\\\\theta_{1:n-1}\\\\in(0,1]$. The GMM shares many properties, such as having sufficient statistics, with exponential models, thus it can be seen as an exponential family with a discrete parameter $\\\\sigma$. This paper shows that computing entropy, crossentropy and Kullback-Leibler divergence in the the class of GMM is tractable, paving the way for a better understanding of this exponential family.',\n",
       " 113: \"Knowledge distillation is a technique used to train a small student network using the output generated by a large teacher network, and has many empirical advantages~\\\\citep{Hinton2015DistillingTK}. While the standard one-shot approach to distillation only uses the output of the final teacher network, recent work~\\\\citep{panigrahi2024progressive} has shown that using intermediate checkpoints from the teacher's training process as an implicit ``curriculum'' for progressive distillation can significantly speed up training. However, such schemes require storing these checkpoints, and often require careful selection of the intermediate checkpoints to train on, which can be impractical for large-scale training. In this paper, we show that a curriculum can be \\\\emph{extracted} from just the fully trained teacher network, and that this extracted curriculum can give similar efficiency benefits to those of progressive distillation. Our extraction scheme is natural; we use a random projection of the hidden representations of the teacher network to progressively train the student network, before training using the output of the full network. We show that our scheme significantly outperforms one-shot distillation and achieves a performance similar to that of progressive distillation for learning sparse parities with two-layer networks, and provide theoretical guarantees for this setting. Additionally, we show that our method outperforms one-shot distillation even when using transformer-based architectures, both for sparse-parity learning, and language modeling tasks.\",\n",
       " 114: 'We extend the celebrated Glivenko-Cantelli theorem, sometimes called the fundamental theorem of statistics, from its standard setting of total variation distance to all $f$-divergences. A key obstacle in this endeavor is to define $f$-divergence on a subcollection of a $\\\\sigma$-algebra that forms a $\\\\pi$-system but not a $\\\\sigma$-subalgebra. This is a side contribution of our work. We will show that this notion of $f$-divergence on the $\\\\pi$-system of rays preserves nearly all known properties of standard $f$-divergence, yields a novel integral representation of the Kolmogorov-Smirnov distance, and has a Glivenko-Cantelli theorem. We will also discuss the prospects of a Vapnik-Chervonenkis theory for $f$-divergence.',\n",
       " 115: 'We investigate differentially private estimators for individual parameters within larger parametric models. While generic private estimators exist, the estimators we provide repose on new local notions of estimand stability, and these notions allow procedures that provide private certificates of their own stability. By leveraging these private certificates, we provide computationally and statistical efficient mechanisms that release private statistics that are, at least asymptotically in the sample size, essentially unimprovable: they achieve instance optimal bounds. Additionally, we investigate the practicality of the algorithms both in simulated data and in real-world data from the American Community Survey and US Census, highlighting scenarios in which the new procedures are successful and identifying areas for future work.',\n",
       " 116: 'The Friedman test has been extensively applied as a nonparametric alternative to the conventional F procedure for comparing treatment effects in randomized complete block designs. A chi-square distribution provides a convenient approximation to determining the critical values for the Friedman procedure in hypothesis testing. However, the chi-square approximation is generally conservative and the accuracy declines with increasing number of treatments. This paper describes an alternative transformation of the Friedman statistic along with an approximate F distribution that has the same numerator degrees of freedom as the ANOVA F test. Moreover, two approximate noncentral F distributions are presented for the proposed F-transformation under the alternative hypothesis of heterogeneous location shifts. Explicit power functions are derived when the underlying populations have the uniform, normal, Laplace, and exponential distributions. Theoretical examination and empirical assessment are presented to validate the advantages of the proposed approaches over the existing methods of the Friedman test. The developed test and power procedures are recommended due to their consistently acceptable Type I error rates and accurate power calculations for the location shift structures and population distributions considered here.',\n",
       " 117: 'Nearly all identifiability results in unsupervised representation learning inspired by, e.g., independent component analysis, factor analysis, and causal representation learning, rely on assumptions of additive independent noise or noiseless regimes. In contrast, we study the more general case where noise can take arbitrary forms, depend on latent variables, and be non-invertibly entangled within a nonlinear function. We propose a general framework for identifying latent variables in the nonparametric noisy settings. We first show that, under suitable conditions, the generative model is identifiable up to certain submanifold indeterminacies even in the presence of non-negligible noise. Furthermore, under the structural or distributional variability conditions, we prove that latent variables of the general nonlinear models are identifiable up to trivial indeterminacies. Based on the proposed theoretical framework, we have also developed corresponding estimation methods and validated them in various synthetic and real-world settings. Interestingly, our estimate of the true GDP growth from alternative measurements suggests more insightful information on the economies than official reports. We expect our framework to provide new insight into how both researchers and practitioners deal with latent variables in real-world scenarios.',\n",
       " 118: \"We consider price competition among multiple sellers over a selling horizon of $T$ periods. In each period, sellers simultaneously offer their prices and subsequently observe their respective demand that is unobservable to competitors. The demand function for each seller depends on all sellers' prices through a private, unknown, and nonlinear relationship. To address this challenge, we propose a semi-parametric least-squares estimation of the nonlinear mean function, which does not require sellers to communicate demand information. We show that when all sellers employ our policy, their prices converge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers would reach if they were fully informed. Each seller incurs a regret of $O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution of our work is proving the existence of equilibrium under shape-constrained demand functions via the concept of $s$-concavity and establishing regret bounds of our proposed policy. Technically, we also establish new concentration results for the least squares estimator under shape constraints. Our findings offer significant insights into dynamic competition-aware pricing and contribute to the broader study of non-parametric learning in strategic decision-making.\",\n",
       " 119: 'We consider estimating the proportion of random variables for two types of composite null hypotheses: (i) the means or medians of the random variables belonging to a non-empty, bounded interval; (ii) the means or medians of the random variables belonging to an unbounded interval that is not the whole real line. For each type of composite null hypotheses, uniformly consistent estimators of the proportion of false null hypotheses are constructed for random variables whose distributions are members of a Type I location-shift family. Further, uniformly consistent estimators of certain functions of a bounded null on the means or medians are provided for the random variables mentioned earlier; these functions are continuous and of bounded variation. The estimators are constructed via solutions to Lebesgue-Stieltjes integral equations and harmonic analysis, do not rely on a concept of p-value, and have various applications.',\n",
       " 120: \"A key trait of stochastic optimizers is that multiple runs of the same optimizer in attempting to solve the same problem can produce different results. As a result, their performance is evaluated over several repeats, or runs, on the problem. However, the accuracy of the estimated performance metrics depends on the number of runs and should be studied using statistical tools. We present a statistical analysis of the common metrics, and develop guidelines for experiment design to measure the optimizer's performance using these metrics to a high level of confidence and accuracy. To this end, we first discuss the confidence interval of the metrics and how they are related to the number of runs of an experiment. We then derive a lower bound on the number of repeats in order to guarantee achieving a given accuracy in the metrics. Using this bound, we propose an algorithm to adaptively adjust the number of repeats needed to ensure the accuracy of the evaluated metric. Our simulation results demonstrate the utility of our analysis and how it allows us to conduct reliable benchmarking as well as hyperparameter tuning and prevent us from drawing premature conclusions regarding the performance of stochastic optimizers.\",\n",
       " 121: 'Estimating the state of a dynamical system from partial and noisy observations is a ubiquitous problem in a large number of applications, such as probabilistic weather forecasting and prediction of epidemics. Particle filters are a widely adopted approach to the problem and provide provably accurate approximations of the statistics of the state, but they perform poorly in high dimensions because of weight collapse. The ensemble Kalman filter does not suffer from this issue, as it relies on an interacting particle system with equal weights. Despite its wide adoption in the geophysical sciences, mathematical analysis of the accuracy of this filter is predominantly confined to the setting of linear dynamical models and linear observations operators, and analysis beyond the linear Gaussian setting is still in its infancy. In this short note, we provide an accessible overview of recent work in which the authors take first steps to analyze the accuracy of the filter beyond the linear Gaussian setting.',\n",
       " 122: 'By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).',\n",
       " 123: 'In this paper, we consider the reproducing property in Reproducing Kernel Hilbert Spaces (RKHS). We establish a reproducing property for the closure of the class of combinations of composition operators under minimal conditions. This allows to revisit the sufficient conditions for the reproducing property to hold for the derivative operator, as well as for the existence of the mean embedding function. These results provide a framework of application of the representer theorem for regularized learning algorithms that involve data for function values, gradients, or any other operator from the considered class.',\n",
       " 124: 'We consider a borderline case: the central limit theorem for a strictly stationary time series with infinite variance but a Gaussian limit. In the iid case a well-known sufficient condition for this central limit theorem is regular variation of the marginal distribution with tail index $\\\\alpha=2$. In the dependent case we assume the stronger condition of sequential regular variation of the time series with tail index $\\\\alpha=2$. We assume that a sample of size $n$ from this time series can be split into $k_n$ blocks of size $r_n\\\\to\\\\infty$ such that $r_n/n\\\\to 0$ as $n\\\\to\\\\infty$ and that the block sums are asymptotically independent. Then we apply classical central limit theory for row-wise iid triangular arrays. The necessary and sufficient conditions for such independent block sums will be verified by using large deviation results for the time series. We derive the central limit theorem for $m$-dependent sequences, linear processes, stochastic volatility processes and solutions to affine stochastic recurrence equations whose marginal distributions have infinite variance and are regularly varying with tail index $\\\\alpha=2$.',\n",
       " 125: 'Given an arbitrary subgraph $H=H_n$ and $p=p_n \\\\in (0,1)$, the planted subgraph model is defined as follows. A statistician observes the union a random copy $H^*$ of $H$, together with random noise in the form of an instance of an Erdos-Renyi graph $G(n,p)$. Their goal is to recover the planted $H^*$ from the observed graph. Our focus in this work is to understand the minimum mean squared error (MMSE) for sufficiently large $n$. A recent paper [MNSSZ23] characterizes the graphs for which the limiting MMSE curve undergoes a sharp phase transition from $0$ to $1$ as $p$ increases, a behavior known as the all-or-nothing phenomenon, up to a mild density assumption on $H$. In this paper, we provide a formula for the limiting MMSE curve for any graph $H=H_n$, up to the same mild density assumption. This curve is expressed in terms of a variational formula over pairs of subgraphs of $H$, and is inspired by the celebrated subgraph expectation thresholds from the probabilistic combinatorics literature [KK07]. Furthermore, we give a polynomial-time description of the optimizers of this variational problem. This allows one to efficiently approximately compute the MMSE curve for any dense graph $H$ when $n$ is large enough. The proof relies on a novel graph decomposition of $H$ as well as a new minimax theorem which may be of independent interest. Our results generalize to the setting of minimax rates of recovering arbitrary monotone boolean properties planted in random noise, where the statistician observes the union of a planted minimal element $A \\\\subseteq [N]$ of a monotone property and a random $Ber(p)^{\\\\otimes N}$ vector. In this setting, we provide a variational formula inspired by the so-called \"fractional\" expectation threshold [Tal10], again describing the MMSE curve (in this case up to a multiplicative constant) for large enough $n$.',\n",
       " 126: 'Undirected graphical models are a widely used class of probabilistic models in machine learning that capture prior knowledge or putative pairwise interactions between variables. Those interactions are encoded in a graph for pairwise interactions; however, generalizations such as factor graphs account for higher-degree interactions using hypergraphs. Inference on such models, which is performed by conditioning on some observed variables, is typically done approximately by optimizing a free energy, which is an instance of variational inference. The Belief Propagation algorithm is a dynamic programming algorithm that finds critical points of that free energy. Recent efforts have been made to unify and extend inference on graphical models and factor graphs to more expressive probabilistic models. A synthesis of these works shows that inference on graphical models, factor graphs, and their generalizations relies on the introduction of presheaves and associated invariants (homology and cohomology groups).We propose to study the impact of the transformation of the presheaves onto the associated message passing algorithms. We show that natural transformations between presheaves associated with graphical models and their generalizations, which can be understood as coherent binning of the set of values of the variables, induce morphisms between associated message-passing algorithms. It is, to our knowledge, the first result on functoriality of the Loopy Belief Propagation.',\n",
       " 127: 'Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications, the theoretical reasons behind it remain largely intuitive, with formal proofs currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. Interestingly, in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension.',\n",
       " 128: 'Nonlinear Bayesian update for a prior ensemble is proposed to extend traditional ensemble Kalman filtering to settings characterized by non-Gaussian priors and nonlinear measurement operators. In this framework, the observed component is first denoised via a standard Kalman update, while the unobserved component is estimated using a nonlinear regression approach based on kernel density estimation. The method incorporates a subsampling strategy to ensure stability and, when necessary, employs unsupervised clustering to refine the conditional estimate. Numerical experiments on Lorenz systems and a PDE-constrained inverse problem illustrate that the proposed nonlinear update can reduce estimation errors compared to standard linear updates, especially in highly nonlinear scenarios.',\n",
       " 129: 'This study introduces a dynamic investment framework to enhance portfolio management in volatile markets, offering clear advantages over traditional static strategies. Evaluates four conventional approaches : equal weighted, minimum variance, maximum diversification, and equal risk contribution under dynamic conditions. Using K means clustering, the market is segmented into ten volatility-based states, with transitions forecasted by a Bayesian Markov switching model employing Dirichlet priors and Gibbs sampling. This enables real-time asset allocation adjustments. Tested across two asset sets, the dynamic portfolio consistently achieves significantly higher risk-adjusted returns and substantially higher total returns, outperforming most static methods. By integrating classical optimization with machine learning and Bayesian techniques, this research provides a robust strategy for optimizing investment outcomes in unpredictable market environments.',\n",
       " 130: \"Although the specification of bivariate probability models using a collection of assumed conditional distributions is not a novel concept, it has received considerable attention in the last decade. In this study, a bivariate distribution-the bivariate Poisson-Gamma conditional distribution-is introduced, combining both univariate continuous and discrete distributions. This work explores aspects of this model's structure and statistical inference that have not been studied before. This paper contributes to the field of statistical modeling and distribution theory through the use of maximum likelihood estimation, along with simulations and analyses of real data.\",\n",
       " 131: 'The recent paper \\\\cite{GSZ2023} on estimation and inference for top-ranking problem in Bradley-Terry-Lice (BTL) model presented a surprising result: componentwise estimation and inference can be done under much weaker conditions on the number of comparison then it is required for the full dimensional estimation. The present paper revisits this finding from completely different viewpoint. Namely, we show how a theoretical study of estimation in sup-norm can be reduced to the analysis of plug-in semiparametric estimation. For the latter, we adopt and extend the general approach from \\\\cite{Sp2024} for high-dimensional estimation. The main tool of the analysis is a theory of perturbed marginal optimization when an objective function depends on a low-dimensional target parameter along with a high-dimensional nuisance parameter. A particular focus of the study is the critical dimension condition. Full-dimensional estimation requires in general the condition \\\\( \\\\mathbbmsl{N} \\\\gg \\\\mathbb{p} \\\\) between the effective parameter dimension \\\\( \\\\mathbb{p} \\\\) and the effective sample size \\\\( \\\\mathbbmsl{N} \\\\) corresponding to the smallest eigenvalue of the Fisher information matrix \\\\( \\\\mathbbmsl{F} \\\\). Inference on the estimated parameter is even more demanding: the condition \\\\( \\\\mathbbmsl{N} \\\\gg \\\\mathbb{p}^{2} \\\\) cannot be generally avoided; see \\\\cite{Sp2024}. However, for the sup-norm estimation, the critical dimension condition can be reduced to \\\\( \\\\mathbbmsl{N} \\\\geq \\\\CONST \\\\log(\\\\dimp) \\\\). Compared to \\\\cite{GSZ2023}, the proposed approach works for the classical MLE and does not require any resampling procedure, applies to more general structure of the comparison graph, and yields more accurate expansions for each component of the parameter vector.',\n",
       " 132: \"This paper investigates the asymptotic properties of least absolute deviation (LAD) regression for linear models with polynomial regressors, highlighting its robustness against heavy-tailed noise and outliers. Assuming independent and identically distributed (i.i.d.) errors, we establish the multiscale asymptotic normality of LAD estimators. A central result is the derivation of the asymptotic precision matrix, shown to be proportional to Hilbert matrices, with the proportionality coefficient depending on the asymptotic variance of the sample median of the noise distribution. We further explore the estimator's convergence properties, both in probability and almost surely, under varying model specifications. Through comprehensive simulations, we evaluate the speed of convergence of the LAD estimator and the empirical coverage probabilities of confidence intervals constructed under different scaling factors (T 1/2 and T $\\\\alpha$ ). These experiments incorporate a range of noise distributions, including Laplace, Gaussian, and Cauchy, to demonstrate the estimator's robustness and efficiency. The findings underscore the versatility and practical relevance of LAD regression in handling non-standard data environments. By connecting the statistical properties of LAD estimators to classical mathematical structures, such as Hilbert matrices, this study offers both theoretical insights and practical tools for robust statistical modeling.\",\n",
       " 133: 'This work extends local linear regression to Banach space-valued time series for estimating smoothly varying means and their derivatives in non-stationary data. The asymptotic properties of both the standard and bias-reduced Jackknife estimators are analyzed under mild moment conditions, establishing their convergence rates. Simulation studies assess the finite sample performance of these estimators and compare them with the Nadaraya-Watson estimator. Additionally, the proposed methods are applied to smooth EEG recordings for reconstructing eye movements and to video analysis for detecting pedestrians and abandoned objects.',\n",
       " 134: 'We consider diffusion of independent molecules in an insulated Euclidean domain with unknown diffusivity parameter. At a random time and position, the molecules may bind and stop diffusing in dependence of a given `binding potential\\'. The binding process can be modeled by an additive random functional corresponding to the canonical construction of a `killed\\' diffusion Markov process. We study the problem of conducting inference on the infinite-dimensional diffusion parameter from a histogram plot of the `killing\\' positions of the process. We show first that these positions follow a Poisson point process whose intensity measure is determined by the solution of a certain Schr\\\\\"odinger equation. The inference problem can then be re-cast as a non-linear inverse problem for this PDE, which we show to be consistently solvable in a Bayesian way under natural conditions on the initial state of the diffusion, provided the binding potential is not too `aggressive\\'. In the course of our proofs we obtain novel posterior contraction rate results for high-dimensional Poisson count data that are of independent interest. A numerical illustration of the algorithm by standard MCMC methods is also provided.',\n",
       " 135: 'We view penalized risks through the lens of the calculus of variations. We consider risks comprised of a fitness-term (e.g. MSE) and a gradient-based penalty. After establishing the Euler-Lagrange field equations as a systematic approach to finding minimizers of risks involving only first derivatives, we proceed to exemplify this approach to the MSE penalized by the integral over the squared l2-norm of the gradient of the regression function. The minimizer of this risk is given as the solution to a second order inhomogeneous PDE, where the inhomogeneity is given as the conditional expectation of the target variable conditioned on the features. We discuss properties of the field equations and practical implications thereof, which also apply to the classical Ridge penalty for linear models, and embed our findings into the existing literature. In particular, we find that we can recover the Rudin-Osher-Fatemi model for image-denoising, if we consider the features as deterministic and evenly distributed. Last, we outline several directions for future research.',\n",
       " 136: 'In reliability theory and survival analysis, observed data are often weakly dependent and subject to additive measurement errors. Such contamination arises when the underlying data are neither independent nor strongly mixed but instead exhibit association. This paper focuses on estimating the hazard rate by deconvolving the density function and constructing an estimator of the distribution function. We assume that the data originate from a strictly stationary sequence satisfying association conditions. Under appropriate smoothness assumptions on the error distribution, we establish the quadratic-mean convergence and asymptotic normality of the proposed estimators. The finite-sample performance of both the hazard rate and distribution function estimators is evaluated through a simulation study. We conclude with a discussion of open problems and potential future research directions.',\n",
       " 137: \"This paper introduces a novel test for conditional stochastic dominance (CSD) at specific values of the conditioning covariates, referred to as target points. The test is relevant for analyzing income inequality, evaluating treatment effects, and studying discrimination. We propose a Kolmogorov-Smirnov-type test statistic that utilizes induced order statistics from independent samples. Notably, the test features a data-independent critical value, eliminating the need for resampling techniques such as the bootstrap. Our approach avoids kernel smoothing and parametric assumptions, instead relying on a tuning parameter to select relevant observations. We establish the asymptotic properties of our test, showing that the induced order statistics converge to independent draws from the true conditional distributions and that the test controls asymptotic size under weak regularity conditions. While our results apply to both continuous and discrete data, in the discrete case, the critical value only provides a valid upper bound. To address this, we propose a refined critical value that significantly enhances power, requiring only knowledge of the support size of the distributions. Additionally, we analyze the test's behavior in the limit experiment, demonstrating that it reduces to a problem analogous to testing unconditional stochastic dominance in finite samples. This framework allows us to prove the validity of permutation-based tests for stochastic dominance when the random variables are continuous. Monte Carlo simulations confirm the strong finite-sample performance of our method.\",\n",
       " 138: 'In statistical inference, confidence set procedures are typically evaluated based on their validity and width properties. Even when procedures achieve rate-optimal widths, confidence sets can still be excessively wide in practice due to elusive constants, leading to extreme conservativeness, where the empirical coverage probability of nominal $1-\\\\alpha$ level confidence sets approaches one. This manuscript studies this gap between validity and conservativeness, using universal inference (Wasserman et al., 2020) with a regular parametric model under model misspecification as a running example. We identify the source of asymptotic conservativeness and propose a general remedy based on studentization and bias correction. The resulting method attains exact asymptotic coverage at the nominal $1-\\\\alpha$ level, even under model misspecification, provided that the product of the estimation errors of two unknowns is negligible, exhibiting an intriguing resemblance to double robustness in semiparametric theory.',\n",
       " 139: \"We revisit the classical broken sample problem: Two samples of i.i.d. data points $\\\\mathbf{X}=\\\\{X_1,\\\\cdots, X_n\\\\}$ and $\\\\mathbf{Y}=\\\\{Y_1,\\\\cdots,Y_m\\\\}$ are observed without correspondence with $m\\\\leq n$. Under the null hypothesis, $\\\\mathbf{X}$ and $\\\\mathbf{Y}$ are independent. Under the alternative hypothesis, $\\\\mathbf{Y}$ is correlated with a random subsample of $\\\\mathbf{X}$, in the sense that $(X_{\\\\pi(i)},Y_i)$'s are drawn independently from some bivariate distribution for some latent injection $\\\\pi:[m] \\\\to [n]$. Originally introduced by DeGroot, Feder, and Goel (1971) to model matching records in census data, this problem has recently gained renewed interest due to its applications in data de-anonymization, data integration, and target tracking. Despite extensive research over the past decades, determining the precise detection threshold has remained an open problem even for equal sample sizes ($m=n$). Assuming $m$ and $n$ grow proportionally, we show that the sharp threshold is given by a spectral and an $L_2$ condition of the likelihood ratio operator, resolving a conjecture of Bai and Hsing (2005) in the positive. These results are extended to high dimensions and settle the sharp detection thresholds for Gaussian and Bernoulli models.\",\n",
       " 140: \"In this paper, we study the minimizers of U-processes and their domains of attraction. U-processes arise in various statistical contexts, particularly in M-estimation, where estimators are defined as minimizers of certain objective functions. Our main results establish necessary and sufficient conditions for the distributional convergence of these minimizers, identifying a broad class of normalizing sequences that go beyond the standard square-root asymptotics with normal limits. We show that the limit distribution belongs to exactly one of the four classes introduced by Smirnov. These results do not only extend Smirnov's theory but also generalize existing asymptotic theories for M-estimators, including classical results by Huber and extensions to higher-degree U-statistics. Furthermore, we analyze the domain of attraction for each class, providing alternative characterizations that determine which types of statistical estimators fall into a given asymptotic regime.\",\n",
       " 141: 'The log-logistic distribution is a versatile parametric family widely used across various applied fields, including survival analysis, reliability engineering, and econometrics. When estimating parameters of the log-logistic distribution, hypothesis testing is necessary to verify assumptions about these parameters. The Wald test and Rao test provide formal methods for testing hypotheses about these parameters. However, these test statistics are not robust, and their rejection decisions may be affected by data contamination. In this paper we develop new families of Wald-type test statistics and Rao-type test statistics based on minimum density power divergence estimators (MDPDEs) for the parameters of the log-logistic distribution. These new families generalize the Wald and Rao test statistics, inheriting the robustness properties from the MDPDEs and thus addressing the lack of robustness of the classical tests. Explicit expressions for the test statistics under the log-logistic model for both simple and composite null hypotheses are derived, and their properties are analyzed in detail. An extensive simulation study empirically demonstrates the robustness of these families and compares their performance with the classical methods.',\n",
       " 142: 'Orthogonal-split trees perform well, but evidence suggests oblique splits can enhance their performance. This paper explores optimizing high-dimensional $s$-sparse oblique splits from $\\\\{(\\\\vec{w}, \\\\vec{w}^{\\\\top}\\\\boldsymbol{X}_{i}) : i\\\\in \\\\{1,\\\\dots, n\\\\}, \\\\vec{w} \\\\in \\\\mathbb{R}^p, \\\\| \\\\vec{w} \\\\|_{2} = 1, \\\\| \\\\vec{w} \\\\|_{0} \\\\leq s \\\\}$ for growing oblique trees, where $ s $ is a user-defined sparsity parameter. We establish a connection between SID convergence and $s_0$-sparse oblique splits with $s_0\\\\ge 1$, showing that the SID function class expands as $s_0$ increases, enabling the capture of more complex data-generating functions such as the $s_0$-dimensional XOR function. Thus, $s_0$ represents the unknown potential complexity of the underlying data-generating function. Learning these complex functions requires an $s$-sparse oblique tree with $s \\\\geq s_0$ and greater computational resources. This highlights a trade-off between statistical accuracy, governed by the SID function class size depending on $s_0$, and computational cost. In contrast, previous studies have explored the problem of SID convergence using orthogonal splits with $ s_0 = s = 1 $, where runtime was less critical. Additionally, we introduce a practical framework for oblique trees that integrates optimized oblique splits alongside orthogonal splits into random forests. The proposed approach is assessed through simulations and real-data experiments, comparing its performance against various oblique tree models.',\n",
       " 143: 'We present a new proof of the sub-Gaussian norm concentration inequality. Our proof is based on an averaged version of the moment generating function termed the averaged moment generating function. Compared with the widely adopted $\\\\varepsilon$-net technique-based proof of the sub-Gaussian norm concentration inequality, our method does not rely on the union bound and promises a tighter concentration bound.',\n",
       " 144: \"Turing's estimator allows one to estimate the probabilities of outcomes that either do not appear or only rarely appear in a given random sample. We perform a simulation study to understand the finite sample performance of several related confidence intervals (CIs) and introduce an approach for selecting the appropriate CI for a given sample. We give an application to the problem of authorship attribution and apply it to a dataset comprised of tweets from users on X (Twitter). Further, we derive several theoretical results about asymptotic normality and asymptotic Poissonity of Turing's estimator for two important discrete distributions.\",\n",
       " 145: 'The aim of distributional regression is to find the best candidate in a given parametric family of conditional distributions to model a given dataset. As each candidate in the distribution family can be identified by the corresponding distribution parameters, a common approach for this task is using the maximum likelihood estimator (MLE) for the parameters. In this paper, we establish theoretical results for this estimator in case the response variable is subject to random right censoring. In particular, we provide proofs of almost sure consistency and asymptotic normality of the MLE under censoring. Further, the finite-sample behavior is exemplarily demonstrated in a simulation study.',\n",
       " 146: 'Previously [Journal of Causal Inference, 10, 90-105 (2022)], we computed the variance of two estimators of causal effects for a v-structure of binary variables. Here we show that a linear combination of these estimators has lower variance than either. Furthermore, we show that this holds also when the treatment variable is block randomised with a predefined number receiving treatment, with analogous results to when it is sampled randomly.',\n",
       " 147: \"The stratified linear permutation statistic arises in various statistics problems, including stratified and post-stratified survey sampling, stratified and post-stratified experiments, conditional permutation tests, etc. Although we can derive the Berry--Esseen bounds for the stratified linear permutation statistic based on existing bounds for the non-stratified statistics, those bounds are not sharp, and moreover, this strategy does not work in general settings with heterogeneous strata with varying sizes. We first use Stein's method to obtain a unified stratified permutational Berry--Esseen bound that can accommodate heterogeneous strata. We then apply the bound to various statistics problems, leading to stronger theoretical quantifications and thereby facilitating statistical inference in those problems.\",\n",
       " 148: 'Quadratic discriminant analysis (QDA) is a widely used method for classification problems, particularly preferable over Linear Discriminant Analysis (LDA) for heterogeneous data. However, QDA loses its effectiveness in high-dimensional settings, where the data dimension and sample size tend to infinity. To address this issue, we propose a novel QDA method utilizing spectral correction and regularization techniques, termed SR-QDA. The regularization parameters in our method are selected by maximizing the Fisher-discriminant ratio. We compare SR-QDA with QDA, regularized quadratic discriminant analysis (R-QDA), and several other competitors. The results indicate that SR-QDA performs exceptionally well, especially in moderate and high-dimensional situations. Empirical experiments across diverse datasets further support this conclusion.',\n",
       " 149: 'Providing theoretical guarantees for parameter estimation in exponential random graph models is a largely open problem. While maximum likelihood estimation has theoretical guarantees in principle, verifying the assumptions for these guarantees to hold can be very difficult. Moreover, in complex networks, numerical maximum likelihood estimation is computer-intensive and may not converge in reasonable time. To ameliorate this issue, local dependency exponential random graph models have been introduced, which assume that the network consists of many independent exponential random graphs. In this setting, progress towards maximum likelihood estimation has been made. However the estimation is still computer-intensive. Instead, we propose to use so-called Stein estimators: we use the Stein characterizations to obtain new estimators for local dependency exponential random graph models.',\n",
       " 150: \"Quantifying the association between two random variables is crucial in applications. Traditional estimation techniques for common association measures, such as Spearman's rank correlation coefficient, $\\\\rho_S$, often fail when data contain ties. This is particularly problematic in zero-inflated contexts and fields like insurance, healthcare, and weather forecasting, where zeros are more frequent and require an extra probability mass. In this paper, we provide a new formulation of Spearman's rho specifically designed for zero-inflated data and propose a novel estimator of Spearman's rho based on our derived expression. Besides, we make our proposed estimator useful in practice by deriving its achievable bounds and suggest how to estimate them. We analyze our method in a comprehensive simulation study and show that our approach overcomes state-of-the-art methods in all the simulated scenarios. Additionally, we illustrate how the proposed theory can be used in practice for a more accurate quantification of association by considering two real-life applications.\",\n",
       " 151: 'The evaluation of G-Wishart normalising constants is a core component for Bayesian analyses for Gaussian graphical models, but remains a computationally intensive task in general. Based on empirical evidence, Roverato [Scandinavian Journal of Statistics, 29:391--411 (2002)] observed and conjectured that such constants can be simplified and rewritten in terms of constants with an identity scale matrix. In this note, we disprove this conjecture for general graphs by showing that the conjecture instead implies an independently-derived approximation for certain ratios of normalising constants.',\n",
       " 152: 'We study the properties of a stochastic heat equation with a generalized mixed fractional Brownian noise. We obtain the covariance structure, stationarity and obtain bounds for the asymptotic behaviour of the solution. We suggest estimators for the unknown parameters based on discrete time observations and study their asymptotic properties.',\n",
       " 153: 'Suppose we observe a trajectory of length $n$ from an $\\\\alpha$-mixing stochastic process over a finite but potentially large state space. We consider the problem of estimating the probability mass placed by the stationary distribution of any such process on elements that occur with a certain frequency in the observed sequence. We estimate this vector of probabilities in total variation distance, showing universal consistency in $n$ and recovering known results for i.i.d. sequences as special cases. Our proposed methodology carefully combines the plug-in (or empirical) estimator with a recently-proposed modification of the Good--Turing estimator called WingIt, which was originally developed for Markovian sequences. En route to controlling the error of our estimator, we develop new performance bounds on WingIt and the plug-in estimator for $\\\\alpha$-mixing stochastic processes. Importantly, the extensively used method of Poissonization can no longer be applied in our non i.i.d. setting, and so we develop complementary tools -- including concentration inequalities for a natural self-normalized statistic of mixing sequences -- that may prove independently useful in the design and analysis of estimators for related problems.',\n",
       " 154: 'We continue our work [arXiv:2403.07628] on asymptotic expansions at the soft edge for the classical $n$-dimensional Gaussian and Laguerre random matrix ensembles. By revisiting the construction of the associated skew-orthogonal polynomials in terms of wave functions, we obtain concise expressions for the level densities that are well suited for proving asymptotic expansions in powers of a certain parameter $h \\\\asymp n^{-2/3}$. In the unitary case, the expansion for the level density can be used to reconstruct the first correction term in an established asymptotic expansion of the associated generating function. In the orthogonal and symplectic cases, we can even reconstruct the conjectured first and second correction terms.',\n",
       " 155: 'Prediction model training is often hindered by limited access to individual-level data due to privacy concerns and logistical challenges, particularly in biomedical research. Resampling-based self-training presents a promising approach for building prediction models using only summary-level data. These methods leverage summary statistics to sample pseudo datasets for model training and parameter optimization, allowing for model development without individual-level data. Although increasingly used in precision medicine, the general behaviors of self-training remain unexplored. In this paper, we leverage a random matrix theory framework to establish the statistical properties of self-training algorithms for high-dimensional sparsity-free summary data. We demonstrate that, within a class of linear estimators, resampling-based self-training achieves the same asymptotic predictive accuracy as conventional training methods that require individual-level datasets. These results suggest that self-training with only summary data incurs no additional cost in prediction accuracy, while offering significant practical convenience. Our analysis provides several valuable insights and counterintuitive findings. For example, while pseudo-training and validation datasets are inherently dependent, their interdependence unexpectedly cancels out when calculating prediction accuracy measures, preventing overfitting in self-training algorithms. Furthermore, we extend our analysis to show that the self-training framework maintains this no-cost advantage when combining multiple methods or when jointly training on data from different distributions. We numerically validate our findings through simulations and real data analyses using the UK Biobank. Our study highlights the potential of resampling-based self-training to advance genetic risk prediction and other fields that make summary data publicly available.',\n",
       " 156: \"This paper proposes new ANOVA-based approximations of functions and emulators of high-dimensional models using either available derivatives or local stochastic evaluations of such models. Our approach makes use of sensitivity indices to design adequate structures of emulators. For high-dimensional models with available derivatives, our derivative-based emulators reach dimension-free mean squared errors (MSEs) and parametric rate of convergence (i.e., $\\\\mathsf{O}(N^{-1})$). This approach is extended to cope with every model (without available derivatives) by deriving global emulators that account for the local properties of models or simulators. Such generic emulators enjoy dimension-free biases, parametric rates of convergence and MSEs that depend on the dimensionality. Dimension-free MSEs are obtained for high-dimensional models with particular inputs' distributions. Our emulators are also competitive in dealing with different distributions of the input variables and for selecting inputs and interactions. Simulations show the efficiency of our approach.\",\n",
       " 157: 'In an earlier work arXiv:2410.22038, it was shown that mixtures of multivariate Gaussian or $t$-distributions can be distinguished by projecting them onto a certain predetermined finite set of lines, the number of lines depending only on the total number of distributions involved and on the ambient dimension. Using this work, we address the following two important statistical problems: that of testing and measuring the agreement between two different random partitions, and that of estimating for mixtures of multivariate normal distributions and mixtures of $t$-distributions based of univariate projections. We also compare our proposal with robust versions of the expectation-maximization method EM. In each case, we present algorithms for effecting the task, and compare them with existing methods by carrying out some simulations.',\n",
       " 158: 'We analyze the landscape and training dynamics of diagonal linear networks in a linear regression task, with the network parameters being perturbed by small isotropic normal noise. The addition of such noise may be interpreted as a stochastic form of sharpness-aware minimization (SAM) and we prove several results that relate its action on the underlying landscape and training dynamics to the sharpness of the loss. In particular, the noise changes the expected gradient to force balancing of the weight matrices at a fast rate along the descent trajectory. In the diagonal linear model, we show that this equates to minimizing the average sharpness, as well as the trace of the Hessian matrix, among all possible factorizations of the same matrix. Further, the noise forces the gradient descent iterates towards a shrinkage-thresholding of the underlying true parameter, with the noise level explicitly regulating both the shrinkage factor and the threshold.',\n",
       " 159: 'Empirical Bayes estimators are based on minimizing the average risk with the hyper-parameters in the weighting function being estimated from observed data. The performance of an empirical Bayes estimator is typically evaluated by its mean squared error (MSE). However, the explicit expression for its MSE is generally unavailable for finite sample sizes. To address this issue, we define a high-order analytical criterion: the excess MSE. It quantifies the performance difference between the maximum likelihood and empirical Bayes estimators. An explicit expression for the excess MSE of an empirical Bayes estimator employing a general data-dependent hyper-parameter estimator is derived. As specific instances, we provide excess MSE expressions for kernel-based regularized estimators using the scaled empirical Bayes, Stein unbiased risk estimation, and generalized cross-validation hyper-parameter estimators. Moreover, we propose a modification to the excess MSE expressions for regularized estimators for moderate sample sizes and show its improvement on accuracy in numerical simulations.',\n",
       " 160: 'Since polynomial regression models are generally quite reliable for data with a linear trend, it is important to note that, in some cases, they may encounter overfitting issues during the training phase, which could result in negative values of the coefficient of determination $R^2$ for unseen data. For this reason, this work proposes the partial implementation of fractional operators in polynomial regression models to generate a fractional regression model. The goal of this proposal is to attempt to mitigate overfitting, which could improve the value of the coefficient of determination for unseen data, compared to the polynomial model, under the assumption that this would contribute to generating predictive models with better performance. The methodology for constructing these fractional regression models is detailed, and examples applicable to both Riemann-Liouville and Caputo fractional operators are presented.',\n",
       " 161: \"Piecewise-Deterministic Markov Processes (PDMPs) hold significant promise for sampling from complex probability distributions. However, their practical implementation is hindered by the need to compute model-specific bounds. Conversely, while Hamiltonian Monte Carlo (HMC) offers a generally efficient approach to sampling, its inability to adaptively tune step sizes impedes its performance when sampling complex distributions like funnels. To address these limitations, we introduce three innovative concepts: (a) a Metropolis-adjusted approximation for PDMP simulation that eliminates the need for explicit bounds without compromising the invariant measure, (b) an adaptive step size mechanism compatible with the Metropolis correction, and (c) a No U-Turn Sampler (NUTS)-inspired scheme for dynamically selecting path lengths in PDMPs. These three ideas can be seamlessly integrated into a single, `doubly-adaptive' PDMP sampler with favourable robustness and efficiency properties.\",\n",
       " 162: 'While measures of concordance -- such as Spearman\\'s rho, Kendall\\'s tau, and Blomqvist\\'s beta -- are continuous with respect to weak convergence, Chatterjee\\'s rank correlation xi recently introduced in Azadkia and Chatterjee [5] does not share this property, causing drawbacks in statistical inference as pointed out in B\\\\\"ucher and Dette [7]. As we study in this paper, xi is instead weakly continuous with respect to conditionally independent copies -- the Markov products. To establish weak continuity of Markov products, we provide several sufficient conditions, including copula-based criteria and conditions relying on the concept of conditional weak convergence in Sweeting [36]. As a consequence, we also obtain continuity results for xi and related dependence measures and verify their continuity in the parameters of standard models such as multivariate elliptical and l1-norm symmetric distributions.',\n",
       " 163: 'We examine a generalisation of the beta distribution that we call the pushed beta distribution. This is a continuous univariate distribution on the unit interval which generalises the beta distribution by \"pushing\" the density in a particular direction using an additional multiplicative term in the density kernel. We examine the properties of this distribution and compare it to the beta distribution. We also examine the use of this distribution in contaminated binary sampling using Bayesian inference. We find that this distribution arises as the appropriate posterior distribution for inference in certain kinds of contaminated binary models. We derive a broad range of properties of the distribution and we also establish some computational methods to compute various functions for the distribution.',\n",
       " 164: 'Aligning large language models (LLMs) with diverse human preferences is critical for ensuring fairness and informed outcomes when deploying these models for decision-making. In this paper, we seek to uncover fundamental statistical limits concerning aligning LLMs with human preferences, with a focus on the probabilistic representation of human preferences and the preservation of diverse preferences in aligned LLMs. We first show that human preferences can be represented by a reward model if and only if the preference among LLM-generated responses is free of any Condorcet cycle. Moreover, we prove that Condorcet cycles exist with probability converging to one exponentially fast under a probabilistic preference model, thereby demonstrating the impossibility of fully aligning human preferences using reward-based approaches such as reinforcement learning from human feedback. Next, we explore the conditions under which LLMs would employ mixed strategies -- meaning they do not collapse to a single response -- when aligned in the limit using a non-reward-based approach, such as Nash learning from human feedback (NLHF). We identify a necessary and sufficient condition for mixed strategies: the absence of a response that is preferred over all others by a majority. As a blessing, we prove that this condition holds with high probability under the probabilistic preference model, thereby highlighting the statistical possibility of preserving minority preferences without explicit regularization in aligning LLMs. Finally, we leverage insights from our statistical results to design a novel, computationally efficient algorithm for finding Nash equilibria in aligning LLMs with NLHF. Our experiments show that Llama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\\\\% against the base model.',\n",
       " 165: 'This paper considers the problem of design-based inference for the average treatment effect in finely stratified experiments. Here, by \"design-based\\'\\' we mean that the only source of uncertainty stems from the randomness in treatment assignment; by \"finely stratified\\'\\' we mean units are first stratified into groups of size k according to baseline covariates and then, within each group, a fixed number l < k are assigned uniformly at random to treatment and the remainder to control. In this setting, we first show under mild conditions that inference using the difference-in-means estimator requires an estimator of its variance that is at least asymptotically upward-biased. We then present a novel estimator of the variance and show that it is upward-biased; furthermore, the magnitude of the bias depends in a natural way on the quality of the stratification. Importantly, this estimator remains well-defined even in the setting in which l = 1 or k - l = 1. We then compare our estimator with some well-known estimators that have been proposed previously for this case. We first show that, while these estimators are also upward-biased, the magnitude of their bias does not change in the natural way with the quality of stratification. To further discriminate among these estimators, we introduce a framework motivated by a thought experiment in which the finite population can be modeled as having been drawn once in an i.i.d. fashion from a well-behaved probability distribution. In this framework, we argue that our estimator dominates the others in terms of limiting bias, and that these improvements are strict except under exceptionally strong restrictions on the treatment effects. Finally, we illustrate our theoretical results through a simulation study, which reveals that our estimator can lead to substantially more precise inferences, especially when the quality of stratification is high.',\n",
       " 166: 'We study the minimax rate of estimation in nonparametric exponential family regression under star-shaped constraints. Specifically, the parameter space $K$ is a star-shaped set contained within a bounded box $[-M, M]^n$, where $M$ is a known positive constant. Moreover, we assume that the exponential family is nonsingular and that its cumulant function is twice continuously differentiable. Our main result shows that the minimax rate for this problem is $\\\\varepsilon^{*2} \\\\wedge \\\\operatorname{diam}(K)^2$, up to absolute constants, where $\\\\varepsilon^*$ is defined as \\\\[ \\\\varepsilon^* = \\\\sup \\\\{\\\\varepsilon: \\\\varepsilon^2 \\\\kappa(M) \\\\leq \\\\log N^{\\\\operatorname{loc}}(\\\\varepsilon)\\\\}, \\\\] with $N^{\\\\operatorname{loc}}(\\\\varepsilon)$ denoting the local entropy and $\\\\kappa(M)$ is an absolute constant allowed to depend on $M$. We also provide an example and derive its corresponding minimax optimal rate.',\n",
       " 167: \"We prove an upper bound on the expected $\\\\ell_p$ injective norm of sums of subgaussian random tensors. Our proof is simple and does not rely on any explicit geometric or chaining arguments. Instead, it follows from a simple application of the PAC-Bayesian lemma, a tool that has proven effective at controlling the suprema of certain ``smooth'' empirical processes in recent years. Our bound strictly improves a very recent result of Bandeira, Gopi, Jiang, Lucca, and Rothvoss. In the Euclidean case ($p=2$), our bound sharpens a result of Lata{\\\\l}a that was central to proving his estimates on the moments of Gaussian chaoses. As a consequence, we obtain an elementary proof of this fundamental result.\",\n",
       " 168: 'Longitudinal networks are becoming increasingly relevant in the study of dynamic processes characterised by known or inferred community structure. Generalised Network Autoregressive (GNAR) models provide a parsimonious framework for exploiting the underlying network and multivariate time series. We introduce the community-$\\\\alpha$ GNAR model with interactions that exploits prior knowledge or exogenous variables for analysing interactions within and between communities, and can describe serial correlation in longitudinal networks. We derive new explicit finite-sample error bounds that validate analysing high-dimensional longitudinal network data with GNAR models, and provide insights into their attractive properties. We further illustrate our approach by analysing the dynamics of $\\\\textit{Red, Blue}$ and $\\\\textit{Swing}$ states throughout presidential elections in the USA from 1976 to 2020, that is, a time series of length twelve on 51 time series (US states and Washington DC). Our analysis connects network autocorrelation to eight-year long terms, highlights a possible change in the system after the 2016 election, and a difference in behaviour between $\\\\textit{Red}$ and $\\\\textit{Blue}$ states.',\n",
       " 169: 'We study the task of list-decodable linear regression using batches. A batch is called clean if it consists of i.i.d. samples from an unknown linear regression distribution. For a parameter $\\\\alpha \\\\in (0, 1/2)$, an unknown $\\\\alpha$-fraction of the batches are clean and no assumptions are made on the remaining ones. The goal is to output a small list of vectors at least one of which is close to the true regressor vector in $\\\\ell_2$-norm. [DJKS23] gave an efficient algorithm, under natural distributional assumptions, with the following guarantee. Assuming that the batch size $n$ satisfies $n \\\\geq \\\\tilde{\\\\Omega}(\\\\alpha^{-1})$ and the number of batches is $m = \\\\mathrm{poly}(d, n, 1/\\\\alpha)$, their algorithm runs in polynomial time and outputs a list of $O(1/\\\\alpha^2)$ vectors at least one of which is $\\\\tilde{O}(\\\\alpha^{-1/2}/\\\\sqrt{n})$ close to the target regressor. Here we design a new polynomial time algorithm with significantly stronger guarantees under the assumption that the low-degree moments of the covariates distribution are Sum-of-Squares (SoS) certifiably bounded. Specifically, for any constant $\\\\delta>0$, as long as the batch size is $n \\\\geq \\\\Omega_{\\\\delta}(\\\\alpha^{-\\\\delta})$ and the degree-$\\\\Theta(1/\\\\delta)$ moments of the covariates are SoS certifiably bounded, our algorithm uses $m = \\\\mathrm{poly}((dn)^{1/\\\\delta}, 1/\\\\alpha)$ batches, runs in polynomial-time, and outputs an $O(1/\\\\alpha)$-sized list of vectors one of which is $O(\\\\alpha^{-\\\\delta/2}/\\\\sqrt{n})$ close to the target. That is, our algorithm achieves substantially smaller minimum batch size and final error, while achieving the optimal list size. Our approach uses higher-order moment information by carefully combining the SoS paradigm interleaved with an iterative method and a novel list pruning procedure. In the process, we give an SoS proof of the Marcinkiewicz-Zygmund inequality that may be of broader applicability.',\n",
       " 170: 'Score-based diffusion models have become a foundational paradigm for modern generative modeling, demonstrating exceptional capability in generating samples from complex high-dimensional distributions. Despite the dominant adoption of probability flow ODE-based samplers in practice due to their superior sampling efficiency and precision, rigorous statistical guarantees for these methods have remained elusive in the literature. This work develops the first end-to-end theoretical framework for deterministic ODE-based samplers that establishes near-minimax optimal guarantees under mild assumptions on target data distributions. Specifically, focusing on subgaussian distributions with $\\\\beta$-H\\\\\"older smooth densities for $\\\\beta\\\\leq 2$, we propose a smooth regularized score estimator that simultaneously controls both the $L^2$ score error and the associated mean Jacobian error. Leveraging this estimator within a refined convergence analysis of the ODE-based sampling process, we demonstrate that the resulting sampler achieves the minimax rate in total variation distance, modulo logarithmic factors. Notably, our theory comprehensively accounts for all sources of error in the sampling process and does not require strong structural conditions such as density lower bounds or Lipschitz/smooth scores on target distributions, thereby covering a broad range of practical data distributions.',\n",
       " 171: 'For one dimensional stochastic Burgers equation driven by space-time white noise we consider the problem of estimation of the diffusivity parameter in front of the second-order spatial derivative. Based on local observations in space, we study the estimator derived in [Altmeyer, Rei{\\\\ss}, Ann. Appl. Probab.(2021)] for linear stochastic heat equation that has also been used in [Altmeyer, Cialenco, Pasemann, Bernoulli (2023)] to cover large class of semilinear SPDEs and has been examined for the stochastic Burgers equation driven by trace class noise. We extend the achieved results by considering the space-time white noise case which has also relevant physical motivations. After we establish new regularity results for the solution, we are able to show that our proposed estimator is strongly consistent and asymptotically normal.',\n",
       " 172: \"The failure of a system can result from the simultaneous effects of multiple causes, where assigning a specific cause may be inappropriate or unavailable. Examples include contributing causes of death in epidemiology and the aetiology of neurodegenerative diseases like Alzheimer's. We propose a parametric Weibull accelerated failure time model for multiple causes, incorporating a data-driven, individualized, and time-varying winning probability (relative importance) matrix. Using maximum likelihood estimation and the expectation-maximization (EM) algorithm, our approach enables simultaneous estimation of regression coefficients and relative cause importance, ensuring consistency and asymptotic normality. A simulation study and an application to Alzheimer's disease demonstrate its effectiveness in addressing cause-mixture problems and identifying informative biomarker combinations, with comparisons to Weibull and Cox proportional hazards models.\",\n",
       " 173: \"This paper tackles the challenge of estimating a low-rank graphon from sampled network data, employing a singular value thresholding (SVT) estimator to create a piecewise-constant graphon based on the network's adjacency matrix. Under certain assumptions about the graphon's structural properties, we establish bounds on the operator norm distance between the true graphon and its estimator, as well as on the rank of the estimated graphon. In the second part of the paper, we apply our estimator to graphon games. We derive bounds on the suboptimality of interventions in the social welfare problem in graphon games when the intervention is based on the estimated graphon. These bounds are expressed in terms of the operator norm of the difference between the true and estimated graphons. We also emphasize the computational benefits of using the low-rank estimated graphon to solve these problems.\",\n",
       " 174: 'Unbiased data synthesis is crucial for evaluating causal discovery algorithms in the presence of unobserved confounding, given the scarcity of real-world datasets. A common approach, implicit parameterization, encodes unobserved confounding by modifying the off-diagonal entries of the idiosyncratic covariance matrix while preserving positive definiteness. Within this approach, we identify that state-of-the-art protocols have two distinct issues that hinder unbiased sampling from the complete space of causal models: first, we give a detailed analysis of use of diagonally dominant constructions restricts the spectrum of partial correlation matrices; and second, the restriction of possible graphical structures when sampling bidirected edges, unnecessarily ruling out valid causal models. To address these limitations, we propose an improved explicit modeling approach for unobserved confounding, leveraging block-hierarchical ancestral generation of ground truth causal graphs. Algorithms for converting the ground truth DAG into ancestral graph is provided so that the output of causal discovery algorithms could be compared with. We draw connections between implicit and explicit parameterization, prove that our approach fully covers the space of causal models, including those generated by the implicit parameterization, thus enabling more robust evaluation of methods for causal discovery and inference.',\n",
       " 175: 'This paper studies the problem of inferring a $k$-factor, specifically a spanning $k$-regular graph, planted within an Erdos-Renyi random graph $G(n,\\\\lambda/n)$. We uncover an interesting \"all-something-nothing\" phase transition. Specifically, we show that as the average degree $\\\\lambda$ surpasses the critical threshold of $1/k$, the inference problem undergoes a transition from almost exact recovery (\"all\" phase) to partial recovery (\"something\" phase). Moreover, as $\\\\lambda$ tends to infinity, the accuracy of recovery diminishes to zero, leading to the onset of the \"nothing\" phase. This finding complements the recent result by Mossel, Niles-Weed, Sohn, Sun, and Zadik who established that for certain sufficiently dense graphs, the problem undergoes an \"all-or-nothing\" phase transition, jumping from near-perfect to near-zero recovery. In addition, we characterize the recovery accuracy of a linear-time iterative pruning algorithm and show that it achieves almost exact recovery when $\\\\lambda < 1/k$. A key component of our analysis is a two-step cycle construction: we first build trees through local neighborhood exploration and then connect them by sprinkling using reserved edges. Interestingly, for proving impossibility of almost exact recovery, we construct $\\\\Theta(n)$ many small trees of size $\\\\Theta(1)$, whereas for establishing the algorithmic lower bound, a single large tree of size $\\\\Theta(\\\\sqrt{n\\\\log n})$ suffices.',\n",
       " 176: 'We consider two random variables $X$ and $Y$ following correlated Gamma distributions, characterized by identical scale and shape parameters and a linear correlation coefficient $\\\\rho$. Our focus is on the parameter: \\\\[ D(X,Y) = \\\\frac{|X - Y|}{X + Y}, \\\\] which appears in applied contexts such as dynamic speckle imaging, where it is known as the \\\\textit{Fujii index}. In this work, we derive a closed-form expression for the probability density function of $D(X,Y)$ as well as analytical formulas for its moments of order $k$. Our derivation starts by representing $X$ and $Y$ as two correlated exponential random variables, obtained from the squared magnitudes of circular complex Gaussian variables. By considering the sum of $k$ independent exponential variables, we then derive the joint density of $(X,Y)$ when $X$ and $Y$ are two correlated Gamma variables. Through appropriate varable transformations, we obtain the theoretical distribution of $D(X,Y)$ and evaluate its moments analytically. These theoretical findings are validated through numerical simulations, with particular attention to two specific cases: zero correlation and unit shape parameter.',\n",
       " 177: 'Early work established convergence of the principal component estimators of the factors and loadings up to a rotation for large dimensional approximate factor models with weak factors in that the factor loading $\\\\Lambda^{(0)}$ scales sublinearly in the number $N$ of cross-section units, i.e., $\\\\Lambda^{(0)\\\\top}\\\\Lambda^{(0)}/N^{\\\\alpha}$ is positive definite in the limit for some $\\\\alpha\\\\in (0,1)$. However, the established convergence rates for weak factors can be much slower especially for small $\\\\alpha$. This article proposes a Transfer Principal Component Analysis (TransPCA) method for enhancing the convergence rates for weak factors by transferring knowledge from large number of available informative panel datasets, which should not be turned a blind eye on in this big data era. We aggregate useful information by analyzing a weighted average projection matrix of the estimated loading spaces from all informative datasets which is highly flexible and computationally efficient. Theoretically, we derive the convergence rates of the estimators of weak/strong loading spaces and factor scores. The results indicate that as long as the auxiliary datasets are similar enough to the target dataset and the auxiliary sample size is sufficiently large, TransPCA estimators can achieve faster convergence rates in contrast to performing PCA solely on the target dataset. To avoid negative transfer, we also investigate the case that the informative datasets are unknown and provide a criterion for selecting useful datasets. Thorough simulation studies and {empirical analysis on real datasets in areas of macroeconomic and finance} are conducted to illustrate the usefulness of our proposed methods where large number of source panel datasets are naturally available.',\n",
       " 178: \"This work addresses the problem of estimating a vector field from a noisy Ordinary Differential Equation (ODE) in a non-parametric regression setting with a random design for initial values. More specifically, given a vector field $ f:\\\\mathbb{R}^{D}\\\\rightarrow \\\\mathbb{R}^{D}$ governing a dynamical system defined by the autonomous ODE: $y' = f(y)$, we assume that the observations are $\\\\tilde{y}_{X_{i}}(t_{j}) = y_{X_{i}}(t_{j}) + \\\\varepsilon_{i,j}$ where $y_{X_{i}}(t_{j})$ is the solution of the ODE at time $t_{j}$ with initial condition $y(0) = X_{i}$, $X_{i}$ is sampled from a probability distribution $\\\\mu$, and $\\\\varepsilon_{i,j}$ some noise. In this context, we investigate, from a minimax perspective, the pointwise reconstruction of $f$ within the envelope of trajectories originating from the support of $\\\\mu$. We propose an estimation strategy based on preliminary flow reconstruction and techniques from derivative estimation in non-parametric regression. Under mild assumptions on $f$, we establish convergence rates that depend on the temporal resolution, the number of sampled initial values and the mass concentration of $\\\\mu$. Importantly, we show that these rates are minimax optimal. Furthermore, we discuss the implications of our results in a manifold learning setting, providing insights into how our approach can mitigate the curse of dimensionality.\",\n",
       " 179: \"This paper examines the evolution of the Finnish electric energy system up to 2035, focusing on the likelihood of different development paths. The primary contribution of this paper is the development of an extensive Bayesian Network, designed to model and analyse the evolution of power generation capacity mix, assess the likelihood of different grid management scenarios, and understand the causal relationships underlying these scenarios. A target optimisation was carried out using the constructed Bayesian Network to explore possibilities to minimise grid management complexity. The results of the optimisation reveal that the authorities and stakeholders should prioritise increasing demand response, gas power, and battery storage capacities. These mature technologies are well-suited to guarantee energy adequacy during peak consumption periods, which in Finland typically occur during consecutive cold, dark and windless winter weeks. Although this study focuses on the evolution of the Finnish power grid, the constructed Bayesian Network approach is broadly applicable and can be utilised to explore causal relationships in other countries by employing the designed questionnaire and engaging a panel of experts specific to the country's energy infrastructure.\",\n",
       " 180: 'Graph sparsification is a well-established technique for accelerating graph-based learning algorithms, which uses edge sampling to approximate dense graphs with sparse ones. Because the sparsification error is random and unknown, users must contend with uncertainty about the reliability of downstream computations. Although it is possible for users to obtain conceptual guidance from theoretical error bounds in the literature, such results are typically impractical at a numerical level. Taking an alternative approach, we propose to address these issues from a data-driven perspective by computing empirical error estimates. The proposed error estimates are highly versatile, and we demonstrate this in four use cases: Laplacian matrix approximation, graph cut queries, graph-structured regression, and spectral clustering. Moreover, we provide two theoretical guarantees for the error estimates, and explain why the cost of computing them is manageable in comparison to the overall cost of a typical graph sparsification workflow.',\n",
       " 181: 'COVID-19 has had a large scale negative impact on the health of opioid users exacerbating the health of an already vulnerable population. Critical information on the total impact of COVID-19 on opioid users is unknown due to a lack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and lack of data coverage. To assess the impact of COVID-19 on small-area opioid mortality, we developed a Bayesian hierarchical excess opioid mortality modeling approach. We incorporate spatio-temporal autocorrelation structures to allow for sharing of information across small areas and time to reduce uncertainty in small area estimates. Excess mortality is defined as the difference between observed trends after a crisis and expected trends based on observed historical trends, which captures the total increase in observed mortality rates compared to what was expected prior to the crisis. We illustrate the application of our approach to assess excess opioid mortality risk estimates for 159 counties in GA. Using our proposed approach will help inform interventions in opioid-related public health responses, policies, and resource allocation. The application of this work also provides a general framework for improving the estimation and mapping of health indicators during crisis periods for the opioid user population.',\n",
       " 182: \"It is a folklore belief that metastable wells in low-temperature statistical mechanics models exhibit high-temperature behavior. We prove a rigorous version of this phenomenon in the setting of the exponential random graph model (ERGM) through the lens of concentration of measure. To do this, we first present a new general result deriving concentration inequalities in a metastable well from the metastable mixing of a Markov chain with the appropriate stationary distribution, extending a result of Chatterjee [Cha05] which is suited for more traditional forms of global mixing. We then apply this result to the supercritical (low-temperature) ERGM which was recently proven to exhibit metastable mixing by Bresler, Nagaraj, and Nichani [BNN24], and obtain a novel concentration inequality for Lipschitz observables of the supercritical ERGM conditioned on a large metastable well, answering a question posed by [BNN24]. This extends a result of Ganguly and Nam [GN24] from the subcritical (high-temperature) regime to a metastable well in the supercritical regime, and we are also able to extend the applications of their concentration inequality to these metastable wells. Namely, we obtain an upper bound on the Wasserstein distance between the ERGM conditioned on a metastable well and an appropriate Erd\\\\H{o}s-R\\\\'enyi model, as well as derive a central limit theorem for the count of edges in certain small subcollections of possible edges. Finally, to supplement the mathematical content of the article, we also discuss the results of what appears to be the first simulation study of a metastable well in the supercritical ERGM.\",\n",
       " 183: 'Language model alignment (or, reinforcement learning) techniques that leverage active exploration -- deliberately encouraging the model to produce diverse, informative responses -- offer the promise of super-human capabilities. However, current understanding of algorithm design primitives for computationally efficient exploration with language models is limited. To better understand how to leverage access to powerful pre-trained generative models to improve the efficiency of exploration, we introduce a new computational framework for RL with language models, in which the learner interacts with the model through a sampling oracle. Focusing on the linear softmax model parameterization, we provide new results that reveal the computational-statistical tradeoffs of efficient exploration: 1. Necessity of coverage: Coverage refers to the extent to which the pre-trained model covers near-optimal responses -- a form of hidden knowledge. We show that coverage, while not necessary for data efficiency, lower bounds the runtime of any algorithm in our framework. 2. Inference-time exploration: We introduce a new algorithm, SpannerSampling, which obtains optimal data efficiency and is computationally efficient whenever the pre-trained model enjoys sufficient coverage, matching our lower bound. SpannerSampling leverages inference-time computation with the pre-trained model to reduce the effective search space for exploration. 3. Insufficiency of training-time interventions: We contrast the result above by showing that training-time interventions that produce proper policies cannot achieve similar guarantees in polynomial time. 4. Computational benefits of multi-turn exploration: Finally, we show that under additional representational assumptions, one can achieve improved runtime (replacing sequence-level coverage with token-level coverage) through multi-turn exploration.',\n",
       " 184: 'Human physiological signals tend to exhibit both global and local structures: the former are shared across a population, while the latter reflect inter-individual variability. For instance, kinetic measurements of the gait cycle during locomotion present common characteristics, although idiosyncrasies may be observed due to biomechanical disposition or pathology. To better represent datasets with local-global structure, this work extends Convolutional Dictionary Learning (CDL), a popular method for learning interpretable representations, or dictionaries, of time-series data. In particular, we propose Personalized CDL (PerCDL), in which a local dictionary models local information as a personalized spatiotemporal transformation of a global dictionary. The transformation is learnable and can combine operations such as time warping and rotation. Formal computational and statistical guarantees for PerCDL are provided and its effectiveness on synthetic and real human locomotion data is demonstrated.',\n",
       " 185: 'A common observation in data-driven applications is that high-dimensional data have a low intrinsic dimension, at least locally. In this work, we consider the problem of point estimation for manifold-valued data. Namely, given a finite set of noisy samples of $\\\\mathcal{M}$, a $d$ dimensional submanifold of $\\\\mathbb{R}^D$, and a point $r$ near the manifold we aim to project $r$ onto the manifold. Assuming that the data was sampled uniformly from a tubular neighborhood of a $k$-times smooth boundaryless and compact manifold, we present an algorithm that takes $r$ from this neighborhood and outputs $\\\\hat p_n\\\\in \\\\mathbb{R}^D$, and $\\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ an element in the Grassmannian $Gr(d, D)$. We prove that as the number of samples $n\\\\to\\\\infty$, the point $\\\\hat p_n$ converges to $\\\\mathbf{p}\\\\in \\\\mathcal{M}$, the projection of $r$ onto $\\\\mathcal{M}$, and $\\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ converges to $T_{\\\\mathbf{p}}\\\\mathcal{M}$ (the tangent space at that point) with high probability. Furthermore, we show that $\\\\hat p_n$ approaches the manifold with an asymptotic rate of $n^{-\\\\frac{k}{2k + d}}$, and that $\\\\hat p_n, \\\\widehat{T_{\\\\hat p_n}\\\\mathcal{M}}$ approach $\\\\mathbf{p}$ and $T_{\\\\mathbf{p}}\\\\mathcal{M}$ correspondingly with asymptotic rates of $n^{-\\\\frac{k-1}{2k + d}}$.',\n",
       " 186: 'We construct a family of estimators for a regression function based on a sample following a qdistribution. Our approach is nonparametric, using kernel methods built from operations that leverage the properties of q-calculus. Furthermore, under appropriate assumptions, we establish the weak convergence and strong consistency of this family of estimators.',\n",
       " 187: 'For some discretely observed path of oscillating Brownian motion with level of self-organized criticality $\\\\rho_0$, we prove in the infill asymptotics that the MLE is $n$-consistent, where $n$ denotes the sample size, and derive its limit distribution with respect to stable convergence. As the transition density of this homogeneous Markov process is not even continuous in $\\\\rho_0$, the analysis is highly non-standard. Therefore, interesting and somewhat unexpected phenomena occur: The likelihood function splits into several components, each of them contributing very differently depending on how close the argument $\\\\rho$ is to $\\\\rho_0$. Correspondingly, the MLE is successively excluded to lay outside a compact set, a $1/\\\\sqrt{n}$-neighborhood and finally a $1/n$-neigborhood of $\\\\rho_0$ asymptotically. The crucial argument to derive the stable convergence is to exploit the semimartingale structure of the sequential suitably rescaled local log-likelihood function (as a process in time). Both sequentially and as a process in $\\\\rho$, it exhibits a bivariate Poissonian behavior in the stable limit with its intensity being a multiple of the local time at $\\\\rho_0$.',\n",
       " 188: 'Community detection, which focuses on recovering the group structure within networks, is a crucial and fundamental task in network analysis. However, the detection process can be quite challenging and unstable when community signals are weak. Motivated by a newly collected large-scale academic network dataset from the Web of Science, which includes multi-layer network information, we propose a Bipartite Assisted Spectral-clustering approach for Identifying Communities (BASIC), which incorporates the bipartite network information into the community structure learning of the primary network. The accuracy and stability enhancement of BASIC is validated theoretically on the basis of the degree-corrected stochastic block model framework, as well as numerically through extensive simulation studies. We rigorously study the convergence rate of BASIC even under weak signal scenarios and prove that BASIC yields a tighter upper error bound than that based on the primary network information alone. We utilize the proposed BASIC method to analyze the newly collected large-scale academic network dataset from statistical papers. During the author collaboration network structure learning, we incorporate the bipartite network information from author-paper, author-institution, and author-region relationships. From both statistical and interpretative perspectives, these bipartite networks greatly aid in identifying communities within the primary collaboration network.',\n",
       " 189: 'We present two limit theorems, a mean ergodic and a central limit theorem, for a specific class of one-dimensional diffusion processes that depend on a small-scale parameter $\\\\varepsilon$ and converge weakly to a homogenized diffusion process in the limit $\\\\varepsilon \\\\rightarrow 0$. In these results, we allow for the time horizon to blow up such that $T_\\\\varepsilon \\\\rightarrow \\\\infty$ as $\\\\varepsilon \\\\rightarrow 0$. The novelty of the results arises from the circumstance that many quantities are unbounded for $\\\\varepsilon \\\\rightarrow 0$, so that formerly established theory is not directly applicable here and a careful investigation of all relevant $\\\\varepsilon$-dependent terms is required. As a mathematical application, we then use these limit theorems to prove asymptotic properties of a minimum distance estimator for parameters in a homogenized diffusion equation.',\n",
       " 190: 'The behavior of extreme observations is well-understood for time series or spatial data, but little is known if the data generating process is a structural causal model (SCM). We study the behavior of extremes in this model class, both for the observational distribution and under extremal interventions. We show that under suitable regularity conditions on the structure functions, the extremal behavior is described by a multivariate Pareto distribution, which can be represented as a new SCM on an extremal graph. Importantly, the latter is a sub-graph of the graph in the original SCM, which means that causal links can disappear in the tails. We further introduce a directed version of extremal graphical models and show that an extremal SCM satisfies the corresponding Markov properties. Based on a new test of extremal conditional independence, we propose two algorithms for learning the extremal causal structure from data. The first is an extremal version of the PC-algorithm, and the second is a pruning algorithm that removes edges from the original graph to consistently recover the extremal graph. The methods are illustrated on river data with known causal ground truth.',\n",
       " 191: \"Consider a pair of sparse correlated stochastic block models $\\\\mathcal S(n,\\\\tfrac{\\\\lambda}{n},\\\\epsilon;s)$ subsampled from a common parent stochastic block model with two symmetric communities, average degree $\\\\lambda=O(1)$ and divergence parameter $\\\\epsilon \\\\in (0,1)$. For all $\\\\epsilon\\\\in(0,1)$, we construct a statistic based on the combination of two low-degree polynomials and show that there exists a sufficiently small constant $\\\\delta=\\\\delta(\\\\epsilon)>0$ and a sufficiently large constant $\\\\Delta=\\\\Delta(\\\\epsilon,\\\\delta)$ such that when $\\\\lambda>\\\\Delta$ and $s>\\\\sqrt{\\\\alpha}-\\\\delta$ where $\\\\alpha\\\\approx 0.338$ is Otter's constant, this statistic can distinguish this model and a pair of independent stochastic block models $\\\\mathcal S(n,\\\\tfrac{\\\\lambda s}{n},\\\\epsilon)$ with probability $1-o(1)$. We also provide an efficient algorithm that approximates this statistic in polynomial time. The crux of our statistic's construction lies in a carefully curated family of multigraphs called \\\\emph{decorated trees}, which enables effective aggregation of the community signal and graph correlation from the counts of the same decorated tree while suppressing the undesirable correlations among counts of different decorated trees.\",\n",
       " 192: 'This paper is devoted to parameter estimation for partially observed polynomial state space models. This class includes discretely observed affine or more generally polynomial Markov processes. The polynomial structure allows for the explicit computation of a Gaussian quasi-likelihood estimator and its asymptotic covariance matrix. We show consistency and asymptotic normality of the estimating sequence and provide explicitly computable expressions for the corresponding asymptotic covariance matrix.',\n",
       " 193: 'This paper is devoted to filtering, smoothing, and prediction of polynomial processes that are partially observed. These problems are known to allow for an explicit solution in the simpler case of linear Gaussian state space models. The key insight underlying the present piece of research is that in filtering applications polynomial processes and their discrete counterpart are indistinguishable from Gaussian processes sharing their first two moments. We describe the construction of these Gaussian equivalents of polynomial processes and explicitly compute optimal linear filters, predictors and smoothers for polynomial processes in discrete and continuous time. The consideration of Gaussian equivalents also opens the door to parameter estimation and linear-quadratic optimal control in the context of polynomial processes.',\n",
       " 194: 'We consider standard gradient descent, gradient flow and conjugate gradients as iterative algorithms for minimizing a penalized ridge criterion in linear regression. While it is well known that conjugate gradients exhibit fast numerical convergence, the statistical properties of their iterates are more difficult to assess due to inherent nonlinearities and dependencies. On the other hand, standard gradient flow is a linear method with well known regularizing properties when stopped early. By an explicit non-standard error decomposition we are able to bound the prediction error for conjugate gradient iterates by a corresponding prediction error of gradient flow at transformed iteration indices. This way, the risk along the entire regularisation path of conjugate gradient iterations can be compared to that for regularisation paths of standard linear methods like gradient flow and ridge regression. In particular, the oracle conjugate gradient iterate shares the optimality properties of the gradient flow and ridge regression oracles up to a constant factor. Numerical examples show the similarity of the regularisation paths in practice.',\n",
       " 195: 'We provide conditions for the stochastic dominance comparisons of a risk $X$ and an associated risk $X+Z$, where $Z$ represents the uncertainty due to the environment and where $X$ and $Z$ can be dependent. The comparisons depend on both the copula $C$ between the distributions of $X$ and $Z$ and on the distribution of $Z$. We provide two different conditions for $C$ which represents new positive dependence properties. Regarding $Z$, we need some symmetry or asymmetry (skew) properties. Some illustrative examples are provided.',\n",
       " 196: 'We consider the graph alignment problem, wherein the objective is to find a vertex correspondence between two graphs that maximizes the edge overlap. The graph alignment problem is an instance of the quadratic assignment problem (QAP), known to be NP-hard in the worst case even to approximately solve. In this paper, we analyze Birkhoff relaxation, a tight convex relaxation of QAP, and present theoretical guarantees on its performance when the inputs follow the Gaussian Wigner Model. More specifically, the weighted adjacency matrices are correlated Gaussian Orthogonal Ensemble with correlation $1/\\\\sqrt{1+\\\\sigma^2}$. Denote the optimal solutions of the QAP and Birkhoff relaxation by $\\\\Pi^\\\\star$ and $X^\\\\star$ respectively. We show that $\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = o(n)$ when $\\\\sigma = o(n^{-1.25})$ and $\\\\|X^\\\\star-\\\\Pi^\\\\star\\\\|_F^2 = \\\\Omega(n)$ when $\\\\sigma = \\\\Omega(n^{-0.5})$. Thus, the optimal solution $X^\\\\star$ transitions from a small perturbation of $\\\\Pi^\\\\star$ for small $\\\\sigma$ to being well separated from $\\\\Pi^\\\\star$ as $\\\\sigma$ becomes larger than $n^{-0.5}$. This result allows us to guarantee that simple rounding procedures on $X^\\\\star$ align $1-o(1)$ fraction of vertices correctly whenever $\\\\sigma = o(n^{-1.25})$. This condition on $\\\\sigma$ to ensure the success of the Birkhoff relaxation is state-of-the-art.',\n",
       " 197: 'We consider the problem of sequential estimation of a single change point in a piecewise linear regression model under a Gaussian setup. We demonstrate that a certain CUSUM-type statistic attains the minimax optimal rates for localizing the change point. Our minimax analysis unveils an interesting phase transition from a jump (discontinuity in values) to a kink (change in slope). Specifically, for a jump, the minimax rate is of order $\\\\log (n) / n$, whereas for a kink it scales as $\\\\bigl(\\\\log (n) / n\\\\bigr)^{1/3}$, given that the sampling rate is of order $1/n$. We further introduce an algorithm for the proposed online change point detector, which requires constant computational steps and constant memory per incoming sample. Finally, the empirical performance of our method is examined on both simulated and real-world data sets. An implementation is available in the R package FLOC on GitHub.',\n",
       " 198: \"We propose causal effect estimators based on empirical Fr\\\\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators. Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\\\\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings.\",\n",
       " 199: \"Given two populations from which independent binary observations are taken with parameters $p_1$ and $p_2$ respectively, estimators are proposed for the relative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their logarithms. The estimators guarantee that the relative mean-square error, or the mean-square error for the logarithmic versions, is less than a target value for any $p_1, p_2 \\\\in (0,1)$, and the ratio of average sample sizes from the two populations is close to a prescribed value. The estimators can also be used with group sampling, whereby samples are taken in batches of fixed size from the two populations. The efficiency of the estimators with respect to the Cram\\\\'er-Rao bound is good, and in particular it is close to $1$ for small values of the target error.\",\n",
       " 200: 'We propose a novel approach for learning causal response representations. Our method aims to extract directions in which a multidimensional outcome is most directly caused by a treatment variable. By bridging conditional independence testing with causal representation learning, we formulate an optimisation problem that maximises the evidence against conditional independence between the treatment and outcome, given a conditioning set. This formulation employs flexible regression models tailored to specific applications, creating a versatile framework. The problem is addressed through a generalised eigenvalue decomposition. We show that, under mild assumptions, the distribution of the largest eigenvalue can be bounded by a known $F$-distribution, enabling testable conditional independence. We also provide theoretical guarantees for the optimality of the learned representation in terms of signal-to-noise ratio and Fisher information maximisation. Finally, we demonstrate the empirical effectiveness of our approach in simulation and real-world experiments. Our results underscore the utility of this framework in uncovering direct causal effects within complex, multivariate settings.',\n",
       " 201: 'Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense. Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.',\n",
       " 202: 'The primary objective of learning methods is generalization. Classic uniform generalization bounds, which rely on VC-dimension or Rademacher complexity, fail to explain the significant attribute that over-parameterized models in deep learning exhibit nice generalizability. On the other hand, algorithm-dependent generalization bounds, like stability bounds, often rely on strict assumptions. To establish generalizability under less stringent assumptions, this paper investigates the generalizability of neural networks that minimize or approximately minimize empirical risk. We establish a lower bound for population accuracy based on the expressiveness of these networks, which indicates that with an adequate large number of training samples and network sizes, these networks, including over-parameterized ones, can generalize effectively. Additionally, we provide a necessary condition for generalization, demonstrating that, for certain data distributions, the quantity of training data required to ensure generalization exceeds the network size needed to represent the corresponding data distribution. Finally, we provide theoretical insights into several phenomena in deep learning, including robust generalization, importance of over-parameterization, and effect of loss function on generalization.',\n",
       " 203: 'We show how to improve the discrepancy of an iid sample by moving only a few points. Specifically, modifying \\\\( O(m) \\\\) sample points on average reduces the Kolmogorov-Smirnov distance to the population distribution to \\\\(1/m\\\\).',\n",
       " 204: 'We propose a new statistical hypothesis testing framework which decides visually, using confidence intervals, whether the means of two samples are equal or if one is larger than the other. With our method, the user can at the same time visualize the confidence region of the means and do a test to decide if the means of the two populations are significantly different or not by looking whether the two confidence intervals overlap. To design this test we use confidence intervals constructed using e-variables, which provide a measure of evidence in hypothesis testing. We propose both a sequential test and a non-sequential test based on the overlap of confidence intervals and for each of these tests we give finite-time error bounds on the probabilities of error. We also illustrate the practicality of our method by applying it to the comparison of sequential learning algorithms.',\n",
       " 205: 'Early-stopped iterative optimization methods are widely used as alternatives to explicit regularization, and direct comparisons between early-stopping and explicit regularization have been established for many optimization geometries. However, most analyses depend heavily on the specific properties of the optimization geometry or strong convexity of the empirical objective, and it remains unclear whether early-stopping could ever be less statistically efficient than explicit regularization for some particular shape constraint, especially in the overparameterized regime. To address this question, we study the setting of high-dimensional linear regression under additive Gaussian noise when the ground truth is assumed to lie in a known convex body and the task is to minimize the in-sample mean squared error. Our main result shows that for any convex body and any design matrix, up to an absolute constant factor, the worst-case risk of unconstrained early-stopped mirror descent with an appropriate potential is at most that of the least squares estimator constrained to the convex body. We achieve this by constructing algorithmic regularizers based on the Minkowski functional of the convex body.',\n",
       " 206: 'We use tools from random matrix theory to study the multi-spiked tensor model, i.e., a rank-$r$ deformation of a symmetric random Gaussian tensor. In particular, thanks to the nature of local optimization methods used to find the maximum likelihood estimator of this model, we propose to study the phase transition phenomenon for finding critical points of the corresponding optimization problem, i.e., those points defined by the Karush-Kuhn-Tucker (KKT) conditions. Moreover, we characterize the limiting alignments between the estimated signals corresponding to a critical point of the likelihood and the ground truth signals. With the help of these results, we propose a new estimator of the rank-$r$ tensor weights by solving a system of polynomial equations, which is asymptotically unbiased contrary the maximum likelihood estimator.',\n",
       " 207: 'We consider a process $X^\\\\varepsilon$ solution of a stochastic Volterra equation with an unknown parameter $\\\\theta^\\\\star$ in the drift function. The Volterra kernel is singular and given by $K(u)=c u^{\\\\alpha-1/2} \\\\mathbb{1}_{u>0}$ with $\\\\alpha \\\\in (0,1/2)$. It is assumed that the diffusion coefficient is proportional to $\\\\varepsilon \\\\to 0$. From an observation of the path $(X^\\\\varepsilon_s)_{s\\\\in[0,T]}$, we construct a Trajectory Fitting Estimator, which is shown to be consistent and asymptotically normal. We also specify identifiability conditions insuring the $L^p$ convergence of the estimator.',\n",
       " 208: 'We address the problem of safety verification for nonlinear stochastic systems, specifically the task of certifying that system trajectories remain within a safe set with high probability. To tackle this challenge, we adopt a set-erosion strategy, which decouples the effects of stochastic disturbances from deterministic dynamics. This approach converts the stochastic safety verification problem on a safe set into a deterministic safety verification problem on an eroded subset of the safe set. The success of this strategy hinges on the depth of erosion, which is determined by a probabilistic tube that bounds the deviation of stochastic trajectories from their corresponding deterministic trajectories. Our main contribution is the establishment of a tight bound for the probabilistic tube of nonlinear stochastic systems. To obtain a probabilistic bound for stochastic trajectories, we adopt a martingale-based approach. The core innovation lies in the design of a novel energy function associated with the averaged moment generating function, which forms an affine martingale, a generalization of the traditional c-martingale. Using this energy function, we derive a precise bound for the probabilistic tube. Furthermore, we enhance this bound by incorporating the union-bound inequality for strictly contractive dynamics. By integrating the derived probabilistic tubes into the set-erosion strategy, we demonstrate that the safety verification problem for nonlinear stochastic systems can be reduced to a deterministic safety verification problem. Our theoretical results are validated through applications in reachability-based safety verification and safe controller synthesis, accompanied by several numerical examples that illustrate their effectiveness.',\n",
       " 209: 'In this paper, we present the asymptotic properties of the moment estimator for autoregressive (AR for short) models subject to Markovian changes in regime under the assumption that the errors are uncorrelated but not necessarily independent. We relax the standard independence assumption on the innovation process to extend considerably the range of application of the Markov-switching AR models. We provide necessary conditions to prove the consistency and asymptotic normality of the moment estimator in a specific case. Particular attention is paid to the estimation of the asymptotic covariance matrix. Finally, some simulation studies and an application to the hourly meteorological data are presented to corroborate theoretical work.',\n",
       " 210: \"We propose a first near complete (that will make explicit sense in the main text) nonasymptotic generalization theory for multilayer neural networks with arbitrary Lipschitz activations and general Lipschitz loss functions (with some very mild conditions). In particular, it doens't require the boundness of loss function, as commonly assumed in the literature. Our theory goes beyond the bias-variance tradeoff, aligned with phenomenon typically encountered in deep learning. It is therefore sharp different with other existing nonasymptotic generalization error bounds for neural networks. More explicitly, we propose an explicit generalization error upper bound for multilayer neural networks with arbitrary Lipschitz activations $\\\\sigma$ with $\\\\sigma(0)=0$ and broad enough Lipschitz loss functions, without requiring either the width, depth or other hyperparameters of the neural network approaching infinity, a specific neural network architect (e.g. sparsity, boundness of some norms), a particular activation function, a particular optimization algorithm or boundness of the loss function, and with taking the approximation error into consideration. General Lipschitz activation can also be accommodated into our framework. A feature of our theory is that it also considers approximation errors. Furthermore, we show the near minimax optimality of our theory for multilayer ReLU networks for regression problems. Notably, our upper bound exhibits the famous double descent phenomenon for such networks, which is the most distinguished characteristic compared with other existing results. This work emphasizes a view that many classical results should be improved to embrace the unintuitive characteristics of deep learning to get a better understanding of it.\",\n",
       " 211: \"We develop a pseudo-likelihood theory for rank one matrix estimation problems in the high dimensional limit. We prove a variational principle for the limiting pseudo-maximum likelihood which also characterizes the performance of the corresponding pseudo-maximum likelihood estimator. We show that this variational principle is universal and depends only on four parameters determined by the corresponding null model. Through this universality, we introduce a notion of equivalence for estimation problems of this type and, in particular, show that a broad class of estimation tasks, including community detection, sparse submatrix detection, and non-linear spiked matrix models, are equivalent to spiked matrix models. As an application, we obtain a complete description of the performance of the least-squares (or ``best rank one'') estimator for any rank one matrix estimation problem.\",\n",
       " 212: 'Multivariate spatial phenomena are ubiquitous, spanning domains such as climate, pandemics, air quality, and social economy. Cross-correlation between different quantities of interest at different locations is asymmetric in general. This paper provides the visualization, structure, and properties of asymmetric cross-correlation as well as symmetric auto-correlation. It reviews mainstream multivariate spatial models and analyzes their capability to accommodate asymmetric cross-correlation. It also illustrates the difference in model accuracy with and without asymmetric accommodation using a 1D simulated example.',\n",
       " 213: 'The failure of key financial institutions may accelerate risk contagion due to their interconnections within the system. In this paper, we propose a robust portfolio strategy to mitigate systemic risks during extreme events. We use the stock returns of key financial institutions as an indicator of their performance, apply extreme value theory to assess the extremal dependence among stocks of financial institutions, and construct a network model based on a threshold approach that captures extremal dependence. Our analysis reveals different dependence structures in the Chinese and U.S. financial systems. By applying the maximum independent set (MIS) from graph theory, we identify a subset of institutions with minimal extremal dependence, facilitating the construction of diversified portfolios resilient to risk contagion. We also compare the performance of our proposed portfolios with that of the market portfolios in the two economies.',\n",
       " 214: 'Upon observing $n$-dimensional multivariate Gaussian data, when can we infer that the largest $K$ observations came from the largest $K$ means? When $K=1$ and the covariance is isotropic, \\\\cite{Gutmann} argue that this inference is justified when the two-sided difference-of-means test comparing the largest and second largest observation rejects. Leveraging tools from selective inference, we provide a generalization of their procedure that applies for both any $K$ and any covariance structure. We show that our procedure draws the desired inference whenever the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference rejects, and sometimes even when this test fails to reject. Using this insight, we argue that our procedure renders existing simultaneous inference approaches inadmissible when $n > 2$. When the observations are independent (with possibly unequal variances) or equicorrelated, our procedure corresponds exactly to running the two-sided difference-of-means test comparing the pair of observations inside and outside the top $K$ with the smallest standardized difference.',\n",
       " 215: 'Laplacian matrices are commonly employed in many real applications, encoding the underlying latent structural information such as graphs and manifolds. The use of the normalization terms naturally gives rise to random matrices with dependency. It is well-known that dependency is a major bottleneck of new random matrix theory (RMT) developments. To this end, in this paper, we formally introduce a class of generalized (and regularized) Laplacian matrices, which contains the Laplacian matrix and the random adjacency matrix as a specific case, and suggest the new framework of the asymptotic theory of eigenvectors for latent embeddings with generalized Laplacian matrices (ATE-GL). Our new theory is empowered by the tool of generalized quadratic vector equation for dealing with RMT under dependency, and delicate high-order asymptotic expansions of the empirical spiked eigenvectors and eigenvalues based on local laws. The asymptotic normalities established for both spiked eigenvectors and eigenvalues will enable us to conduct precise inference and uncertainty quantification for applications involving the generalized Laplacian matrices with flexibility. We discuss some applications of the suggested ATE-GL framework and showcase its validity through some numerical examples.',\n",
       " 216: 'The multi-armed bandits (MAB) framework is a widely used approach for sequential decision-making, where a decision-maker selects an arm in each round with the goal of maximizing long-term rewards. Moreover, in many practical applications, such as personalized medicine and recommendation systems, feedback is provided in batches, contextual information is available at the time of decision-making, and rewards from different arms are related rather than independent. We propose a novel semi-parametric framework for batched bandits with covariates and a shared parameter across arms, leveraging the single-index regression (SIR) model to capture relationships between arm rewards while balancing interpretability and flexibility. Our algorithm, Batched single-Index Dynamic binning and Successive arm elimination (BIDS), employs a batched successive arm elimination strategy with a dynamic binning mechanism guided by the single-index direction. We consider two settings: one where a pilot direction is available and another where the direction is estimated from data, deriving theoretical regret bounds for both cases. When a pilot direction is available with sufficient accuracy, our approach achieves minimax-optimal rates (with $d = 1$) for nonparametric batched bandits, circumventing the curse of dimensionality. Extensive experiments on simulated and real-world datasets demonstrate the effectiveness of our algorithm compared to the nonparametric batched bandit method introduced by \\\\cite{jiang2024batched}.',\n",
       " 217: 'In this paper, we consider a two-stage Gibbs sampler for a normal linear regression model with a horseshoe prior. Under some assumptions, we show that it produces a geometrically ergodic Markov chain. In particular, we prove geometric ergodicity under some three-parameter beta global prior which does not have a finite $(p / 5)$-th negative moment, where $p$ is the number of regression coefficients. This is in contrast to the case of a known general result which is applicable if the global parameter has a finite approximately $(p / 2)$-th negative moment.',\n",
       " 218: 'I present a novel uniform law of large numbers (ULLN) for network-dependent data. While Kojevnikov, Marmer, and Song (KMS, 2021) provide a comprehensive suite of limit theorems and a robust variance estimator for network-dependent processes, their analysis focuses on pointwise convergence. On the other hand, uniform convergence is essential for nonlinear estimators such as M and GMM estimators (e.g., Newey and McFadden, 1994, Section 2). Building on KMS, I establish the ULLN under network dependence and demonstrate its utility by proving the consistency of both M and GMM estimators. A byproduct of this work is a novel maximal inequality for network data, which may prove useful for future research beyond the scope of this paper.',\n",
       " 219: \"We revisit the problem of constructing predictive confidence sets for which we wish to obtain some type of conditional validity. We provide new arguments showing how ``split conformal'' methods achieve near desired coverage levels with high probability, a guarantee conditional on the validation data rather than marginal over it. In addition, we directly consider (approximate) conditional coverage, where, e.g., conditional on a covariate $X$ belonging to some group of interest, we would like a guarantee that a predictive set covers the true outcome $Y$. We show that the natural method of performing quantile regression on a held-out (validation) dataset yields minimax optimal guarantees of coverage here. Complementing these positive results, we also provide experimental evidence that interesting work remains to be done to develop computationally efficient but valid predictive inference methods.\",\n",
       " 220: 'Linear inverse problems are ubiquitous in various science and engineering disciplines. Of particular importance in the past few decades, is the incorporation of sparsity based priors, in particular $\\\\ell_1$ priors, into linear inverse problems, which led to the flowering of fields of compressive sensing (CS) and sparsity based signal processing. More recently, methods based on a Compound Gaussian (CG) prior have been investigated and demonstrate improved results over CS in practice. This paper is the first attempt to identify and elucidate the fundamental structures underlying the success of CG methods by studying CG in the context of a broader framework of generalized-sparsity-based-inference. After defining our notion of generalized sparsity we introduce a weak null space property and proceed to generalize two well-known methods in CS, basis pursuit and iteratively reweighted least squares (IRLS). We show how a subset of CG-induced regularizers fits into this framework.',\n",
       " 221: 'Given a tree $T$, its path polytope is the convex hull of the edge indicator vectors for the paths between any two distinct leaves in $T$. These polytopes arise naturally in polyhedral geometry and applications, such as phylogenetics, tropical geometry, and algebraic statistics. We provide a minimal halfspace representation of these polytopes. The construction is made inductively using toric fiber products.',\n",
       " 222: 'We introduce a novel class of bivariate common-shock discrete phase-type (CDPH) distributions to describe dependencies in loss modeling, with an emphasis on those induced by common shocks. By constructing two jointly evolving terminating Markov chains that share a common evolution up to a random time corresponding to the common shock component, and then proceed independently, we capture the essential features of risk events influenced by shared and individual-specific factors. We derive explicit expressions for the joint distribution of the termination times and prove various class and distributional properties, facilitating tractable analysis of the risks. Extending this framework, we model random sums where aggregate claims are sums of continuous phase-type random variables with counts determined by these termination times, and show that their joint distribution belongs to the multivariate phase-type or matrix-exponential class. We develop estimation procedures for the CDPH distributions using the expectation-maximization algorithm and demonstrate the applicability of our models through simulation studies and an application to bivariate insurance claim frequency data.',\n",
       " 223: 'We examine the location characteristics of a conditional selective confidence interval based on the polyhedral method. This interval is constructed from the distribution of a test statistic conditional upon the event of statistical significance. In the case of a one-sided test, the behavior of the interval varies depending on whether the parameter is highly significant or only marginally significant. When the parameter is highly significant, the interval is similar to the usual confidence interval derived without considering selection. However, when the parameter is only marginally significant, the interval falls into an extreme range and deviates greatly from the estimated value of the parameter. In contrast, an interval conditional on two-sided significance does not yield extreme results, although it may exclude the estimated parameter value.',\n",
       " 224: 'Reproducing Kernel Hilbert Space (RKHS) embedding of probability distributions has proved to be an effective approach, via MMD (maximum mean discrepancy) for nonparametric hypothesis testing problems involving distributions defined over general (non-Euclidean) domains. While a substantial amount of work has been done on this topic, only recently, minimax optimal two-sample tests have been constructed that incorporate, unlike MMD, both the mean element and a regularized version of the covariance operator. However, as with most kernel algorithms, the computational complexity of the optimal test scales cubically in the sample size, limiting its applicability. In this paper, we propose a spectral regularized two-sample test based on random Fourier feature (RFF) approximation and investigate the trade-offs between statistical optimality and computational efficiency. We show the proposed test to be minimax optimal if the approximation order of RFF (which depends on the smoothness of the likelihood ratio and the decay rate of the eigenvalues of the integral operator) is sufficiently large. We develop a practically implementable permutation-based version of the proposed test with a data-adaptive strategy for selecting the regularization parameter and the kernel. Finally, through numerical experiments on simulated and benchmark datasets, we demonstrate that the proposed RFF-based test is computationally efficient and performs almost similar (with a small drop in power) to the exact test.',\n",
       " 225: 'We study the coverage properties of full conformal regression in the proportional asymptotic regime where the ratio of the dimension and the sample size converges to a constant. In this setting, existing theory tells us only that full conformal inference is unbiased, in the sense that its average coverage lies at the desired level when marginalized over both the new test point and the training data. Considerably less is known about the behaviour of these methods conditional on the training set. As a result, the exact benefits of full conformal inference over much simpler alternative methods is unclear. This paper investigates the behaviour of full conformal inference and natural uncorrected alternatives for a broad class of $L_2$-regularized linear regression models. We show that in the proportional asymptotic regime the training-conditional coverage of full conformal inference concentrates at the target value. On the other hand, simple alternatives that directly compare test and training residuals realize constant undercoverage bias. While these results demonstrate the necessity of full conformal in correcting for high-dimensional overfitting, we also show that this same methodology is redundant for the related task of tuning the regularization level. In particular, we show that full conformal inference still yields asymptotically valid coverage when the regularization level is selected using only the training set, without consideration of the test point. Simulations show that our asymptotic approximations are accurate in finite samples and can be readily extended to other popular full conformal variants, such as full conformal quantile regression and the LASSO, that do not directly meet our assumptions.',\n",
       " 226: 'The Kolmogorov-Smirnov (KS) statistic is a classical nonparametric test widely used for comparing an empirical distribution function with a reference distribution or for comparing two empirical distributions. Despite its broad applicability in statistical hypothesis testing and model validation, certain aspects of the KS statistic remain under-explored among the young generation, particularly under finite sample conditions. This paper revisits the KS statistic in both one-sample and two-sample scenarios, considering one-sided and two-sided variants. We derive exact probabilities for the supremum of the empirical process and present a unified treatment of the KS statistic under diverse settings. Additionally, we explore the discrete nature of the hitting times of the normalized empirical process, providing practical insights into the computation of KS test p-values. The study also discusses the Dvoretzky-Kiefer-Wolfowitz-Massart (DKWM) inequality, highlighting its role in constructing confidence bands for distribution functions. Using empirical process theory, we establish the limit distribution of the KS statistic when the true distribution includes unknown parameters. Our findings extend existing results, offering improved methodologies for statistical analysis and hypothesis testing using the KS statistic, particularly in finite sample scenarios.',\n",
       " 227: 'A one-shot device is a unit that operates only once, after which it is either destroyed or needs to be rebuilt. For this type of device, the operational status can only be assessed at a specific inspection time, determining whether failure occurred before or after it. Consequently, lifetimes are subject to left- or right-censoring. One-shot devices are usually highly reliables. To analyze the reliability of such products, an accelerated life test (ALT) plan is typically employed by subjecting the devices to increased levels of stress factors, thus allowing life characteristics observed under high-stress conditions to be extrapolated to normal operating conditions. By accelerating the degradation process, ALT significantly reduces both the time required for testing and the associated experimental costs. Recently, robust inferential methods have gained considerable interest in statistical analysis. Among them, weighted minimum density power divergence estimators (WMDPDEs) are widely recognized for their robust statistical properties with small loss of efficiency. In this work, robust WMDPDE and associated statistical tests are developed under a log-logistic lifetime distribution with multiple stresses. Explicit expressions for the estimating equations and asymptotic distribution of the estimators are obtained. Further, a Monte Carlo simulation study is presented to evaluate the performance of the WMDPDE in practical applications.',\n",
       " 228: 'Learning kernels in operators from data lies at the intersection of inverse problems and statistical learning, offering a powerful framework for capturing nonlocal dependency in function spaces and high-dimensional settings. In contrast to classical nonparametric regression, where the inverse problem is well-posed, kernel estimation involves a compact normal operator and an ill-posed deconvolution. To address these challenges, we introduce adaptive spectral Sobolev spaces, unifying Sobolev spaces and reproducing kernel Hilbert spaces, that automatically discard non-identifiable components and control terms with small eigenvalues. Within this framework, we establish the minimax convergence rates for the mean squared error under both polynomial and exponential spectral decay regimes. Methodologically, we develop a tamed least squares estimator achieving the minimax upper rates via controlling the left-tail probability for eigenvalues of the random normal matrix; and for the minimax lower rates, we resolve challenges from infinite-dimensional measures through their projections.',\n",
       " 229: 'This paper investigates conditional specifications for multivariate count variables. Recently, the spatial count data literature has proposed several conditional models such that the conditional expectations are linear in the conditioning variables. These models are much easier to estimate than existing spatial count models based on Gaussian random field. However, whether or not such conditional specifications are compatible have not been addressed. We investigate two large families of conditional models, that are the compound autoregressive model and the random coefficient integer autoregressive model. We characterize all the solutions to these two families of models at arbitrary dimensions, and find that only a handful of them admit non-trivial solutions. We then show that if we focus on the linearity condition of the conditional expectations only, a considerable larger family of solutions can be obtained. This suggests that for spatial count data modeling, semi-parametric type specifications that impose the conditional expectation structure is preferable.',\n",
       " 230: 'In this paper we extend the classical Glivenko-Cantelli theorem to real-valued empirical functions under dependence structures characterised by $\\\\alpha$-mixing and $\\\\beta$-mixing conditions. We investigate sufficient conditions ensuring that families of real-valued functions exhibit the Glivenko-Cantelli (GC) property in these dependence settings. Our analysis focuses on function classes satisfying uniform entropy conditions and establishes deviation bounds under mixing coefficients that decay at appropriate rates. Our results refine the existing literature by relaxing the independence assumptions and highlighting the role of dependence in empirical process convergence.',\n",
       " 231: 'We study two G-modeling strategies for estimating the signal distribution (the empirical Bayesian\\'s prior) from observations corrupted with normal noise. First, we choose the signal distribution by minimizing Stein\\'s unbiased risk estimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach motivated by optimal empirical Bayesian shrinkage estimation of the signals. Second, we select the signal distribution by minimizing Hyv\\\\\"arinen\\'s score matching objective for the implied score (derivative of log-marginal density), targeting minimal Fisher divergence between estimated and true marginal densities. While these strategies appear distinct, they are known to be mathematically equivalent. We provide a unified analysis of SURE and score matching under both well-specified signal distribution classes and misspecification. In the classical well-specified setting with homoscedastic noise and compactly supported signal distribution, we establish nearly parametric rates of convergence of the empirical Bayes regret and the Fisher divergence. In a commonly studied misspecified model, we establish fast rates of convergence to the oracle denoiser and corresponding oracle inequalities. Our empirical results demonstrate competitiveness with nonparametric maximum likelihood in well-specified settings, while showing superior performance under misspecification, particularly in settings involving heteroscedasticity and side information.',\n",
       " 232: \"In many random phenomena, such as life-testing experiments and environmental data (like rainfall data), there are often positive values and an excess of zeros, which create modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called zero inliers, are difficult to model using standard approaches. When studying extreme values in the above scenarios, a key issue is selecting an appropriate threshold for accurate tail approximation of the population using asymptotic models. While some extreme value mixture models address threshold estimation and tail approximation, conventional parametric and non-parametric bulk and generalised Pareto distribution (GPD) approaches often neglect inliers, leading to suboptimal results. This paper introduces a framework for modeling extreme events and inliers using the GPD, addressing threshold uncertainty and effectively capturing inliers at zero. The model's parameters are estimated using the maximum likelihood estimation (MLE) method, ensuring optimal precision. Through simulation studies and real-world applications, we demonstrate that the proposed model significantly outperforms the traditional methods, which typically neglect inliers at the origin.\",\n",
       " 233: 'This paper continues the study of the efficiency of conformal prediction as compared with more general randomness prediction and exchangeability prediction. It does not restrict itself to the case of classification, and our results will also be applicable to the case of regression. The price to pay is that efficiency will be attained only on average, albeit with respect to a wide range of probability measures on the label space.',\n",
       " 234: 'Time series in natural sciences, such as hydrology and climatology, and other environmental applications, often consist of continuous observations constrained to the unit interval (0,1). Traditional Gaussian-based models fail to capture these bounds, requiring more flexible approaches. This paper introduces the Matsuoka Autoregressive Moving Average (MARMA) model, extending the GARMA framework by assuming a Matsuoka-distributed random component taking values in (0,1) and an ARMA-like systematic structure allowing for random time-dependent covariates. Parameter estimation is performed via partial maximum likelihood (PMLE), for which we present the asymptotic theory. It enables statistical inference, including confidence intervals and model selection. To construct prediction intervals, we propose a novel bootstrap-based method that accounts for dependence structure uncertainty. A comprehensive Monte Carlo simulation study assesses the finite sample performance of the proposed methodologies, while an application to forecasting the useful water volume of the Guarapiranga Reservoir in Brazil showcases their practical usefulness.',\n",
       " 235: 'Many scientific problems involve data exhibiting both temporal and cross-sectional dependencies. While linear dependencies have been extensively studied, the theoretical analysis of regression estimators under nonlinear dependencies remains scarce. This work studies a kernel-based estimation procedure for nonlinear dynamics within the reproducing kernel Hilbert space framework, focusing on nonlinear vector autoregressive models. We derive nonasymptotic probabilistic bounds on the deviation between a regularized kernel estimator and the nonlinear regression function. A key technical contribution is a concentration bound for quadratic forms of stochastic matrices in the presence of dependent data, which is of independent interest. Additionally, we characterize conditions on multivariate kernels that guarantee optimal convergence rates.',\n",
       " 236: \"In statistics, generalized linear models (GLMs) are widely used for modeling data and can expressively capture potential nonlinear dependence of the model's outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in a general class of sparse binary GLMs. Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron (Kakade et al. 2011; Bahmani et al. 2016), BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications, logistic and probit regression are additionally studied. In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression.\",\n",
       " 237: \"We study high-dimensional random geometric graphs (RGGs) of edge-density $p$ with vertices uniformly distributed on the $d$-dimensional torus and edges inserted between sufficiently close vertices with respect to an $L_q$-norm. We focus on distinguishing an RGG from an Erd\\\\H{o}s--R\\\\'enyi (ER) graph if both models have edge probability $p$. So far, most results considered either spherical RGGs with $L_2$-distance or toroidal RGGs under $L_\\\\infty$-distance. However, for general $L_q$-distances, many questions remain open, especially if $p$ is allowed to depend on $n$. The main reason for this is that RGGs under $L_q$-distances can not easily be represented as the logical AND of their 1-dimensional counterparts, as for $L_\\\\infty$ geometries. To overcome this, we devise a novel technique for quantifying the dependence between edges based on modified Edgeworth expansions. Our technique yields the first tight algorithmic upper bounds for distinguishing toroidal RGGs under general $L_q$ norms from ER-graphs for fixed $p$ and $q$. We achieve this by showing that signed triangles can distinguish the two models when $d\\\\ll n^3p^3$ for the whole regime of $c/n<p<1$. Additionally, our technique yields an improved information-theoretic lower bound for this task, showing that the two distributions converge whenever $d=\\\\tilde{\\\\Omega}(n^3p^2)$, which is just as strong as the currently best known lower bound for spherical RGGs in case of general $p$ from Liu et al. [STOC'22]. Finally, our expansions allow us to tightly characterize the spectral properties of toroidal RGGs both under $L_q$-distances for fixed $1\\\\le q<\\\\infty$, and $L_\\\\infty$-distance. Our results partially resolve a conjecture of Bangachev and Bresler [COLT'24] and prove that the distance metric, rather than the underlying space, is responsible for the observed differences in the behavior of spherical and toroidal RGGs.\",\n",
       " 238: 'In this work, we present a new perspective on the origin and interpretation of adaptive filters. By applying Bayesian principles of recursive inference from the state-space model and using a series of simplifications regarding the structure of the solution, we can present, in a unified framework, derivations of many adaptive filters which depend on the probabilistic model of the observational noise. In particular, under a Gaussian model, we obtain solutions well-known in the literature (such as LMS, NLMS, or Kalman filter), while using non-Gaussian noise, we obtain new families of adaptive filter. Notably, under assumption of Laplacian noise, we obtain a family of robust filters of which the signed-error algorithm is a well-known member, while other algorithms, derived effortlessly in the proposed framework, are entirely new. Numerical examples are shown to illustrate the properties and provide a better insight into the performance of the derived adaptive filters.',\n",
       " 239: \"A novel method for sequential outlier detection in non-stationary time series is proposed. The method tests the null hypothesis of ``no outlier'' at each time point, addressing the multiple testing problem by bounding the error probability of successive tests, using extreme value theory. The asymptotic properties of the test statistic are studied under the null hypothesis and alternative. The finite sample properties of the new detection scheme are investigated by means of a simulation study, and the method is compared with alternative procedures which have recently been proposed in the statistics and machine learning literature.\",\n",
       " 240: 'This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference.',\n",
       " 241: 'Hypothesis tests and confidence intervals are ubiquitous in empirical research, yet their connection to subsequent decision-making is often unclear. We develop a theory of certified decisions that pairs recommended decisions with inferential guarantees. Specifically, we attach P-certificates -- upper bounds on loss that hold with probability at least $1-\\\\alpha$ -- to recommended actions. We show that such certificates allow \"safe,\" risk-controlling adoption decisions for ambiguity-averse downstream decision-makers. We further prove that it is without loss to limit attention to P-certificates arising as minimax decisions over confidence sets, or what Manski (2021) terms \"as-if decisions with a set estimate.\" A parallel argument applies to E-certified decisions obtained from e-values in settings with unbounded loss.',\n",
       " 242: 'In many real applications of statistical learning, collecting sufficiently many training data is often expensive, time-consuming, or even unrealistic. In this case, a transfer learning approach, which aims to leverage knowledge from a related source domain to improve the learning performance in the target domain, is more beneficial. There have been many transfer learning methods developed under various distributional assumptions. In this article, we study a particular type of classification problem, called conformal prediction, under a new distributional assumption for transfer learning. Classifiers under the conformal prediction framework predict a set of plausible labels instead of one single label for each data instance, affording a more cautious and safer decision. We consider a generalization of the \\\\textit{covariate shift with posterior drift} setting for transfer learning. Under this setting, we propose a weighted conformal classifier that leverages both the source and target samples, with a coverage guarantee in the target domain. Theoretical studies demonstrate favorable asymptotic properties. Numerical studies further illustrate the usefulness of the proposed method.',\n",
       " 243: 'We consider statistical inference under a semi-supervised setting where we have access to both a labeled dataset consisting of pairs $\\\\{X_i, Y_i \\\\}_{i=1}^n$ and an unlabeled dataset $\\\\{ X_i \\\\}_{i=n+1}^{n+N}$. We ask the question: under what circumstances, and by how much, can incorporating the unlabeled dataset improve upon inference using the labeled data? To answer this question, we investigate semi-supervised learning through the lens of semiparametric efficiency theory. We characterize the efficiency lower bound under the semi-supervised setting for an arbitrary inferential problem, and show that incorporating unlabeled data can potentially improve efficiency if the parameter is not well-specified. We then propose two types of semi-supervised estimators: a safe estimator that imposes minimal assumptions, is simple to compute, and is guaranteed to be at least as efficient as the initial supervised estimator; and an efficient estimator, which -- under stronger assumptions -- achieves the semiparametric efficiency bound. Our findings unify existing semiparametric efficiency results for particular special cases, and extend these results to a much more general class of problems. Moreover, we show that our estimators can flexibly incorporate predicted outcomes arising from ``black-box\" machine learning models, and thereby achieve the same goal as prediction-powered inference (PPI), but with superior theoretical guarantees. We also provide a complete understanding of the theoretical basis for the existing set of PPI methods. Finally, we apply the theoretical framework developed to derive and analyze efficient semi-supervised estimators in a number of settings, including M-estimation, U-statistics, and average treatment effect estimation, and demonstrate the performance of the proposed estimators via simulations.',\n",
       " 244: 'Motivated by learning dynamical structures from static snapshot data, this paper presents a distribution-on-scalar regression approach for estimating the density evolution of a stochastic process from its noisy temporal point clouds. We propose an entropy-regularized nonparametric maximum likelihood estimator (E-NPMLE), which leverages the entropic optimal transport as a smoothing regularizer for the density flow. We show that the E-NPMLE has almost dimension-free statistical rates of convergence to the ground truth distributions, which exhibit a striking phase transition phenomenon in terms of the number of snapshots and per-snapshot sample size. To efficiently compute the E-NPMLE, we design a novel particle-based and grid-free coordinate KL divergence gradient descent (CKLGD) algorithm and prove its polynomial iteration complexity. Moreover, we provide numerical evidence on synthetic data to support our theoretical findings. This work contributes to the theoretical understanding and practical computation of estimating density evolution from noisy observations in arbitrary dimensions.',\n",
       " 245: 'A fundamental problem in statistics and machine learning is to estimate a function $f$ from possibly noisy observations of its point samples. The goal is to design a numerical algorithm to construct an approximation $\\\\hat f$ to $f$ in a prescribed norm that asymptotically achieves the best possible error (as a function of the number $m$ of observations and the variance $\\\\sigma^2$ of the noise). This problem has received considerable attention in both nonparametric statistics (noisy observations) and optimal recovery (noiseless observations). Quantitative bounds require assumptions on $f$, known as model class assumptions. Classical results assume that $f$ is in the unit ball of a Besov space. In nonparametric statistics, the best possible performance of an algorithm for finding $\\\\hat f$ is known as the minimax rate and has been studied in this setting under the assumption that the noise is Gaussian. In optimal recovery, the best possible performance of an algorithm is known as the optimal recovery rate and has also been determined in this setting. While one would expect that the minimax rate recovers the optimal recovery rate when the noise level $\\\\sigma$ tends to zero, it turns out that the current results on minimax rates do not carefully determine the dependence on $\\\\sigma$ and the limit cannot be taken. This paper handles this issue and determines the noise-level-aware (NLA) minimax rates for Besov classes when error is measured in an $L_q$-norm with matching upper and lower bounds. The end result is a reconciliation between minimax rates and optimal recovery rates. The NLA minimax rate continuously depends on the noise level and recovers the optimal recovery rate when $\\\\sigma$ tends to zero.',\n",
       " 246: 'We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering sublinear Neyman regret, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\\\\widetilde{O}(\\\\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\\\\widetilde{O}(\\\\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual \"multigroup\" Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group\\'s best non-adaptive designs. In particular, we develop a contextual adaptive design with $\\\\widetilde{O}(\\\\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments.',\n",
       " 247: 'Gaussian multiplicative chaos (GMC) is a canonical random fractal measure obtained by exponentiating log-correlated Gaussian processes, first constructed in the seminal work of Kahane (1985). Since then it has served as an important building block in constructions of quantum field theories and Liouville quantum gravity. However, in many natural settings, non-Gaussian log-correlated processes arise. In this paper, we investigate the universality of GMC through an invariance principle. We consider the model of a random Fourier series, a process known to be log-correlated. While the Gaussian Fourier series has been a classical object of study, recently, the non-Gaussian counterpart was investigated and the associated multiplicative chaos constructed by Junnila in 2016. We show that the Gaussian and non-Gaussian variables can be coupled so that the associated chaos measures are almost surely mutually absolutely continuous throughout the entire sub-critical regime. This solves the main open problem from Kim and Kriechbaum (2024) who had earlier established such a result for a part of the regime. The main ingredient is a new high dimensional CLT for a sum of independent (but not i.i.d.) random vectors belonging to rank one subspaces with error bounds involving the isotropic properties of the covariance matrix of the sum, which we expect will find other applications. The proof relies on a path-wise analysis of Skorokhod embeddings as well as a perturbative result about square roots of positive semi-definite matrices which, surprisingly, appears to be new.',\n",
       " 248: 'In this paper, we introduce a unified framework, inspired by classical regularization theory, for designing and analyzing a broad class of linear regression approaches. Our framework encompasses traditional methods like least squares regression and Ridge regression, as well as innovative techniques, including seven novel regression methods such as Landweber and Showalter regressions. Within this framework, we further propose a class of debiased and thresholded regression methods to promote feature selection, particularly in terms of sparsity. These methods may offer advantages over conventional regression techniques, including Lasso, due to their ease of computation via a closed-form expression. Theoretically, we establish consistency results and Gaussian approximation theorems for this new class of regularization methods. Extensive numerical simulations further demonstrate that the debiased and thresholded counterparts of linear regression methods exhibit favorable finite sample performance and may be preferable in certain settings.',\n",
       " 249: 'We address the problem of producing a lower bound for the mean of a discrete probability distribution, with known support over a finite set of real numbers, from an iid sample of that distribution. Up to a constant, this is equivalent to bounding the mean of a multinomial distribution (with known support) from a sample of that distribution. Our main contribution is to characterize the complete set of admissible bound functions for any sample space, and to show that certain previously published bounds are admissible. We prove that the solution to each one of a set of simple-to-state optimization problems yields such an admissible bound. Single examples of such bounds, such as the trinomial bound by Miratrix and Stark [2009] have been previously published, but without an analysis of admissibility, and without a discussion of the full set of alternative admissible bounds. In addition to a variety of results about admissible bounds, we prove the non-existence of optimal bounds for sample spaces with supports of size greater than 1 and samples sizes greater than 1.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e27c9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_keywords_with_chunks(row):\n",
    "    article_id = row.name\n",
    "    original_keywords = set(row[\"cleaned_keywords\"]) if isinstance(row[\"cleaned_keywords\"], list) else set()\n",
    "    \n",
    "    chunk_text = article_chunks.get(article_id, \"\")\n",
    "    new_keywords = extract_keywords(chunk_text)\n",
    "    new_keywords_lemmatized = lemmatize_keywords_stanza(new_keywords)\n",
    "    new_keywords_cleaned = clean_keywords(new_keywords_lemmatized)\n",
    "\n",
    "    combined = original_keywords.union(new_keywords_cleaned)\n",
    "    return list(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ae76850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"enriched_keywords\"] = df.apply(update_keywords_with_chunks, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7230f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_enriched_text(row):\n",
    "    new_keywords = \", \".join(df.loc[row.article_id, \"enriched_keywords\"])\n",
    "    return f\"Title: {row.title}\\nAuthors: {row.authors}\\nKeywords: {new_keywords}\\nText: {row.chunk}\"\n",
    "\n",
    "df_enriched_chunks[\"enriched_text\"] = df_enriched_chunks.apply(update_enriched_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc5023cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/maksvell/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "00af0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_and_clean_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    words = [word.lemma for word in doc.iter_words() if word.lemma not in stop_words and word.lemma.isalpha()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df_enriched_chunks[\"chunk_lemmatized\"] = df_enriched_chunks[\"chunk\"].apply(lemmatize_and_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a47e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_intro(article_id):\n",
    "    try:\n",
    "        summary = df.loc[article_id, \"summary\"]\n",
    "        sentences = sent_tokenize(summary)\n",
    "        return \" \".join(sentences[:2]) \n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "df_enriched_chunks[\"summary_intro\"] = df_enriched_chunks[\"article_id\"].apply(get_summary_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be1bfab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_from_keywords(article_id):\n",
    "    keywords = df.loc[article_id, \"enriched_keywords\"]\n",
    "    return keywords[0] if isinstance(keywords, list) and len(keywords) > 0 else \"other\"\n",
    "\n",
    "df_enriched_chunks[\"topic\"] = df_enriched_chunks[\"article_id\"].apply(get_topic_from_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa429ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rich_enriched_text(row):\n",
    "    enriched_keywords = \", \".join(df.loc[row.article_id, \"enriched_keywords\"])\n",
    "    return (\n",
    "        f\"Title: {row.title}\\n\"\n",
    "        f\"Authors: {row.authors}\\n\"\n",
    "        f\"Topic: {row.topic}\\n\"\n",
    "        f\"Keywords: {enriched_keywords}\\n\"\n",
    "        f\"Intro: {row.summary_intro}\\n\"\n",
    "        f\"Lemmatized Chunk: {row.chunk_lemmatized}\"\n",
    "    )\n",
    "\n",
    "df_enriched_chunks[\"enriched_text_rich\"] = df_enriched_chunks.apply(make_rich_enriched_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fba3d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_enriched_chunks[\"chunk_lemmatized\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_terms = []\n",
    "for row in tfidf_matrix:\n",
    "    row_data = row.toarray().flatten()\n",
    "    top_indices = row_data.argsort()[-5:][::-1]  \n",
    "    top_words = [feature_names[i] for i in top_indices if row_data[i] > 0]\n",
    "    top_terms.append(\", \".join(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4078b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_chunks[\"tfidf_top_terms\"] = top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ba38e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_chunks = df_enriched_chunks[[\n",
    "    \"article_id\", \"title\", \"authors\", \"topic\",\n",
    "    \"summary_intro\", \"keywords\", \"chunk\", \"chunk_lemmatized\",\n",
    "    \"enriched_text_rich\", \"tfidf_top_terms\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c76527a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>topic</th>\n",
       "      <th>summary_intro</th>\n",
       "      <th>keywords</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_lemmatized</th>\n",
       "      <th>enriched_text_rich</th>\n",
       "      <th>tfidf_top_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>213</td>\n",
       "      <td>Systemic Risk Management via Maximum Independe...</td>\n",
       "      <td>Qian Hui, Tiandong Wang</td>\n",
       "      <td>financial institution may accelerate</td>\n",
       "      <td></td>\n",
       "      <td>risk management, key financial institution, sy...</td>\n",
       "      <td>The failure of key financial institutions may ...</td>\n",
       "      <td>failure key financial institution may accelera...</td>\n",
       "      <td>Title: Systemic Risk Management via Maximum In...</td>\n",
       "      <td>institution, financial, extremal, dependence, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>202</td>\n",
       "      <td>Generalizability of Neural Networks Minimizing...</td>\n",
       "      <td>Lijia Yu, Yibo Miao, Yifan Zhu, Xiao-Shan Gao,...</td>\n",
       "      <td>learn exhibit nice generalizability</td>\n",
       "      <td></td>\n",
       "      <td>minimize empirical, generalizability, learn ex...</td>\n",
       "      <td>The primary objective of learning methods is g...</td>\n",
       "      <td>primary objective learning method generalizati...</td>\n",
       "      <td>Title: Generalizability of Neural Networks Min...</td>\n",
       "      <td>generalization, network, deep, minimize, learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>The level of self-organized criticality in osc...</td>\n",
       "      <td>Johannes Brutsche, Angelika Rohde</td>\n",
       "      <td>rho</td>\n",
       "      <td></td>\n",
       "      <td>rho, self - organize criticality, self - organ...</td>\n",
       "      <td>For some discretely observed path of oscillati...</td>\n",
       "      <td>discretely observe path oscillate brownian mot...</td>\n",
       "      <td>Title: The level of self-organized criticality...</td>\n",
       "      <td>mle, argument, process, local, limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>200</td>\n",
       "      <td>Learning Causal Response Representations throu...</td>\n",
       "      <td>Homer Durand, Gherardo Varando, Gustau Camps-V...</td>\n",
       "      <td>conditional independence</td>\n",
       "      <td></td>\n",
       "      <td>conditional independence, learn causal respons...</td>\n",
       "      <td>We propose a novel approach for learning causa...</td>\n",
       "      <td>propose novel approach learn causal response r...</td>\n",
       "      <td>Title: Learning Causal Response Representation...</td>\n",
       "      <td>independence, representation, causal, conditio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>A New Design-Based Variance Estimator for Fine...</td>\n",
       "      <td>Yuehao Bai, Xun Huang, Joseph P. Romano, Azeem...</td>\n",
       "      <td>treatment effect</td>\n",
       "      <td></td>\n",
       "      <td>variance estimator finely, treatment effect fi...</td>\n",
       "      <td>This paper considers the problem of design-bas...</td>\n",
       "      <td>paper consider problem design base inference a...</td>\n",
       "      <td>Title: A New Design-Based Variance Estimator f...</td>\n",
       "      <td>estimator, stratify, treatment, biased, quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                              title  \\\n",
       "213         213  Systemic Risk Management via Maximum Independe...   \n",
       "202         202  Generalizability of Neural Networks Minimizing...   \n",
       "187         187  The level of self-organized criticality in osc...   \n",
       "200         200  Learning Causal Response Representations throu...   \n",
       "165         165  A New Design-Based Variance Estimator for Fine...   \n",
       "\n",
       "                                               authors  \\\n",
       "213                            Qian Hui, Tiandong Wang   \n",
       "202  Lijia Yu, Yibo Miao, Yifan Zhu, Xiao-Shan Gao,...   \n",
       "187                  Johannes Brutsche, Angelika Rohde   \n",
       "200  Homer Durand, Gherardo Varando, Gustau Camps-V...   \n",
       "165  Yuehao Bai, Xun Huang, Joseph P. Romano, Azeem...   \n",
       "\n",
       "                                    topic summary_intro  \\\n",
       "213  financial institution may accelerate                 \n",
       "202   learn exhibit nice generalizability                 \n",
       "187                                   rho                 \n",
       "200              conditional independence                 \n",
       "165                      treatment effect                 \n",
       "\n",
       "                                              keywords  \\\n",
       "213  risk management, key financial institution, sy...   \n",
       "202  minimize empirical, generalizability, learn ex...   \n",
       "187  rho, self - organize criticality, self - organ...   \n",
       "200  conditional independence, learn causal respons...   \n",
       "165  variance estimator finely, treatment effect fi...   \n",
       "\n",
       "                                                 chunk  \\\n",
       "213  The failure of key financial institutions may ...   \n",
       "202  The primary objective of learning methods is g...   \n",
       "187  For some discretely observed path of oscillati...   \n",
       "200  We propose a novel approach for learning causa...   \n",
       "165  This paper considers the problem of design-bas...   \n",
       "\n",
       "                                      chunk_lemmatized  \\\n",
       "213  failure key financial institution may accelera...   \n",
       "202  primary objective learning method generalizati...   \n",
       "187  discretely observe path oscillate brownian mot...   \n",
       "200  propose novel approach learn causal response r...   \n",
       "165  paper consider problem design base inference a...   \n",
       "\n",
       "                                    enriched_text_rich  \\\n",
       "213  Title: Systemic Risk Management via Maximum In...   \n",
       "202  Title: Generalizability of Neural Networks Min...   \n",
       "187  Title: The level of self-organized criticality...   \n",
       "200  Title: Learning Causal Response Representation...   \n",
       "165  Title: A New Design-Based Variance Estimator f...   \n",
       "\n",
       "                                       tfidf_top_terms  \n",
       "213  institution, financial, extremal, dependence, ...  \n",
       "202  generalization, network, deep, minimize, learning  \n",
       "187               mle, argument, process, local, limit  \n",
       "200  independence, representation, causal, conditio...  \n",
       "165    estimator, stratify, treatment, biased, quality  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_chunks.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bde6cc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "summary_intro\n",
       "    250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_chunks.summary_intro.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6889e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_chunks = df_final_chunks.drop('summary_intro', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0bd16a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>topic</th>\n",
       "      <th>keywords</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_lemmatized</th>\n",
       "      <th>enriched_text_rich</th>\n",
       "      <th>tfidf_top_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nonparametric local polynomial regression for ...</td>\n",
       "      <td>Moritz Jirak, Alois Kneip, Alexander Meister, ...</td>\n",
       "      <td>space</td>\n",
       "      <td>space, polynomial, nonparametric local, hilber...</td>\n",
       "      <td>We consider nonparametric regression with func...</td>\n",
       "      <td>consider nonparametric regression functional c...</td>\n",
       "      <td>Title: Nonparametric local polynomial regressi...</td>\n",
       "      <td>regression, polynomial, tuning, functional, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Revisiting poverty measures using quantile fun...</td>\n",
       "      <td>N. Unnikrishnan Nair, S. M. Sunoj</td>\n",
       "      <td>redefine various poverty</td>\n",
       "      <td>flexible quantile function model, measure lite...</td>\n",
       "      <td>In this article we redefine various poverty me...</td>\n",
       "      <td>article redefine various poverty measure liter...</td>\n",
       "      <td>Title: Revisiting poverty measures using quant...</td>\n",
       "      <td>quantile, function, exist, difficult, flexible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Online Bernstein-von Mises theorem</td>\n",
       "      <td>Jeyong Lee, Junhyeok Choi, Minwoo Chae</td>\n",
       "      <td>bernstein - von mise theorem</td>\n",
       "      <td>sequentially update posterior, update posterio...</td>\n",
       "      <td>Online learning is an inferential paradigm in ...</td>\n",
       "      <td>online learning inferential paradigm parameter...</td>\n",
       "      <td>Title: Online Bernstein-von Mises theorem\\nAut...</td>\n",
       "      <td>update, posterior, batch, sequentially, step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Debiasing Continuous-time Nonlinear Autoregres...</td>\n",
       "      <td>Simon Kuang, Xinfan Lin</td>\n",
       "      <td>class continuous - time system</td>\n",
       "      <td>debiase continuous - time nonlinear, ordinary ...</td>\n",
       "      <td>We study how to identify a class of continuous...</td>\n",
       "      <td>study identify class continuous time nonlinear...</td>\n",
       "      <td>Title: Debiasing Continuous-time Nonlinear Aut...</td>\n",
       "      <td>step, direct, continuous, time, second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A Generalized Tangent Approximation Framework ...</td>\n",
       "      <td>Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. ...</td>\n",
       "      <td>approximation base variational inference</td>\n",
       "      <td>framework strongly super - gaussian, analysis ...</td>\n",
       "      <td>Tangent approximation form a popular class of ...</td>\n",
       "      <td>tangent approximation form popular class varia...</td>\n",
       "      <td>Title: A Generalized Tangent Approximation Fra...</td>\n",
       "      <td>likelihood, approximation, variational, optima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>245</td>\n",
       "      <td>Optimal Recovery Meets Minimax Estimation</td>\n",
       "      <td>Ronald DeVore, Robert D. Nowak, Rahul Parhi, G...</td>\n",
       "      <td>meet minimax estimation</td>\n",
       "      <td>recovery, estimation fundamental problem, rate...</td>\n",
       "      <td>A fundamental problem in statistics and machin...</td>\n",
       "      <td>fundamental problem statistic machine learning...</td>\n",
       "      <td>Title: Optimal Recovery Meets Minimax Estimati...</td>\n",
       "      <td>recovery, rate, minimax, noise, optimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>Stronger Neyman Regret Guarantees for Adaptive...</td>\n",
       "      <td>Georgy Noarov, Riccardo Fogliato, Martin Bertr...</td>\n",
       "      <td>design - base potential</td>\n",
       "      <td>potential outcome, treatment effect, potential...</td>\n",
       "      <td>We study the design of adaptive, sequential ex...</td>\n",
       "      <td>study design adaptive sequential experiment un...</td>\n",
       "      <td>Title: Stronger Neyman Regret Guarantees for A...</td>\n",
       "      <td>neyman, design, regret, adaptive, group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>Invariance principle for the Gaussian Multipli...</td>\n",
       "      <td>Mriganka Basu Roy Chowdhury, Shirshendu Ganguly</td>\n",
       "      <td>exponentiate log - correlate gaussian process</td>\n",
       "      <td>random fractal measure obtain, non-gaussian lo...</td>\n",
       "      <td>Gaussian multiplicative chaos (GMC) is a canon...</td>\n",
       "      <td>gaussian multiplicative chaos gmc canonical ra...</td>\n",
       "      <td>Title: Invariance principle for the Gaussian M...</td>\n",
       "      <td>correlate, log, gaussian, multiplicative, sum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>248</td>\n",
       "      <td>On a class of high dimensional linear regressi...</td>\n",
       "      <td>Ying-Ao Wang, Yunyi Zhang, Ye Zhang</td>\n",
       "      <td>design analyze</td>\n",
       "      <td>inspire classical, broad class linear, method,...</td>\n",
       "      <td>In this paper, we introduce a unified framewor...</td>\n",
       "      <td>paper introduce unified framework inspire clas...</td>\n",
       "      <td>Title: On a class of high dimensional linear r...</td>\n",
       "      <td>regression, method, regularization, class, fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>249</td>\n",
       "      <td>On the admissibility of bounds on the mean of ...</td>\n",
       "      <td>Erik Learned-Miller</td>\n",
       "      <td>distribution</td>\n",
       "      <td>distribution, bound, finite set real, scalar p...</td>\n",
       "      <td>We address the problem of producing a lower bo...</td>\n",
       "      <td>address problem produce low bind mean discrete...</td>\n",
       "      <td>Title: On the admissibility of bounds on the m...</td>\n",
       "      <td>admissible, bound, bind, set, support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                              title  \\\n",
       "0             0  Nonparametric local polynomial regression for ...   \n",
       "1             1  Revisiting poverty measures using quantile fun...   \n",
       "2             2                 Online Bernstein-von Mises theorem   \n",
       "3             3  Debiasing Continuous-time Nonlinear Autoregres...   \n",
       "4             4  A Generalized Tangent Approximation Framework ...   \n",
       "..          ...                                                ...   \n",
       "245         245          Optimal Recovery Meets Minimax Estimation   \n",
       "246         246  Stronger Neyman Regret Guarantees for Adaptive...   \n",
       "247         247  Invariance principle for the Gaussian Multipli...   \n",
       "248         248  On a class of high dimensional linear regressi...   \n",
       "249         249  On the admissibility of bounds on the mean of ...   \n",
       "\n",
       "                                               authors  \\\n",
       "0    Moritz Jirak, Alois Kneip, Alexander Meister, ...   \n",
       "1                    N. Unnikrishnan Nair, S. M. Sunoj   \n",
       "2               Jeyong Lee, Junhyeok Choi, Minwoo Chae   \n",
       "3                              Simon Kuang, Xinfan Lin   \n",
       "4    Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. ...   \n",
       "..                                                 ...   \n",
       "245  Ronald DeVore, Robert D. Nowak, Rahul Parhi, G...   \n",
       "246  Georgy Noarov, Riccardo Fogliato, Martin Bertr...   \n",
       "247    Mriganka Basu Roy Chowdhury, Shirshendu Ganguly   \n",
       "248                Ying-Ao Wang, Yunyi Zhang, Ye Zhang   \n",
       "249                                Erik Learned-Miller   \n",
       "\n",
       "                                             topic  \\\n",
       "0                                            space   \n",
       "1                         redefine various poverty   \n",
       "2                     bernstein - von mise theorem   \n",
       "3                   class continuous - time system   \n",
       "4         approximation base variational inference   \n",
       "..                                             ...   \n",
       "245                        meet minimax estimation   \n",
       "246                        design - base potential   \n",
       "247  exponentiate log - correlate gaussian process   \n",
       "248                                 design analyze   \n",
       "249                                   distribution   \n",
       "\n",
       "                                              keywords  \\\n",
       "0    space, polynomial, nonparametric local, hilber...   \n",
       "1    flexible quantile function model, measure lite...   \n",
       "2    sequentially update posterior, update posterio...   \n",
       "3    debiase continuous - time nonlinear, ordinary ...   \n",
       "4    framework strongly super - gaussian, analysis ...   \n",
       "..                                                 ...   \n",
       "245  recovery, estimation fundamental problem, rate...   \n",
       "246  potential outcome, treatment effect, potential...   \n",
       "247  random fractal measure obtain, non-gaussian lo...   \n",
       "248  inspire classical, broad class linear, method,...   \n",
       "249  distribution, bound, finite set real, scalar p...   \n",
       "\n",
       "                                                 chunk  \\\n",
       "0    We consider nonparametric regression with func...   \n",
       "1    In this article we redefine various poverty me...   \n",
       "2    Online learning is an inferential paradigm in ...   \n",
       "3    We study how to identify a class of continuous...   \n",
       "4    Tangent approximation form a popular class of ...   \n",
       "..                                                 ...   \n",
       "245  A fundamental problem in statistics and machin...   \n",
       "246  We study the design of adaptive, sequential ex...   \n",
       "247  Gaussian multiplicative chaos (GMC) is a canon...   \n",
       "248  In this paper, we introduce a unified framewor...   \n",
       "249  We address the problem of producing a lower bo...   \n",
       "\n",
       "                                      chunk_lemmatized  \\\n",
       "0    consider nonparametric regression functional c...   \n",
       "1    article redefine various poverty measure liter...   \n",
       "2    online learning inferential paradigm parameter...   \n",
       "3    study identify class continuous time nonlinear...   \n",
       "4    tangent approximation form popular class varia...   \n",
       "..                                                 ...   \n",
       "245  fundamental problem statistic machine learning...   \n",
       "246  study design adaptive sequential experiment un...   \n",
       "247  gaussian multiplicative chaos gmc canonical ra...   \n",
       "248  paper introduce unified framework inspire clas...   \n",
       "249  address problem produce low bind mean discrete...   \n",
       "\n",
       "                                    enriched_text_rich  \\\n",
       "0    Title: Nonparametric local polynomial regressi...   \n",
       "1    Title: Revisiting poverty measures using quant...   \n",
       "2    Title: Online Bernstein-von Mises theorem\\nAut...   \n",
       "3    Title: Debiasing Continuous-time Nonlinear Aut...   \n",
       "4    Title: A Generalized Tangent Approximation Fra...   \n",
       "..                                                 ...   \n",
       "245  Title: Optimal Recovery Meets Minimax Estimati...   \n",
       "246  Title: Stronger Neyman Regret Guarantees for A...   \n",
       "247  Title: Invariance principle for the Gaussian M...   \n",
       "248  Title: On a class of high dimensional linear r...   \n",
       "249  Title: On the admissibility of bounds on the m...   \n",
       "\n",
       "                                       tfidf_top_terms  \n",
       "0    regression, polynomial, tuning, functional, no...  \n",
       "1       quantile, function, exist, difficult, flexible  \n",
       "2         update, posterior, batch, sequentially, step  \n",
       "3               step, direct, continuous, time, second  \n",
       "4    likelihood, approximation, variational, optima...  \n",
       "..                                                 ...  \n",
       "245            recovery, rate, minimax, noise, optimal  \n",
       "246            neyman, design, regret, adaptive, group  \n",
       "247      correlate, log, gaussian, multiplicative, sum  \n",
       "248  regression, method, regularization, class, fra...  \n",
       "249              admissible, bound, bind, set, support  \n",
       "\n",
       "[250 rows x 9 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fef20585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tangent approximation form a popular class of variational inference (VI) techniques for Bayesian analysis in intractable non-conjugate models. It is based on the principle of convex duality to construct a minorant of the marginal likelihood, making the problem tractable. Despite its extensive applications, a general methodology for tangent approximation encompassing a large class of likelihoods beyond logit models with provable optimality guarantees is still elusive. In this article, we propose a general Tangent Approximation based Variational InferencE (TAVIE) framework for strongly super-Gaussian (SSG) likelihood functions which includes a broad class of flexible probability models. Specifically, TAVIE obtains a quadratic lower bound of the corresponding log-likelihood, thus inducing conjugacy with Gaussian priors over the model parameters. Under mild assumptions on the data-generating process, we demonstrate the optimality of our proposed methodology in the fractional likelihood setup. Furthermore, we illustrate the empirical performance of TAVIE through extensive simulations and an application on the U.S. 2000 Census real data.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_chunks.loc[df_final_chunks.article_id == 4].chunk.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81516e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (0.29.0)\n",
      "Requirement already satisfied: Pillow in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from faiss-cpu) (2.2.2)\n",
      "Requirement already satisfied: packaging in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
      "Downloading faiss_cpu-1.10.0-cp313-cp313-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu, sentence-transformers\n",
      "Successfully installed faiss-cpu-1.10.0 sentence-transformers-4.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5c939d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eb4cd204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1413cda3e824549b3e3655635ae35b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff377884f134b2896fffa28789f0eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcfcc1a60604b8983729d336cf865a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e078294020c8485fa214df528cabb0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5bc7f3706a41548e42d3f7ae66276f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e33971bc35f4054b3e674e99aed588f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54534f0e8b9b4842aee247fe1e6820f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8862d59c4643bf8a7f6bb5ca1fe708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daad759a9448481fb4b0bf6df4d88562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8392d652184723a8d6291b9d4a9fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8bcdefc438469fbd1a801d082707ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d35b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_embed = df_final_chunks[\"enriched_text_rich\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c78a9e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title: Nonparametric local polynomial regression for functional covariates\\nAuthors: Moritz Jirak, Alois Kneip, Alexander Meister, Mario Pahl\\nTopic: space\\nKeywords: space, polynomial estimator construct, polynomial, polynomial convergence rate, infinite - dimensional hilbert, convergence, nonparametric local, hilbert space, nonparametric local polynomial regression, local polynomial, regression, nonparametric, regression functional covariate, nonparametric local polynomial, local polynomial regression, regression function, polynomial convergence, achieve polynomial convergence rate, element, infinite - dimensional hilbert space, function, functional, hilbert, functional covariate, convergence rate, infinite - dimensional, covariate\\nIntro: \\nLemmatized Chunk: consider nonparametric regression functional covariate element infinite dimensional hilbert space locally polynomial estimator construct orthonormal basis various tuning parameter remain select provide general asymptotic upper bind estimation error show procedure achieve polynomial convergence rate appropriate tuning supersmoothness regression function polynomial convergence rate usually consider nonparametric functional regression without additional strong structural constraint linearity regression function',\n",
       " 'Title: Revisiting poverty measures using quantile functions\\nAuthors: N. Unnikrishnan Nair, S. M. Sunoj\\nTopic: redefine various poverty\\nKeywords: redefine various poverty, measure literature, revisit poverty measure, measure use quantile function, measure, provision alternative methodology, redefine various poverty measure, poverty measurement analysis, revisit poverty, article redefine, term quantile, poverty, flexible quantile function model, literature term, quantile function model, enable provision alternative, flexible quantile function, methodology poverty measurement, quantile function, term quantile function, poverty measure, poverty measure use quantile, poverty measure literature, distribution function, prevail approach, alternative methodology poverty\\nIntro: \\nLemmatized Chunk: article redefine various poverty measure literature term quantile function instead distribution function prevail approach enable provision alternative methodology poverty measurement analysis along new result difficult obtain exist framework several flexible quantile function model enrich exist one propose utility demonstrate real datum',\n",
       " 'Title: Online Bernstein-von Mises theorem\\nAuthors: Jeyong Lee, Junhyeok Choi, Minwoo Chae\\nTopic: bernstein - von mise theorem\\nKeywords: bernstein - von mise theorem, bernstein - von mise theorem online, contrast batch, posterior, online learning, suit online learning, update incrementally sequentially, sequentially, sequentially update posterior, parameter update incrementally, entire dataset process, mise theorem online, batch learning, online bernstein - von mise, update, update posterior, inferential paradigm, contrast batch learning, learning, sequentially update, dataset process, theorem online learning, mise theorem online learning, update posterior serving, online bernstein - von mise theorem, entire dataset, update incrementally\\nIntro: \\nLemmatized Chunk: online learning inferential paradigm parameter update incrementally sequentially available datum contrast batch learning entire dataset process paper assume mini batch full dataset become available sequentially bayesian framework update belief unknown parameter observe mini batch naturally suit online learning step update posterior distribution use current prior new observation update posterior serving prior next step however recursive bayesian updating rarely computationally tractable unless model prior conjugate model regular update posterior approximate normal distribution justify bernstein von mise theorem adopt variational approximation step investigate frequentist property final posterior obtain sequential procedure mild assumption show accumulate approximation error become negligible mini batch size exceed threshold depend parameter dimension result sequentially update posterior asymptotically indistinguishable full posterior',\n",
       " 'Title: Debiasing Continuous-time Nonlinear Autoregressions\\nAuthors: Simon Kuang, Xinfan Lin\\nTopic: class continuous - time system\\nKeywords: class continuous - time system, ordinary differential, equation affine, continuous - time nonlinear system define, autoregression study, differential equation, class continuous - time nonlinear, step, plug - linear estimator, system define, continuous - time nonlinear autoregression, debiase continuous - time nonlinear autoregression, nonlinear system, continuous - time nonlinear system, debiase continuous - time nonlinear, unknown parameter, ordinary differential equation, ordinary differential equation affine, define ordinary, study identify, class continuous - time, debiase continuous - time, continuous - time nonlinear, differential equation affine, nonlinear autoregression, nonlinear autoregression study, nonlinear system define\\nIntro: \\nLemmatized Chunk: study identify class continuous time nonlinear system define ordinary differential equation affine unknown parameter define notion asymptotic consistency n h achieve use family direct method first step differentiate noisy time series second step plug linear estimator first step differentiation signal process adaptation nonparametric statistical technique local polynomial regression second step generalize linear regression consistent use least square estimator demonstrate two novel bias correction improve accuracy finite method significantly broaden class continuous time system consistently estimate direct method',\n",
       " 'Title: A Generalized Tangent Approximation Framework for Strongly\\n  Super-Gaussian Likelihoods\\nAuthors: Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. Mallick\\nTopic: approximation base variational inference\\nKeywords: approximation base variational inference, approximation form popular, tangent approximation form, bayesian analysis, variational inference, approximation form, super - gaussian likelihood tangent approximation, tangent approximation base variational, analysis intractable non-conjugate, generalize tangent approximation, strongly super - gaussian likelihood tangent, technique bayesian, form popular, popular class variational, likelihood tangent approximation form, tangent approximation, bayesian analysis intractable, analysis intractable, base variational inference, intractable non-conjugate, form popular class, generalize tangent approximation framework, general tangent approximation base, intractable non-conjugate model, framework strongly super - gaussian, technique bayesian analysis\\nIntro: \\nLemmatized Chunk: tangent approximation form popular class variational inference vil technique bayesian analysis intractable model base principle convex duality construct minorant marginal likelihood make problem tractable despite extensive application general methodology tangent approximation encompass large class likelihood beyond logit model provable optimality guarantee still elusive article propose general tangent approximation base variational inference tavie framework strongly super gaussian ssg likelihood function include broad class flexible probability model specifically tavie obtain quadratic low bind corresponding log likelihood thus induce conjugacy gaussian prior model parameter mild assumption datum generating process demonstrate optimality propose methodology fractional likelihood setup furthermore illustrate empirical performance tavie extensive simulation application census real datum',\n",
       " 'Title: Dimension-Free Convergence of Diffusion Models for Approximate Gaussian\\n  Mixtures\\nAuthors: Gen Li, Changxiao Cai, Yuting Wei\\nTopic: exceptional generative performance\\nKeywords: exceptional generative performance, model, produce high - quality sample, diffusion, iterative denoising, gaussian mixture diffusion, mixture diffusion model, gaussian mixture diffusion model, dimension - free convergence diffusion, approximate gaussian mixture diffusion, sample iterative denoising, dimension - free convergence, generative performance, approximate gaussian mixture, diffusion model distinguished, denoise step require, denoise diffusion probabilistic, produce high - quality, effectiveness diffusion model, denoise diffusion probabilistic model, high - quality sample iterative, diffusion model, high - quality sample, sample generation scale, accurate sample generation, exceptional generative, gaussian mixture model, diffusion probabilistic model, convergence diffusion model\\nIntro: \\nLemmatized Chunk: diffusion model distinguish exceptional generative performance particularly produce high quality sample iterative denoising current theory suggest number denoise step require accurate sample generation scale linearly datum dimension reflect practical efficiency widely use algorithm like denoise diffusion probabilistic model ddpm paper investigate effectiveness diffusion model sampling complex high dimensional distribution well approximate gaussian mixture model gmm distribution main result show ddpm take iteration attain accurate distribution total variation TV distance independent ambient dimension number component logarithmic factor furthermore result remain robust score estimation error finding highlight remarkable effectiveness diffusion model high dimensional setting give universal approximation capability gmm provide theoretical insight practical success',\n",
       " 'Title: DDPM Score Matching and Distribution Learning\\nAuthors: Sinho Chewi, Alkis Kalavasis, Anay Mehrotra, Omar Montasser\\nTopic: distribution learning score estimation\\nKeywords: distribution learning score estimation, pac density estimation, learning score estimation, distribution, low bound score, score estimation relate, score - base generative model, diffusion probabilistic, probabilistic model, density estimation, ddpm score matching, accurate score estimate, distribution learning score, generative model, score - base generative, score matching, denoise diffusion probabilistic, denoise diffusion probabilistic model, score estimation, score, backbone score - base generative, distribution learning, bound score estimation, estimation, backbone score - base, diffusion probabilistic model, density\\nIntro: \\nLemmatized Chunk: score estimation backbone score base generative model sgm especially denoise diffusion probabilistic model ddpm key result area show accurate score estimate sgm efficiently generate sample realistic datum distribution chen et iclr Lee et alt distribution learning result learn distribution implicitly sampler output explain score estimation relate classical task parameter density estimation paper introduce framework reduce score estimation two task various implication statistical computational learning theory parameter estimation koehler et iclr demonstrate score matching variant statistically inefficient parametric estimation multimodal density common practice contrast show mild condition denoise score matching ddpm asymptotically efficient density estimation link generation score estimation lift exist score estimation guarantee pac density estimation function approximate target log density within fraction space provide minimax rate density estimation old class ii quasi pac density estimation algorithm classical gaussian location mixture model build address open problem gatmiry et arxiv low bound score estimation framework offer first principled method prove computational low bound score estimation across general distribution application establish cryptographic low bound score estimation general gaussian mixture model conceptually recover song neurip result advance key open problem',\n",
       " 'Title: Detecting relevant dependencies under measurement error with\\n  applications to the analysis of planetary system evolution\\nAuthors: Patrick Bastian, Nicolai Bissantz\\nTopic: planetary system\\nKeywords: planetary system, system formation, detect relevant dependency, formation orbital evolution, orbital evolution, play important role, planetary system formation, important role understand, evolution exoplanet play, important role, formation orbital, measurement, planetary system evolution, mechanics planetary system, system evolution, evolution exoplanet, play important, role understanding, exoplanet play important, system evolution exoplanet, exoplanet play, system formation orbital, correlation, system evolution exoplanet play, analysis planetary system, measurement error, understand mechanics, planetary system evolution exoplanet, datum\\nIntro: \\nLemmatized Chunk: exoplanet play important role understand mechanics planetary system formation orbital evolution context correlation different parameter planet host star useful guide search explanatory mechanism base reanalysis datum set study still poorly understand correlation planetary surface gravity stellar activity hot jupiter unfortunately datum collection often suffer measurement error due complicated indirect measurement setup rendering standard inference technique unreliable present new method estimate test correlation deconvolution framework thereby improve state art analysis datum two direction first able account additive measurement error facilitate reliable inference second test relevant change test correlation exceed certain threshold reflect fact small nonzero correlation expect real life datum almost always standard statistical test therefore always reject null correlation give sufficient datum theory focus quantity estimate contain variety correlation measure propose bootstrap test establish theoretical validity product also obtain confidence interval apply method hot jupiter datum set observe take account measurement error yield small point estimate null relevant correlation reject small demonstrate importance consider impact measurement error avoid mislead conclusion result statistical analysis',\n",
       " 'Title: SurvSurf: a partially monotonic neural network for first-hitting time\\n  prediction of intermittently observed discrete and continuous sequential\\n  events\\nAuthors: Yichen Kelly Chen, Sören Dittmer, Kinga Bernatowicz, Josep Arús-Pous, Kamen Bliznashki, John Aston, James H. F. Rudd, Carola-Bibiane Schönlieb, James Jones, Michael Roberts\\nTopic: hit time sequential\\nKeywords: hit time sequential, partially monotonic neural network, simultaneous probabilistic prediction, prediction intermittently observe, design direct, neural - network base, discrete continuous sequential, sequential event baseline, time sequential event, prediction intermittently, partially monotonic neural, neural - network base survival, neural - network base survival model, neural network first - hit, network first - hit time, first - hit time prediction, propose neural - network, specifically design, monotonic neural network, propose neural - network base, function sequential event, base survival model, simultaneous probabilistic, probabilistic prediction, sequential event, time prediction intermittently, continuous sequential event, intermittently observe discrete, incidence function sequential, specifically design direct, direct simultaneous, observe discrete continuous, direct simultaneous probabilistic\\nIntro: \\nLemmatized Chunk: propose neural network base survival model survsurf specifically design direct simultaneous probabilistic prediction first hit time sequential event baseline unlike exist model survsurf theoretically guarantee never violate monotonic relationship cumulative incidence function sequential event allow nonlinear influence predictor also incorporate implicit truth unobserved intermediate event model fitting support discrete continuous time event also identify variant integrate brier score ibs show robust correlation mean square error mse true predict probability accounting imply truth missing intermediate event demonstrate superiority survsurf compare modern traditional predictive survival model two simulate dataset two real world dataset use mse robust ibs measure extent monotonicity violation',\n",
       " 'Title: Truncated sequential guaranteed estimation for the Cox-Ingersoll-Ross\\n  models\\nAuthors: Mohamed Ben Alaya, Thi-Bao Trâm Ngô, Serguei Pergamenchtchikov\\nTopic: sequential parameter estimation problem\\nKeywords: sequential parameter estimation problem, propose sequential procedure, truncate sequential guarantee estimation, observation, limited duration, sequential parameter estimation, truncate sequential guarantee, fix observation, truncate sequential, parameter estimation problem, dimensional parameter case, drift sequential parameter, sequential estimation method, observation duration, sequential guarantee estimation, truncate sequential estimation method, fix observation duration, truncate sequential estimation, estimation problem, drift sequential, estimation, model drift, drift sequential parameter estimation, sequential, model drift sequential, sequential parameter\\nIntro: \\nLemmatized Chunk: drift sequential parameter estimation problem cox ingersoll ross cir process limited duration observation study truncate sequential estimation method scalar dimensional parameter case propose setting propose truncate estimator property guarantee mean square estimation accuracy establish asymptotic formulation observation time tend infinity show propose sequential procedure asymptotically optimal among possible sequential estimate average estimation time less fix observation duration also turn asymptotically without degrade estimation quality significantly reduce observation duration compare classical maximum likelihood estimation base fix observation duration',\n",
       " \"Title: Extension of Yager's negation of probability distribution based on\\n  uncertainty measures\\nAuthors: Santosh Kumar Chaudhary, Pradeep Kumar Sahu, Nitin Gupta\\nTopic: negation primarily focus\\nKeywords: negation primarily focus, exist research, distribution, primarily focus, uncertainty measure exist research, measure, research, base uncertainty, exist research negation, entropy extropy, varextropy, uncertainty measure exist, yager negation, entropy, probability distribution, probability distribution base, primarily focus entropy, extension yager negation, exist, probability, negation primarily, shannon entropy, distribution base uncertainty, negation, varentropy varextropy, primarily, base uncertainty measure, measure exist research, research negation primarily, extension yager\\nIntro: \\nLemmatized Chunk: exist research negation primarily focus entropy extropy recently new function varentropy varextropy develop consider extension entropy extropy however impact negation extend measure particularly varentropy varextropy extensively explore address gap paper investigate effect negation shannon entropy varentropy varextropy explore negation probability distribution influence measure show negate distribution consistently lead high value shannon entropy varentropy varextropy compare original distribution additionally prove negation probability distribution maximize measure process paper provide theoretical proof detailed analysis behaviour measure contribute good understanding interplay probability distribution negation information theoretic quantity\",\n",
       " 'Title: Gaussian Mean Testing under Truncation\\nAuthors: Clément L. Canonne, Themis Gouleakis, Yuhao Wang, Joy Qiping Yang\\nTopic: large magnitude\\nKeywords: large magnitude, vector perturb, computer science community, task, perturb white, high - dimensional gaussian, vector perturb white, testing, gaussian, signal processing community, vector, perturb white noise, underlying high - dimensional gaussian, white noise, noise large, truncation, noise large magnitude, theoretical computer science, high - dimensional vector, white noise large, high - dimensional vector perturb, theoretical computer science community\\nIntro: \\nLemmatized Chunk: consider task gaussian mean testing test whether high dimensional vector perturb white noise large magnitude zero vector question originate signal processing community recently see surge interest machine learning theoretical computer science community fairly well understand much less understand focus work perform task truncation observation sample underlying high dimensional gaussian observe fall give subset domain truncation model previously study context learning instead testing mean vector range application particular economics social science work show sample truncation affect complexity testing task rather subtle surprising way',\n",
       " 'Title: On the bias of the Gini coefficient estimator for zero-truncated Poisson\\n  distributions\\nAuthors: Roberto Vila, Helton Saulo\\nTopic: analyze gini coefficient\\nKeywords: analyze gini coefficient, zero - truncate poisson, carlo simulation method, evaluate use monte carlo, paper analyze gini, distribution paper analyze, monte carlo simulation, reveal presence, monte carlo simulation method, coefficient estimator, coefficient estimator zero - truncate, paper analyze, gini coefficient estimator, zero - truncate poisson distribution, estimator zero - truncate, evaluate use monte, mathematical expression, bias gini, simulation method, poisson population, gini coefficient, expression bias, analyze gini, estimator zero - truncate poisson, poisson distribution paper, zero - truncate poisson population\\nIntro: \\nLemmatized Chunk: paper analyze gini coefficient estimator zero truncate poisson population reveal presence bias provide mathematical expression bias along bias correct estimator evaluate use monte carlo simulation method',\n",
       " 'Title: Balancing Complexity and Informativeness in LLM-Based Clustering:\\n  Finding the Goldilocks Zone\\nAuthors: Justin Miller, Tristram Alexander\\nTopic: short text\\nKeywords: short text, llm - base clustering, text datum lie, traditional evaluation metric, datum lie, semantic, balancing, balancing complexity informativeness, cluster short text datum, cluster short text, short text datum, short text datum lie, lie balancing informativeness, challenge cluster short, balancing complexity, cluster, datum lie balancing, cluster short, clustering, informativeness, balancing informativeness, balancing informativeness interpretability, semantic density, lie balancing, text datum\\nIntro: \\nLemmatized Chunk: challenge cluster short text datum lie balancing informativeness interpretability traditional evaluation metric often overlook trade inspire linguistic principle communicative efficiency paper investigate optimal number cluster quantify trade informativeness cognitive simplicity use large language model llm generate cluster name evaluate effectiveness semantic density information theory cluster accuracy result show gaussian mixture model gmm cluster embedding generate llm increase semantic density compare random assignment effectively grouping similar bio however cluster increase interpretability decline measure generative llm ability correctly assign bio base cluster name logistic regression analysis confirm classification accuracy depend semantic similarity bio assign cluster name well distinction alternative finding reveal goldilock zone cluster remain distinct yet interpretable identify optimal range cluster parallel linguistic efficiency lexical categorization insight inform theoretical model practical application guide future research toward optimise cluster interpretability usefulness',\n",
       " 'Title: Regression Discontinuity Design with Distribution-Valued Outcomes\\nAuthors: David Van Dijcke\\nTopic: local average quantile treatment\\nKeywords: local average quantile treatment, distribution - value, rdd framework, regression discontinuity, introduce regression discontinuity, subsidy allocate base, regression, outcome article introduce, regression discontinuity design, article introduce, rdd framework setting, outcome distribution, standard rdd method, discontinuity design distribution - value, introduce regression discontinuity design, extend standard rdd, standard rdd framework, standard rdd, distribution - value outcome article, article introduce regression discontinuity, discontinuity design, discontinuity, design distribution - value outcome, distribution - value outcome, article introduce regression, rdd\\nIntro: \\nLemmatized Chunk: article introduce regression discontinuity design rdd distribution value outcome extend standard rdd framework setting outcome distribution rather scalar setting arise treatment assign high level aggregation outcome example subsidy allocate base firm level revenue cutoff outcome interest distribution employee wage within firm since standard rdd method accommodate two level randomness I propose novel approach base random distribution target estimand local average quantile treatment effect average across random quantile estimate target I introduce two related approach one extend local polynomial regression random quantile another base local regression form functional regression estimator I establish asymptotic normality develop uniform debiase confidence band together datum drive bandwidth selection procedure simulation validate theoretical property show exist method biased inconsistent setting I apply propose method study effect gubernatorial party control within state income distribution use close election design result suggest classic equality efficiency tradeoff democratic governorship drive reduction income top distribution',\n",
       " 'Title: Common Drivers in Sparsely Interacting Hawkes Processes\\nAuthors: Alexander Kreiss, Enno Mammen, Wolfgang Polonik\\nTopic: parameter\\nKeywords: parameter, model, model time - continuous relational, study, relational event, multivariate, model time - continuous, multivariate hawkes process, sparsely interact, sparsely interact hawke, sparsely interact hawkes process, local parameter specific, time - continuous relational, hawkes process study, event network, hawkes process, interact hawkes, interact hawkes process, time - continuous relational event, study multivariate hawke, relational event network, multivariate hawkes, study multivariate, common driver sparsely, network, common driver, driver sparsely interact, process study, time - continuous relational event network, model parameter\\nIntro: \\nLemmatized Chunk: study multivariate hawke process model time continuous relational event network model assume network know include covariate allow common driver parameter common actor network also local parameter specific actor derive rate convergence model parameter number actor time horizon tend infinity prevent explode network sparseness assume also discuss numerical aspect',\n",
       " 'Title: Stochastic Optimization with Optimal Importance Sampling\\nAuthors: Liviu Aolaritei, Bart P. G. Van Parys, Henry Lam, Michael I. Jordan\\nTopic: reduction technique\\nKeywords: reduction technique, efficiency monte carlo, monte carlo method, widely use variance reduction, importance sampling, frequently require stochastic, rare - event simulation, simulation related application, optimization optimal importance, rare - event simulation related, efficiency monte, simulation related, variance reduction technique, optimal importance, importance sampling importance sampling, reduction technique enhance, monte carlo, related application, require stochastic calibration technique, sampling importance, enhance efficiency, variance reduction, optimal importance sampling, importance sampling importance, optimal importance sampling importance, frequently require stochastic calibration, stochastic calibration technique, sampling importance sampling\\nIntro: \\nLemmatized Chunk: importance sampling widely use variance reduction technique enhance efficiency monte carlo method particularly rare event simulation related application despite power performance often highly sensitive choice proposal distribution frequently require stochastic calibration technique design analysis extensively study estimation setting apply within stochastic optimization introduce unique challenge decision distribution mutually dependent create circular optimization structure interdependence complicate analysis convergence decision iterate efficiency scheme paper propose iterative gradient base algorithm jointly update decision variable distribution without require time scale separation two method achieve low possible asymptotic variance guarantee global convergence convexity objective mild assumption distribution family furthermore show property preserve linear constraint incorporate recent variant nesterov dual average method',\n",
       " 'Title: Identifiability of VAR(1) model in a stationary setting\\nAuthors: Bixuan Liu\\nTopic: direct graph\\nKeywords: direct graph, weight direct, matrix influence relationship, interpret autoregressive interaction, vector autoregressive, component var, autoregressive interaction matrix, classical first - order vector, classical first - order, first - order vector, var, classical first - order vector autoregressive, influence relationship, influence graph underlying, weight direct graph, first - order vector autoregressive, interaction matrix influence, interaction matrix, autoregressive interaction, interpret autoregressive, stationary setting, study structural identifiability\\nIntro: \\nLemmatized Chunk: consider classical first order vector autoregressive var model interpret autoregressive interaction matrix influence relationship among component var process encode weight direct graph majority previous work study structural identifiability graph base time series observation therefore rely dynamical information work assume equilibrium exist study instead identifiability graph stationary distribution mean seek way reconstruct influence graph underlie dynamic network use static information use approach algebraic statistic characterize model use jacobian matroid associate parametrization model introduce sufficient graphical condition different graph yield distinct steady state distribution additionally illustrate result could apply characterize network inspire ecological research',\n",
       " 'Title: On empirical Hodge Laplacians under the manifold hypothesis\\nAuthors: Jan-Paul Lerch, Martin Wahl\\nTopic: observation uniformly distribute\\nKeywords: observation uniformly distribute, high - order generalization graph, exist dirichlet form error, study high - order generalization, observation uniformly, dirichlet form error bound, high - order generalization, - call hodge laplacian, - call hodge, form error bound, generalization graph laplacian, manifold hypothesis, dirichlet form error, laplace - beltrami operator differential, euclidean space, close submanifold, hodge laplacian, uniformly distribute, operator differential form, study high - order, laplace - beltrami operator, empirical hodge laplacian, empirical hodge, operator differential\\nIntro: \\nLemmatized Chunk: give observation uniformly distribute close submanifold euclidean space study high order generalization graph laplacian call hodge laplacian graph approximation laplace beltrami operator differential form main result high probability error bind associate dirichlet form bind improve exist dirichlet form error bound graph laplacian context laplacian eigenmap provide insight betti number study topological datum analysis complement positive part spectrum',\n",
       " 'Title: On the rate of convergence of an over-parametrized deep neural network\\n  regression estimate learned by gradient descent\\nAuthors: Michael Kohler\\nTopic: neural network regression\\nKeywords: neural network regression, deep neural network, network regression estimate learn, network regression estimate, gradient descent, consider, learn gradient descent, deep neural network regression, estimate learn gradient, regression random, neural network, regression, error criterion, activation function define, nonparametric, gradient descent nonparametric, - parametrize deep, neural network regression estimate, design, integration respect, descent nonparametric regression, deep neural, regression random design, design measure, nonparametric regression, gradient descent nonparametric regression, random design consider, - parametrize deep neural, random design, function, error, - parametrize deep neural network, nonparametric regression random, design consider, learn gradient, random\\nIntro: \\nLemmatized Chunk: nonparametric regression random design consider error integration respect design measure use error criterion parametrize deep neural network regression estimate logistic activation function define weight learn gradient descent show estimate achieve nearly optimal rate convergence case regression function p c smooth',\n",
       " 'Title: Eigen-inference by Marchenko-Pastur inversion\\nAuthors: Ben Deitmar\\nTopic: spectral\\nKeywords: spectral, population linear spectral statistic, eigen - inference marchenko - pastur inversion, marchenko - pastur inversion derive, marchenko - pastur, linear spectral, inference population linear, linear spectral statistic, population spectral distribution, formula marchenko - pastur inversion, spectral statistic, estimator population linear, population linear, population spectral, frac, spectral distribution, inversion derive, marchenko - pastur inversion, eigen - inference marchenko - pastur, spectral statistic construct, population linear spectral, population, varepsilon\\nIntro: \\nLemmatized Chunk: new formula marchenko pastur inversion derive use inference population linear spectral statistic formula allow estimation stieltje transform population spectral distribution z sufficiently far support population spectral distribution dimension sample size go infinity simultaneously rightarrow estimation error show asymptotically less arbitrary integrate along curve around support estimator population linear spectral statistic construct benefit convergence speed',\n",
       " 'Title: Existence and non-existence of consistent estimators in supercritical\\n  controlled branching processes\\nAuthors: Peter Braunsteins, Sophie Hautphenne, James Kerlidis\\nTopic: supercritical control branching process\\nKeywords: supercritical control branching process, alongside population size count, branching process, control branching process, control branching process consistently, parameter supercritical, trajectory population size, estimator supercritical control, population size count, supercritical control branching, branching process consistently, single observe, problem estimating, existence non-existence, consistently, consistently estimate, non-existence consistent estimator, observe trajectory, population size, control branching, process consistently, size count, single observe trajectory, supercritical control, non-existence consistent, control, consistent estimator supercritical\\nIntro: \\nLemmatized Chunk: consider problem estimate parameter supercritical control branching process consistently single observe trajectory population size count goal establish parameter consistently estimate parameter consistently estimate derive explicit expression estimator address question three scenario distribution control function distribution know unknown progenitor number observe alongside population size count result offer theoretical justification common practice population ecology estimate demographic environmental stochasticity use separate observation scheme',\n",
       " 'Title: Adaptive sparse variational approximations for Gaussian process\\n  regression\\nAuthors: Dennis Nieman, Botond Szabó\\nTopic: effectively\\nKeywords: effectively, hyperparameter selection, accurate tuning hyperparameter, effectively across different setting, generalise, model generalise, gaussian process regression accurate, regression model, model generalise effectively, crucial ensure, ensure model, tuning hyperparameter, variational, regression accurate, sparse variational, accurate tuning, process regression accurate, crucial, regression accurate tuning, generalise effectively, gaussian process, adaptive sparse, adaptive sparse variational approximation, ensure, sparse variational approximation, nonparametric regression model, process regression accurate tuning, tuning, gaussian process regression, accurate, hyperparameter crucial, adaptive sparse variational\\nIntro: \\nLemmatized Chunk: accurate tuning hyperparameter crucial ensure model generalise effectively across different setting paper present theoretical guarantee hyperparameter selection use variational bay nonparametric regression model construct variational approximation hierarchical bay procedure derive upper bound contraction rate variational posterior abstract setting theory apply various gaussian process prior variational class result minimax optimal rate theoretical result accompany numerical analysis synthetic real world datum set',\n",
       " 'Title: A computational transition for detecting multivariate shuffled linear\\n  regression by low-degree polynomials\\nAuthors: Zhangsong Li\\nTopic: standard gaussian design matrix\\nKeywords: standard gaussian design matrix, linear regression, detect multivariate shuffle linear, latent permutation, shuffle linear, linear model, response linear, linear model obfuscate, sigma, transition detect multivariate, detect multivariate, detect multivariate shuffle, gaussian design matrix, shuffle linear regression, correspondence predictor, multivariate shuffle linear, linear, predictor response, multivariate shuffle linear regression, low - degree polynomial algorithm, gaussian noise matrix, multivariate shuffle, permutation matrix, problem multivariate shuffle, linear regression low - degree\\nIntro: \\nLemmatized Chunk: paper study problem multivariate shuffle linear regression correspondence predictor response linear model obfuscate latent permutation specifically investigate model x sigma z standard gaussian design matrix gaussian noise matrix unknown permutation matrix unknown grassmanian manifold satisfy mathbb consider hypothesis testing problem distinguish model case independent gaussian random matrice size respectively result reveal phase transition phenomenon performance low degree polynomial algorithm task would show degree polynomial fail distinguish two model even provide big show degree polynomial fail distinguish two model provide show exist constant degree polynomial strongly distinguish two model result establish smooth transition effectiveness low degree polynomial algorithm problem highlight interplay dimension noise level computational complexity testing task',\n",
       " 'Title: A Lanczos-Based Algorithmic Approach for Spike Detection in Large Sample\\n  Covariance Matrices\\nAuthors: Charbel Abi Younes, Xiucai Ding, Thomas Trogdon\\nTopic: asymptotic\\nKeywords: asymptotic, estimate number, large sample, lanczo - base algorithmic approach, algorithmic approach spike, model without directly computing, large sample covariance, directly computing, computing eigenvalue, covariance model without directly, class spike covariance, spike covariance model, approach spike detection, general class spike, jacobi matrix, sample covariance, directly computing eigenvalue, general class, covariance matrice introduce, jacobi matrix enable, matrice introduce, detection large sample, spike general, sample covariance matrice, large sample covariance matrice, asymptotic spectral distribution, spike detection, number spike, spike covariance, detection large, spike detection large, standard spike covariance model, sample covariance matrix, lanczo - base algorithmic\\nIntro: \\nLemmatized Chunk: introduce new approach estimate number spike general class spike covariance model without directly computing eigenvalue sample covariance matrix approach base lanczo algorithm asymptotic property associate jacobi matrix cholesky factorization key aspect analysis interpret eigenvector spectral distribution perturbation asymptotic counterpart specific exponential type asymptotics jacobi matrix enable efficient approximation stieltje transform asymptotic spectral distribution via finite continue fraction consequence also obtain estimate density asymptotic distribution location outlier provide consistency guarantee propose estimator prove convergence high dimensional regime demonstrate apply standard spike covariance model approach outperform exist method computational efficiency runtime still maintain robustness exotic population covariance',\n",
       " 'Title: High-dimensional ridge regression with random features for\\n  non-identically distributed data with a variance profile\\nAuthors: Issa-Mbenard Dabo, Jérémie Bigot\\nTopic: model\\nKeywords: model, random matrix theory, random feature model, random vector, issue interest, feature vector, popular issue interest, random feature non-identically, feature model, regression random feature, random feature vector, random feature, non-iid feature vector, non-identically distribute, machine learning, machine learning literature, ridge regression random, learning literature, non-identically distribute datum, popular issue, vector make independent, positive definite matrix represent, feature non-identically distribute, random vector make, feature, random\\nIntro: \\nLemmatized Chunk: behavior random feature model high dimensional regression framework become popular issue interest machine learning literature model generally consider feature vector random vector make independent identically distribute iid entrie positive definite matrix represent covariance feature paper move beyond standard assumption study performance random feature model setting feature vector approach related analysis spectrum large random matrice random matrix theory rmt free probability result turn analysis datum use notion variance profile well study rmt main contribution study limit training prediction risk associate ridge estimator random feature model dimension grow provide asymptotic equivalent risk capture behavior ridge regression random feature high dimensional framework asymptotic equivalent prove sharp numerical experiment retrieve adapt setting establish result operator value free probability theory moreover various class random feature vector consider far approach allow show appearance double descent phenomenon ridge regularization parameter small enough',\n",
       " 'Title: E-variables for hypotheses generated by constraints\\nAuthors: Martin Larsson, Aaditya Ramdas, Johannes Ruf\\nTopic: distribution\\nKeywords: distribution, mathcal, nonnegative random, attempt derive admissible, family, variable whose expect, object hypothesis testing, e - variable, e - variable hypothese, fundamental object hypothesis, random variable, derive admissible optimal, recognize fundamental object, admissible optimal e-variable, hypothese generate constraint, grow body work, nonnegative random variable, rapidly grow body, hypothese generate, nonnegative, e - variable hypothese generate, random variable whose expect, optimal e-variable, constraint\\nIntro: \\nLemmatized Chunk: family distribution nonnegative random variable whose expect value every distribution one variable recently recognize fundamental object hypothesis testing rapidly grow body work attempt derive admissible optimal various family paper study class specify constraint simple example include bound moment general theory cover arbitrary set measurable constraint main result characterize set class well maximal one three case study illustrate scope theory finite constraint set one sided sub distribution distribution invariant group symmetry particular generalize recent result clerico drop assumption constraint',\n",
       " 'Title: Kullback-Leibler Consistency of $p$-dimensional Pólya Tree Posteriors\\n  and Differential Entropy Estimation\\nAuthors: Fernando Corrêa, Rafael Bassi Stern, Julio Michael Stern\\nTopic: density differential entropy\\nKeywords: density differential entropy, posterior differential entropy, dimension, differential entropy estimation, olya tree prior, multiplicative structure, olya, obtain kullback - leibler total, dimensional pólya tree, exploit multiplicative structure, entropy estimation exploit, tree prior, tree prior density, dimensional pólya tree posterior, entropy, kullback - leibler total variation, differential entropy, exploit multiplicative, olya tree obtain, dimensional pólya, prior density, entropy estimation, pólya tree posterior, pólya tree, olya tree level, tree posterior differential, olya tree, density differential\\nIntro: \\nLemmatized Chunk: exploit multiplicative structure tree prior density differential entropy estimation dimension establish representation theorem entropy functional ii condition parameter tree obtain kullback leibler total variation consistency vector compact support result motivate novel differential entropy estimator consistent probability compact support vector mild condition order enable application result also provide theoretical motivation truncation univariate tree level',\n",
       " 'Title: Universal Log-Optimality for General Classes of e-processes and\\n  Sequential Hypothesis Tests\\nAuthors: Ian Waudby-Smith, Ricardo Sandoval, Michael I. Jordan\\nTopic: bound random tuple\\nKeywords: bound random tuple, hypothesis test, hypothesis testing, testing, special case, universal log - optimality general, testing problem, hypothesis testing betting, problem sequential hypothesis, composite testing problem, general case, class e-process, sequential hypothesis testing, two - sample independence testing, sublinear regret bind, general class, independence testing, universal log - optimality, key ingredient two - sample, hypothesis, general class e-process, class composite testing, include bound mean testing, composite testing, testing special case, log - optimality general class, sequential hypothesis, testing betting, e - process sequential hypothesis\\nIntro: \\nLemmatized Chunk: consider problem sequential hypothesis testing bet general class composite testing problem include bound mean testing equal mean testing bound random tuple key ingredient two sample independence testing special case show process satisfy certain sublinear regret bind adaptively asymptotically almost surely log optimal composite alternative strong notion optimality previously establish aforementioned problem provide explicit test supermartingale process satisfy notion general case furthermore derive match low upper bound expect rejection time result sequential test case proof result make weak algorithm agnostic moment assumption rely general purpose proof technique involve aforementioned regret family numeraire portfolios finally discuss theorem hold distribution uniform sense notion log optimality strong still seem new literature',\n",
       " 'Title: Computing High-dimensional Confidence Sets for Arbitrary Distributions\\nAuthors: Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan\\nTopic: confidence set\\nKeywords: confidence set, arbitrary distribution, distribution, high - dimensional confidence set, region arbitrary, competitive, set, volume, factor competitive, high - density region, mathbb, computing high - dimensional, delta, arbitrary distribution study, arbitrary, exp, tilde, coverage, algorithm, learn high - density region, computing high - dimensional confidence set, confidence, problem\\nIntro: \\nLemmatized Chunk: study problem learn high density region arbitrary distribution give target coverage parameter sample access arbitrary distribution want output confidence set subset achieve coverage sim volume small possible central problem high dimensional statistic application find confidence set uncertainty quantification support estimation general setting problem statistically intractable restrict attention compete set concept class bound vc dimension algorithm competitive class give sample arbitrary distribution output polynomial time set achieve coverage whose volume competitive small set require coverage problem computationally challenging even basic setting set euclidean ball exist algorithm base coreset find polynomial time ball whose volume would factor competitive volume good ball main result algorithm find confidence set whose volume factor competitive optimal ball desire coverage algorithm improper output ellipsoid combine computational intractability result proper learning ball within approximation factor volume result provide interesting separation proper improper learning confidence set',\n",
       " 'Title: The Markov approximation of the periodic multivariate Poisson\\n  autoregression\\nAuthors: Mahmoud Khabou, Edward A. K. Cohen, Almut E. D. Veraart\\nTopic: multivariate poisson\\nKeywords: multivariate poisson, focus network, potentially infinite memory, paper introduce periodic, special focus, periodic multivariate, autoregression paper introduce, paper introduce, network setting, periodic multivariate poisson, multivariate poisson autoregression, periodic multivariate poisson autoregression, potentially infinite, computationally efficient markov approximation, poisson autoregression, infinite memory, autoregression potentially infinite, poisson autoregression paper, poisson autoregression potentially, introduce periodic, introduce periodic multivariate, approximation periodic, autoregression potentially\\nIntro: \\nLemmatized Chunk: paper introduce periodic multivariate poisson autoregression potentially infinite memory special focus network setting use contraction technique study stability process provide upper bound fast reach periodically stationary regime propose computationally efficient markov approximation use property exponential function density result furthermore prove strong consistency maximum likelihood estimator markov approximation empirically test robustness case misspecification model apply prediction weekly rotavirus case berlin demonstrate superior performance compare exist pnar model',\n",
       " 'Title: Fermat Distance-to-Measure: a robust Fermat-like metric\\nAuthors: Jérôme Taupin, Frédéric Chazal\\nTopic: distance low density\\nKeywords: distance low density, robust fermat - like metric, metric shrink distance, euclidean metric shrink, enlarge distance low, density - drive metric conformal, shrink distance high, distance high, shrink distance, conformal transformation, fermat distance density - drive, distance high density, density area, robust fermat - like, enlarge distance, density area enlarge, distance density - drive metric, distance low, high density area, low density area, metric conformal transformation, area enlarge distance\\nIntro: \\nLemmatized Chunk: give probability measure density fermat distance density drive metric conformal transformation euclidean metric shrink distance high density area enlarge distance low density area although widely study show useful various machine learning task limit measure density respect lebesgue measure volume form manifold paper replace density distance measure introduce new metric fermat distance measure define probability measure derive strong stability property fermat distance measure respect measure propose estimator random sampling measure feature explicit bind convergence speed',\n",
       " 'Title: Unifying Different Theories of Conformal Prediction\\nAuthors: Rina Foygel Barber, Ryan J. Tibshirani\\nTopic: present unified framework\\nKeywords: present unified framework, randomly - localize conformal prediction, conformal prediction, nonexchangeable conformal prediction, unified framework understanding, prediction, include standard conformal, weight conformal, nonexchangeable conformal, prediction paper present, theory conformal prediction, prediction literature, conformal prediction paper, weight conformal prediction, paper present, conformal prediction literature, include standard conformal prediction, include standard, partial information, conformal, paper present unified, standard conformal prediction, present unified\\nIntro: \\nLemmatized Chunk: paper present unified framework understand methodology theory behind several different method conformal prediction literature include standard conformal prediction cp weight conformal prediction wcp nonexchangeable conformal prediction nexcp randomly localize conformal prediction rlcp among crux framework idea conformal method base reveal partial information datum hand positioning conditional distribution datum give partial information different method arise different choice partial information corresponding approximate conditional distribution addition recover unify exist result framework lead new theoretical guarantee exist method new extension conformal methodology',\n",
       " 'Title: Testing independence and conditional independence in high dimensions via\\n  coordinatewise Gaussianization\\nAuthors: Jinyuan Chang, Yue Du, Jing He, Qiwei Yao\\nTopic: conditional independence\\nKeywords: conditional independence, propose new statistical test, conditional independence high, test, independence conditional, coordinatewise gaussianization propose, transform random vector, random vector, vector, propose test, statistical test, dimension random vector, gaussianization propose, coordinatewise gaussianization, high - dimensional setting, independence two random, type test statistic, independence, propose new statistical, vector conditional, dimension via coordinatewise gaussianization, random vector diverge, high dimension via coordinatewise, independence conditional independence, conditional, random\\nIntro: \\nLemmatized Chunk: propose new statistical test high dimensional setting test independence two random vector conditional independence give third random vector key idea simple first transform component variable standard normal via marginal empirical distribution test independence conditional independence transform random vector use appropriate type test statistic test necessary condition independence conditional independence new test outperform frequently use testing method large scale simulation comparison advantage new test summarize follow require moment condition ii allow arbitrary dependence structure component among random vector iii allow dimension random vector diverge exponential rate sample size critical value propose test determine computationally efficient multiplier bootstrap procedure theoretical analysis show size propose test well control nominal significance level propose test also consistent certain local alternative finite sample performance new test illustrate via extensive simulation study real datum application',\n",
       " 'Title: On the Geometry of Receiver Operating Characteristic and\\n  Precision-Recall Curves\\nAuthors: Reza Sameni\\nTopic: geometry receiver operating\\nKeywords: geometry receiver operating, characteristic precision - recall curve, classifier, precision - recall curve study, receiver operating characteristic, binary classification, characteristic precision - recall, positive negative class, binary classification problem, cdot, class - conditional cumulative distribution function, geometry receiver, operating point selection, classification problem, precision - recall curve, classification metric, binary classification metric, commonly use binary classification, operate characteristic precision - recall, receiver operating, receiver, cumulative distribution function, curve reflect classifier behavior, class - conditional cumulative distribution, characteristic, curve binary classification, operate characteristic, precision - recall\\nIntro: \\nLemmatized Chunk: study geometry receiver operating characteristic roc precision recall pr curve binary classification problem key finding many commonly use binary classification metric merely function composition function circ class conditional cumulative distribution function classifier score positive negative class respectively geometric perspective facilitate selection operating point understand effect decision threshold comparison classifier also help explain shape geometry roc pr curve reflect classify behavior provide objective tool build classifier optimize specific application context specific constraint far explore condition classifier dominance present analytical numerical example demonstrate effect class separability variance roc pr geometry derive link positive negative class leakage function kullback leibler divergence framework highlight practical consideration model calibration cost sensitive optimization operating point selection real world capacity constraint enable inform approach classify deployment decision making',\n",
       " 'Title: Estimation of the complier causal hazard ratio under dependent censoring\\nAuthors: Gilles Crommen, Jad Beyhum, Ingrid Van Keilegom\\nTopic: censor duration outcome .\\nKeywords: censor duration outcome ., conditioning measure, statistically independent, dependent censoring problem, dependently censor, duration outcome . dependent, complier causal, endogenous binary, causal hazard ratio, outcome . dependent censoring, measure covariate, interested study, estimation procedure, binary treatment, dependently censor duration outcome ., endogenous binary treatment, binary instrumental variable, complier causal hazard, estimation complier, complier causal hazard ratio, dependently censor duration, causal hazard, treatment dependently, dependent censoring, study causal effect\\nIntro: \\nLemmatized Chunk: work interested study causal effect endogenous binary treatment dependently censor duration outcome dependent censoring mean duration time right censoring time statistically independent even conditioning measure covariate endogeneity issue handle make use binary instrumental variable treatment deal dependent censoring problem assume stratum complier follow semiparametric proportional hazard model ii follow fully parametric model iii relation model parametric copula association parameter leave unspecified framework treatment effect interest complier causal hazard ratio cchr devise estimation procedure base weight maximum likelihood approach weight probability observation come complier weight estimate first stage follow estimation cchr novel condition model identifiable give two step estimation procedure propose important asymptotic property establish simulation use assess validity finite sample performance estimation procedure finally apply approach estimate cchr job training program unemployment duration periodic screen examination time death breast cancer datum come national job training partnership act study health insurance plan great new york experiment respectively',\n",
       " 'Title: Cramér--Rao Inequalities for Several Generalized Fisher Information\\nAuthors: Hao Wu, Lei Yu\\nTopic: half derivative\\nKeywords: half derivative, identity state, information de bruijn, enyi, information, shannon differential entropy, derivative shannon differential, generalize fisher information, derivative shannon, bruijn identity, bruijn identity state, shannon differential, entropy along heat flow, fisher information, heat flow, identity state fisher, information along heat flow, entropy along heat, flow, differential entropy along heat, fisher, state fisher information\\nIntro: \\nLemmatized Chunk: de bruijn identity state fisher information half derivative shannon differential entropy along heat flow spirit paper introduce generalize version fisher information name fisher information half derivative information along heat flow base fisher information establish sharp isoperimetric inequality generalize classic entropic isoperimetric inequality setting utilize isoperimetric inequality extend classical rao inequality fisher information fisher information lastly use generalize rao inequality determine sign derivative entropy along heat flow strengthen exist result complete monotonicity entropy',\n",
       " 'Title: Estimating hazard rates from $δ$-records in discrete distributions\\nAuthors: Martín Alcalde, Miguel Lafuente, F. Javier López, Lina Maldonado, Gerardo Sanz\\nTopic: hazard rate function\\nKeywords: hazard rate function, paper focus nonparametric, record discrete distribution, determine exact distribution, estimate hazard, distribution paper focus, statistical inference, focus nonparametric statistical, likelihood estimator determine, discrete distribution base, paper focus, rate function discrete, discrete distribution paper, rate function base, nonparametric statistical, hazard rate, estimate hazard rate, function discrete distribution, focus nonparametric, derive explicit expression, discrete distribution, rate function, hazard rate function base, nonparametric statistical inference, discrete, maximum likelihood estimator\\nIntro: \\nLemmatized Chunk: paper focus nonparametric statistical inference hazard rate function discrete distribution base record datum derive explicit expression maximum likelihood estimator determine exact distribution well important characteristic bias mean square error discuss construction confidence interval goodness fit test performance proposal evaluate use simulation method application real datum give well estimation hazard rate function base usual record study literature although many procedure require several sample record contrast approach rely single sequence record simplify experimental design increase applicability method',\n",
       " 'Title: Proper scoring rules for estimation and forecast evaluation\\nAuthors: Kartik Waghmare, Johanna Ziegel\\nTopic: foundation proper scoring\\nKeywords: foundation proper scoring, interest recent, subject grow, method estimate probability, proper scoring, evaluation proper scoring rule, estimate probability distribution, scoring rule, scoring rule include, subject grow interest, method estimating, estimate probability, probability distribution, recent year, rule include general characterization, interest recent year, forecast evaluation proper scoring, score rule include general, proper scoring rule include, proper scoring rule, evaluation proper scoring, grow interest recent, grow interest\\nIntro: \\nLemmatized Chunk: proper scoring rule subject grow interest recent year tool evaluation probabilistic forecast also method estimate probability distribution article review mathematical foundation proper scoring rule include general characterization result important family scoring rule discuss role statistic machine learning estimation forecast evaluation furthermore comment interesting development usage application',\n",
       " 'Title: Asymptotic analysis of the finite predictor for the fractional Gaussian\\n  noise\\nAuthors: P. Chigansky, M. Kleptsyna\\nTopic: paper propose\\nKeywords: paper propose, finite predictor stationary, gaussian noise goal, propose new approach, exact asymptotics, gaussian noise, analysis, fractional gaussian, partial correlation coefficient, predictor, stationary sequence, fractional gaussian noise, correlation coefficient, finite, long range dependence, asymptotic analysis, sequence, finite predictor, approach asymptotic, approach asymptotic analysis, arima type process drive, relative prediction error, predictor stationary sequence, analysis finite, process long range, goal paper, predictor stationary, produce exact asymptotics\\nIntro: \\nLemmatized Chunk: goal paper propose new approach asymptotic analysis finite predictor stationary sequence produce exact asymptotics relative prediction error partial correlation coefficient assumption analytic nature applicable process long range dependence arima type process drive fractional gaussian noise fgn previously remain elusive serve study case',\n",
       " 'Title: On Robust Empirical Likelihood for Nonparametric Regression with\\n  Application to Regression Discontinuity Designs\\nAuthors: Qin Fang, Shaojun Guo, Yang Hong, Xinghao Qiao\\nTopic: regression discontinuity design empirical\\nKeywords: regression discontinuity design empirical, original empirical likelihood, original empirical likelihood framework, powerful tool, regression discontinuity, powerful tool construct, regression, discontinuity design empirical likelihood, regression discontinuity design, regression regression discontinuity, likelihood, robust empirical likelihood, design empirical likelihood, nonparametric regression, empirical likelihood, likelihood serve, discontinuity design empirical, construct confidence, discontinuity design, design empirical likelihood serve, empirical, empirical likelihood framework, tool construct confidence, design empirical, empirical likelihood serve, construct confidence interval\\nIntro: \\nLemmatized Chunk: empirical likelihood serve powerful tool construct confidence interval nonparametric regression regression discontinuity design rdd original empirical likelihood framework naturally extend setting use local linear smoother wilks theorem hold undersmoothed bandwidth select however generalization bias correct version empirical likelihood realistic condition remain open challenge literature paper provide satisfactory solution propose novel approach refer robust empirical likelihood design nonparametric regression rdd core idea construct robust weight simultaneously achieve bias correction account additional variability introduce estimate bias thereby enable valid confidence interval construction without extra estimation step involve demonstrate wilk phenomenon still hold weak condition nonparametric regression sharp fuzzy rdd setting extensive simulation study confirm effectiveness propose approach show superior performance exist method term coverage probability interval length moreover propose procedure exhibit robustness bandwidth selection make flexible reliable tool empirical analysis practical usefulness far illustrate application two real dataset',\n",
       " 'Title: Tail Bounds for Canonical $U$-Statistics and $U$-Processes with\\n  Unbounded Kernels\\nAuthors: Abhishek Chakrabortty, Arun K. Kuchibhotla\\nTopic: tail assumption\\nKeywords: tail assumption, paper, sub -optimal tail behavior, prove exponential, canonical, obtain sub-optimal tail behavior, tail behavior, tail bound, process unbound kernel, process, tail bound canonical, exponential tail, prove exponential tail bound, exponential - type tail, unbound kernel, process exponential - type tail, exponential tail bound, bound, kernel, exponential - type tail assumption, tail, bound canonical, prove exponential tail, degenerate\\nIntro: \\nLemmatized Chunk: paper prove exponential tail bound canonical degenerate statistic process exponential type tail assumption kernel exist result relevant literature often assume bound kernel obtain tail behavior unbound kernel obtain sharp rate optimal tail behavior sub kernel function example nonparametric semiparametric statistic literature consider',\n",
       " 'Title: On spectral gap decomposition for Markov chains\\nAuthors: Qian Qin\\nTopic: idealize simplify\\nKeywords: idealize simplify, simplify version, gap, mathrm, markov chain, convergence analysis markov, work regard convergence analysis, markov chain multiple work, markov transition kernel, multiple work regard convergence, transition kernel, mtk characterize difference, gap reversible, collection mtk characterize, idealize simplify version, work regard convergence, mtk characterize, chain multiple work, spectral gap decomposition, gap decomposition, bar, reversible markov operator, convergence analysis, spectral gap\\nIntro: \\nLemmatized Chunk: multiple work regard convergence analysis markov chain lead spectral gap decomposition formula form mathrm geq leave mathrm gap mathrm constant denote right spectral gap reversible markov operator markov transition kernel mtk interest idealize simplify version collection mtk characterize difference type relationship establish various context include decomposition markov chain base finite cover state space hybrid gibb sampler spectral independence localization scheme show multiple key decomposition result across domain connect within unified framework root simple sandwich structure within general framework establish new instance spectral gap decomposition hybrid hit run sampler hybrid datum augmentation algorithm two intractable conditional distribution additionally explore several property sandwich structure derive extension spectral gap decomposition formula',\n",
       " 'Title: Confidence Bands for Multiparameter Persistence Landscapes\\nAuthors: Inés García-Redondo, Anthea Monod, Qiquan Wang\\nTopic: tool datum analysis\\nKeywords: tool datum analysis, persistence landscape, generalization classical persistent, central widely - use methodology, datum analysis, analysis presence, effective tool, density estimation, topological datum analysis, widely - use methodology topological, multiparameter persistent homology, persistent homology, multiparameter persistence, methodology topological, account density, multiparameter persistence landscape multiparameter, persistence landscape multiparameter persistent, methodology topological datum, landscape multiparameter persistent homology, widely - use methodology, account density estimation, classical persistent homology, presence noise, multiparameter persistence landscape\\nIntro: \\nLemmatized Chunk: multiparameter persistent homology generalization classical persistent homology central widely use methodology topological datum analysis take account density estimation effective tool datum analysis presence noise similar classical single parameter counterpart however challenging compute use practice due complex algebraic construction paper study popular tractable invariant multiparameter persistent homology statistical setting multiparameter persistence landscape derive functional central limit theorem multiparameter persistence landscape compute confidence band give rise one first statistical inference methodology multiparameter persistence landscape provide implementation confidence band demonstrate application machine learning task synthetic datum',\n",
       " 'Title: Causal Models for Growing Networks\\nAuthors: Gecia Bravo-Hermsdorff, Lee M. Gunderson, Kayvan Sadeghi\\nTopic: network real - world network\\nKeywords: network real - world network, model, network grow time, grow network real - world, statistical model base, base node exchangeability, network grow, real - world network, grow network real - world network, node exchangeability, network real - world, ancestral set, real - world, causal, real - world network grow, network real - world network grow, model base, relevant symmetry refer, node, time, textit, grow, grow time, statistical, statistical model\\nIntro: \\nLemmatized Chunk: real world network grow time statistical model base node exchangeability appropriate instead constrain structure edge propose relevant symmetry refer structure first enumerate causal direct acyclic graph dag model pair node dyad variable grow network finite ancestral set invariant node deletion partition class ancestral set close node marginalization several class remarkably amenable distribute asynchronous evaluation example highlight simple model exhibit flexible power law degree distribution emergent phase transition sparsity characterize analytically parameter much conditional independence propose framework provide natural baseline model causal inference relational datum',\n",
       " 'Title: Nonparametric spectral density estimation using interactive mechanisms\\n  under local differential privacy\\nAuthors: Cristina Butucea, Karolina Klockmann, Tatyana Krivobokova\\nTopic: stationary gaussian time series\\nKeywords: stationary gaussian time series, nonparametric estimation, spectral density, differential privacy address, center stationary gaussian, spectral density function, local differential privacy, center stationary, density center, problem nonparametric estimation, nonparametric spectral density estimation, address problem, nonparametric spectral density, stationary gaussian time, spectral density estimation, entire spectral density function, entire spectral density, local differential, series local differential, mechanism local differential, differential privacy constraint, center stationary gaussian time, problem nonparametric, gaussian time series, differential privacy, estimation spectral, local differential privacy constraint\\nIntro: \\nLemmatized Chunk: address problem nonparametric estimation spectral density center stationary gaussian time series local differential privacy constraint specifically propose new interactive privacy mechanism three task estimate single covariance coefficient estimate spectral density fix frequency estimate entire spectral density function approach achieve fast rate two stage process apply first laplace mechanism truncate value use former privatize sample gain knowledge dependence mechanism time series spectral density belong old sobolev smoothness class demonstrate estimator improve upon mechanism kroll small privacy parameter since pointwise rate depend instead moreover show rate optimal estimate covariance coefficient mechanism however rate interactive estimator slow pointwise rate show use estimator provide bona fide locally differentially private covariance matrix estimator',\n",
       " 'Title: Power comparison of sequential testing by betting procedures\\nAuthors: Amaury Durand, Olivier Wintenberger\\nTopic: sequential testing\\nKeywords: sequential testing, paper, sequential test bound, finite time, testing, procedure, power, power guarantee, sequential test, sequential testing betting, testing procedure, general alternative, betting procedure, power comparison sequential, derive, bound, guarantee, alternative, reject finite time, general, sequential, derive power guarantee, testing betting procedure, comparison sequential testing, keep power guarantee, derive power\\nIntro: \\nLemmatized Chunk: paper derive power guarantee sequential test bound mean general alternative focus testing procedure use nonnegative supermartingale anytime valid consider alternative coincide asymptotically null vanish mean still allow reject finite time introduce variance constraint show alternative broaden keep power guarantee certain second order testing procedure also compare different test procedure multidimensional setting use characteristic rejection time finally extend analysis functional well testing compare forecaster result illustrate numerical simulation include bound mean testing comparison forecaster',\n",
       " 'Title: Graphical Models and Efficient Inference Methods for Multivariate Phase\\n  Probability Distributions\\nAuthors: Andrew S. Perley, Todd P. Coleman\\nTopic: numerous physical\\nKeywords: numerous physical, multivariate phase, multivariate phase probability distribution, chemical system, understand numerous, distribution multivariate phase, multivariate, system exhibit complex spatiotemporal, characterize understand numerous, electromagnetic wave neural, phase probability, neural oscillation, characterize multivariate phase relationship, phase probability distribution multivariate, distribution multivariate phase relationship, phase probability distribution, multivariate phase relationship, wave neural oscillation, important characterize, probability distribution multivariate phase, understand numerous physical, characterize multivariate phase, system, multivariate phase probability, phase relationship, relationship, phase relationship important, phase\\nIntro: \\nLemmatized Chunk: multivariate phase relationship important characterize understand numerous physical biological chemical system electromagnetic wave neural oscillation system exhibit complex spatiotemporal dynamic intricate interdependencyncie among constituent element classical model multivariate phase relationship wave equation kuramoto model give theoretical model describe phenomenon development statistical tool hypothesis testing inference multivariate phase relationship complex system remain limit paper introduce novel probabilistic modeling framework characterize multivariate phase relationship wave like phenomenon serve key example approach describe spatial pattern interaction oscillator pairwise exponential family distribution build upon literature graphical model inference include method like ising model graphical lasso interaction screen work bridge gap classical wave dynamic modern statistical approach efficient inference method introduce leverage chow liu algorithm direct tree approximation interaction screen general graphical model simulate experiment demonstrate utility method uncover wave property sparse interaction structure highlight applicability diverse scientific domain framework establish new paradigm statistical modeling multivariate phase relationship provide powerful toolset explore complexity system',\n",
       " 'Title: Non-parametric cure models through extreme-value tail estimation\\nAuthors: Jan Beirlant, Martin Bladt, Ingrid Van Keilegom\\nTopic: extreme - value tail\\nKeywords: extreme - value tail, survival analysis, receive considerable attention, susceptible population, attention recently, cure, non-parametric cure model, extreme, experience event, rate extreme, cure rate, extreme - value tail estimation, tail estimation survival, receive considerable, estimation survival analysis, receive considerable attention recently, considerable attention recently, cure model extreme - value, considerable attention, estimate cure rate, estimation, term cure, rate estimation, term cure rate, event interest, cure rate estimation\\nIntro: \\nLemmatized Chunk: survival analysis estimation proportion subject never experience event interest term cure rate receive considerable attention recently estimation particularly difficult task follow sufficient censoring mechanism small support distribution target datum latter case estimator recently propose use extreme value methodology assume distribution susceptible population gumbel max domain attraction paper take extreme value technique one step far jointly estimate cure rate extreme value index use probability plotting methodology particular use full information contain top order statistic word sufficient insufficient follow reconstruct immune proportion end peak threshold approach propose gumbel max domain assumption next approach also transfer specific model pareto log normal weibull tail model allow recognize important tail characteristic susceptible population establish asymptotic behavior estimator regularization simulation study estimator show rival often outperform establish model even purely consider cure rate estimation finally provide application method norwegian birth registry datum',\n",
       " 'Title: Non-Asymptotic Analysis of Classical Spectrum Estimators for $L$-mixing\\n  Time-series Data with Unknown Means\\nAuthors: Yuping Zheng, Andrew Lamperski\\nTopic: series analysis\\nKeywords: series analysis, important tool, error bound, analysis classical spectrum, mix time - series datum, unknown mean spectral estimation, mixing, non-parametric estimation well - know, classical spectrum estimator, non-asymptotic, time series, mix time - series, model time series, theory non-parametric estimation, tool time series, time - series datum, classical spectrum, non-parametric estimation, time series analysis, application include, spectral estimation, application include economics, important tool time, estimation, tool time, include economics, non-asymptotic error bound, non-asymptotic error\\nIntro: \\nLemmatized Chunk: spectral estimation important tool time series analysis application include economics astronomy climatology asymptotic theory estimation well know development theory still ongoing recent work obtain first error bound bartlett welch method mix stochastic process class mixing process contain common model time series analysis include autoregressive process measurement geometrically ergodic markov chain prior analysis assume process zero mean zero mean assumption common real world time series datum often unknown mean work derive error bound bartlett welch estimator mixing time series datum unknown means obtain error bound number datum segment use algorithm tight previous result zero mean assumption',\n",
       " \"Title: Estimating a graph's spectrum via random Kirchhoff forests\\nAuthors: Simon Barthelmé, Fabienne Castell, Alexandre Gaudillière, Clothilde Melot, Matteo Quattropani, Nicolas Tremblay\\nTopic: forest exact\\nKeywords: forest exact, estimate graph spectrum, spectral, exact, exact eigendecomposition, spectral estimation typically work, exact eigenvalue, impossible compute exact, compute exact, spectral density, compute exact eigenvalue, random kirchhoff forest exact, modest goal approach, eigendecomposition large matrice, moment, practically impossible, kirchhoff forest exact eigendecomposition, exact eigendecomposition large, large matrice, approach empirical distribution, impossible compute, estimate, kirchhoff forest exact, kirchhoff forest, practically impossible compute, eigenvalue, forest exact eigendecomposition\\nIntro: \\nLemmatized Chunk: exact eigendecomposition large matrice expensive practically impossible compute exact eigenvalue instead one may set modest goal approach empirical distribution eigenvalue recover overall shape eigenspectrum current approach spectral estimation typically work spectral distribution moment first estimate use monte carlo trace estimator estimate combine approximate spectral density article show random forest graph use estimate certain moment large graph laplacian show combine moment estimate spectral density estimate desire precision high approach pave way estimation graph spectrum time sublinear number link\",\n",
       " 'Title: Optimal low-rank approximations for linear Gaussian inverse problems on\\n  Hilbert spaces, Part II: posterior mean approximation\\nAuthors: Giuseppe Carere, Han Cheng Lie\\nTopic: distribution linear\\nKeywords: distribution linear, gaussian, separable hilbert space, gaussian posterior, linear gaussian inverse, joint approximation, optimal low - rank approximation, gaussian posterior distribution, posterior, posterior distribution linear, approximation, gaussian inverse problem, optimal, inverse problem, construct optimal low - rank approximation, posterior distribution, construct optimal low - rank, gaussian inverse, problem hilbert space, linear gaussian inverse problem, inverse problem hilbert, distribution linear gaussian, approximation gaussian, linear gaussian\\nIntro: \\nLemmatized Chunk: work construct optimal low rank approximation gaussian posterior distribution linear gaussian inverse problem parameter space separable hilbert space possibly infinite dimension datum space assume finite dimensional consider various type approximation family posterior first consider approximate posterior means vary among class either structure preserve structure ignore low rank transformation datum posterior covariance keep fix give necessary sufficient condition approximate posterior equivalent exact posterior possible realisation datum simultaneously approximation measure approximation error kullback leibler amari divergence hellinger distance average datum distribution loss find optimal approximation formulate equivalent condition uniqueness extend work finite dimension spantini et siam consider joint approximation mean covariance also vary posterior covariance low rank update consider part work reverse kullback leibler divergence show separate optimal approximation mean covariance combine yield optimal joint approximation mean covariance addition interpret joint approximation optimal structure ignore approximate mean term optimal projector parameter space',\n",
       " 'Title: Asymptotically distribution-free goodness-of-fit testing for point\\n  processes\\nAuthors: Justin Baars, Sami Umut Can, Roger J. A. Laeven\\nTopic: temporal point process\\nKeywords: temporal point process, multivariate temporal point, mathcal, multivariate temporal, point process, convergent compensate counting process, law, construct convergent compensate, interval, convergent compensate counting, multivariate, point, process, temporal, asymptotically distribution - free, temporal point, time interval, innovation martingale transformation, time, observation multivariate, multivariate temporal point process, distribution - free, apply innovation martingale, observation\\nIntro: \\nLemmatized Chunk: consider observation multivariate temporal point process law time interval test null hypothesis belong give parametric family construct convergent compensate counting process apply innovation martingale transformation prove result process converge weakly standard wiener process consequently take suitable functional process yield asymptotically distribution free goodness fit test point process several standard test base increment transform process establish consistency alternative hypothese finally assess performance propose testing procedure monte carlo simulation study illustrate practical utility two real datum example',\n",
       " 'Title: Smooth and rough paths in mean derivative estimation for functional data\\nAuthors: Max Berger, Hajo Holzmann\\nTopic: fix synchronous\\nKeywords: fix synchronous, partial derivative, minimax sense, derivative estimation functional, derive near optimal, minimax sense estimating, convergence minimax, sense estimate partial, old smoothness class, functional datum, derive near optimal rate, sense estimating, optimal rate convergence, estimation functional datum, datum observe, multivariate setting derive, estimate partial derivative, datum paper, optimal rate, fix synchronous design, set derive, functional datum observe, function functional datum, path mean derivative\\nIntro: \\nLemmatized Chunk: paper multivariate setting derive near optimal rate convergence minimax sense estimate partial derivative mean function functional datum observe fix synchronous design old smoothness class focus supremum norm since correspond visualisation estimation error closely related construction uniform confidence band contrast mean function estimation derivative estimation smoothness path process crucial rate convergence one hand path high order smoothness order partial derivative estimate parametric rate achieve sufficiently dense design hand process rough path low order smoothness show rate convergence necessarily slow parametric rate determine near optimal rate estimation still possible implement multivariate local polynomial derivative estimator illustrate finite sample performance simulation well two real datum set assess smoothness sample path application far discuss method base compare restrict estimate partial derivative covariance kernel',\n",
       " 'Title: Wasserstein KL-divergence for Gaussian distributions\\nAuthors: Adwait Datar, Nihat Ay\\nTopic: sample space\\nKeywords: sample space, kl - divergence, gaussian distribution introduce, wasserstein kl - divergence gaussian, geometry, distribution, version, gaussian, kl - divergence gaussian distribution, base wasserstein, geometry refer, base wasserstein geometry, base, version kl - divergence, refer, gaussian distribution, introduce new version, wasserstein geometry, wasserstein geometry refer, introduce, wasserstein kl - divergence, distribution introduce, wasserstein, kl - divergence gaussian\\nIntro: \\nLemmatized Chunk: introduce new version kl divergence gaussian distribution base wasserstein geometry refer wkl divergence show version consistent geometry sample space particular evaluate wkl divergence dirac measure concentrate two point turn proportional square distance point',\n",
       " 'Title: Optimal low-rank approximations for linear Gaussian inverse problems on\\n  Hilbert spaces, Part I: posterior covariance approximation\\nAuthors: Giuseppe Carere, Han Cheng Lie\\nTopic: conditioning formula\\nKeywords: conditioning formula, posterior covariance approximation, gaussian, covariance approximation linear, approximation yield approximate posterior, low - rank approximation linear, low - rank approximation, linear gaussian inverse, prior gaussian observation, gaussian prior, gaussian prior gaussian, posterior, covariance determine, approximation, approximation linear gaussian, covariance approximation, low- rank, gaussian inverse problem, linear gaussian, determine conditioning, gaussian observation, low - rank covariance approximation, low - rank covariance approximation yield, low - rank covariance, observation noise, linear gaussian inverse problem, gaussian observation noise, linear inverse problem\\nIntro: \\nLemmatized Chunk: linear inverse problem gaussian prior gaussian observation noise posterior gaussian mean covariance determine conditioning formula use feldman hajek theorem analyse prior posterior update low rank approximation infinite dimensional hilbert parameter space finite dimensional observation show posterior distribution differ prior finite dimensional subspace construct low rank approximation posterior covariance keep mean fix since infinite dimension low rank covariance approximation yield approximate posterior distribution equivalent posterior prior distribution characterise low rank covariance approximation yield equivalence respective inverse precision approximation family measure approximation problem solve identify low rank approximation optimal various loss simultaneously loss function include family divergence amari divergence hellinger metric kullback leibler divergence result extend spantini et siam hilbertian parameter space provide theoretical underpin construction low rank approximation discretise version infinite dimensional inverse problem formulate discretization independent result',\n",
       " 'Title: Multivariate Species Sampling Models\\nAuthors: Beatrice Franzolini, Antonio Lijoi, Igor Prünster, Giovanni Rebaudo\\nTopic: model\\nKeywords: model, multivariate species sampling model, regular multivariate species, study random, multivariate species, sampling model species sampling, study random discrete distribution, species, long serve, framework study random, random discrete, species sampling model species, regular multivariate species sampling, species sampling model, species sampling, multivariate species sampling, sampling, random discrete distribution, sampling model, process long serve, discrete distribution, study random discrete\\nIntro: \\nLemmatized Chunk: species sampling process long serve framework study random discrete distribution however statistical applicability limit partial exchangeability assume probabilistic invariance observable despite numerous discrete model partially exchangeable observation unify framework currently miss leave many question induce learning mechanism unanswered setting fill gap consider natural extension species sample model multivariate framework obtain general class model characterize partially exchangeable partition probability function notable subclass name regular multivariate species sampling model exist among model subclass dependence across process accurately capture correlation among correlation one equal full exchangeability null correlation correspond independence regular multivariate species sampling model encompass discrete process partial exchangeable datum use bayesian model thereby highlight core distributional property provide means develop new model',\n",
       " 'Title: Operator limit of Wigner matrices I\\nAuthors: Debapratim Banerjee\\nTopic: suitable hilbert\\nKeywords: suitable hilbert, limit wigner matrice, dimension, construct operator, mathcal, suitable notion, convergence, suitable notion convergence, define suitable, measure, infty, wigner, define suitable notion, operator limit wigner, wigner matrice, suitable, operator limit, notion convergence, matrice, notion, suitable hilbert space, matrix, limit wigner, limit, operator, time, wigner matrix, hilbert space\\nIntro: \\nLemmatized Chunk: consider wigner matrix dimension objective paper two fold first construct operator suitable hilbert space define suitable notion convergence matrice converge notion convergence far investigate property show nontrivial extension respect lebesgue measure spectral measure function almost surely semicircular law',\n",
       " 'Title: Distributional regression with reject option\\nAuthors: Ahmed Zaoui, Clément Dombry\\nTopic: mistake costly\\nKeywords: mistake costly, make decision, distributional regression reject, reject option selective, option selective prediction, regression reject option, reject option, rate, selective prediction, crucial machine learning, machine learning, rejection, learning application, focus distributional regression, machine learning application, option selective, model option, crucial machine, regression reject, reject option selective prediction, distributional regression, option, abstain make, optimal rule, rejection rate\\nIntro: \\nLemmatized Chunk: selective prediction model option abstain make decision crucial machine learning application mistake costly work focus distributional regression introduce framework enable model abstain estimation situation high uncertainty refer approach distributional regression reject option inspire similar concept classification regression reject option study scenario rejection rate fix derive close form expression optimal rule rely threshold entropy function continuous rank probability score crp propose semi supervise estimation procedure optimal rule use two dataset first label use estimate conditional distribution function entropy function crp second unlabel employ calibrate desire rejection rate notably control rejection rate distribution free mild condition show procedure asymptotically effective optimal rule term error rate rejection rate additionally establish rate convergence approach base distributional neighbor numerical analysis real world dataset demonstrate strong performance procedure',\n",
       " 'Title: Finite sample valid confidence sets of mode\\nAuthors: Manit Paul, Arun Kumar Kuchibhotla\\nTopic: confidence set\\nKeywords: confidence set, sample valid confidence, unimodal distribution, classical problem statistic, valid confidence, problem statistic, estimate mode, sample valid, finite sample valid confidence, set, finite sample, propose confidence set, mode estimate mode, sample valid confidence set, obtain finite sample valid, mode estimating, build finite sample valid, classical problem, confidence set shrink, confidence, valid confidence set, mode, finite sample valid\\nIntro: \\nLemmatized Chunk: estimate mode unimodal distribution classical problem statistic although several approach point estimation mode literature little explore interval estimation mode work propose collection novel method obtain finite sample valid confidence set mode unimodal distribution analyze behaviour width propose confidence set regularity assumption density mode show width confidence set shrink zero near optimally simply put show possible build finite sample valid confidence set mode shrink singleton sample size increase support theoretical result show performance propose method synthetic datum set believe confidence set improve construction term rate',\n",
       " 'Title: On Finite Time Span Estimators of Parameters for Ornstein-Uhlenbeck\\n  Processes\\nAuthors: Jun S. Han, Nino Kordzakhia\\nTopic: mle mean - reverting\\nKeywords: mle mean - reverting, likelihood ratio process, maximum likelihood, finite time span, finite time, error maximum, two - parameter mean - reverting, bias mse, general ornstein - uhlenbeck process, time span, span estimator, time span estimator, two - parameter mean - reverting process, process, ornstein - uhlenbeck process study, likelihood estimator, mean - reverting speed parameter, process finite, span estimator parameter, mse mle, mean - square error, finite time span estimator, measure ito formula, mean - reverting process, parameter ornstein - uhlenbeck process, likelihood ratio, bias, maximum likelihood estimator\\nIntro: \\nLemmatized Chunk: study bias mean square error maximum likelihood estimator mle parameter associate two parameter mean reverting process finite time use likelihood ratio process derive expression mle compute bias mse via change measure ito formula apply derive expression general ornstein uhlenbeck process bias mse numerically compute joint moment generate function key functional process numerical study provide illustrate behaviour bias mse mle mean revert speed parameter',\n",
       " 'Title: Bayesian Inference for High-dimensional Time Series with a Directed\\n  Acyclic Graphical Structure\\nAuthors: Arkaprava Roy, Anindya Roy, Subhashis Ghosal\\nTopic: underlying causal relationship\\nKeywords: underlying causal relationship, understand underlying, high - dimensional time series, represent causal dependency, direct acyclic graph, framework represent causal, time series, causal, understand underlying causal, acyclic graphical structure, time series analysis, model multivariate time series, relationship among variable, direct acyclic graphical structure, acyclic graphical, time, multivariate time series analysis, high - dimensional time, causal relationship among variable, model multivariate time, multivariate time series, direct acyclic, causal relationship, underlying causal, multivariate time, matrix - variate time series, direct acyclic graphical\\nIntro: \\nLemmatized Chunk: multivariate time series analysis understand underlying causal relationship among variable often interest various application direct acyclic graph dag provide powerful framework represent causal dependency paper propose novel bayesian approach model multivariate time series conditional independency causal structure encode dag propose model allow structural property stationarity easily accommodate give application far extend model matrix variate time series take bayesian approach inference projection posterior base efficient computational algorithm develop posterior convergence property propose method establish along two identifiability result unrestricted structural equation model utility propose method demonstrate simulation study real datum analysis',\n",
       " 'Title: Modeling Maximum drawdown Records with Piecewise Deterministic Markov\\n  Processe in Capital Markets\\nAuthors: Rolando Rubilar-Torrealba, Lisandro Fermin, Soledad Torres\\nTopic: record maximum\\nKeywords: record maximum, maximum drawdown capital, pdmp, piecewise deterministic markov, model maximum drawdown record, deterministic markov process, capital market propose, model maximum, deterministic markov, maximum drawdown, drawdown record piecewise, piecewise deterministic markov process, model record, sequence maximum drawdown, record piecewise deterministic, processe capital market, derive statistical result, govern stochastic process, piecewise deterministic markov processe, propose model, piecewise deterministic, capital market, markov processe capital, model maximum drawdown, capital market illustrate, drawdown capital market, drawdown record, markov process, deterministic markov processe, markov processe, maximum drawdown record\\nIntro: \\nLemmatized Chunk: propose model record maximum drawdown capital market mean piecewise deterministic markov process pdmp derive statistical result mean variance describe sequence maximum drawdown record addition develop simulation study technique estimate parameter govern stochastic process use practical example capital market illustrate procedure',\n",
       " 'Title: Optimal Change Point Detection and Inference in the Spectral Density of\\n  General Time Series Models\\nAuthors: Sepideh Mosaferi, Abolfazl Safikhani, Peiliang Bai\\nTopic: series model paper\\nKeywords: series model paper, density general time, seizure patient, represent general time series, spectral density, general time series, analysis seizure patient, spectral density general, detect change point, detect change, density time series, time series, problem detecting, model paper, eeg analysis seizure, paper address, address problem, coherence functional connectivity, eeg analysis, time series model, model paper address, seizure disrupt coherence, spectral density time, time, general time series model, problem detect change, paper address problem, motivate eeg analysis\\nIntro: \\nLemmatized Chunk: paper address problem detect change point spectral density time series motivate eeg analysis seizure patient seizure disrupt coherence functional connectivity necessitate precise detection depart traditional parametric approach utilize wold decomposition represent general time series autoregressive process infinite lag truncate estimate around change point detection procedure employ initial estimator systematically search across time point examine localization error dependence time series property sample size enhance accuracy introduce optimal rate method asymptotic distribution facilitate construction confidence interval propose method effectively identify seizure onset eeg datum extend event detection video datum comprehensive numerical experiment demonstrate superior performance compare exist technique',\n",
       " 'Title: Tracy-Widom, Gaussian, and Bootstrap: Approximations for Leading\\n  Eigenvalues in High-Dimensional PCA\\nAuthors: Nina Dörnemann, Miles E. Lopes\\nTopic: covariance matrix\\nKeywords: covariance matrix, diverge proportionally, undergoe well - know phase, pca certain condition, lead sample eigenvalue, subcritical regime, sample covariance, sample covariance matrix undergoe, large eigenvalue, eigenvalue high - dimensional pca, matrix undergoe well - know, undergoe well - know, regime, approximation lead eigenvalue, datum dimension, high - dimensional pca, fluctuation order, well - know phase, sample size, matrix undergoe, phase transition, sample covariance matrix, well - know phase transition, covariance matrix undergoe\\nIntro: \\nLemmatized Chunk: certain condition large eigenvalue sample covariance matrix undergoe well know phase transition sample size datum dimension diverge proportionally subcritical regime eigenvalue fluctuation order approximate tracy widom distribution supercritical regime fluctuation order approximate gaussian distribution however statistical problem determine regime underly give dataset far resolve develop new testing framework procedure address problem particular demonstrate procedure asymptotically control level power consistent certain alternative also testing procedure enable design new bootstrap method approximate distribution functional lead sample eigenvalue within subcritical regime first method support theoretical guarantee',\n",
       " 'Title: Optimal treatment regimes for the net benefit of a treatment\\nAuthors: François Petit, Gérard Biau, Raphaël Porcher\\nTopic: treatment regime\\nKeywords: treatment regime, individualized treatment rule, pairwise comparison define, optimal treatment regime, inspire buyse, buyse generalize pairwise, setup inspire, buyse generalize pairwise comparison, optimal individualized treatment rule, notion optimal individualized, generalize pairwise comparison, pairwise optimal itr, develop mathematical setup, setup inspire buyse, itr pairwise optimal, mathematical setup inspire, mathematical setup, inspire buyse generalize, individualized treatment, develop mathematical, benefit treatment, treatment rule, estimate pairwise optimal itr, presence prioritize outcome\\nIntro: \\nLemmatized Chunk: develop mathematical setup inspire buyse generalize pairwise comparison define notion optimal individualized treatment rule itr presence prioritize outcome randomize control trial term itr pairwise optimal present two approach estimate pairwise optimal itr first variant near neighbor algorithm second meta learner base randomize bagging scheme allow use classification algorithm construct itr study behavior estimation scheme theoretical standpoint monte carlo simulation illustrate use trial datum',\n",
       " 'Title: Inference on effect size after multiple hypothesis testing\\nAuthors: Andreas Dzemski, Ryo Okui, Wenjie Wang\\nTopic: treatment effect\\nKeywords: treatment effect, emphasize interpret, test significant treatment, test significant treatment effect, hypothesis testing, significant effect, effect size, interpret summarizing, confidence interval, effect, treatment, hypothesis test significant treatment, multiple hypothesis testing significant, finding study, study estimate multiple, significant, treatment effect estimate, multiple hypothesis, significant treatment, empirical finding study, conventional treatment effect, summarize empirical finding, multiple hypothesis testing, interpret summarize empirical, significant treatment effect, confidence, conventional treatment effect estimate\\nIntro: \\nLemmatized Chunk: significant treatment effect often emphasize interpret summarize empirical finding study estimate multiple possibly many treatment effect kind selective reporting conventional treatment effect estimate may biased corresponding confidence interval may undercover true effect size propose new estimator confidence interval provide valid inference effect size significant effect multiple hypothesis testing method base principle selective conditional inference complement wide range test include step test bootstrap base step test approach scalable allow study application estimate effect justify procedure asymptotically normal treatment effect estimator provide two empirical example demonstrate bias correction confidence interval adjustment significant effect magnitude direction bias correction depend correlation structure estimate effect whether interpretation significant effect depend significance effect',\n",
       " 'Title: Conditional Extreme Value Estimation for Dependent Time Series\\nAuthors: Martin Bladt, Laurits Glargaard, Theodor Henningsen\\nTopic: conditional tail\\nKeywords: conditional tail, tail function, broad dependence assumption, time series, time series study, estimation dependent, estimator broad, dependent time series, sequence, conditional tail function, covariate sequence, estimator broad dependence, hill estimator broad, response sequence, sequence covariate, function conditional hill, estimation dependent time, heavy - tail response sequence, weak convergence, consistency weak convergence, conditional hill estimator, dependent time, series study, hill estimator, conditional hill, heavy - tail response, broad dependence\\nIntro: \\nLemmatized Chunk: study consistency weak convergence conditional tail function conditional hill estimator broad dependence assumption heavy tail response sequence covariate sequence consistency establish mixing asymptotic normality follow mixing second order condition key aspect approach versatile functional formulation term conditional tail process simulation demonstrate performance across dependence scenario apply method extreme event modeling oil industry reveal distinct tail behavior vary conditioning value',\n",
       " 'Title: Asymptotic Behavior of Principal Component Projections for Multivariate\\n  Extremes\\nAuthors: Holger Drees\\nTopic: dimensional subspace obtain\\nKeywords: dimensional subspace obtain, dependence structure, principal component, random vector, random vector large, multivariate extreme, measure, measure empirical, dimensional random, vector large norm, dimensional subspace, low dimensional, regularly vary, extreme extremal dependence, extremal dependence structure, extremal dependence, low dimensional subspace, estimator measure, principal component projection, multivariate extreme extremal, standard nonparametric estimator, dimensional random vector, low dimensional subspace obtain, angular measure, structure regularly\\nIntro: \\nLemmatized Chunk: extremal dependence structure regularly vary dimensional random vector describe angular measure standard nonparametric estimator measure empirical measure observe angle random vector large norm suitably choose number due curse dimensionality moderate large estimator often inaccurate angular measure concentrate vicinity low dimensional subspace first project datum low dimensional subspace obtain principal component analysis angle extreme observation substantially improve performance estimator derive asymptotic behavior pca projection result excess risk particular show mild condition excess risk function decrease much fast suggest empirical risk bound obtain cite moreover functional limit theorem local empirical process empirical reconstruction error projection uniformly neighborhood true optimal projection establish base asymptotic result propose datum drive method select dimension projection space finally finite sample performance result estimator examine simulation study',\n",
       " 'Title: An Improved Satterthwaite Effective Degrees of Freedom Correction for\\n  Weighted Syntheses of Variance\\nAuthors: Matthias von Davier\\nTopic: method estimate\\nKeywords: method estimate, composite variance estimator, effective degree, article present improve, scal chisquare distribution, standard satterthwaite approximation assume, assume scal chisquare, degree freedom, improve satterthwaite, article present, standard satterthwaite, degree freedom correction, component degree freedom, improve satterthwaite effective degree, satterthwaite effective, satterthwaite effective degree, present improve approximation, effective degree freedom, biased downward component, distribution weight, standard satterthwaite approximation, chisquare distribution, improve satterthwaite effective, approximation assume scal, freedom satterthwaite, component standard satterthwaite, satterthwaite approximation assume\\nIntro: \\nLemmatized Chunk: article present improve approximation effective degree freedom satterthwaite method estimate distribution weight combination variance component standard satterthwaite approximation assume scal chisquare distribution composite variance estimator know biased downward component degree freedom small build recent work von davier propose adjust estimator correct bias modify numerator denominator traditional formula new approximation incorporate weight average component degree freedom scaling factor ensure consistency number component degree freedom increase demonstrate utility adjustment practical setting include rubin total variance estimation multiple imputation weight variance combination common propose estimator generalize von davier unweight case accurately approximate synthetic variance estimator arbitrary weight',\n",
       " 'Title: Rolled Gaussian process models for curves on manifolds\\nAuthors: Simon Preston, Karthik Bharath, Pablo Lopez-Custodio, Alfred Kume\\nTopic: slip twist\\nKeywords: slip twist, trace curve, gaussian, roll gaussian process model, roll, induce local isometry, twist, operation induce local, euclidean gaussian process, roll gaussian, process, roll gaussian process, sphere, curve, planar curve, imagine roll, curve uniquely, curve uniquely determine, gaussian process model, imagine roll sphere, planar, gaussian process, roll operation, roll operation induce, imagine\\nIntro: \\nLemmatized Chunk: give planar curve imagine roll sphere along curve without slip twist mean trace curve sphere well know roll operation induce local isometry sphere plane two curve uniquely determine moreover operation extend general class manifold dimension use roll construct analogue gaussian process manifold start euclidean gaussian process result model generative amenable statistical inference give datum curve manifold illustrate example unit sphere symmetric positive definite matrice robotics application involve orientation',\n",
       " 'Title: Wasserstein bounds for non-linear Gaussian filters\\nAuthors: Toni Karvonen, Simo Särkkä\\nTopic: gaussian filter\\nKeywords: gaussian filter, non-linear, filter kalman filter, non-linear gaussian, gaussian, filter non-linear system, base gaussian, base gaussian approximation, kalman filter, non-linear system, approximation, wasserstein bound non-linear, kalman, kalman filter non-linear, unscented kalman, unscented kalman filter, non-linear gaussian filter, determine filter approximation, system, filter, true joint distribution, performance non-linear gaussian, bound non-linear gaussian, gaussian filter kalman, unscented, gaussian approximation, wasserstein bound\\nIntro: \\nLemmatized Chunk: kalman filter system unscented kalman filter base gaussian approximation use inequality bind wasserstein distance true joint distribution prediction measurement gaussian approximation bound use assess performance gaussian filter determine filter approximation likely induce error',\n",
       " 'Title: Locally minimax optimal and dimension-agnostic discrete argmin inference\\nAuthors: Ilmun Kim, Aaditya Ramdas\\nTopic: discrete argmin inference\\nKeywords: discrete argmin inference, dimension - agnostic discrete argmin inference, inference problem, optimal dimension - agnostic discrete, inference revisit, test, argmin inference revisit, vector, argmin inference, argmin inference problem, inference, dimension - agnostic discrete argmin, problem high - dimensional, revisit discrete, discrete argmin, problem high - dimensional setting, inference problem high - dimensional, high - dimensional setting, minimax optimal dimension - agnostic, dimensional vector, locally minimax optimal, dimension - agnostic discrete, propose dimension - agnostic test, setting, revisit discrete argmin, revisit, discrete argmin inference problem, discrete\\nIntro: \\nLemmatized Chunk: revisit discrete argmin inference problem high dimensional setting give observation dimensional vector goal test whether component mean vector small among component propose dimension agnostic test maintain validity regardless scale regardless arbitrary tie mean vector notably validity hold mild moment condition require little finiteness second moment permitt possibly strong dependence coordinate addition establish local minimax separation rate problem adapt cardinality confusion set show propose test attain rate method use sample split self normalization approach kim ramdas test easily invert yield confidence set argmin index empirical result illustrate strong performance approach term type error control power compare exist method',\n",
       " 'Title: Empirical Measures and Strong Laws of Large Numbers in Categorical\\n  Probability\\nAuthors: Tobias Fritz, Tomáš Gonda, Antonio Lorenzin, Paolo Perrone, Areeb Shah Mohammed\\nTopic: well - define empirical measure\\nKeywords: well - define empirical measure, empirical sampling morphism, law large number, large, measure, law, theorem, strong law, strong, glivenko - cantelli theorem, empirical measure, law large, empirical sampling morphism live, empirical measure converge, sampling morphism, empirical sampling, sampling, glivenko - cantelli, empirical, number, categorical probability, strong law large, large number\\nIntro: \\nLemmatized Chunk: glivenko cantelli theorem uniform version strong law large number state every iid sequence random variable empirical measure converge underlying distribution sense uniform convergence cdf work provide tool study limit empirical measure categorical probability propose two axiom permutation invariance empirical adequacy morphism type satisfy interpretable take infinite sequence input produce sample empirical measure output since sequence well define empirical measure empirical sampling live quasi markov category unlike markov category allow partial morphism give empirical sampling morphism property prove representability well abstract version de finetti theorem glivenko cantelli theorem strong law large number provide several concrete construction empirical sampling morphism partially define markov kernel standard borel space instantiate abstract result recover standard glivenko cantelli theorem strong law large number random variable finite first moment work thus provide joint proof two theorem conjunction de finetti theorem first principle',\n",
       " 'Title: Constraint-based causal discovery with tiered background knowledge and\\n  latent variables in single or overlapping datasets\\nAuthors: Christine W. Bang, Vanessa Didelez\\nTopic: incorporate tiered background knowledge\\nKeywords: incorporate tiered background knowledge, tier background, base causal discovery, constraint base causal discovery, multiple overlapp dataset, causal discovery tier, setting relax causal sufficiency, tier background knowledge, base causal, background, single overlapp dataset, knowledge within constraint base, causal discovery, background knowledge within constraint, latent variable, dataset paper, constraint base causal, background knowledge, incorporate tiered background, constraint base, tier, algorithm incorporate tier background, tier background knowledge tfci, latent variable single, relax causal sufficiency, overlapp dataset, background knowledge tfci\\nIntro: \\nLemmatized Chunk: paper consider use tier background knowledge within constraint base causal discovery focus setting relax causal sufficiency allow latent variable may arise relevant information could measure jointly case multiple overlapp dataset first present novel insight property tier tfci algorithm build introduce new extension iod integrate overlapp dataset algorithm incorporate tier background knowledge tier iod tiod algorithm show full usage tiered background knowledge tfci tiod sound simple version tiod tfci sound complete far show tiod algorithm often expect considerably efficient informative iod algorithm even beyond obvious restriction markov equivalence class provide formal result condition gain efficiency informativeness result accompany series example illustrate exact role usefulness tier background knowledge',\n",
       " 'Title: Sparse Bayesian Learning for Label Efficiency in Cardiac Real-Time MRI\\nAuthors: Felix Terhag, Philipp Knechtges, Achim Basermann, Anja Bach, Darius Gerlach, Jens Tank, Raúl Tempone\\nTopic: real - time mri cardiac\\nKeywords: real - time mri cardiac, respiratory effect, resonance imaging, cardiac real - time magnetic resonance, cardiac real - time magnetic, real - time magnetic resonance imaging, offer insight, mri cardiac real - time, label efficiency cardiac, image, cardiac real - time mri, outer slice, real - time magnetic resonance, emerge technology image, derive critical health indicator, mri cardiac, real - time magnetic, real - time mri cardiac real - time, emerge technology, real - time mri, sparse frequency, mri cardiac real - time magnetic, cardiac real - time, effect heartbeat, cardiac real - time mri cardiac, outer slice image, efficiency cardiac real - time, slice, magnetic resonance, magnetic resonance imaging\\nIntro: \\nLemmatized Chunk: cardiac real time magnetic resonance imaging mri emerge technology image heart frame per second offer insight respiratory effect heartbeat however method significantly increase number image must segment derive critical health indicator although neural network perform well inner slice prediction outer slice often unreliable work propose sparse bayesian learning sbl predict ventricular volume outer slice minimal manual labeling address challenge ventricular volume time assume dominate sparse frequency corresponding heart respiratory rate moreover sbl identify sparse frequency well segment inner slice optimize hyperparameter via type ii likelihood automatically pruning irrelevant component identify sparse frequency guide selection outer slice image labeling minimize posterior variance work provide performance guarantee greedy algorithm testing patient datum demonstrate label image necessary accurate volume prediction labeling procedure effectively avoid select inefficient image furthermore bayesian approach provide uncertainty estimate highlight unreliable prediction choose suboptimal label',\n",
       " 'Title: Robust Mean Estimation for Optimization: The Impact of Heavy Tails\\nAuthors: Bart P. G. van Parys, Bert Zwart\\nTopic: heavy tail\\nKeywords: heavy tail, robust optimization, construct least conservative, impact heavy, heavy - tail random, distributionally robust optimization, overestimate expect, variable, problem construct, decision process anticipate, random variable, non-negative, estimator expect, expect, non-negative heavy - tail, estimation optimization, conservative, impact heavy tail, robust mean estimation, heavy - tail random variable, non-negative heavy - tail random, process anticipate, non-negative heavy - tail random variable, conservative estimator, appropriately small, construct, heavy - tail, random\\nIntro: \\nLemmatized Chunk: consider problem construct least conservative estimator expect value heavy tail random variable require probability overestimate expect value keep appropriately small natural requirement subsequent use decision process anticipate setting show optimal estimate solve distributionally robust optimization dro problem use kullback leibler kl divergence far show statistical property kl dro compare favorably estimator base truncation variance regularization wasserstein dro',\n",
       " 'Title: Safety of particle filters: Some results on the time evolution of\\n  particle filter estimates\\nAuthors: Mathieu Gerber\\nTopic: evolution particle filter\\nKeywords: evolution particle filter, eta, monte carlo algorithm, particle filter, time evolution, define state - space, geq, filter estimate, filter problem, filter estimate particle, particle filter estimate, hat, state - space model, particle filter estimate particle, kappa, sequence filter distribution, safety particle filter, monte carlo, carlo algorithm propagate, online fashion, estimate, class monte, class monte carlo, pfs, estimate particle filter, filter, time evolution particle, filter estimate particle filter, filter distribution, propagate time\\nIntro: \\nLemmatized Chunk: particle filter pfs class monte carlo algorithm propagate time set particle use estimate online fashion sequence filter distribution define state space model despite popularity pfs study time evolution estimate receive little attention literature denot p estimate let work first show number particle hold probability one infinitely many measure distance probability distribution consider simple filter problem provide reassure result concern ability pfs estimate jointly finite set filter distribution study kappa finally toy filter problem prove sequential carlo randomize carlo version p algorithm offer great safety guarantee pfs sense algorithm hold probability one',\n",
       " 'Title: G{é}n{é}ration de Matrices de Corr{é}lation avec des Structures de\\n  Graphe par Optimisation Convexe\\nAuthors: Ali Fahkar, Kévin Polisano, Irène Gannaz, Sophie Achard\\nTopic: convexe work deal\\nKeywords: convexe work deal, work deal, specific sparsity, compare exist technique, graph structure, theoretical correlation matrice, specific sparsity pattern, offer great flexibility compare, generate correlation matrice, correlation matrice specific, graphe par optimisation, avec des, convexe work, base convex optimization, lation avec des, correlation matrice, lation avec, optimisation convexe work, matrice, matrice specific sparsity, des structure de graphe, par optimisation, generation correlation matrice, correlation, generation theoretical correlation, optimisation convexe, avec des structure, par optimisation convexe, lation avec des structure, offer great flexibility, graphe par, structure de graphe par, sparsity pattern, generation theoretical, theoretical correlation, graphe par optimisation convexe\\nIntro: \\nLemmatized Chunk: work deal generation theoretical correlation matrice specific sparsity pattern associate graph structure present novel approach base convex optimization offer great flexibility compare exist technique notably control mean entry distribution generate correlation matrice allow generation correlation matrice well represent realistic datum use benchmark statistical method graph inference',\n",
       " 'Title: Use of copula functions in error assessment due to deviation from\\n  dependence assumption\\nAuthors: Subarna Bhattacharjee, Aninda Kumar Nanda, Subhashree Patra\\nTopic: parallel system independently\\nKeywords: parallel system independently, reliability measure due, system parallel, component series, error assessment due, deviation dependence, measure due, deviation dependence assumption, assumption paper, analyze relative error, independently work, due tacit, dependence assumption, system independently work, component, copula function, component dependent, analyze relative, reliability measure, parallel system, due deviation, tacit assumption, system, assessment due deviation, relative error, series system, system independently, component series system\\nIntro: \\nLemmatized Chunk: paper analyze relative error various reliability measure due tacit assumption component associate component series system parallel system independently work component dependent use copula function say error analysis technique generalize exist work error assessment many wide class distribution',\n",
       " 'Title: Use of stochastic orders and statistical dependence in error analysis\\n  for multi-component system\\nAuthors: Subarna Bhattacharjee, Aninda Kumar Nanda, Subhashree Patra\\nTopic: reliability measure due\\nKeywords: reliability measure due, multivariate weibull distribution, component series, component independently work, measure due, stochastic order statistical, component independently, weibull exponential distribution, follow well - define multivariate, analyze relative error, independently work, order statistical dependence, well - define multivariate weibull, relative error crop, dependent follow, analyze relative, reliability measure, tacit assumption, follow well - define, assumption component, component series system, multivariate weibull exponential\\nIntro: \\nLemmatized Chunk: paper analyze relative error crop various reliability measure due tacit assumption component independently work associate component series system parallel system component dependent follow well define multivariate weibull exponential distribution also list important observation previous author note early work paper focus incur error multi component series parallel system multivariate weibull distribution upcoming section establish present study relevance stochastic order statistical dependence previously point previous author',\n",
       " 'Title: A computational theory of evaluation for parameterisable subject\\nAuthors: Hedong Yan\\nTopic: methodology often struggle\\nKeywords: methodology often struggle, decision making, critical advance, evaluation, struggle balance theoretical, critical advance decision, rigor practical scalability, struggle balance, advance decision making, make across domain, computational theory, balance theoretical, exist methodology often struggle, decision making across domain, balance theoretical rigor, practical scalability, rigor practical, exist methodology, advance decision, theoretical rigor practical, theoretical rigor\\nIntro: \\nLemmatized Chunk: evaluation critical advance decision making across domain yet exist methodology often struggle balance theoretical rigor practical scalability order reduce cost experimental evaluation introduce computational theory evaluation parameterisable subject prove upper bound generalize evaluation error generalize causal effect error evaluation metric subject also prove efficiency consistency estimate causal effect subject metric prediction optimize evaluation model propose meta learner handle heterogeneous evaluation subject space compare computational approach conditional evaluation model reduce evaluation error across scene include individual medicine scientific simulation business activity quantum trade evaluation time reduce order magnitude compare experiment simulation',\n",
       " 'Title: Variable selection via thresholding\\nAuthors: Ka Long Keith Ho, Hien Duy Nguyen\\nTopic: inference procedure\\nKeywords: inference procedure, variable selection, modern statistical inference, variable selection comprise, comprise, modern statistical, statistical inference, threshold variable selection, selection, thresholding, selection via thresholding variable, selection comprise important, comprise important step, variable selection via thresholding, statistical inference procedure, modern statistical inference procedure, step many modern, important step, important, selection comprise, threshold variable selection comprise, response often manifest small, shrink irrelevant signal, comprise important\\nIntro: \\nLemmatized Chunk: variable selection comprise important step many modern statistical inference procedure regression setting estimator shrink irrelevant signal zero covariate without relationship response often manifest small regression coefficient ad hoc procedure discard variable whose coefficient small threshold often employ practice formally analyze version thresholding procedure develop simple thresholding method consistently estimate set relevant variable mild regularity assumption use thresholding procedure propose sparse consistent asymptotically normal estimator whose element exhibit shrinkage performance applicability approach examine via numerical study simulate real datum',\n",
       " 'Title: Teachable normal approximations to binomial and related probabilities or\\n  confidence bounds\\nAuthors: Lutz Mattner\\nTopic: propose approximation result\\nKeywords: propose approximation result, related probability confidence, standard normal distribution, sufficiently advanced high school, standard normal distribution function, range sufficiently advanced, advanced high school pupil, deutsche arbeitsgemeinschaft statistik, confidence bound, arbeitsgemeinschaft statistik, probability confidence bound, statistical meeting, teachable normal approximation, normal distribution function, bound document, university student mathematics, version abstract, joint statistical meeting, related probability, confidence bound document, normal random variable, joint statistical, person range sufficiently, extend version, deutsche arbeitsgemeinschaft, research trier university, apply person range, probability confidence, binomial related probability\\nIntro: \\nLemmatized Chunk: document extend version abstract talk approximately title hold joint statistical meeting deutsche arbeitsgemeinschaft statistik march berlin teachable mean apply person range sufficiently advanced high school pupil university student mathematics statistic understand propose approximation result suffice know binomial law means variance standard normal distribution function necessarily concept corresponding normal random variable propose approximation well know least expert base teaching experience research trier university',\n",
       " 'Title: Parameter estimation for fractional autoregressive process with periodic\\n  structure\\nAuthors: Chunhao Cai, Yiwu Shang\\nTopic: autoregressive process\\nKeywords: autoregressive process, periodic fractional autoregressive, gaussian noise, fractional gaussian, kind periodic fractional, specialize vary coefficient fractional, paper introduce, fractional gaussian noise, coefficient fractional autoregressive model, process, coefficient fractional, fractional autoregressive model, periodic fractional, fractional autoregressive, vary coefficient fractional, estimator, autoregressive process periodic, periodic structure, process periodic structure, vary coefficient fractional autoregressive, autoregressive, fractional, drive fractional gaussian, periodic fractional autoregressive process, fractional autoregressive process\\nIntro: \\nLemmatized Chunk: paper introduce new kind periodic fractional autoregressive process pfar drive fractional gaussian noise fgn new model specialized vary coefficient fractional autoregressive model coefficient adhere periodic structure work generalize least square estimation gph method employ construct initial estimator estimate joint estimation parameter model one step procedure use obtain asymptotically efficient estimator paper prove estimator consistent asymptotically normal performance demonstrate simulation study use finite size sample via monte carlo simulation simulation study suggest estimation method accurately estimate model one step estimator outperform initial estimator',\n",
       " 'Title: Stochastic Transport Maps in Diffusion Models and Sampling\\nAuthors: Xicheng Zhang\\nTopic: construct stochastic\\nKeywords: construct stochastic, diffusion process, stochastic transport map, diffusion, diffusion model sampling, arbitrary probability distribution, transport map diffusion, computational framework, probability distribution use diffusion, process, satisfy fokker - planck equation, probability distribution, independent diffusion process satisfy, process satisfy fokker - planck, independent diffusion process, stochastic differential equation, transport, transport map, construct stochastic transport map, stochastic transport, diffusion process satisfy, computational framework construct, distribution use diffusion process, map probability distribution\\nIntro: \\nLemmatized Chunk: work present theoretical computational framework construct stochastic transport map probability distribution use diffusion process begin prove time marginal distribution sum two independent diffusion process satisfy fokker planck equation build result apply ambrosio figalli trevisan superposition principle establish existence uniqueness solution associate stochastic differential equation sde leverage theoretical foundation develop method construct stochastic transport map arbitrary probability distribution use dynamical ordinary differential equation ode sde furthermore introduce unified framework generalize extend broad class diffusion base generative model sampling technique finally analyze convergence property particle approximation sde underlie framework provide theoretical guarantee practical implementation work bridge theoretical insight practical application offer new tool generative modeling sampling high dimensional space',\n",
       " 'Title: Revisiting general source condition in learning over a Hilbert space\\nAuthors: Naveen Gupta, S. Sivananthan\\nTopic: general source\\nKeywords: general source, smoothness assumption, space learning theory, hilbert space, factor establish theoretical, revisit general source, optimal convergence rate, establish optimal convergence rate, revisit general source condition, convergence rate least - square, target function, establish theoretical convergence rate, learning theory literature, source condition learning, establish theoretical, learning theory, source condition, hilbert space learning, discuss learning theory, theoretical convergence rate, condition, key factor, key factor establish, function, factor establish, general source condition, convergence rate\\nIntro: \\nLemmatized Chunk: learning theory smoothness assumption target function know source condition key factor establish theoretical convergence rate estimator exist general form source condition discuss learning theory literature traditionally restrict class function express product operator monotone function lipschitz continuous function note remove restriction index function establish optimal convergence rate least square regression hilbert space general regularization general source condition thereby significantly broaden scope exist theoretical result',\n",
       " 'Title: Concentration inequalities for the sum in sampling without replacement:\\n  an approach via majorization\\nAuthors: Jianhang Ai, Ondřej Kuželka, Christos Pelekis\\nTopic: approach\\nKeywords: approach, number whose sum, concentration, integer, real, sum, sampling without replacement, concentration inequality, sample, replacement, real number whose sum, replacement denote, ldot , x, exceed expect, deduce non-asymptotic low, approach via majorization, consist, positive integer, element, theory majorization, sampling, positive, number, non-asymptotic low upper, sum sampling, population, inequality, population consisting, real number\\nIntro: \\nLemmatized Chunk: let p population consist real number whose sum zero let thousand positive integer sample element without replacement denote sum element sample article use idea theory majorization deduce low upper bound probability exceed expect value',\n",
       " 'Title: Estimation and variable selection in nonlinear mixed-effects models\\nAuthors: Antoine Caillebotte, Estelle Kuhn, Sarah Lemler\\nTopic: parameter\\nKeywords: parameter, model, estimation variable selection, model include, variable selection, individual parameter, estimate model parameter, variable selection nonlinear, include high - dimensional covariate, effect model include, model include high - dimensional, mix effect, model individual, identify relevant covariate, model individual parameter, include high - dimensional, covariate model individual, mix effect model include, estimation variable, high - dimensional covariate model, model include high - dimensional covariate, effect model include high - dimensional, high - dimensional covariate, covariate\\nIntro: \\nLemmatized Chunk: consider nonlinear mix effect model include high dimensional covariate model individual parameter objective identify relevant covariate estimate model parameter combine penalize lasso type estimator ebic model choice criterion select covariate interest estimate parameter maximum likelihood reduce model calculate lasso type penalize estimator weight proximal gradient descent algorithm adaptive learning rate choice allow particular consider model necessarily belong curved exponential family compare first performance propose methodology glmmlasso procedure linear mix effect model simulation study illustrate performance nonlinear mix effect logistic growth model simulation',\n",
       " 'Title: Nonparametric MLE for Gaussian Location Mixtures: Certified Computation\\n  and Generic Behavior\\nAuthors: Yury Polyanskiy, Mark Sellke\\nTopic: maximum likelihood\\nKeywords: maximum likelihood, dimension, study, bind subgaussian datum, mixture one dimension, study nonparametric, supp, mle gaussian location, certify computation generic, gaussian location, generic behavior, nonparametric mle, widehat, log, nonparametric, nonparametric maximum, nonparametric mle gaussian, likelihood estimator, study nonparametric maximum, location mixture, certify computation, generic behavior study, estimator, mle gaussian, computation generic, nonparametric maximum likelihood, nonparametric maximum likelihood estimator, gaussian location mixture, varepsilon, maximum likelihood estimator, computation generic behavior\\nIntro: \\nLemmatized Chunk: study nonparametric maximum likelihood estimator gaussian location mixture one dimension know since lindsay give point dataset estimator always return mixture component recently wu polyanskiy give sharp n bind subgaussian datum work study computational aspect provide algorithm small enough compute approximation wasserstein distance time datum dependent independent absolute constant widehat number atom also certifiably compute exact value finite time guarantee hold almost surely whenever dataset consist independent point probability distribution density relative lebesgue measure also show distribution condition atomic admit density associate thousand dimensional parameter space sqrt almost sure locally linear convergence algorithm one key tool classical fourier analytic estimate curve',\n",
       " 'Title: Functional structural equation models with out-of-sample guarantees\\nAuthors: Philip Kennerberg, Ernst C. Wit\\nTopic: learning method typically assume\\nKeywords: learning method typically assume, functional structural equation model, enable effective, statistical learning method, effective risk, structural equation, method typically assume, training test, real - world application frequently involve, datum originate, enable effective risk minimization, test datum, statistical learning method typically, equation model, effective risk minimization, statistical learning, learning method typically, test datum originate, enable effective risk, functional, training test datum, structural equation model, typically assume, method typically\\nIntro: \\nLemmatized Chunk: statistical learning method typically assume training test datum originate distribution enable effective risk minimization however real world application frequently involve distributional shift lead poor model generalization address recent advance causal inference robust learning introduce strategy invariant causal prediction anchor regression approach explore traditional structural equation model sem extension functional system remain limit paper develop risk minimization framework functional sem use linear potentially unbounded operator introduce functional bad risk minimization approach ensure robust predictive performance across shift environment key contribution novel bad risk decomposition theorem express maximum sample risk term observe environment establish condition existence uniqueness bad risk minimizer provide consistent estimation procedure empirical result functional system illustrate advantage method mitigating distributional shift finding contribute grow literature robust functional regression causal learning offer practical guarantee sample generalization dynamic environment',\n",
       " 'Title: A Martingale Approach to Large-$θ$ Ewens-Pitman Model\\nAuthors: Rodrigo Ribeiro\\nTopic: number part\\nKeywords: number part, model investigate, law large number, behavior number, partition model, parameter scal, parameter scal linearly, investigate asymptotic, yield significantly short proof, result regime, diversity parameter, investigate asymptotic behavior, lambda, martingale approach, diversity parameter scal, ewen - pitman model, asymptotic behavior, pitman partition, martingale approach large, sample size, central limit theorem, scal linearly, ewen - pitman model investigate, recent work establish, pitman partition model\\nIntro: \\nLemmatized Chunk: investigate asymptotic behavior number part ewen pitman partition model regime diversity parameter scal linearly sample size lambda recent work establish law large number lln central limit theorem clt regime revisit result martingale base approach method yield significantly short proof lead sharper convergence rate clt include improve berry esseen bound case new result regime fill gap literature',\n",
       " 'Title: Estimation of accuracy and reliability of models of\\n  $\\\\varphi$-sub-Gaussian stochastic processes in $C(T)$ spaces\\nAuthors: Oleksandr Mokliachuk\\nTopic: series independent term\\nKeywords: series independent term, series independent, process, process model problem, sub - gaussian stochastic, independent, stochastic, series independent element, process series, stochastic process, reliability accuracy, sub - gaussian stochastic process, study reliability accuracy, stochastic process model, accuracy stochastic process, independent element, series, independent term, modeling stochastic process, theory stochastic process, independent element explicitly, problem assessment\\nIntro: \\nLemmatized Chunk: present theory stochastic process model problem assessment reliability accuracy stochastic process model space study case implicit decomposition process form series independent term goal study reliability accuracy model process decompose series independent element explicitly use previous research field modeling stochastic process assumption consider possibility decomposition stochastic process series independent element find use approximation impact approximation error process decomposition series independent element reliability accuracy modeling stochastic process study theorem prove allow estimation reliability accuracy model stochastic process case decomposition process series independent element find error example use numerical approximation',\n",
       " 'Title: Interpretable Deep Regression Models with Interval-Censored Failure Time\\n  Data\\nAuthors: Changhui Yuan, Shishun Zhao, Shuwei Li, Xinyuan Song, Zhao Chen\\nTopic: deep neural network\\nKeywords: deep neural network, hide layer, powerful tool, datum structure sequentially, complex datum, integrate simple function, neural network, powerful tool modeling, interpretable deep regression model, sequentially integrate simple function, sequentially integrate simple, tool modeling, integrate simple, datum deep neural network, deep neural, model complex datum, tool modeling complex, model complex, sequentially integrate, structure sequentially integrate, complex datum structure, model complex datum structure, time datum deep neural\\nIntro: \\nLemmatized Chunk: deep neural network dnn become powerful tool model complex datum structure sequentially integrate simple function hide layer survival analysis recent advance dnn primarily focus enhance model capability especially explore nonlinear covariate effect right censoring however deep learning method interval censor datum unobservable failure time know lie interval remain underexplored limit specific datum type model work propose general regression framework interval censor datum broad class partially linear transformation model key covariate effect model parametrically nonlinear effect nuisance covariate approximate via dnn balancing interpretability flexibility employ sieve maximum likelihood estimation leverage monotone spline approximate cumulative baseline hazard function ensure reliable tractable estimation develop algorithm incorporate stochastic gradient descent establish asymptotic property parameter estimator show dnn estimator achieve minimax optimal convergence extensive simulation demonstrate superior estimation prediction accuracy state art method apply method alzheimer disease neuroimaging initiative dataset yield novel insight improve predictive performance compare traditional approach',\n",
       " 'Title: No-prior Bayesian inference reIMagined: probabilistic approximations of\\n  inferential models\\nAuthors: Ryan Martin\\nTopic: go - strategy probabilistic\\nKeywords: go - strategy probabilistic, information lack, bay, - prior bayesian, prior information lack, likelihood via bay theorem, prior, valid uncertainty quantification, inference, approximately valid uncertainty quantification, bayesian inference, model prior information, go - strategy, approximation, - prior bayesian inference reimagine, - prior bayesian inference, uncertainty quantification, valid uncertainty, bay theorem, bayesian inference reimagine, probabilistic, prior information, probabilistic approximation, default prior, fiducial inference, strategy probabilistic inference, probabilistic inference\\nIntro: \\nLemmatized Chunk: prior information lack go strategy probabilistic inference combine default prior likelihood via bay theorem objective bay generalize fiducial inference fall umbrella construction natural corresponding posterior distribution generally offer limited approximately valid uncertainty quantification present paper take reimagine approach offer posterior distribution strong reliability property propose construction start inferential model im one take mathematical form datum drive possibility measure feature exactly valid uncertainty quantification return call inner probabilistic approximation thereof inner probabilistic approximation inherit many original im desirable property include credible set exact coverage asymptotic efficiency approximation also agree familiar bay fiducial solution obtain application model group transformation structure monte carlo method evaluate probabilistic approximation present along numerical illustration',\n",
       " 'Title: Lean Formalization of Generalization Error Bound by Rademacher\\n  Complexity\\nAuthors: Sho Sonoda, Kazumi Kasaura, Yuma Mizuno, Kei Tsukamoto, Naoto Onda\\nTopic: generalization error bind\\nKeywords: generalization error bind, formalize generalization error, lean formalization generalization, error quantify gap, establish generalization error bound, generalization error, rademacher, learning machine performance, versus unseen test datum, lean formalization, formalization generalization error, error bind, establish generalization error, training datum versus unseen, learning, rademacher complexity, rademacher complexity serve, generalization, generalization error quantify, error, theorem prover, complexity, datum versus unseen test, lean\\nIntro: \\nLemmatized Chunk: formalize generalization error bind use rademacher complexity lean theorem prover generalization error quantify gap learning machine performance give training datum versus unseen test datum rademacher complexity serve estimate error base complexity learning machine hypothesis class unlike traditional method pac learning vc dimension rademacher complexity applicable across diverse machine learning scenario include deep learning kernel method formalize key concept theorem include empirical population rademacher complexity establish generalization error bound formal proof mcdiarmid inequality hoeffding lemma symmetrization argument',\n",
       " 'Title: Detecting Arbitrary Planted Subgraphs in Random Graphs\\nAuthors: Dor Elimelech, Wasim Huleihel\\nTopic: plant subgraph\\nKeywords: plant subgraph, recover plant, detect recover plant, nyi random, gamma, hoc plant structure, significant attention, plant structure inferential, random graph, past three decade, receive significant, mathematical technique, arbitrary plant subgraph, nyi random graph, recover plant structure, plant, structure inferential setting, detect arbitrary plant, detect arbitrary plant subgraph, receive significant attention, plant structure, exciting result mathematical, exciting result, result mathematical technique\\nIntro: \\nLemmatized Chunk: problem detect recover plant structure subgraph e random graph receive significant attention past three decade lead many exciting result mathematical technique however prior work largely focus specific ad hoc plant structure inferential setting general theory remain elusive paper bridge gap investigate detection plant subgraph e random graph n edge probability within examine statistical computational aspect problem establish follow result dense regime edge probability fix tightly characterize information theoretic computational threshold detect provide condition computational statistical gap arise notably threshold depend number edge maximum degree maximum subgraph density low upper bound general apply value function accordingly also analyze sparse regime theta well critical regime theta widely study specific choice regime show bound tight plant subgraph investigate literature thus many finally identify condition detection undergoe sharp phase transition boundary algorithm succeed fail shift abruptly function',\n",
       " 'Title: Calibration Bands for Mean Estimates within the Exponential Dispersion\\n  Family\\nAuthors: Łukasz Delong, Selim Gatti, Mario V. Wüthrich\\nTopic: calibration\\nKeywords: calibration, band, dispersion family, sample noisy observation, auto - calibration, obtain calibration band, estimate match estimate, result mean estimate, deal finite sample, dispersion family statistical, estimate perfectly, perfectly match, perfectly match true, estimate perfectly match, finite sample noisy, exponential dispersion, result mean estimate perfectly, estimate, calibration band denote, calibration band, underlying response, exponential dispersion family, match true, family statistical model, statistical model\\nIntro: \\nLemmatized Chunk: statistical model say calibrate result mean estimate perfectly match true means underlying response aim calibration often achievable practice one deal finite sample noisy observation weak notion calibration auto calibration auto calibrate model satisfy expect value response give mean estimate match estimate testing auto calibration consider recently literature propose new approach base calibration band calibration band denote set low upper bound probability true means lie simultaneously inside bound exceed give confidence level band construct yang barber sub distribution dimitriadis et introduce narrower band bernoulli distribution use idea order extend construction entire exponential dispersion family contain example binomial poisson negative binomial gamma normal distribution moreover show obtain calibration band allow construct various test calibration auto calibration respectively',\n",
       " 'Title: An improved central limit theorem for the empirical sliced Wasserstein\\n  distance\\nAuthors: David Rodríguez-Vítores, Eustasio del Barrio, Jean-Michel Loubes\\nTopic: application across various field\\nKeywords: application across various field, wasserstein distance present, improve central limit, diverse type datum, tool handle diverse, fundamental tool, optimal transport theory, optimal transport, fundamental tool handling, wasserstein distance optimal, grow application, improve central limit theorem, handle diverse, distance present significant computational, slice wasserstein distance optimal, wasserstein distance, distance optimal transport, distance optimal transport theory, slice wasserstein, slice wasserstein distance, type datum, empirical slice wasserstein distance, tool handling, diverse type, wasserstein distance present significant, central limit theorem, transport theory, wasserstein distance optimal transport, slice wasserstein distance applicable, handle diverse type\\nIntro: \\nLemmatized Chunk: optimal transport theory become fundamental tool handle diverse type datum grow application across various field however wasserstein distance present significant computational statistical challenge high dimensional setting address issue alternative distance slice wasserstein distance leverage one dimensional projection introduce work establish novel central limit theorem slice wasserstein distance use efron stein inequality technique prove effective related problem approach yield central limit theorem center expect value empirical cost mild regularity condition notably unlike general wasserstein distance arbitrary dimension demonstrate specific assumption centering constant replace population cost essential statistical inference generalize significantly refine exist result one dimensional case consequently present first asymptotically valid inference framework slice wasserstein distance applicable measure necessarily compactly support finally address key practical aspect inference include monte carlo estimation integral estimation asymptotic variance ensure applicability real world scenario',\n",
       " 'Title: Benign landscapes for synchronization on spheres via normalized\\n  Laplacian matrices\\nAuthors: Andrew D. McRae\\nTopic: condition number\\nKeywords: condition number, laplacian matrice study, nonconvex optimization, model, normalize laplacian, result, geq, nonconvex, optimization landscape, synchronization problem sphere, sphere via normalize laplacian, study nonconvex optimization, optimization landscape synchronization, statistical problem, matrice study, study nonconvex, mathbf, nonconvex least - square problem, normalize laplacian matrice, synchronization, synchronization problem, network, nonconvex optimization landscape, condition, laplacian matrice, problem\\nIntro: \\nLemmatized Chunk: study nonconvex optimization landscape synchronization problem sphere first present new result statistical problem synchronization two element group consider nonconvex least square problem relaxed unit sphere geq several popular model include graph clustering binary stochastic block model show geq every second order critical point recover ground truth asymptotic regime exact recovery information theoretically possible statistical optimality via spherical relaxation previously show potentially arbitrarily large relaxation dimension second consider global synchronization network couple oscillator homogeneous kuramoto model prove new optimal asymptotic result random sign network graph give new simple proof several exist state art result key tool deterministic landscape condition extend recent result rakoto endor waldspurger result say certain problem dependent laplacian matrix small enough condition number nonconvex landscape benign extension allow condition number include arbitrary diagonal preconditioner give tight result many problem show synchronization kuramoto oscillator network near neighbor circulant graph study wiley strogatz girvan condition optimal also prove natural complex extension may interest synchronization special orthogonal group',\n",
       " 'Title: Confidence set for mixture order selection\\nAuthors: Alessandro Casa, Davide Ferrari\\nTopic: true number component\\nKeywords: true number component, confidence set, model, finite mixture, mixture, order selection fundamental, mixture order selection, model selection confidence set, set, application finite, selection, finite mixture model, application finite mixture, mixture order, select single model, selection fundamental challenge, order, order selection, component, mixture component, model selection, fundamental challenge, identify true number, number mixture component, select single, traditional approach rely, select, confidence\\nIntro: \\nLemmatized Chunk: fundamental challenge application finite mixture model select number mixture component also know order traditional approach rely select single good model use information criterion however presence noisy datum model different order yield similar fit model selection uncertainty substantial make challenging confidently identify true number component paper introduce model selection confidence set msc order selection set value estimator predefine confidence level include true mixture order across repeat sample rather select single model msc identify plausible order determine whether candidate model least plausible well select one use screen test base penalize likelihood ratio statistic provide theoretical guarantee asymptotic coverage confidence set demonstrate practical advantage simulation real datum analysis',\n",
       " 'Title: Differentially Private Joint Independence Test\\nAuthors: Xingwei Liu, Yuexin Chen, Wangli Xu\\nTopic: vector play important\\nKeywords: vector play important, identification joint dependence, hilbert - schmidt independence criterion, sensitive confidential information, power, play important role, random vector, statistical application, confidential information, datum may contain sensitive, important role, differentially private joint, joint independence test, differentially private joint independence, dhsic, independence test identification, context differential privacy, play important, random vector play, - variable hilbert - schmidt independence criterion, private joint independence, joint dependence, test identification joint, joint independence test identification, vector play, detect joint dependence, sensitive confidential, private joint independence test\\nIntro: \\nLemmatized Chunk: identification joint dependence among two random vector play important role many statistical application datum may contain sensitive confidential information paper consider hilbert schmidt independence criterion dhsic context differential privacy give limit distribution empirical estimate dhsic complicated gaussian chaos construct test regime typically base permutation bootstrap detect joint dependence privacy propose dhsic base testing procedure employ differentially private permutation methodology method enjoy privacy guarantee valid level pointwise consistency bootstrap counterpart suffer inconsistent power far investigate uniform power propose test dhsic metric metric indicate propose test attain minimax optimal power across different privacy regime byproduct result also contain pointwise uniform power permutation dhsic address unsolved question remain pfister et',\n",
       " 'Title: AutoBayes: A Compositional Framework for Generalized Variational\\n  Inference\\nAuthors: Toby St Clere Smithe, Marco Perin\\nTopic: generalize variational inference\\nKeywords: generalize variational inference, loss function typical, generalize variational, compositional framework, typical variational inference, inference, variational inference, exact bayesian inference, framework generalize, local loss function, bayesian inference, reverse - mode automatic differentiation, generalize, clarify different part, attach local loss function, variational free energy, framework generalize variational, variational, satisfy chain rule akin, satisfy chain rule, loss function, variational inference introduce, compositional framework generalize, build optimize model\\nIntro: \\nLemmatized Chunk: introduce new compositional framework generalize variational inference clarify different part model interact compose explain exact bayesian inference loss function typical variational inference variational free energy generalization satisfy chain rule akin reverse mode automatic differentiation advocate exploit build optimize model accordingly end construct series compositional tool building model construct inversion attach local loss function expose parameter finally explain result parameterize statistical game may optimize locally illustrate framework number classic example point new area extensibility reveal',\n",
       " 'Title: Minimax Rate-Optimal Inference for Individualized Quantile Treatment\\n  Effects in High-dimensional Models\\nAuthors: Jiachen Sun, Yin Xia\\nTopic: vary treatment effect\\nKeywords: vary treatment effect, individualized quantile treatment, treatment effect play, treatment effect, individualized quantile treatment effect, policy making, high - dimensional model quantification, bio- pharmaceutical research, play important role, range application, role wide, policy making bio -pharmaceutical, important role, effect high - dimensional model, effect play important, play important, wide range application, wide range, treatment effect high - dimensional, include policy making, quantification treatment effect, quantile treatment, make bio - pharmaceutical research, quantile treatment effect\\nIntro: \\nLemmatized Chunk: quantification treatment effect play important role wide range application include policy making bio pharmaceutical research article study quantile treatment effect qte address two specific type heterogeneity personalize heterogeneity capture vary treatment effect different individual benefit quantile heterogeneity account impact covariate vary across different quantile level well design debiase estimator individualized quantile treatment effect iqte propose capture heterogeneity effectively show estimator converge weakly gaussian process function quantile level propose valid statistical inference method include construction confidence interval development hypothesis testing decision rule addition minimax optimality framework inference procedure establish specifically derive minimax optimal rate expect length confidence interval magnitude detection boundary hypothesis testing procedure illustrate superiority propose estimator effectiveness method demonstrate extensive simulation analysis national health nutrition examination survey nhane dataset',\n",
       " 'Title: Recovering a (1+1)-dimensional wave equation from a single white noise\\n  boundary measurement\\nAuthors: Emilia L. K. Blåsten, Tapio Helin, Antti Kujanpää, Lauri Oksanen, Jesse Railo\\nTopic: unknown first order coefficient\\nKeywords: unknown first order coefficient, order coefficient function, neumann boundary, wave equation, boundary datum, white noise process, initial condition, single white, initial condition excited, noise process, modell white, condition excited, single white noise boundary, dimensional wave, white noise boundary measurement, neumann boundary datum modell, white noise boundary, datum modell, single white noise, inverse problem, neumann boundary datum, noise boundary measurement, boundary datum modell, datum, boundary datum determine, white noise, dimensional wave equation\\nIntro: \\nLemmatized Chunk: consider follow inverse problem suppose dimensional wave equation zero initial condition excite neumann boundary datum modell white noise process give also dirichlet datum point determine unknown first order coefficient function system first establish direct problem well pose inverse problem solve show correlation boundary datum determine neumann dirichlet operator sense distribution know uniquely identify coefficient approach application acoustic measurement internal fluid pipe pressurise water supply pipe vocal tract shape determination',\n",
       " 'Title: Asymptotically uniformly most powerful tests for diffusion processes\\n  with nonsynchronous observations\\nAuthors: Teppei Ogihara, Futo Ueno\\nTopic: nonsynchronous observation\\nKeywords: nonsynchronous observation, observe nonsynchronous sampling, introduce quasi-likelihood ratio, diffusion process, quasi - likelihood ratio testing, quasi - likelihood ratio, testing procedure, testing procedure diffusion, paper introduce, quasi - likelihood ratio testing procedure, nonsynchronous observation paper, observation paper introduce, diffusion process nonsynchronous, process observe, observe nonsynchronous, process nonsynchronous observation, ratio testing procedure, ratio testing, process observe nonsynchronous, diffusion process observe, observation paper, sampling scheme, procedure diffusion process, nonsynchronous sampling scheme, introduce quasi-likelihood, paper introduce quasi-likelihood\\nIntro: \\nLemmatized Chunk: paper introduce quasi likelihood ratio testing procedure diffusion process observe nonsynchronous sampling scheme high frequency datum particularly financial econometrics often record irregular time point challenge conventional synchronous method parameter estimation hypothesis testing address challenge develop quasi likelihood framework accommodate irregular sampling integrate adaptive estimation technique drift diffusion coefficient thereby enhance optimization stability reduce computational burden rigorously derive asymptotic property propose test statistic show converge distribution null hypothesis exhibit consistency alternative moreover establish result test asymptotically uniformly powerful extensive numerical experiment corroborate theoretical finding demonstrate method outperform exist nonparametric approach',\n",
       " 'Title: Non-Bayesian Learning in Misspecified Models\\nAuthors: Sebastian Bervoets, Mathieu Faure, Ludovic Renou\\nTopic: nuanced view\\nKeywords: nuanced view, non-bayesian updating, view, imply inherent, misspecified model, model deviation, inherent, deviation bayesian, offer nuanced, non-bayesian learning misspecified, outperform bayesian updating, update, deviation, model deviation bayesian, deviation bayesian updating, update traditionally categorize, misspecified model deviation, problem misspecified model, bayesian updating, error, fallacy, bayesian updating traditionally, non-bayesian learning, traditionally categorize, categorize bias, bias, traditionally categorize bias\\nIntro: \\nLemmatized Chunk: deviation bayesian updating traditionally categorize bias error fallacy thus imply inherent sub optimality offer nuanced view demonstrate learning problem misspecified model updating outperform bayesian updating',\n",
       " 'Title: Two-Sample Tests for Optimal Lifts, Manifold Stability and Reverse\\n  Labeling Reflection Shap\\nAuthors: Do Tran Van, Susovan Pal, Benjamin Eltzner, Stephan F. Huckemann\\nTopic: respect reference\\nKeywords: respect reference, push forward riemannian, complete riemannian manifold, act lie group, isometrically properly act, properly act lie, derive continuity uniqueness, lift quotient, forward riemannian volume, push forward riemannian volume, properly act lie group, manifold modulo isometrically, riemannian manifold modulo, smoothness large extent, complete riemannian manifold modulo, quotient complete, point manifold ., position reference, reference point, quotient manifold\\nIntro: \\nLemmatized Chunk: consider quotient complete riemannian manifold modulo isometrically properly acting lie group lift quotient manifold optimal position reference point manifold respect push forward riemannian volume onto quotient derive continuity uniqueness smoothness large extent also respect reference point consequence derive general manifold stability theorem mean lie high dimensional stratum assume positive probability strong law optimal lift allow define new two sample test utilize individual optimal lift outperform exist two sample test simulate datum also outperform exist test newly derive reverse labeling reflection shape space use model filament datum microtubule within cell biological application',\n",
       " 'Title: Poisson-Process Topic Model for Integrating Knowledge from Pre-trained\\n  Language Models\\nAuthors: Morgane Austern, Yuanchuan Guo, Zheng Tracy Ke, Tianle Liu\\nTopic: topic modeling\\nKeywords: topic modeling, word, topic model integrating, large language model, count without accounting, traditional topic, topic modeling method, knowledge pre-trained language, traditionally apply, model topic modeling, offer contextualize word embedding, apply, language model, language model topic, traditionally apply word, embedding, integrate knowledge, contextualize word embedding, word count, modeling, word embedding, language model topic modeling, traditional topic modeling, integrate knowledge pre-trained, apply word count, topic modeling traditionally, word count without accounting, pre-trained language model topic, model integrate knowledge, traditional topic modeling method, topic, poisson - process topic model, pre-trained language model\\nIntro: \\nLemmatized Chunk: topic modeling traditionally apply word count without accounting context word appear recent advancement large language model llm offer contextualize word embedding capture deep meaning relationship word aim leverage embedding improve topic modeling use llm convert document sequence word embedding sequence model poisson point process intensity measure express convex combination base measure corresponding topic estimate topic propose flexible algorithm integrate traditional topic modeling method enhance net rounding apply kernel smoothing apply one advantage framework treat llm black box require fine tuning parameter another advantage ability seamlessly integrate traditional topic modeling approach plug module without need modification assume topic smooth intensity measure embed space establish rate convergence method also provide minimax low bind show rate method match low bind additionally apply method several dataset provide evidence offer advantage traditional topic modeling approach',\n",
       " 'Title: A new tail bound for the sum of bounded independent random variables\\nAuthors: Jackson Loper, Jeffrey Regier\\nTopic: sum bound independent\\nKeywords: sum bound independent, independent random, bind, variable lie, variable, sum bound, sum, interval, random variable, random variable situation, independent, sum independent random, bound independent random variable, random, variable situation, random variable construct, bound independent random, convex optimization problem, tail bind, random variable lie, solve two - dimensional convex, sum independent, construct, independent random variable, tail, two - dimensional convex optimization problem, two - dimensional convex optimization, bound independent\\nIntro: \\nLemmatized Chunk: construct new tail bind sum independent random variable situation expect value sum know random variable lie within specify interval may different variable new bind compute solve two dimensional convex optimization problem simulation demonstrate new bind often substantially tight hoeffding inequality case bound applicable',\n",
       " 'Title: A Statistical Theory of Contrastive Learning via Approximate Sufficient\\n  Statistics\\nAuthors: Licong Lin, Song Mei\\nTopic: drive significant\\nKeywords: drive significant, datum training model, drive significant progress, approximate sufficient statistic, significant progress, sufficient statistic contrastive learning, significant progress foundation, statistical theory, training model distinguish, similar sample, contrastive learning, approximate sufficient statistic contrastive, foundation model, distinguish similar, training model, distinguish similar sample, sample dissimilar, extract useful representation, datum augmentation - base contrastive learning, modern approach extract, progress foundation model, model distinguish similar, representation unlabeled datum, similar sample dissimilar\\nIntro: \\nLemmatized Chunk: contrastive learning modern approach extract useful representation unlabeled datum training model distinguish similar sample dissimilar one drive significant progress foundation model work develop new theoretical framework analyze datum augmentation base contrastive learning focus simclr representative example approach base concept approximate sufficient extend beyond original definition cite contrastive language image pretraining clip use kl divergence generalize equivalent form general divergence show minimize simclr contrastive loss yield encoder approximately sufficient furthermore demonstrate near sufficient encoder effectively adapt downstream regression classification task performance depend sufficiency error induce datum augmentation contrastive learning concrete example linear regression topic classification provide illustrate broad applicability result',\n",
       " 'Title: Modeling of stochastic processes in $L_p(T)$ using orthogonal\\n  polynomials\\nAuthors: Oleksandr Mokliachuk\\nTopic: space\\nKeywords: space, paper, model, approximate stochastic, modeling stochastic, orthonormal basis, orthogonal polynomial, specific function, series use orthonormal basis, process, varphi, stochastic, consider specific, reliability accuracy, stochastic process, modeling, find explicitly, approximate stochastic process, function, approximate, model approximate stochastic, omega, modeling stochastic process, polynomial paper, model approximate, process space\\nIntro: \\nLemmatized Chunk: paper model approximate stochastic process space give reliability accuracy consider specific function process decomposite series use orthonormal basis model construct case element decomposition find explicitly',\n",
       " 'Title: The Entropy and Crossentropy of Generalized Mallows Models\\nAuthors: Marina Meilă\\nTopic: parameter\\nKeywords: parameter, model, rank datum, family, exponential family, crossentropy generalize mallow, model generalize mallow, exponential, mallow model, set dispersion parameter, generalize mallow model, model rank datum, generalize mallow, gmm, generalize, sigma, mathbb, gmm share many property, location parameter, mallow, dispersion parameter, mallow model generalize, rank, datum\\nIntro: \\nLemmatized Chunk: generalize mallow model gmm well know family model rank datum gmm distribution set permutation n object characterize location parameter know central permutation set dispersion parameter gmm share many property sufficient statistic exponential model thus see exponential family discrete parameter paper show computing entropy crossentropy kullback leibler divergence class gmm tractable pave way good understanding exponential family',\n",
       " 'Title: Efficient Knowledge Distillation via Curriculum Extraction\\nAuthors: Shivam Gupta, Sushrut Karmalkar\\nTopic: output\\nKeywords: output, final teacher network, small student, output generate, curriculum extraction knowledge distillation, student network, knowledge, knowledge distillation, curriculum extraction knowledge, progressive distillation, curriculum, outperform one - shoot distillation, teacher training process, small student network, efficient knowledge, teacher, citep, network, knowledge distillation via curriculum, intermediate checkpoint, distillation, training, large teacher network, empirical advantage, teacher network, extraction knowledge, extraction knowledge distillation, large teacher, efficient knowledge distillation\\nIntro: \\nLemmatized Chunk: knowledge distillation technique use train small student network use output generate large teacher network many empirical advantage distillingtk standard one shot approach distillation use output final teacher network recent work progressive show use intermediate checkpoint teacher training process implicit curriculum progressive distillation significantly speed training however scheme require store checkpoint often require careful selection intermediate checkpoint train impractical large scale training paper show curriculum fully train teacher network extract curriculum give similar efficiency benefit progressive distillation extraction scheme natural use random projection hide representation teacher network progressively train student network training use output full network show scheme significantly outperform one shot distillation achieve performance similar progressive distillation learn sparse parity two layer network provide theoretical guarantee setting additionally show method outperform one shot distillation even use transformer base architecture sparse parity learning language modeling task',\n",
       " 'Title: Glivenko-Cantelli for $f$-divergence\\nAuthors: Haoming Wang, Lek-Heng Lim\\nTopic: call fundamental theorem\\nKeywords: call fundamental theorem, setting total variation, celebrate glivenko - cantelli, setting total, theorem, total variation distance, glivenko - cantelli theorem, fundamental theorem, sigma, variation distance, theorem statistic, fundamental theorem statistic, celebrate glivenko - cantelli theorem, glivenko - cantelli, divergence, extend celebrate glivenko - cantelli, total variation, extend celebrate, standard setting, standard setting total, call fundamental\\nIntro: \\nLemmatized Chunk: extend celebrate glivenko cantelli theorem sometimes call fundamental theorem statistic standard setting total variation distance divergence key obstacle endeavor define divergence subcollection algebra form system subalgebra side contribution work show notion divergence system ray preserve nearly know property standard divergence yield novel integral representation kolmogorov smirnov distance glivenko cantelli theorem also discuss prospect vapnik chervonenkis theory divergence',\n",
       " 'Title: On Privately Estimating a Single Parameter\\nAuthors: Hilal Asi, John C. Duchi, Kunal Talwar\\nTopic: investigate differentially\\nKeywords: investigate differentially, private estimator, private estimator individual, individual parameter, large parametric, estimate single parameter, privately estimate single, large parametric model, generic private estimator, parameter within large, estimator individual parameter, differentially private estimator, parameter investigate differentially, privately estimate, generic private estimator exist, parametric model, individual parameter within large, single parameter, achieve instance optimal bound, estimate single, differentially private, investigate differentially private estimator, private estimator exist, investigate differentially private, parameter within large parametric, provide private certificate, single parameter investigate\\nIntro: \\nLemmatized Chunk: investigate differentially private estimator individual parameter within large parametric model generic private estimator exist estimator provide repose new local notion estimand stability notion allow procedure provide private certificate stability leverage private certificate provide computationally statistical efficient mechanism release private statistic least asymptotically sample size essentially unimprovable achieve instance optimal bound additionally investigate practicality algorithm simulate datum real world datum American community survey census highlight scenario new procedure successful identify area future work',\n",
       " 'Title: An improved nonparametric test and sample size procedures for the\\n  randomized complete block designs\\nAuthors: Show-Li Jan, Gwowen Shieh\\nTopic: treatment effect\\nKeywords: treatment effect, block design friedman, test, test sample size, treatment effect randomize, friedman test, apply nonparametric, block design, effect randomize complete, compare treatment, nonparametric test sample, extensively apply, friedman procedure hypothesis, randomize complete block design, compare treatment effect, nonparametric alternative, procedure compare treatment, randomize complete block, complete block, randomize complete, sample size, complete block design, effect randomize, improve nonparametric, friedman, procedure randomize, sample size procedure, design friedman test, improve nonparametric test, friedman procedure\\nIntro: \\nLemmatized Chunk: friedman test extensively apply nonparametric alternative conventional f procedure compare treatment effect randomize complete block design chi distribution provide convenient approximation determine critical value friedman procedure hypothesis testing however approximation generally conservative accuracy decline increase number treatment paper describe alternative transformation friedman statistic along approximate f distribution numerator degree freedom anova f test moreover two approximate noncentral f distribution present propose transformation alternative hypothesis heterogeneous location shift explicit power function derive underlying population uniform normal laplace exponential distribution theoretical examination empirical assessment present validate advantage propose approach exist method friedman test develop test power procedure recommend due consistently acceptable type error rate accurate power calculation location shift structure population distribution consider',\n",
       " 'Title: Nonparametric Factor Analysis and Beyond\\nAuthors: Yujia Zheng, Yang Liu, Jiaxiong Yao, Yingyao Hu, Kun Zhang\\nTopic: factor analysis\\nKeywords: factor analysis, additive independent, identifiability result unsupervised, unsupervised representation learning, independent component analysis, unsupervised representation learning inspire, independent component, representation learning inspire, representation learning, component analysis, unsupervised representation, causal representation learning, nonparametric factor analysis, learning inspire, rely assumption, additive independent noise, assumption additive independent, independent noise noiseless, result unsupervised representation, causal representation, noise noiseless regime\\nIntro: \\nLemmatized Chunk: nearly identifiability result unsupervised representation learning inspire independent component analysis factor analysis causal representation learning rely assumption additive independent noise noiseless regime contrast study general case noise take arbitrary form depend latent variable entangle within nonlinear function propose general framework identify latent variable nonparametric noisy setting first show suitable condition generative model identifiable certain submanifold indeterminacycy even presence noise furthermore structural distributional variability condition prove latent variable general nonlinear model identifiable trivial indeterminacycy base propose theoretical framework also develop corresponding estimation method validate various synthetic real world setting interestingly estimate true gdp growth alternative measurement suggest insightful information economy official report expect framework provide new insight researcher practitioner deal latent variable real world scenario',\n",
       " 'Title: Optimal Nonlinear Online Learning under Sequential Price Competition via\\n  s-Concavity\\nAuthors: Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun\\nTopic: price\\nKeywords: price, price competition via s-concavity, nonlinear online learning, optimal nonlinear online learning, competition among multiple, policy, seller, sell horizon, competition via s-concavity, price competition, price competition among multiple, competition among multiple seller, sell, unobservable competitor, sequential price, seller simultaneously offer, multiple, period, online learning sequential, multiple seller, optimal nonlinear, function, optimal nonlinear online, subsequently observe respective, nonlinear online, sequential price competition, observe respective demand, competition, horizon, demand\\nIntro: \\nLemmatized Chunk: consider price competition among multiple seller sell horizon period period seller simultaneously offer price subsequently observe respective demand unobservable competitor demand function seller depend seller price private unknown nonlinear relationship address challenge propose semi parametric least square estimation nonlinear mean function require seller communicate demand information show seller employ policy price converge rate nash equilibrium price seller would reach fully inform seller incur regret relative dynamic benchmark policy theoretical contribution work prove existence equilibrium shape constrain demand function via concept concavity establish regret bound propose policy technically also establish new concentration result least square estimator shape constraint finding offer significant insight dynamic competition aware pricing contribute broad study learning strategic decision making',\n",
       " 'Title: Uniformly consistent proportion estimation for composite hypotheses via\\n  integral equations: \"the case of location-shift families\"\\nAuthors: Xiongzhi Chen\\nTopic: null hypothese\\nKeywords: null hypothese, consistent proportion estimation, random variable belong, uniformly consistent proportion estimation, proportion false null, real line, estimation composite hypothese, uniformly consistent, unbound interval, variable two type, estimate proportion, composite null, random variable, random, uniformly consistent estimator, median random, uniformly consistent proportion, variable belong, proportion random variable, composite null hypothese, belong non-empty, case location - shift family, type composite null, false null hypothese, random variable mention, proportion estimation composite, random variable mention early\\nIntro: \\nLemmatized Chunk: consider estimate proportion random variable two type composite null hypothese means median random variable belong bound interval ii means median random variable belong unbounded interval whole real line type composite null hypothese uniformly consistent estimator proportion false null hypothese construct random variable whose distribution member type location shift family far uniformly consistent estimator certain function bound null means median provide random variable mention early function continuous bound variation estimator construct via solution lebesgue stieltje integral equation harmonic analysis rely concept value various application',\n",
       " 'Title: A Statistical Analysis for Per-Instance Evaluation of Stochastic\\n  Optimizers: How Many Repeats Are Enough?\\nAuthors: Moslem Noori, Elisabetta Valiante, Thomas Van Vaerenbergh, Masoud Mohseni, Ignacio Rozada\\nTopic: trait stochastic optimizer\\nKeywords: trait stochastic optimizer, metric, multiple run, statistical analysis, stochastic optimizer, estimate performance metric depend, result, problem, statistical analysis per-instance, analysis per-instance evaluation, accuracy, problem produce, evaluation stochastic, key trait, optimizer, stochastic, estimate performance metric, per - instance evaluation stochastic, per - instance evaluation, attempt solve, trait stochastic, performance stochastic optimizer, run, number, performance, performance metric depend, evaluation stochastic optimizer, number run, key trait stochastic, repeat, number repeat\\nIntro: \\nLemmatized Chunk: key trait stochastic optimizer multiple run optimizer attempt solve problem produce different result result performance evaluate several repeat run problem however accuracy estimate performance metric depend number run study use statistical tool present statistical analysis common metric develop guideline experiment design measure optimizer performance use metric high level confidence accuracy end first discuss confidence interval metric related number run experiment derive low bind number repeat order guarantee achieve give accuracy metric use bind propose algorithm adaptively adjust number repeat need ensure accuracy evaluate metric simulation result demonstrate utility analysis allow conduct reliable benchmarking well hyperparameter tuning prevent draw premature conclusion regard performance stochastic optimizer',\n",
       " 'Title: Statistical accuracy of the ensemble Kalman filter in the near-linear\\n  setting\\nAuthors: E. Calvello, J. A. Carrillo, F. Hoffmann, P. Monmarché, A. M. Stuart, U. Vaes\\nTopic: weather forecasting\\nKeywords: weather forecasting, noisy observation, forecasting prediction, dynamical system partial, probabilistic weather, partial noisy, estimate state, linear gaussian setting, weather forecasting prediction, set estimate state, prediction epidemics, number application, ensemble kalman, state dynamical, probabilistic weather forecasting, interact particle system, large number application, ensemble kalman filter, filter near - linear, problem large, partial noisy observation, provide provably accurate approximation, ubiquitous problem, near - linear setting estimating, large number\\nIntro: \\nLemmatized Chunk: estimate state dynamical system partial noisy observation ubiquitous problem large number application probabilistic weather forecasting prediction epidemics particle filter widely adopt approach problem provide provably accurate approximation statistic state perform poorly high dimension weight collapse ensemble kalman filter suffer issue rely interact particle system equal weight despite wide adoption geophysical science mathematical analysis accuracy filter predominantly confine setting linear dynamical model linear observation operator analysis beyond linear gaussian setting still infancy short note provide accessible overview recent work author take first step analyze accuracy filter beyond linear gaussian setting',\n",
       " 'Title: Sequential Monte Carlo with Gaussian Mixture Approximation for\\n  Infinite-Dimensional Statistical Inverse Problems\\nAuthors: Haoyu Lu, Junxiong Jia, Deyu Meng\\nTopic: inverse problem pde\\nKeywords: inverse problem pde, statistical inverse problem, framework quantify, framework quantify uncertainty, inference problem, differential equation, computationally intensive likelihood function, partial differential, quantify uncertainty, general framework quantify, formulate inverse, statistical inference, infinite - dimensional function space, infinite - dimensional statistical inverse problem, problem partial differential, sequential monte carlo, statistical inverse, formulate inverse problem, inverse problem, inverse problem formulate, infinite - dimensional statistical inverse, general framework, pde induce computationally, statistical inference problem, inverse problem partial, partial differential equation\\nIntro: \\nLemmatized Chunk: formulate inverse problem partial differential equation pde statistical inference problem bayesian approach provide general framework quantify uncertainty inverse problem pde parameter define infinite dimensional function space pde induce computationally intensive likelihood function additionally sparse datum tend lead posterior feature make difficult apply exist sequential monte carlo smc algorithm overcome difficulty propose new condition likelihood function construct gaussian mixture base precondition crank nicolson transition kernel demonstrate universal approximation property infinite dimensional gaussian mixture probability measure combine three novel tool propose new smc algorithm name new algorithm obtain convergence theorem allow gaussian prior illustrate sequential particle filter actually reproduce true posterior distribution furthermore propose new algorithm rigorously define infinite dimensional function space naturally exhibit discretization invariant property numerical experiment demonstrate new approach strong ability probe multi modality posterior significantly reduce computational burden numerically exhibit discretization invariant property important large scale problem',\n",
       " 'Title: General reproducing properties in RKHS with application to derivative\\n  and integral operators\\nAuthors: Fatima-Zahrae El-Boukkouri, Josselin Garnier, Olivier Roustant\\nTopic: kernel hilbert space\\nKeywords: kernel hilbert space, paper, property rkh, reproduce property reproduce, reproduce property, establish reproduce property, space, reproduce property rkh, derivative integral operator, general reproduce property, reproduce kernel, rkh, property reproduce kernel, kernel hilbert, reproduce kernel hilbert space, property, reproduce property hold, general reproduce, integral operator, operator minimal condition, combination composition operator, kernel, reproduce, reproduce kernel hilbert, revisit sufficient condition, hilbert space\\nIntro: \\nLemmatized Chunk: paper consider reproduce property reproduce kernel hilbert space rkh establish reproduce property closure class combination composition operator minimal condition allow revisit sufficient condition reproduce property hold derivative operator well existence mean embed function result provide framework application representer theorem regularize learning algorithm involve datum function value gradient operator consider class',\n",
       " 'Title: The Gaussian central limit theorem for a stationary time series with\\n  infinite variance\\nAuthors: Muneya Matsui, Thomas Mikosch\\nTopic: gaussian limit\\nKeywords: gaussian limit, gaussian central limit, series tail index, strictly stationary, strictly stationary time series, tail index, stationary time series, theorem, gaussian central limit theorem, time series, theorem strictly, time series infinite, borderline case, limit theorem, time series tail, limit, strictly stationary time, time, theorem stationary, central limit theorem, central limit, alpha, central, series, stationary time\\nIntro: \\nLemmatized Chunk: consider borderline case central limit theorem strictly stationary time series infinite variance gaussian limit iid case well know sufficient condition central limit theorem regular variation marginal distribution tail index dependent case assume strong condition sequential regular variation time series tail index assume sample size time series split block size block sum asymptotically independent apply classical central limit theory row wise iid triangular array necessary sufficient condition independent block sum verify use large deviation result time series derive central limit theorem dependent sequence linear process stochastic volatility process solution affine stochastic recurrence equation whose marginal distribution infinite variance regularly vary tail index',\n",
       " 'Title: The Fundamental Limits of Recovering Planted Subgraphs\\nAuthors: Daniel Lee, Francisco Pernice, Amit Rajaraman, Ilias Zadik\\nTopic: plant subgraph\\nKeywords: plant subgraph, plant subgraph model, fundamental limit recover, limit recover plant, random noise, mmse curve, mmse curve undergoe, graph, recover plant subgraph, limit mmse, subgraph, curve, subgraph model, subgraph model define, mild density assumption, variational formula, plant, fundamental limit, mmse, model define, statistician observe union, limit mmse curve, limit recover, limit mmse curve undergoe, random\\nIntro: \\nLemmatized Chunk: give arbitrary subgraph plant subgraph model define follow statistician observe union random copy together random noise form instance erdo renyi graph n p goal recover plant observe graph focus work understand minimum mean square error mmse sufficiently large recent paper characterize graph limit mmse curve undergoe sharp phase transition increase behavior know nothing phenomenon mild density assumption paper provide formula limit mmse curve graph mild density assumption curve express term variational formula pair subgraph inspire celebrate subgraph expectation threshold probabilistic combinatorics literature furthermore give polynomial time description optimizer variational problem allow one efficiently approximately compute mmse curve dense graph large enough proof rely novel graph decomposition well new minimax theorem may independent interest result generalize setting minimax rate recover arbitrary monotone boolean property plant random noise statistician observe union plant minimal element subseteq n monotone property random p vector setting provide variational formula inspire call fractional expectation threshold describe mmse curve case multiplicative constant large enough',\n",
       " 'Title: On the Functoriality of Belief Propagation Algorithms on finite\\n  Partially Ordered Sets\\nAuthors: Grégoire Sergeant-Perthuis, Toby St Clere Smithe, Léo Boitel\\nTopic: pairwise interaction\\nKeywords: pairwise interaction, model, undirected graphical, putative pairwise, factor graph, putative pairwise interaction, learn capture, model machine learning, capture prior, knowledge putative pairwise, machine learning capture, partially order set undirect, machine learning, order set undirect graphical, finite partially order set, set undirected graphical model, graphical model, finite partially order, class probabilistic model, prior knowledge putative, capture prior knowledge, order set undirect, learn capture prior, interaction, partially order set, widely use class, finite partially, partially order, undirected graphical model\\nIntro: \\nLemmatized Chunk: undirected graphical model widely use class probabilistic model machine learning capture prior knowledge putative pairwise interaction variable interaction encode graph pairwise interaction however generalization factor graph account high degree interaction use hypergraph inference model perform conditioning observe variable typically approximately optimize free energy instance variational inference belief propagation algorithm dynamic programming algorithm find critical point free energy recent effort make unify extend inference graphical model factor graph expressive probabilistic model synthesis work show inference graphical model factor graph generalization rely introduction presheave associate invariant homology cohomology group propose study impact transformation presheave onto associate message pass algorithm show natural transformation presheave associate graphical model generalization understand coherent binning set value variable induce morphism associate message pass algorithm knowledge first result functoriality loopy belief propagation',\n",
       " 'Title: Optimal Data Splitting for Holdout Cross-Validation in Large Covariance\\n  Matrix Estimation\\nAuthors: Lamia Lamrani, Christian Bongiorno, Marc Potters\\nTopic: covariance matrix\\nKeywords: covariance matrix, datum split, statistical tool, matrix estimation cross-validation, large, improve large covariance matrix, matrix dimension, split holdout cross-validation, covariance matrix estimation, optimal datum split, large covariance matrix estimation, improve large, matrix estimation, cross-validation, covariance, optimal train - test split scale, covariance matrix estimation cross-validation, observe practical application, square root, large covariance, improve large covariance, matrix, cross - validation large covariance, datum split holdout, fold cross -validation method converge, large covariance matrix, holdout cross-validation large, fold cross-validation method, formal proof currently lack, remain largely intuitive\\nIntro: \\nLemmatized Chunk: cross validation statistical tool use improve large covariance matrix estimation although efficiency observe practical application theoretical reason behind remain largely intuitive formal proof currently lack carry analytical analysis focus holdout method single iteration rather traditional fold approach derive close form expression estimation error population matrix follow white inverse wishart distribution observe optimal train test split scale square root matrix dimension general population matrice connect error variance eigenvalue distribution approximation necessary interestingly high dimensional asymptotic regime holdout fold method converge optimal estimator train test ratio scale square root matrix dimension',\n",
       " 'Title: Nonlinear Bayesian Update via Ensemble Kernel Regression with Clustering\\n  and Subsampling\\nAuthors: Yoonsang Lee\\nTopic: bayesian update\\nKeywords: bayesian update, characterize non-gaussian prior, characterize non-gaussian, setting characterize non-gaussian, nonlinear bayesian, propose extend traditional, measurement operator, setting characterize, prior ensemble, filter setting, non-gaussian prior, traditional ensemble kalman filter, subsampling nonlinear bayesian update, subsample nonlinear bayesian, kalman filter setting, nonlinear bayesian update, traditional ensemble kalman, extend traditional ensemble, extend traditional, filter setting characterize, prior ensemble propose, ensemble kalman filter, traditional ensemble, ensemble kernel regression, nonlinear measurement operator, extend traditional ensemble kalman\\nIntro: \\nLemmatized Chunk: nonlinear bayesian update prior ensemble propose extend traditional ensemble kalman filter setting characterize prior nonlinear measurement operator framework observe component first denoise via standard kalman update unobserved component estimate use nonlinear regression approach base kernel density estimation method incorporate subsampling strategy ensure stability necessary employ unsupervised clustering refine conditional estimate numerical experiment lorenz system inverse problem illustrate propose nonlinear update reduce estimation error compare standard linear update especially highly nonlinear scenario',\n",
       " 'Title: Dynamic Investment Strategies Through Market Classification and\\n  Volatility: A Machine Learning Approach\\nAuthors: Jinhui Li, Wenjia Xie, Luis Seco\\nTopic: enhance portfolio management\\nKeywords: enhance portfolio management, investment framework enhance, offer clear, market classification volatility, evaluate four conventional approach, traditional static strategy, portfolio management volatile, clear advantage traditional, study introduce dynamic, learn approach study, classification volatility, clear advantage, approach study, advantage traditional static, static strategy, offer clear advantage, advantage traditional, approach study introduce, framework enhance portfolio, study introduce, management volatile market, investment framework, framework enhance, management volatile, dynamic investment framework\\nIntro: \\nLemmatized Chunk: study introduce dynamic investment framework enhance portfolio management volatile market offer clear advantage traditional static strategy evaluate four conventional approach equal weight minimum variance maximum diversification equal risk contribution dynamic condition use k mean clustering market segment ten volatility base state transition forecast bayesian markov switch model employ dirichlet prior gibb sampling enable real time asset allocation adjustment test across two asset set dynamic portfolio consistently achieve significantly high risk adjust return substantially high total return outperform static method integrate classical optimization machine learning bayesian technique research provide robust strategy optimize investment outcome unpredictable market environment',\n",
       " 'Title: A Bivariate Poisson-Gamma Distribution: Statistical Properties and\\n  Practical Applications\\nAuthors: Indranil Ghosh, Mina Norouzirad, Filipe J. Marques\\nTopic: bivariate poisson - gamma\\nKeywords: bivariate poisson - gamma, collection assume conditional, receive considerable attention, bivariate probability, collection assume, distribution - bivariate poisson - gamma conditional, assume conditional distribution, statistical property practical, assume conditional, conditional distribution - introduce, probability model, bivariate probability model, bivariate poisson - gamma conditional distribution -, property practical, poisson - gamma conditional distribution -, bivariate poisson - gamma distribution, receive considerable, poisson - gamma conditional distribution - introduce, bivariate distribution - bivariate poisson - gamma, distribution - bivariate poisson - gamma, considerable attention, bivariate distribution - bivariate, property practical application, practical application, bivariate poisson - gamma conditional, specification bivariate probability\\nIntro: \\nLemmatized Chunk: although specification bivariate probability model use collection assume conditional distribution novel concept receive considerable attention last decade study bivariate distribution bivariate poisson gamma conditional distribution introduce combine univariate continuous discrete distribution work explore aspect model structure statistical inference study paper contribute field statistical modeling distribution theory use maximum likelihood estimation along simulation analysis real datum',\n",
       " 'Title: Finite sample expansions for semiparametric plug-in estimation and\\n  inference for BTL model\\nAuthors: Vladimir Spokoiny\\nTopic: inference top - rank problem\\nKeywords: inference top - rank problem, paper revisit finding, btl model, model present surprising, present surprising result, present surprising, componentwise estimation, semiparametric plug - estimation, full dimensional, recent paper, mathbbmsl, cite, critical dimension condition, inference btl model, full dimensional estimation, componentwise estimation inference, btl model recent, model present, weak condition, estimation, present paper revisit, surprising result, top - rank problem, model recent paper, dimensional estimation, plug - semiparametric estimation\\nIntro: \\nLemmatized Chunk: recent paper cite estimation inference top rank problem bradley terry lice btl model present surprising result componentwise estimation inference much weak condition number comparison require full dimensional estimation present paper revisit finding completely different viewpoint namely show theoretical study estimation sup norm reduce analysis plug semiparametric estimation latter adopt extend general approach cite high dimensional estimation main tool analysis theory perturb marginal optimization objective function depend low dimensional target parameter along high dimensional nuisance parameter particular focus study critical dimension condition full dimensional estimation require general condition effective parameter dimension effective sample size corresponding small eigenvalue fisher information matrix mathbbmsl inference estimate parameter even demand condition generally avoid see however sup norm estimation critical dimension condition reduce geq const log compare cite propose approach work classical mle require resampling procedure apply general structure comparison graph yield accurate expansion component parameter vector',\n",
       " 'Title: Asymptotic Normality in LAD Polynomial Regression and Hilbert Matrices\\nAuthors: Saïd Maanan, Azzouz Dermoune, Ahmed El Ghini\\nTopic: asymptotic\\nKeywords: asymptotic, heavy - tail noise outlier, lad estimator, polynomial regressor, asymptotic normality, asymptotic normality lad, absolute deviation, regression linear model, lad polynomial, lad, linear model polynomial, investigate asymptotic, paper investigate asymptotic, heavy - tail noise, lad polynomial regression, linear model, normality lad polynomial, normality lad estimator, establish multiscale asymptotic, hilbert matrice, robustness heavy - tail noise, matrice paper investigate, investigate asymptotic property, multiscale asymptotic normality, paper investigate, model polynomial regressor, hilbert matrice paper\\nIntro: \\nLemmatized Chunk: paper investigate asymptotic property least absolute deviation lad regression linear model polynomial regressor highlight robustness heavy tail noise outlier assume independent identically distribute error establish multiscale asymptotic normality lad estimator central result derivation asymptotic precision matrix show proportional hilbert matrice proportionality coefficient depend asymptotic variance sample median noise distribution far explore estimator convergence property probability almost surely vary model specification comprehensive simulation evaluate speed convergence lad estimator empirical coverage probability confidence interval construct different scaling factor experiment incorporate range noise distribution include laplace gaussian cauchy demonstrate estimator robustness efficiency finding underscore versatility practical relevance lad regression handle datum environment connect statistical property lad estimator classical mathematical structure hilbert matrice study offer theoretical insight practical tool robust statistical modeling',\n",
       " 'Title: A Note on Local Linear Regression for Time Series in Banach Spaces\\nAuthors: Florian Heinrichs\\nTopic: estimate smoothly vary\\nKeywords: estimate smoothly vary, banach space - value time series, linear regression, linear regression banach, banach space - value time, space - value time series, series estimate smoothly, non-stationary datum, time series estimating, local linear, work extend local linear, time series, regression banach, linear regression time, local linear regression, work extend local, regression time series, extend local linear regression, space work extend, extend local, banach space work, derivative non-stationary datum, extend local linear, work extend, note local linear, regression banach space - value\\nIntro: \\nLemmatized Chunk: work extend local linear regression banach space value time series estimate smoothly vary means derivative datum asymptotic property standard bias reduce jackknife estimator analyze mild moment condition establish convergence rate simulation study assess finite sample performance estimator compare nadaraya watson estimator additionally propose method apply smooth eeg recording reconstruct eye movement video analysis detect pedestrian abandon object',\n",
       " 'Title: Inferring diffusivity from killed diffusion\\nAuthors: Richard Nickl, Fanny Seizilles\\nTopic: diffusivity parameter\\nKeywords: diffusivity parameter, infinite - dimensional diffusion parameter, inferr diffusivity, domain unknown, unknown diffusivity parameter, euclidean domain unknown, insulate euclidean, diffusion, diffusivity, euclidean domain, insulate euclidean domain, inferr diffusivity kill, independent molecule, process, domain unknown diffusivity, binding, molecule insulated, diffusivity kill diffusion, diffusion markov process, bind stop diffuse, unknown diffusivity, stop diffuse dependence, bind potential, diffusion independent molecule\\nIntro: \\nLemmatized Chunk: consider diffusion independent molecule insulate euclidean domain unknown diffusivity parameter random time position molecule may bind stop diffuse dependence give binding potential binding process model additive random functional corresponding canonical construction kill diffusion markov process study problem conduct inference infinite dimensional diffusion parameter histogram plot killing position process show first position follow poisson point process whose intensity measure determine solution certain odinger equation inference problem inverse problem pde show consistently solvable bayesian way natural condition initial state diffusion provide binding potential aggressive course proof obtain novel posterior contraction rate result high dimensional poisson count datum independent interest numerical illustration algorithm standard mcmc method also provide',\n",
       " 'Title: The Field Equations of Penalized non-Parametric Regression\\nAuthors: Sven Pappert\\nTopic: field equation penalize\\nKeywords: field equation penalize, penalize non-parametric, penalize, establish euler - lagrange field, equation penalize non-parametric, risk comprise, minimizer risk involve, view, non-parametric regression, find minimizer risk, variation, lens, field, risk involve, view penalize risk, mse penalize, penalize non-parametric regression, field equation, calculus variation, equation, calculus, non-parametric regression view, approach find minimizer, find minimizer, view penalize, regression view, euler - lagrange field equation, regression view penalize, penalize risk, risk\\nIntro: \\nLemmatized Chunk: view penalize risk lens calculus variation consider risk comprise fitness term mse gradient base penalty establish euler lagrange field equation systematic approach find minimizer risk involve first derivative proceed exemplify approach mse penalize integral square norm gradient regression function minimizer risk give solution second order inhomogeneous pde inhomogeneity give conditional expectation target variable condition feature discuss property field equation practical implication thereof also apply classical ridge penalty linear model emb finding exist literature particular find recover rudin osher fatemi model image denoising consider feature deterministic evenly distribute last outline several direction future research',\n",
       " 'Title: Hazard Rate for Associated Data in Deconvolution Problems: Asymptotic\\n  Normality\\nAuthors: Benjrada Mohammed Essalih\\nTopic: datum deconvolution problem\\nKeywords: datum deconvolution problem, survival analysis, normality reliability theory, asymptotic normality, additive measurement error, additive measurement, subject additive measurement, weakly dependent subject, subject additive, underlying datum, observe datum, hazard rate, asymptotic normality reliability, theory survival, dependent subject, theory survival analysis, independent strongly mix, reliability theory, reliability theory survival, measurement error, weakly dependent, distribution function estimator, datum, distribution function\\nIntro: \\nLemmatized Chunk: reliability theory survival analysis observe datum often weakly dependent subject additive measurement error contamination arise underlying datum neither independent strongly mix instead exhibit association paper focus estimate hazard rate deconvolve density function construct estimator distribution function assume datum originate strictly stationary sequence satisfy association condition appropriate smoothness assumption error distribution establish quadratic mean convergence asymptotic normality propose estimator finite sample performance hazard rate distribution function estimator evaluate simulation study conclude discussion open problem potential future research direction',\n",
       " 'Title: Testing Conditional Stochastic Dominance at Target Points\\nAuthors: Federico A. Bugni, Ivan A. Canay, Deborah Kim\\nTopic: target point paper\\nKeywords: target point paper, test, conditional stochastic dominance, target, utilize induce order statistic, paper introduce, evaluate treatment effect, conditioning covariate, test control asymptotic size, induce order statistic converge, point, stochastic dominance target, csd, stochastic, induce order statistic, test statistic, order statistic, stochastic dominance, point paper introduce, test conditional stochastic dominance, test feature datum - independent, dominance target point, analyze income inequality, critical, test control asymptotic, induce order, conditional stochastic, point paper, refer target, target point\\nIntro: \\nLemmatized Chunk: paper introduce novel test conditional stochastic dominance csd specific value conditioning covariate refer target point test relevant analyze income inequality evaluate treatment effect study discrimination propose kolmogorov smirnov type test statistic utilize induce order statistic independent sample notably test feature datum independent critical value eliminate need resampling technique bootstrap approach avoid kernel smoothing parametric assumption instead rely tuning parameter select relevant observation establish asymptotic property test show induce order statistic converge independent draw true conditional distribution test control asymptotic size weak regularity condition result apply continuous discrete datum discrete case critical value provide valid upper bind address propose refine critical value significantly enhance power require knowledge support size distribution additionally analyze test behavior limit experiment demonstrate reduce problem analogous test unconditional stochastic dominance finite sample framework allow prove validity permutation base test stochastic dominance random variable continuous monte carlo simulation confirm strong finite sample performance method',\n",
       " 'Title: On the Precise Asymptotics of Universal Inference\\nAuthors: Kenta Takatsu\\nTopic: confidence set\\nKeywords: confidence set, procedure typically evaluate, confidence set approach, level confidence set approach, level confidence set, statistical inference, width property, procedure achieve rate - optimal, achieve rate - optimal width, universal inference, universal inference statistical, confidence set procedure, level confidence, set approach, set procedure typically, evaluate base, inference statistical inference, typically evaluate base, validity width property, confidence, typically evaluate, set procedure, procedure achieve rate - optimal width\\nIntro: \\nLemmatized Chunk: statistical inference confidence set procedure typically evaluate base validity width property even procedure achieve rate optimal width confidence set still excessively wide practice due elusive constant lead extreme conservativeness empirical coverage probability nominal level confidence set approach one manuscript study gap validity conservativeness use universal inference wasserman et regular parametric model model misspecification run example identify source asymptotic conservativeness propose general remedy base studentization bias correction result method attain exact asymptotic coverage nominal level even model misspecification provide product estimation error two unknown negligible exhibit intrigue resemblance double robustness semiparametric theory',\n",
       " 'Title: The broken sample problem revisited: Proof of a conjecture by Bai-Hsing\\n  and high-dimensional extensions\\nAuthors: Simiao Jiao, Yihong Wu, Jiaming Xu\\nTopic: classical break sample\\nKeywords: classical break sample, classical break, problem, sample problem revisite, break sample problem, revisit classical break, revisit classical, cdot ,, break, classical, extension revisit, observe without correspondence, break sample, bai-hsing high - dimensional, cdot, point, classical break sample problem, datum point, hypothesis, mathbf, break sample problem revisite, leq, high - dimensional extension revisit, null hypothesis, high - dimensional extension, bai-hsing high - dimensional extension, datum, revisit, sample problem, problem revisite\\nIntro: \\nLemmatized Chunk: revisit classical break sample problem two sample datum point observe without correspondence null hypothesis independent alternative hypothesis correlate random subsample sense draw independently bivariate distribution latent injection n originally introduce degroot feder goel model matching record census datum problem recently gain renew interest due application datum datum integration target tracking despite extensive research past decade determine precise detection threshold remain open problem even equal sample size assume grow proportionally show sharp threshold give spectral condition likelihood ratio operator resolve conjecture bai hsing positive result extend high dimension settle sharp detection threshold gaussian bernoulli model',\n",
       " 'Title: Minimizers of U-processes and their domains of attraction\\nAuthors: Dietmar Ferger\\nTopic: asymptotic\\nKeywords: asymptotic, domain, paper, standard square - root asymptotics, study, identify broad class, smirnov, result, domain attraction, minimizer u-process, u - process domain, u - process arise, main result establish, statistical context, u - process, study minimizer, estimator, attraction, class, objective function, statistical, minimizer\\nIntro: \\nLemmatized Chunk: paper study minimizer domain attraction u process arise various statistical context particularly estimator define minimizer certain objective function main result establish necessary sufficient condition distributional convergence minimizer identify broad class normalize sequence go beyond standard square root asymptotics normal limit show limit distribution belong exactly one four class introduce smirnov result extend smirnov theory also generalize exist asymptotic theory include classical result huber extension high degree furthermore analyze domain attraction class provide alternative characterization determine type statistical estimator fall give asymptotic regime',\n",
       " 'Title: Robust tests for log-logistic models based on minimum density power\\n  divergence estimators\\nAuthors: A. Felipe, M. Jaenada, P. Miranda, L. Pardo\\nTopic: parameter\\nKeywords: parameter, density power divergence, survival analysis, distribution, reliability engineering, test, minimum density power, minimum density power divergence, family widely, log - logistic distribution, versatile parametric family, log - logistic, density power divergence estimator, test statistic, base minimum density, apply field, rao test statistic, include survival, rao test, versatile parametric, parametric family widely, parametric family, statistic, power divergence estimator, include survival analysis, versatile parametric family widely\\nIntro: \\nLemmatized Chunk: log logistic distribution versatile parametric family widely use across various apply field include survival analysis reliability engineering econometrics estimate parameter log logistic distribution hypothesis testing necessary verify assumption parameter wald test rao test provide formal method test hypothese parameter however test statistic robust rejection decision may affect datum contamination paper develop new family wald type test statistic test statistic base minimum density power divergence estimator mdpde parameter log logistic distribution new family generalize wald rao test statistic inherit robustness property mdpde thus address lack robustness classical test explicit expression test statistic log logistic model simple composite null hypothese derive property analyze detail extensive simulation study empirically demonstrate robustness family compare performance classical method',\n",
       " 'Title: Optimizing High-Dimensional Oblique Splits\\nAuthors: Chien-Ming Chi\\nTopic: sparse oblique tree\\nKeywords: sparse oblique tree, grow oblique tree, suggest oblique split, vec, oblique split orthogonal - split, paper explore optimize high - dimensional, evidence suggest, orthogonal - split tree, oblique split enhance, optimize high - dimensional, high - dimensional oblique split, oblique, optimize high - dimensional oblique, orthogonal - split tree perform, sparse oblique, oblique split orthogonal - split tree, split orthogonal - split tree perform, split orthogonal - split tree, sid function class, optimize high - dimensional oblique split, oblique split, sparse oblique split, oblique tree, suggest oblique, split alongside orthogonal split, evidence suggest oblique, split, oblique split alongside orthogonal, evidence suggest oblique split, high - dimensional oblique split orthogonal - split\\nIntro: \\nLemmatized Chunk: orthogonal split tree perform well evidence suggest oblique split enhance performance paper explore optimize high dimensional sparse oblique split grow oblique tree user define sparsity parameter establish connection sid convergence sparse oblique split show sid function class expand increase enable capture complex datum generate function dimensional xor function thus represent unknown potential complexity underlying datum generate function learn complex function require sparse oblique tree geq great computational resource highlight trade statistical accuracy govern sid function class size depend computational cost contrast previous study explore problem sid convergence use orthogonal split runtime less critical additionally introduce practical framework oblique tree integrate optimize oblique split alongside orthogonal split random forest propose approach assess simulation real datum experiment compare performance various oblique tree model',\n",
       " 'Title: A New Proof of Sub-Gaussian Norm Concentration Inequality\\nAuthors: Zishun Liu, Yongxin Chen\\nTopic: sub -gaussian\\nKeywords: sub -gaussian, norm concentration inequality, concentration inequality present, moment generate, proof, concentration, moment generate function term, sub - gaussian norm concentration inequality, proof sub-gaussian norm, concentration inequality, sub -gaussian norm, generate function, average moment generate function, norm concentration, net technique - base proof, moment generate function, norm, sub - gaussian norm concentration, generate function term, inequality, average moment generating, proof sub-gaussian\\nIntro: \\nLemmatized Chunk: present new proof sub gaussian norm concentration inequality proof base average version moment generate function term average moment generate function compare widely adopt net technique base proof sub norm concentration inequality method rely union bind promise tight concentration bind',\n",
       " \"Title: Confidence Intervals Using Turing's Estimator: Simulations and\\n  Applications\\nAuthors: Jie Chang, Michael Grabchak, Jialin Zhang\\nTopic: application ture estimator\\nKeywords: application ture estimator, related confidence, simulation study understand, related confidence interval, finite sample performance, confidence interval use turing, confidence interval, understand finite sample, turing, interval, finite sample, estimate probability, simulation application turing, outcome, sample, probability outcome, sample performance, random sample, perform simulation study, probability, simulation study, estimator, estimate, interval use turing estimator, turing estimator, confidence, rarely, random\\nIntro: \\nLemmatized Chunk: turing estimator allow one estimate probability outcome either appear rarely appear give random sample perform simulation study understand finite sample performance several related confidence interval cis introduce approach select appropriate ci give sample give application problem authorship attribution apply dataset comprise tweet user x twitter far derive several theoretical result asymptotic normality asymptotic poissonity turing estimator two important discrete distribution\",\n",
       " 'Title: Asymptotic properties of the MLE in distributional regression under\\n  random censoring\\nAuthors: Gitte Kremling, Gerhard Dikta\\nTopic: dataset\\nKeywords: dataset, distribution, distributional, family, candidate distribution, aim distributional regression, maximum likelihood estimator, regression, regression find, asymptotic property, distributional regression random, property mle, mle distributional regression, distribution parameter, distribution family, candidate, conditional distribution, parametric family conditional, aim distributional, random censor aim, parametric family, distribution model, family conditional distribution, distributional regression, regression random censoring, family conditional, model give dataset, conditional distribution model\\nIntro: \\nLemmatized Chunk: aim distributional regression find good candidate give parametric family conditional distribution model give dataset candidate distribution family identify corresponding distribution parameter common approach task use maximum likelihood estimator mle parameter paper establish theoretical result estimator case response variable subject random right censoring particular provide proof almost sure consistency asymptotic normality mle censoring far finite sample behavior exemplarily demonstrate simulation study',\n",
       " 'Title: The covariance of causal effect estimators for binary v-structures\\nAuthors: Jack Kuipers, Giusi Moffa\\nTopic: compute variance\\nKeywords: compute variance, journal causal, 90-105, causal effect estimator, v - structure previously, inference, journal causal inference, v - structure binary variable, variance, binary v-structure, causal, effect estimator binary, journal, previously, low variance, binary v -structure previously, estimator causal, estimator low variance, causal inference, estimator causal effect, estimator binary v -structure, causal effect, covariance causal, estimator, effect estimator, covariance causal effect, effect v - structure, binary, v - structure binary, binary variable\\nIntro: \\nLemmatized Chunk: previously journal causal inference compute variance two estimator causal effect v structure binary variable show linear combination estimator low variance either furthermore show hold also treatment variable block randomise predefined number receive treatment analogous result sample randomly',\n",
       " 'Title: Stratified Permutational Berry--Esseen Bounds and Their Applications to\\n  Statistics\\nAuthors: Pengfei Tian, Fan Yang, Peng Ding\\nTopic: stratify linear\\nKeywords: stratify linear, permutation statistic arise, stratify post - stratify experiment, permutation statistic, post - stratify survey sampling, include stratify, post - stratify experiment, linear permutation statistic, permutation test, unified stratify permutational berry, statistic problem, stratify linear permutation statistic, conditional permutation test, stratify post - stratify survey, stratify permutational berry, post - stratify survey, conditional permutation, esseen bound, linear permutation statistic arise, survey sampling, linear permutation, include stratify post - stratify, permutational berry, stratify linear permutation\\nIntro: \\nLemmatized Chunk: stratify linear permutation statistic arise various statistic problem include stratify post stratify survey sampling stratify post stratify experiment conditional permutation test although derive berry esseen bound stratify linear permutation statistic base exist bound statistic bound sharp moreover strategy work general setting heterogeneous strata vary size first use stein method obtain unified stratify permutational berry esseen bind accommodate heterogeneous strata apply bind various statistic problem lead strong theoretical quantification thereby facilitate statistical inference problem',\n",
       " 'Title: Spectrally-Corrected and Regularized QDA Classifier for Spiked\\n  Covariance Model\\nAuthors: Wenya Luo, Hua Li, Zhidong Bai, Zhijun Liu\\nTopic: preferable linear\\nKeywords: preferable linear, quadratic discriminant analysis, qda classifier spike, linear discriminant, analysis, heterogeneous datum, spike covariance, spike covariance model, covariance model quadratic, spike covariance model quadratic, regularize quadratic discriminant analysis, discriminant, quadratic discriminant, discriminant analysis, classifier spike covariance, method classification problem, method, datum dimension sample, covariance model, classifier spike, classification problem, model quadratic discriminant analysis, preferable linear discriminant, model quadratic discriminant, lda, qda, sr-qda, linear discriminant analysis, datum, problem, covariance model quadratic discriminant\\nIntro: \\nLemmatized Chunk: quadratic discriminant analysis qda widely use method classification problem particularly preferable linear discriminant analysis lda heterogeneous datum however qda lose effectiveness high dimensional setting datum dimension sample size tend infinity address issue propose novel qda method utilize spectral correction regularization technique term regularization parameter method select maximize fisher discriminant ratio compare qda qda regularize quadratic discriminant analysis qda several competitor result indicate sr qda perform exceptionally well especially moderate high dimensional situation empirical experiment across diverse dataset far support conclusion',\n",
       " \"Title: Stein's method of moment estimators for local dependency exponential\\n  random graph models\\nAuthors: Adrian Fischer, Gesine Reinert, Wenkai Xu\\nTopic: largely open problem\\nKeywords: largely open problem, maximum likelihood, open problem, exponential random graph, maximum likelihood estimation, random graph model, exponential random graph model, local dependency exponential, model provide theoretical guarantee, provide theoretical guarantee, random graph, theoretical guarantee, dependency exponential random graph, exponential random, local dependency exponential random, random graph model provide, dependency exponential random, method moment estimator, graph model provide theoretical, graph model, estimation, provide theoretical, theoretical guarantee parameter, largely open, likelihood estimation, random\\nIntro: \\nLemmatized Chunk: provide theoretical guarantee parameter estimation exponential random graph model largely open problem maximum likelihood estimation theoretical guarantee principle verify assumption guarantee hold difficult moreover complex network numerical maximum likelihood estimation computer intensive may converge reasonable time ameliorate issue local dependency exponential random graph model introduce assume network consist many independent exponential random graph setting progress towards maximum likelihood estimation make however estimation still computer intensive instead propose use call stein estimator use stein characterization obtain new estimator local dependency exponential random graph model\",\n",
       " \"Title: Spearman's rho for bivariate zero-inflated data\\nAuthors: Jasper Arends, Elisa Perrone\\nTopic: rho\\nKeywords: rho, spearman rank correlation, quantify, quantify association, spearman rho specifically design, estimation technique common, bivariate zero - inflate, datum quantify association, random variable crucial, variable crucial, random variable, bivariate zero - inflate datum, common association measure, traditional estimation technique, association measure, zero - inflate datum quantify, spearman rho specifically, technique common association, spearman rho base, association, spearman rho, spearman, spearman rho bivariate, bivariate zero - inflate datum quantify, rho bivariate zero - inflate, zero - inflate datum, spearman rank correlation coefficient, datum quantify, rank correlation coefficient\\nIntro: \\nLemmatized Chunk: quantify association two random variable crucial application traditional estimation technique common association measure spearman rank correlation coefficient often fail datum contain tie particularly problematic zero inflate context field like insurance healthcare weather forecasting zero frequent require extra probability mass paper provide new formulation spearman rho specifically design zero inflate datum propose novel estimator spearman rho base derive expression besides make propose estimator useful practice derive achievable bound suggest estimate analyze method comprehensive simulation study show approach overcome state art method simulate scenario additionally illustrate propose theory use practice accurate quantification association consider two real life application\",\n",
       " 'Title: On a conjecture of Roverato regarding G-Wishart normalising constants\\nAuthors: Ching Wong, Giusi Moffa, Jack Kuipers\\nTopic: remain computationally intensive\\nKeywords: remain computationally intensive, remain computationally, intensive task, gaussian graphical, bayesian analysis, computationally intensive task, component bayesian, gaussian graphical model, core component, computationally intensive, core component bayesian, gram - wishart normalise constant, graphical model, bayesian analysis gaussian, evaluation g-wishart, component bayesian analysis, analysis gaussian graphical, analysis gaussian, gram - wishart normalising, intensive task general, evaluation g-wishart normalising\\nIntro: \\nLemmatized Chunk: evaluation normalising constant core component bayesian analysis gaussian graphical model remain computationally intensive task general base empirical evidence roverato scandinavian journal statistic observe conjecture constant simplify rewritt term constant identity scale matrix note disprove conjecture general graph show conjecture instead imply independently derive approximation certain ratio normalise constant',\n",
       " 'Title: Parameter estimation for generalized mixed fractional stochastic heat\\n  equation\\nAuthors: B. L. S. Prakasa Rao\\nTopic: mix fractional stochastic heat\\nKeywords: mix fractional stochastic heat, estimation generalize mix, fractional stochastic heat equation, mix fractional stochastic, fractional brownian, heat equation study, generalize mix fractional stochastic, mix fractional brownian noise, stochastic heat, fractional brownian noise, brownian noise, noise, fractional stochastic, stationarity obtain bound, stochastic, equation generalize, heat equation, mix fractional brownian, equation, parameter estimation generalize, stochastic heat equation, heat, fractional stochastic heat, generalize mix fractional brownian, covariance structure, obtain covariance structure, mix fractional, generalize mix fractional, generalize mix\\nIntro: \\nLemmatized Chunk: study property stochastic heat equation generalize mix fractional brownian noise obtain covariance structure stationarity obtain bound asymptotic behaviour solution suggest estimator unknown parameter base discrete time observation study asymptotic property',\n",
       " 'Title: Estimating stationary mass, frequency by frequency\\nAuthors: Milind Nakul, Vidya Muthukumar, Ashwin Pananjady\\nTopic: potentially large state space\\nKeywords: potentially large state space, stationary mass, frequency, state space, finite potentially large, mix stochastic process, potentially large state, mix stochastic, mixing, frequency frequency suppose, estimate stationary mass, frequency suppose observe, trajectory length, process, sequence, observe trajectory, frequency suppose, stochastic process, suppose observe, large state space, finite potentially, estimator, estimate stationary, alpha, large state, potentially large, estimate probability mass\\nIntro: \\nLemmatized Chunk: suppose observe trajectory length mix stochastic process finite potentially large state space consider problem estimate probability mass place stationary distribution process element occur certain frequency observe sequence estimate vector probability total variation distance show universal consistency recover know result sequence special case propose methodology carefully combine plug empirical estimator recently propose modification good turing estimator call wingit originally develop markovian sequence en route control error estimator develop new performance bound wingit plug estimator mix stochastic process importantly extensively use method poissonization long apply non setting develop complementary tool include concentration inequality natural self normalize statistic mixing sequence may prove independently useful design analysis estimator related problem',\n",
       " 'Title: Asymptotic Expansions of Gaussian and Laguerre Ensembles at the Soft\\n  Edge II: Level Densities\\nAuthors: Folkmar Bornemann\\nTopic: laguerre random matrix ensemble .\\nKeywords: laguerre random matrix ensemble ., prove asymptotic expansion, skew - orthogonal polynomial term, continue work, establish asymptotic expansion, term wave function, gaussian laguerre, dimensional gaussian laguerre, laguerre random matrix, obtain concise expression, matrix ensemble . revisit, random matrix ensemble ., soft edge, laguerre random, expansion level, dimensional gaussian, asymptotic expansion, gaussian laguerre random, gaussian laguerre ensemble, level density continue, asymptotic expansion gaussian, asymptotic expansion power, level density, suit prove asymptotic, random matrix\\nIntro: \\nLemmatized Chunk: continue work arxiv asymptotic expansion soft edge classical dimensional gaussian laguerre random matrix ensemble revisit construction associate skew orthogonal polynomial term wave function obtain concise expression level density well suit prove asymptotic expansion power certain parameter h unitary case expansion level density use reconstruct first correction term establish asymptotic expansion associate generating function orthogonal symplectic case even reconstruct conjecture first second correction term',\n",
       " 'Title: On self-training of summary data with genetic applications\\nAuthors: Buxin Su, Jiaoyang Huang, Jin Jin, Bingxin Zhao\\nTopic: biomedical research\\nKeywords: biomedical research, due privacy concern, model, summary datum, limited access, genetic application prediction model, limited access individual - level, individual - level datum due, prediction, privacy concern logistical, application prediction model training, application prediction model, datum due privacy, hinder limited, logistical challenge, hinder limited access, due privacy, model training, training, self - training, concern logistical, datum, concern logistical challenge, privacy concern\\nIntro: \\nLemmatized Chunk: prediction model training often hinder limited access individual level datum due privacy concern logistical challenge particularly biomedical research resampling base self training present promising approach build prediction model use summary level datum method leverage summary statistic sample pseudo dataset model training parameter optimization allow model development without individual level datum although increasingly use precision medicine general behavior self training remain unexplored paper leverage random matrix theory framework establish statistical property self training algorithm high dimensional sparsity free summary datum demonstrate within class linear estimator resampling base self training achieve asymptotic predictive accuracy conventional training method require individual level dataset result suggest self training summary datum incur additional cost prediction accuracy offer significant practical convenience analysis provide several valuable insight counterintuitive finding example pseudo training validation dataset inherently dependent interdependence unexpectedly cancel calculate prediction accuracy measure prevent overfitt self training algorithm furthermore extend analysis show self training framework maintain cost advantage combine multiple method jointly train datum different distribution numerically validate finding simulation real datum analysis use uk biobank study highlight potential resampling base self training advance genetic risk prediction field make summary datum publicly available',\n",
       " 'Title: Optimal ANOVA-based emulators of models with(out) derivatives\\nAuthors: Matieyendou Lamboni\\nTopic: paper propose\\nKeywords: paper propose, model, approach, local stochastic evaluation, propose new anova - base approximation, high - dimensional model, mse, parametric rate convergence, optimal anova - base emulator, derivative local stochastic, dimension - free, local stochastic, anova - base emulator, anova - base approximation, anova - base, optimal anova- base, paper propose new anova- base, propose new anova- base, emulator, derivative paper propose, high - dimensional, anova - base approximation function, stochastic evaluation, derivative, anova - base emulator model, approximation function\\nIntro: \\nLemmatized Chunk: paper propose new anova base approximation function emulator high dimensional model use either available derivative local stochastic evaluation model approach make use sensitivity index design adequate structure emulator high dimensional model available derivative derivative base emulator reach dimension free mean square error mse parametric rate convergence approach extend cope every model without available derivative derive global emulator account local property model simulator generic emulator enjoy dimension free bias parametric rate convergence mse depend dimensionality dimension free mse obtain high dimensional model particular input distribution emulator also competitive deal different distribution input variable select input interaction simulation show efficiency approach',\n",
       " 'Title: Two statistical problems for multivariate mixture distributions\\nAuthors: Ricardo Fraiman, Leonardo Moreno, Thomas Ransford\\nTopic: finite set line\\nKeywords: finite set line, mixture multivariate, work arxiv, ambient dimension, important statistical problem, number line depend, total number distribution, multivariate normal distribution, predetermine finite set, finite set, number distribution involve, multivariate gaussian, distinguish project, number line, total number, statistical problem, set line, mixture multivariate normal, mixture multivariate gaussian, problem multivariate mixture, line depend, predetermine finite, early work arxiv, multivariate mixture distribution\\nIntro: \\nLemmatized Chunk: early work arxiv show mixture multivariate gaussian distribution distinguish project onto certain predetermine finite set line number line depend total number distribution involve ambient dimension use work address follow two important statistical problem testing measure agreement two different random partition estimate mixture multivariate normal distribution mixture distribution base univariate projection also compare proposal robust version expectation maximization method case present algorithm effect task compare exist method carry simulation',\n",
       " 'Title: Training Diagonal Linear Networks with Stochastic Sharpness-Aware\\n  Minimization\\nAuthors: Gabriel Clara, Sophie Langer, Johannes Schmidt-Hieber\\nTopic: training dynamic\\nKeywords: training dynamic, small isotropic, landscape training dynamic, stochastic sharpness - aware minimization, dynamic sharpness, perturb small, linear regression task, linear network stochastic, underlying landscape training, training diagonal linear, network stochastic sharpness - aware, sharpness - aware minimization analyze, small isotropic normal, normal noise, sharpness - aware minimization, perturb small isotropic, landscape training, isotropic normal, diagonal linear network, regression task, network parameter perturb, training diagonal linear network, linear network, small isotropic normal noise, network linear, training dynamic diagonal, isotropic normal noise\\nIntro: \\nLemmatized Chunk: analyze landscape training dynamic diagonal linear network linear regression task network parameter perturb small isotropic normal noise addition noise may interpret stochastic form sharpness aware minimization sam prove several result relate action underlying landscape training dynamic sharpness loss particular noise change expect gradient force balancing weight matrice fast rate along descent trajectory diagonal linear model show equate minimize average sharpness well trace hessian matrix among possible factorization matrix far noise force gradient descent iterate towards shrinkage thresholding underlying true parameter noise level explicitly regulate shrinkage factor threshold',\n",
       " 'Title: Excess Mean Squared Error of Empirical Bayes Estimators\\nAuthors: Yue Ju, Bo Wahlberg, Håkan Hjalmarsson\\nTopic: bay\\nKeywords: bay, estimate observe datum, estimate observe, function estimate, mse expression, mse, excess mse expression, empirical bay estimator, empirical bay, bay estimator, base minimize, minimize average risk, square error, bay estimator empirical bay, observe datum, weighting function, estimator, empirical bay estimator empirical, minimize average, excess mse, empirical, empirical bay estimator employ, estimator empirical bay estimator, weighting function estimate\\nIntro: \\nLemmatized Chunk: empirical bay estimator base minimize average risk hyper parameter weighting function estimate observe datum performance empirical bay estimator typically evaluate mean square error mse however explicit expression mse generally unavailable finite sample size address issue define high order analytical criterion excess mse quantify performance difference maximum likelihood empirical bay estimator explicit expression excess mse empirical bay estimator employ general datum dependent hyper parameter estimator derive specific instance provide excess mse expression kernel base regularize estimator use scal empirical bay stein unbiased risk estimation generalize hyper parameter estimator moreover propose modification excess mse expression regularize estimator moderate sample size show improvement accuracy numerical simulation',\n",
       " 'Title: Proposal for the Application of Fractional Operators in Polynomial\\n  Regression Models to Enhance the Determination Coefficient $R^2$ on Unseen\\n  Data\\nAuthors: Anthony Torres-Hernandez\\nTopic: model\\nKeywords: model, generally quite reliable, polynomial regression model, regression model generally, unseen datum, regression model, regression, training phase, encounter overfitting, fractional operator polynomial, result negative, linear trend, operator polynomial regression, polynomial regression, coefficient determination, determination unseen datum, proposal application, fractional operator, regression model enhance, overfitt issue, important note, fractional regression model, fractional regression, application fractional operator, encounter overfitt issue\\nIntro: \\nLemmatized Chunk: since polynomial regression model generally quite reliable datum linear trend important note case may encounter overfitt issue training phase could result negative value coefficient determination unseen datum reason work propose partial implementation fractional operator polynomial regression model generate fractional regression model goal proposal attempt mitigate overfitting could improve value coefficient determination unseen datum compare polynomial model assumption would contribute generate predictive model good performance methodology construct fractional regression model detailed example applicable riemann liouville caputo fractional operator present',\n",
       " 'Title: Towards practical PDMP sampling: Metropolis adjustments, locally\\n  adaptive step-sizes, and NUTS-based time lengths\\nAuthors: Augustin Chevallier, Sam Power, Matthew Sutton\\nTopic: piecewise - deterministic markov process\\nKeywords: piecewise - deterministic markov process, hold significant, sampling complex probability, locally adaptive step - size, locally adaptive, length piecewise - deterministic markov process, metropolis adjustment, time length piecewise - deterministic, process, markov, probability distribution, hold significant promise, complex probability, adaptive step - size, piecewise - deterministic markov, significant promise sampling, significant, sampling, complex probability distribution, nut - base time length, compute model - specific bound, time length piecewise - deterministic markov, piecewise - deterministic, sample complex distribution, significant promise, nut - base time length piecewise - deterministic, hold, practical implementation hinder, markov process, nut - base time\\nIntro: \\nLemmatized Chunk: piecewise deterministic markov process pdmp hold significant promise sampling complex probability distribution however practical implementation hinder need compute model specific bound conversely hamiltonian monte carlo hmc offer generally efficient approach sampling inability adaptively tune step size impede performance sample complex distribution like funnel address limitation introduce three innovative concept metropolis adjust approximation pdmp simulation eliminate need explicit bound without compromise invariant measure benefit adaptive step size mechanism compatible metropolis correction c sampler nut inspire scheme dynamically select path length pdmp three idea seamlessly integrate single doubly adaptive pdmp sampler favourable robustness efficiency property',\n",
       " \"Title: On continuity of Chatterjee's rank correlation and related dependence\\n  measures\\nAuthors: Jonathan Ansari, Sebastian Fuchs\\nTopic: rank correlation xi recently\\nKeywords: rank correlation xi recently, continuity chatterjee rank, weakly continuous respect, share property, recently introduce, blomqvist beta, statistical inference point, correlation xi recently, kendall tau, cause drawback statistical, drawback statistical inference, correlation xi recently introduce, related dependence measure, rank correlation related, cause drawback, chatterjee rank, continuous respect, ucher dette, spearman rho, introduce azadkia, recently introduce azadkia, rank correlation, chatterjee rank correlation\\nIntro: \\nLemmatized Chunk: measure concordance spearman rho kendall tau blomqvist beta continuous respect weak convergence chatterjee rank correlation xi recently introduce azadkia chatterjee share property cause drawback statistical inference point ucher dette study paper xi instead weakly continuous respect conditionally independent copy markov product establish weak continuity markov product provide several sufficient condition include copula base criterion condition rely concept conditional weak convergence sweeting consequence also obtain continuity result xi related dependence measure verify continuity parameter standard model multivariate elliptical norm symmetric distribution\",\n",
       " \"Title: The pushed beta distribution and contaminated binary sampling\\nAuthors: Ben O'Neill\\nTopic: push beta\\nKeywords: push beta, generalisation beta, beta distribution, distribution, call, unit interval generalise, binary sampling examine, contaminate, beta distribution contaminate, beta, call push beta, continuous univariate distribution, contaminate binary sampling, density kernel, additional multiplicative term, generalisation, binary sampling, call push, generalise beta distribution, contaminate binary, push beta distribution, examine, binary, push, generalise beta, density\\nIntro: \\nLemmatized Chunk: examine generalisation beta distribution call push beta distribution continuous univariate distribution unit interval generalise beta distribution push density particular direction use additional multiplicative term density kernel examine property distribution compare beta distribution also examine use distribution contaminate binary sampling use bayesian inference find distribution arise appropriate posterior distribution inference certain kind contaminate binary model derive broad range property distribution also establish computational method compute various function distribution\",\n",
       " 'Title: Statistical Impossibility and Possibility of Aligning LLMs with Human\\n  Preferences: From Condorcet Paradox to Nash Equilibrium\\nAuthors: Kaizhao Liu, Qi Long, Zhekun Shi, Weijie J. Su, Jiancong Xiao\\nTopic: ensure fairness inform\\nKeywords: ensure fairness inform, fairness inform, align large language, llm, large language, inform outcome, equilibrium align large, probabilistic preference model, nash equilibrium align large, critical ensure, preference, align llm, align large language model, human preference, inform outcome deploy, diverse human preference, ensure fairness, equilibrium align large language, fairness inform outcome, outcome deploy, critical ensure fairness, large language model, human\\nIntro: \\nLemmatized Chunk: align large language model llm diverse human preference critical ensure fairness inform outcome deploy model decision making paper seek uncover fundamental statistical limit concern align llm human preference focus probabilistic representation human preference preservation diverse preference align llm first show human preference represent reward model preference among llm generate response free condorcet cycle moreover prove condorcet cycle exist probability converg one exponentially fast probabilistic preference model thereby demonstrate impossibility fully align human preference use reward base approach reinforcement learning human feedback next explore condition llm would employ mix strategy mean collapse single response align limit use base approach nash learning human feedback nlhf identify necessary sufficient condition mix strategy absence response prefer majority blessing prove condition hold high probability probabilistic preference model thereby highlight statistical possibility preserve minority preference without explicit regularization align llm finally leverage insight statistical result design novel computationally efficient algorithm find nash equilibria align llm nlhf experiment show llama benefit align algorithm achieve win rate base model',\n",
       " 'Title: A New Design-Based Variance Estimator for Finely Stratified Experiments\\nAuthors: Yuehao Bai, Xun Huang, Joseph P. Romano, Azeem M. Shaikh, Max Tabord-Meehan\\nTopic: treatment effect\\nKeywords: treatment effect, effect finely stratify, stratify, design - base, stratify experiment, finely, treatment, upward - biased, treatment assignment, problem design - base inference, design - base inference, problem design - base, estimator finely stratify, quality, variance estimator finely, design - base variance estimator, show, average treatment, estimator, stratify experiment paper, average treatment effect, treatment effect finely, design - base variance, finely stratify experiment, finely stratify\\nIntro: \\nLemmatized Chunk: paper consider problem design base inference average treatment effect finely stratify experiment design base mean source uncertainty stem randomness treatment assignment finely stratify mean unit first stratify group size thousand accord baseline covariate within group fix number l k assign uniformly random treatment remainder control setting first show mild condition inference use difference means estimator require estimator variance least asymptotically upward biased present novel estimator variance show upward biased furthermore magnitude bias depend natural way quality stratification importantly estimator remain well define even setting l thousand l compare estimator well know estimator propose previously case first show estimator also upward biased magnitude bias change natural way quality stratification far discriminate among estimator introduce framework motivate thought experiment finite population model draw fashion well behave probability distribution framework argue estimator dominate term limit bias improvement strict except exceptionally strong restriction treatment effect finally illustrate theoretical result simulation study reveal estimator lead substantially precise inference especially quality stratification high',\n",
       " 'Title: Nonparametric Exponential Family Regression Under Star-Shaped\\n  Constraints\\nAuthors: Guanghong Yi, Matey Neykov\\nTopic: nonparametric exponential family\\nKeywords: nonparametric exponential family, nonparametric exponential family regression, constraint study, exponential family, star - shape constraint study, star - shape, regression, exponential family regression, study minimax rate, nonparametric, minimax rate estimation, family regression, nonparametric exponential, minimax, operatorname, regression star - shape, regression star - shape constraint, minimax rate, family regression star - shape, star - shape constraint, constraint, star - shape set contain, varepsilon, estimation nonparametric exponential, estimation nonparametric\\nIntro: \\nLemmatized Chunk: study minimax rate estimation nonparametric exponential family regression star shape constraint specifically parameter space star shape set contain within bound box know positive constant moreover assume exponential family nonsingular cumulant function twice continuously differentiable main result show minimax rate problem wedge diam thousand absolute constant define sup varepsilon million leq denot local entropy absolute constant allow depend also provide example derive corresponding minimax optimal rate',\n",
       " 'Title: On the Injective Norm of Sums of Random Tensors and the Moments of\\n  Gaussian Chaoses\\nAuthors: Ishaq Aden-Ali\\nTopic: ell\\nKeywords: ell, chain argument, norm sum, prove upper, result, bind, sum, random tensor, injective, subgaussian random, gaussian chaose prove, expect, sum random, injective norm sum, subgaussian random tensor, tensor moment, upper bound, sum random tensor, geometric chaining argument, injective norm, norm, prove upper bind, sum subgaussian, sum subgaussian random, explicit geometric chaining, moment gaussian chaose, tensor, random\\nIntro: \\nLemmatized Chunk: prove upper bind expect injective norm sum subgaussian random tensor proof simple rely explicit geometric chaining argument instead follow simple application pac bayesian lemma tool prove effective control suprema certain smooth empirical process recent year bind strictly improve recent result bandeira gopi jiang lucca rothvoss euclidean case bind sharpen result lata central prove estimate moment gaussian chaose consequence obtain elementary proof fundamental result',\n",
       " 'Title: Generalized network autoregressive modelling of longitudinal networks\\n  with application to presidential elections in the USA\\nAuthors: Guy Nason, Daniel Salnikov, Mario Cortina-Borja\\nTopic: infer community structure\\nKeywords: infer community structure, generalize network, network autoregressive modelling, longitudinal, generalize network autoregressive, dynamic process, generalise network autoregressive, increasingly relevant, generalize network autoregressive modelling, time series, exploit underlying network, study dynamic process, autoregressive modelling longitudinal, community structure, autoregressive modelling, application presidential election, network, dynamic process characterise, time, textit, gnar, network autoregressive, infer community, multivariate time series, series, longitudinal network, analyse high - dimensional longitudinal network, process characterise\\nIntro: \\nLemmatized Chunk: longitudinal network become increasingly relevant study dynamic process characterise know infer community structure generalise network autoregressive gnar model provide parsimonious framework exploit underlying network multivariate time series introduce community gnar model interaction exploit prior knowledge exogenous variable analyse interaction within community describe serial correlation longitudinal network derive new explicit finite sample error bound validate analyse high dimensional longitudinal network datum gnar model provide insight attractive property far illustrate approach analyse dynamic swing state throughout presidential election usa time series length twelve time series states washington dc analysis connect network autocorrelation eight year long term highlight possible change system election difference behaviour state',\n",
       " 'Title: Batch List-Decodable Linear Regression via Higher Moments\\nAuthors: Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Sihan Liu, Thanasis Pittas\\nTopic: linear regression\\nKeywords: linear regression, size, unknown linear, study task, batch size, batch, linear regression use batch, regression via high moment, task list - decodable, list - decodable linear regression, list, list - decodable, delta, linear regression via high, list - decodable linear, batch list - decodable linear regression, linear regression distribution, batch list - decodable, algorithm, linear, task list - decodable linear, unknown linear regression distribution, alpha, sos, high moment study, unknown linear regression, batch list - decodable linear\\nIntro: \\nLemmatized Chunk: study task list decodable linear regression use batch batch call clean consist sample unknown linear regression distribution parameter unknown fraction batch clean assumption make remain one goal output small list vector least one close true regressor vector norm djk give efficient algorithm natural distributional assumption follow guarantee assume batch size satisfy geq tilde number batch mathrm poly would n algorithm run polynomial time output list vector least one close target regressor design new polynomial time algorithm significantly strong guarantee assumption low degree moment covariate distribution sum square sos certifiably bound specifically constant long batch size degree delta moment covariate sos certifiably bound algorithm use mathrm poly dn batch run polynomial time output size list vector one close target algorithm achieve substantially small minimum batch size final error achieve optimal list size approach use high order moment information carefully combine sos paradigm interleave iterative method novel list pruning procedure process give sos proof marcinkiewicz zygmund inequality may broad applicability',\n",
       " 'Title: Minimax Optimality of the Probability Flow ODE for Diffusion Models\\nAuthors: Changxiao Cai, Gen Li\\nTopic: model score - base diffusion model\\nKeywords: model score - base diffusion model, ode diffusion model, complex high - dimensional distribution, paradigm modern generative, optimality probability, generative modeling, score - base diffusion model, model score - base diffusion, exceptional capability, modern generative modeling, diffusion model score - base diffusion, generate sample, score - base diffusion, diffusion model score - base, probability flow ode - base sampler, capability generating, foundational paradigm, exceptional capability generating, modern generative, capability generate sample, demonstrate exceptional capability, probability flow ode, diffusion model, paradigm modern, demonstrate exceptional, sample complex high - dimensional, foundational paradigm modern, flow ode diffusion, generate sample complex\\nIntro: \\nLemmatized Chunk: score base diffusion model become foundational paradigm modern generative modeling demonstrate exceptional capability generate sample complex high dimensional distribution despite dominant adoption probability flow ode base sampler practice due superior sampling efficiency precision rigorous statistical guarantee method remain elusive literature work develop first end end theoretical framework deterministic ode base sampler establish near minimax optimal guarantee mild assumption target datum distribution specifically focus subgaussian distribution h old smooth density propose smooth regularize score estimator simultaneously control score error associate mean jacobian error leverage estimator within refine convergence analysis ode base sampling process demonstrate result sampler achieve minimax rate total variation distance modulo logarithmic factor notably theory comprehensively account source error sampling process require strong structural condition density low bound lipschitz smooth score target distribution thereby cover broad range practical datum distribution',\n",
       " 'Title: Parameter estimation for the stochastic Burgers equation driven by white\\n  noise from local measurements\\nAuthors: Josef Janák, Enrico Priola\\nTopic: diffusivity parameter\\nKeywords: diffusivity parameter, diffusivity parameter front, equation drive, estimation stochastic, parameter front, equation drive trace, spatial derivative, stochastic burger equation, dimensional stochastic burger, second - order spatial derivative, dimensional stochastic, second - order spatial, linear stochastic heat equation, stochastic burger, parameter estimation, space - time white noise case, space - time white noise, burger equation, burger equation drive, stochastic burger equation drive, white noise, dimensional stochastic burger equation, problem estimation\\nIntro: \\nLemmatized Chunk: one dimensional stochastic burger equation drive space time white noise consider problem estimation diffusivity parameter front second order spatial derivative base local observation space study estimator derive altmeyer probab linear stochastic heat equation also use altmeyer cialenco pasemann bernoulli cover large class semilinear spde examine stochastic burger equation drive trace class noise extend achieve result consider space time white noise case also relevant physical motivation establish new regularity result solution able show propose estimator strongly consistent asymptotically normal',\n",
       " 'Title: Competing-risk Weibull survival model with multiple causes\\nAuthors: Kai Wang, Yuqin Mu, Shenyi Zhang, Zhengjun Zhang, Chengxiu Ling\\nTopic: inappropriate unavailable\\nKeywords: inappropriate unavailable, simultaneous effect multiple, parametric weibull accelerate, parametric weibull accelerate failure, multiple cause failure, approach enable simultaneous estimation, compete - risk weibull survival model, weibull accelerate failure time, aetiology neurodegenerative disease, accelerate failure time model, failure system, assign specific, weibull survival model, survival model multiple, simultaneous effect, compete - risk weibull, system result, survival model, weibull survival, time - vary win probability, effect multiple, compete - risk weibull survival, failure time model, time model multiple, accelerate failure time, enable simultaneous estimation, propose parametric weibull, weibull accelerate failure\\nIntro: \\nLemmatized Chunk: failure system result simultaneous effect multiple cause assign specific cause may inappropriate unavailable example include contribute cause death epidemiology aetiology neurodegenerative disease like alzheimer propose parametric weibull accelerate failure time model multiple cause incorporate datum drive individualized time vary win probability relative importance matrix use maximum likelihood estimation expectation maximization algorithm approach enable simultaneous estimation regression coefficient relative cause importance ensure consistency asymptotic normality simulation study application alzheimer disease demonstrate effectiveness address cause mixture problem identify informative biomarker combination comparison weibull cox proportional hazard model',\n",
       " 'Title: Low-Rank Graphon Estimation: Theory and Applications to Graphon Games\\nAuthors: Olga Klopp, Fedor Noskov\\nTopic: piecewise - constant graphon base\\nKeywords: piecewise - constant graphon base, tackle challenge, sample network datum, estimate low - rank graphon, graphon game, employ singular, adjacency matrix, graphon, network datum, singular value thresholding, paper tackle challenge, sample network, challenge estimating, estimate graphon, graphon sample network, theory application, low - rank estimate graphon, network adjacency matrix, create piecewise - constant graphon, low - rank graphon estimation, create piecewise - constant, piecewise - constant graphon, network adjacency, graphon estimator\\nIntro: \\nLemmatized Chunk: paper tackle challenge estimate low rank graphon sample network datum employ singular value thresholding svt estimator create piecewise constant graphon base network adjacency matrix certain assumption graphon structural property establish bound operator norm distance true graphon estimator well rank estimate graphon second part paper apply estimator graphon game derive bound suboptimality intervention social welfare problem graphon game intervention base estimate graphon bound express term operator norm difference true estimate graphon also emphasize computational benefit use low rank estimate graphon solve problem',\n",
       " 'Title: Addressing pitfalls in implicit unobserved confounding synthesis using\\n  explicit block hierarchical ancestral sampling\\nAuthors: Xudong Sun, Alex Markham, Pratik Misra, Carsten Marr\\nTopic: causal model\\nKeywords: causal model, unobserved confound synthesis, block hierarchical ancestral sampling, address pitfall implicit, unobserved confounding, ancestral sampling unbiased datum, encode unobserved confounding, block hierarchical ancestral, explicit block hierarchical ancestral, implicit unobserved confounding, sample unbiased datum, datum synthesis crucial, unbiased datum synthesis, evaluate causal discovery, scarcity real - world dataset, implicit unobserved confound synthesis, causal, causal discovery, evaluate causal, crucial evaluate causal, evaluate causal discovery algorithm, sample unbiased datum synthesis, hierarchical ancestral sampling unbiased, causal discovery algorithm, synthesis crucial, real - world dataset, scarcity real - world, space causal model, presence unobserved confounding, explicit block hierarchical, datum synthesis, crucial evaluate\\nIntro: \\nLemmatized Chunk: unbiased datum synthesis crucial evaluate causal discovery algorithm presence unobserved confounding give scarcity real world dataset common approach implicit parameterization encode unobserve confounding modify diagonal entrie idiosyncratic covariance matrix preserve positive definiteness within approach identify state art protocol two distinct issue hinder unbiased sampling complete space causal model first give detailed analysis use diagonally dominant construction restrict spectrum partial correlation matrice second restriction possible graphical structure sample bidirect edge unnecessarily rule valid causal model address limitation propose improve explicit modeling approach unobserved confounding leverage block hierarchical ancestral generation ground truth causal graph algorithm convert ground truth dag ancestral graph provide output causal discovery algorithm could compare draw connection implicit explicit parameterization prove approach fully cover space causal model include generate implicit parameterization thus enable robust evaluation method causal discovery inference',\n",
       " 'Title: \"All-Something-Nothing\" Phase Transitions in Planted k-Factor Recovery\\nAuthors: Julia Gaudio, Colin Sandon, Jiaming Xu, Dana Yang\\nTopic: problem undergoe\\nKeywords: problem undergoe, erdo - renyi random graph, factor, paper study problem, erdo - renyi random, plant within erdos-renyi, exact, problem, k - factor recovery paper, paper study, specifically span, random graph, k - factor recovery, transition, lambda, transition plant k- factor, plant k- factor recovery, phase transition plant, plant, theta, exact recovery, phase transition, plant k- factor, regular graph, recovery paper study, recovery, study problem, phase\\nIntro: \\nLemmatized Chunk: paper study problem inferr factor specifically span regular graph plant within erdo renyi random graph n n uncover interesting something nothing phase transition specifically show average degree surpass critical threshold inference problem undergoe transition almost exact recovery phase partial recovery something phase moreover tend infinity accuracy recovery diminish zero lead onset nothing phase finding complement recent result mossel niles weed sohn sun zadik establish certain sufficiently dense graph problem undergoe nothing phase transition jump near perfect near zero recovery addition characterize recovery accuracy linear time iterative pruning algorithm show achieve almost exact recovery key component analysis two step cycle construction first build tree local neighborhood exploration connect sprinkle use reserve edge interestingly prove impossibility almost exact recovery construct n many small tree size whereas establish algorithmic low bind single large tree size suffice',\n",
       " 'Title: Distribution and Moments of a Normalized Dissimilarity Ratio for two\\n  Correlated Gamma Variables\\nAuthors: Elise Colin, Razvigor Ossikovski\\nTopic: correlate gamma distribution\\nKeywords: correlate gamma distribution, correlate gamma variable ., dynamic speckle imaging, identical scale shape, normalize dissimilarity, linear correlation, correlate gamma, variable, correlation coefficient, characterize identical, linear correlation coefficient, random variable, identical scale, correlate gamma variable, scale shape parameter, parameter linear, independent exponential variable, exponential random variable, correlate exponential random, dissimilarity ratio, correlate, characterize identical scale, normalize dissimilarity ratio, correlate exponential random variable\\nIntro: \\nLemmatized Chunk: consider two random variable follow correlate gamma distribution characterize identical scale shape parameter linear correlation coefficient focus parameter x frac appear apply context dynamic speckle imaging know fujii work derive close form expression probability density function x well analytical formula moment order derivation start represent two correlate exponential random variable obtain square magnitude circular complex gaussian variable consider sum independent exponential variable derive joint density x two correlate gamma variable appropriate varable transformation obtain theoretical distribution x evaluate moment analytically theoretical finding validate numerical simulation particular attention two specific case zero correlation unit shape parameter',\n",
       " 'Title: TransPCA for Large-dimensional Factor Analysis with Weak Factors: Power\\n  Enhancement via Knowledge Transfer\\nAuthors: Yong He, Dong Liu, Yunjing Sun, Yalin Wang\\nTopic: weak factor\\nKeywords: weak factor, power enhancement via knowledge, dimensional approximate factor, cross -section unit, transfer early work establish, large dimensional approximate, definite limit, large dimensional approximate factor, knowledge transfer early work, transfer principal component analysis, approximate factor model, transfer early work, lambda, convergence rate weak, early work, scale sublinearly, sublinearly number, work establish convergence, establish convergence rate, establish convergence, early work establish convergence, rate weak factor, dimensional approximate factor model, large - dimensional factor analysis, early work establish, convergence rate\\nIntro: \\nLemmatized Chunk: early work establish convergence principal component estimator factor loading rotation large dimensional approximate factor model weak factor factor loading scale sublinearly number unit positive definite limit however establish convergence rate weak factor much slow especially small article propose transfer principal component analysis transpca method enhance convergence rate weak factor transfer knowledge large number available informative panel dataset turn blind eye big datum era aggregate useful information analyze weight average projection matrix estimate loading space informative dataset highly flexible computationally efficient theoretically derive convergence rate estimator weak strong loading space factor score result indicate long auxiliary dataset similar enough target dataset auxiliary sample size sufficiently large transpca estimator achieve fast convergence rate contrast perform pca solely target dataset avoid negative transfer also investigate case informative dataset unknown provide criterion select useful dataset thorough simulation study analysis real dataset area macroeconomic finance conduct illustrate usefulness propose method large number source panel dataset naturally available',\n",
       " 'Title: Pointwise Minimax Vector Field Reconstruction from Noisy ODE\\nAuthors: Hugo Henneuse\\nTopic: ordinary differential\\nKeywords: ordinary differential, ode, differential equation, field noisy, estimate vector field, pointwise minimax vector, solution ode, non-parametric regression, minimax vector field, non-parametric regression setting, govern dynamical system, estimate vector, problem estimating, ordinary differential equation, address problem, work address problem, vector field, minimax vector field reconstruction, noisy ordinary, vector field reconstruction, ode work address, noisy ode work, random design, work address, pointwise minimax vector field, noisy ordinary differential, noisy ode, random design initial, noisy ordinary differential equation\\nIntro: \\nLemmatized Chunk: work address problem estimate vector field noisy ordinary differential equation ode regression setting random design initial value specifically give vector field govern dynamical system define autonomous ode assume observation solution ode time initial condition sample probability distribution noise context investigate minimax perspective pointwise reconstruction within envelope trajectory originate support propose estimation strategy base preliminary flow reconstruction technique derivative estimation regression mild assumption establish convergence rate depend temporal resolution number sample initial value mass concentration importantly show rate minimax optimal furthermore discuss implication result manifold learning setting provide insight approach mitigate curse dimensionality',\n",
       " 'Title: Generation and Balancing Capacity in Future Electric Power Systems --\\n  Scenario Analysis Using Bayesian Networks\\nAuthors: Seppo Borenius, Pekka Kekolahti, Petri Mähönen, Matti Lehtonen\\nTopic: electric power system\\nKeywords: electric power system, paper examine, grid management, capacity future electric, finnish electric energy, extensive bayesian network, finnish electric energy system, minimise grid management, paper examine evolution, construct bayesian network, future electric, future electric power, construct bayesian, development path, generation balancing capacity, balancing capacity future, future electric power system, power generation capacity mix, examine evolution, scenario analysis use bayesian, construct bayesian network approach, paper development, minimise grid management complexity, finnish power grid, grid management scenario, grid management complexity, bayesian network, generation capacity mix, electric energy system\\nIntro: \\nLemmatized Chunk: paper examine evolution finnish electric energy system focus likelihood different development path primary contribution paper development extensive bayesian network design model analyse evolution power generation capacity mix assess likelihood different grid management scenario understand causal relationship underlie scenario target optimisation carry use construct bayesian network explore possibility minimise grid management complexity result optimisation reveal authority stakeholder prioritise increase demand response gas power battery storage capacity mature technology well suit guarantee energy adequacy peak consumption period finland typically occur consecutive cold dark windless winter week although study focus evolution finnish power grid construct bayesian network approach broadly applicable utilise explore causal relationship country employ design questionnaire engage panel expert specific country energy infrastructure',\n",
       " 'Title: Empirical Error Estimates for Graph Sparsification\\nAuthors: Siyao Wang, Miles E. Lopes\\nTopic: graph sparsification\\nKeywords: graph sparsification, well - establish technique accelerate, accelerate graph - base learning, graph - base learning, approximate dense, error estimate, sampling approximate, technique accelerate graph - base, graph - base learning algorithm, learn algorithm, accelerate graph - base learning algorithm, dense graph sparse, edge sampling approximate, approximate dense graph, sampling approximate dense, accelerate graph - base, edge sampling, empirical error estimate, well - establish technique, error, technique accelerate\\nIntro: \\nLemmatized Chunk: graph sparsification well establish technique accelerate graph base learning algorithm use edge sampling approximate dense graph sparse one sparsification error random unknown user must contend uncertainty reliability downstream computation although possible user obtain conceptual guidance theoretical error bound literature result typically impractical numerical level take alternative approach propose address issue datum drive perspective computing empirical error estimate propose error estimate highly versatile demonstrate four use case laplacian matrix approximation graph cut query graph structure regression spectral clustering moreover provide two theoretical guarantee error estimate explain cost computing manageable comparison overall cost typical graph sparsification workflow',\n",
       " 'Title: The Hidden Toll of COVID-19 on Opioid Mortality in Georgia: A Bayesian\\n  Excess Opioid Mortality Analysis\\nAuthors: Cyen J. Peterkin, Lance A. Waller, Emily N. Peterson\\nTopic: user exacerbate health\\nKeywords: user exacerbate health, opioid user population, vulnerable population, opioid mortality, scale negative, scale negative impact, opioid user exacerbate, user exacerbate, opioid mortality analysis, large scale negative impact, opioid, excess opioid mortality, large scale negative, assess excess opioid mortality, hide toll, mortality, bayesian excess opioid mortality, opioid user, hierarchical excess opioid mortality, excess opioid mortality modeling, large scale, excess opioid mortality analysis, negative impact, bayesian hierarchical excess opioid, excess opioid, bayesian excess opioid\\nIntro: \\nLemmatized Chunk: covid large scale negative impact health opioid user exacerbate health already vulnerable population critical information total impact covid opioid user unknown due lack comprehensive datum covid case inaccurate diagnostic coding lack datum coverage assess impact covid small area opioid mortality develop bayesian hierarchical excess opioid mortality modeling approach incorporate spatio temporal autocorrelation structure allow sharing information across small area time reduce uncertainty small area estimate excess mortality define difference observe trend crisis expect trend base observe historical trend capture total increase observe mortality rate compare expect prior crisis illustrate application approach assess excess opioid mortality risk estimate county use propose approach help inform intervention opioid related public health response policy resource allocation application work also provide general framework improve estimation mapping health indicator crisis period opioid user population',\n",
       " 'Title: Concentration via Metastable Mixing, with Applications to the\\n  Supercritical Exponential Random Graph Model\\nAuthors: Vilas Winstein\\nTopic: model exhibit high - temperature behavior\\nKeywords: model exhibit high - temperature behavior, low - temperature statistical mechanics model, exponential random graph, statistical mechanics model exhibit, supercritical exponential random graph, supercritical ergm condition, exponential random graph model, random graph model, mechanics model exhibit, random graph, folklore belief metastable, model exhibit high - temperature, mechanics model, low - temperature statistical mechanics, exponential random, supercritical exponential random, exhibit high - temperature behavior, mechanics model exhibit high - temperature, statistical mechanics model, supercritical ergm, folklore belief, statistical mechanics, graph model, exhibit metastable mixing, metastable, metastable mixing, ergm\\nIntro: \\nLemmatized Chunk: folklore belief metastable well low temperature statistical mechanics model exhibit high temperature behavior prove rigorous version phenomenon setting exponential random graph model ergm lens concentration measure first present new general result derive concentration inequality metastable well metastable mixing markov chain appropriate stationary distribution extend result chatterjee suit traditional form global mixing apply result supercritical low temperature ergm recently prove exhibit metastable mixing bresler nagaraj nichani obtain novel concentration inequality lipschitz observable supercritical ergm condition large metastable well answer question pose extend result ganguly nam gn subcritical high temperature regime metastable well supercritical regime also able extend application concentration inequality metastable well namely obtain upper bind wasserstein distance ergm condition metastable well appropriate model well derive central limit theorem count edge certain small subcollection possible edge finally supplement mathematical content article also discuss result appear first simulation study metastable well supercritical ergm',\n",
       " 'Title: Is a Good Foundation Necessary for Efficient Reinforcement Learning? The\\n  Computational Role of the Base Model in Exploration\\nAuthors: Dylan J. Foster, Zakaria Mhammedi, Dhruv Rohatgi\\nTopic: model\\nKeywords: model, reinforcement learning, deliberately encourage, reinforcement, language model alignment, leverage active, super - human capability, language model, pre-trained model, exploration language model alignment, efficient exploration, computationally efficient, efficient reinforcement learning, promise super - human, active exploration, offer promise, efficient reinforcement, language, good foundation, promise super - human capability, efficient, coverage, learning, model alignment, deliberately encourage model, exploration, exploration language model, informative response, pre-trained, technique leverage active, leverage active exploration\\nIntro: \\nLemmatized Chunk: language model alignment reinforcement learning technique leverage active exploration deliberately encourage model produce diverse informative response offer promise super human capability however current understanding algorithm design primitive computationally efficient exploration language model limit well understand leverage access powerful generative model improve efficiency exploration introduce new computational framework rl language model learner interact model sampling oracle focus linear softmax model parameterization provide new result reveal computational statistical tradeoff efficient exploration necessity coverage coverage refer extent model cover near optimal response form hide knowledge show coverage necessary datum efficiency low bound runtime algorithm framework inference time exploration introduce new algorithm spannersampling obtain optimal datum efficiency computationally efficient whenever model enjoy sufficient coverage match low bind spannersampling leverage inference time computation model reduce effective search space exploration insufficiency training time intervention contrast result show training time intervention produce proper policy achieve similar guarantee polynomial time computational benefit exploration finally show additional representational assumption one achieve improve runtime replace sequence level coverage token level coverage exploration',\n",
       " 'Title: Personalized Convolutional Dictionary Learning of Physiological Time\\n  Series\\nAuthors: Axel Roques, Samuel Gruffaz, Kyurae Kim, Alain Oliviero-Durmus, Laurent Oudre\\nTopic: physiological time series\\nKeywords: physiological time series, tend exhibit, convolutional dictionary learning, due biomechanical disposition, physiological signal tend, dataset local - global structure, time series human physiological, time series human, extend convolutional dictionary learning, observe due biomechanical, inter - individual variability, share across population, local structure, series human physiological, series human physiological signal, signal tend exhibit, personalize convolutional dictionary learning, population, convolutional dictionary, physiological signal, present common characteristic, reflect inter-individual, reflect inter- individual variability, human physiological signal, signal tend, human physiological, human physiological signal tend, physiological time series human, biomechanical disposition pathology, locomotion present common characteristic\\nIntro: \\nLemmatized Chunk: human physiological signal tend exhibit global local structure former share across population latter reflect individual variability instance kinetic measurement gait cycle locomotion present common characteristic although idiosyncrasy may observe due biomechanical disposition pathology well represent dataset local global structure work extend convolutional dictionary learning cdl popular method learn interpretable representation dictionary time series datum particular propose personalize cdl percdl local dictionary model local information personalize spatiotemporal transformation global dictionary transformation learnable combine operation time warping rotation formal computational statistical guarantee percdl provide effectiveness synthetic real human locomotion datum demonstrate',\n",
       " 'Title: Estimation of Local Geometric Structure on Manifolds from Noisy Data\\nAuthors: Yariv Aizenbud, Barak Sober\\nTopic: datum common observation\\nKeywords: datum common observation, dimension, common observation datum - drive, low intrinsic dimension, mathcal, low intrinsic, manifold, observation datum - drive, geometric structure manifold, datum - drive application, observation datum - drive application, point, intrinsic dimension, hat, widehat, estimation manifold - value datum, mathbf, point estimation manifold - value, mathbb, local geometric structure, common observation, high - dimensional datum, datum, local geometric, estimation local geometric, geometric structure, noisy datum common\\nIntro: \\nLemmatized Chunk: common observation datum drive application high dimensional datum low intrinsic dimension least locally work consider problem point estimation manifold value datum namely give finite set noisy sample dimensional submanifold point near manifold aim project onto manifold assume datum sample uniformly tubular neighborhood time smooth boundaryless compact manifold present algorithm take neighborhood output element grassmannian would would prove number sample point converge mathcal projection onto converge tangent space point high probability furthermore show approach manifold asymptotic rate k widehat approach correspondingly asymptotic rate thousand',\n",
       " 'Title: Asymptotic normality and strong consistency of kernel regression\\n  estimation in q-calculus\\nAuthors: Emmanuel De Dieu Nkou, Fridolin Melong\\nTopic: asymptotic normality\\nKeywords: asymptotic normality, family, approach nonparametric, regression estimation, regression function base, regression, base, kernel regression estimation, regression estimation q -calculus, function base, sample, construct family, normality strong, regression function, kernel regression, method build operation, property q -calculus, asymptotic normality strong, kernel method build, strong consistency kernel, normality strong consistency, strong consistency, consistency kernel regression, kernel method, estimator, qdistribution, function, convergence strong consistency, weak convergence strong, construct, base sample, family estimator, sample follow qdistribution\\nIntro: \\nLemmatized Chunk: construct family estimator regression function base sample follow qdistribution approach nonparametric use kernel method build operation leverage property furthermore appropriate assumption establish weak convergence strong consistency family estimator',\n",
       " 'Title: The level of self-organized criticality in oscillating Brownian motion:\\n  $n$-consistency and stable Poisson-type convergence of the MLE\\nAuthors: Johannes Brutsche, Angelika Rohde\\nTopic: rho\\nKeywords: rho, prove infill, oscillate brownian, level self - organize criticality, path oscillating, observe path oscillating, oscillate brownian motion, self - organize criticality oscillating, discretely observe, brownian motion level, path oscillate brownian, level self - organize, self - organize criticality, denote sample size, motion level, discretely observe path, observe path, brownian motion, sample size, limit distribution respect, stable poisson - type convergence, criticality oscillate brownian, denote sample\\nIntro: \\nLemmatized Chunk: discretely observe path oscillate brownian motion level self organize criticality prove infill asymptotics mle consistent denote sample size derive limit distribution respect stable convergence transition density homogeneous markov process even continuous analysis highly therefore interesting somewhat unexpected phenomenon occur likelihood function split several component contribute differently depend close argument correspondingly mle successively exclude lay outside compact set neighborhood finally neigborhood asymptotically crucial argument derive stable convergence exploit semimartingale structure sequential suitably rescal local log likelihood function process time sequentially process exhibit bivariate poissonian behavior stable limit intensity multiple local time',\n",
       " 'Title: BASIC: Bipartite Assisted Spectral-clustering for Identifying\\n  Communities in Large-scale Networks\\nAuthors: Tianchen Gao, Jingyuan Liu, Rui Pan, Ao Sun\\nTopic: large - scale network community detection\\nKeywords: large - scale network community detection, academic network dataset, collect large - scale academic network, network community detection, primary network, bipartite assist spectral - clustering approach, fundamental task network, crucial fundamental task, focus recover, bipartite network information, recover group structure, large - scale network community, recover group, fundamental task, bipartite network, primary network information, bipartite assist spectral - clustering, crucial fundamental, large - scale academic network, large - scale academic network dataset, bipartite assist, network, newly collect large - scale academic, network analysis, identify community, task network analysis, network information\\nIntro: \\nLemmatized Chunk: community detection focus recover group structure within network crucial fundamental task network analysis however detection process quite challenging unstable community signal weak motivate newly collect large scale academic network dataset web science include multi layer network information propose bipartite assist spectral cluster approach identify community basic incorporate bipartite network information community structure learning primary network accuracy stability enhancement basic validate theoretically basis degree correct stochastic block model framework well numerically extensive simulation study rigorously study convergence rate basic even weak signal scenario prove basic yield tight upper error bound base primary network information alone utilize propose basic method analyze newly collect large scale academic network dataset statistical paper author collaboration network structure learning incorporate bipartite network information author paper author institution author region relationship statistical interpretative perspective bipartite network greatly aid identify community within primary collaboration network',\n",
       " 'Title: Limit Theorems for One-Dimensional Homogenized Diffusion Processes\\nAuthors: Jaroslav I. Borodavka, Sebastian Krumscheid\\nTopic: diffusion process\\nKeywords: diffusion process, diffusion process present, homogenize diffusion process, specific class one - dimensional, class one - dimensional, one - dimensional homogenize, one - dimensional diffusion, diffusion process depend, one - dimensional diffusion process, homogenize diffusion, one - dimensional homogenize diffusion process, limit theorem, rightarrow, class one - dimensional diffusion, process present, specific class, process limit, converge weakly, process depend, small - scale parameter, central limit theorem, central limit, present two limit theorem, one - dimensional homogenize diffusion, varepsilon\\nIntro: \\nLemmatized Chunk: present two limit theorem mean ergodic central limit theorem specific class one dimensional diffusion process depend small scale parameter converge weakly homogenize diffusion process limit result allow time horizon blow infty novelty result arise circumstance many quantity unbound formerly establish theory directly applicable careful investigation relevant dependent term require mathematical application use limit theorem prove asymptotic property minimum distance estimator parameter homogenize diffusion equation',\n",
       " 'Title: Extremes of structural causal models\\nAuthors: Sebastian Engelke, Nicola Gnecco, Frank Röttger\\nTopic: behavior\\nKeywords: behavior, extremal graph, well - understand time series, series spatial datum, structural, extremal graphical model, extreme structural causal, extremal behavior, time series, causal, extreme observation well - understand, time series spatial, structural causal model, extreme observation, scm, behavior extreme observation, series spatial, well - understand time, structural causal, extremal, generate process, observation well - understand, datum generating process\\nIntro: \\nLemmatized Chunk: behavior extreme observation well understand time series spatial datum little know datum generate process structural causal model scm study behavior extreme model class observational distribution extremal intervention show suitable regularity condition structure function extremal behavior describe multivariate pareto distribution represent new scm extremal graph importantly latter sub graph original scm mean causal link disappear tail far introduce direct version extremal graphical model show extremal scm satisfy corresponding markov property base new test extremal conditional independence propose two algorithm learn extremal causal structure datum first extremal version second pruning algorithm remove edge original graph consistently recover extremal graph method illustrate river datum know causal ground truth',\n",
       " \"Title: Detecting correlation efficiently in stochastic block models: breaking\\n  Otter's threshold by counting decorated trees\\nAuthors: Guanyi Chen, Jian Ding, Shuyang Gong, Zhangsong Li\\nTopic: independent stochastic block model\\nKeywords: independent stochastic block model, stochastic block model, sparse correlate stochastic, stochastic block, pair independent stochastic, common parent stochastic block, divergence parameter, common parent stochastic, parent stochastic block, correlate stochastic block model, symmetric community, block model, epsilon, correlate stochastic block, sparse correlate stochastic block, delta, detect correlation efficiently, break otter threshold, average degree, otter threshold counting, parent stochastic block model, independent stochastic block, pair sparse correlate, threshold counting decorate\\nIntro: \\nLemmatized Chunk: consider pair sparse correlate stochastic block model n subsample common parent stochastic block model two symmetric community average degree divergence parameter construct statistic base combination two low degree polynomial show exist sufficiently small constant sufficiently large constant otter constant statistic distinguish model pair independent stochastic block model n probability also provide efficient algorithm approximate statistic polynomial time crux statistic construction lie carefully curate family multigraph call decorate enable effective aggregation community signal graph correlation count decorate tree suppress undesirable correlation among count different decorate tree\",\n",
       " 'Title: Parameter Estimation for Partially Observed Affine and Polynomial\\n  Processes\\nAuthors: Jan Kallsen, Ivo Richert\\nTopic: paper devote\\nKeywords: paper devote, partially observe polynomial state, polynomial state space, parameter estimation partially, covariance matrix, include discretely observe affine, space model, partially observe, partially observe polynomial, observe polynomial state, state space model, devote parameter estimation, polynomial state space model, observe polynomial state space, parameter estimation, estimation partially, asymptotic covariance matrix, partially observe affine, class include discretely observe, devote parameter, estimation, estimation partially observe, state space, observe affine\\nIntro: \\nLemmatized Chunk: paper devote parameter estimation partially observe polynomial state space model class include discretely observe affine generally polynomial markov process polynomial structure allow explicit computation gaussian estimator asymptotic covariance matrix show consistency asymptotic normality estimating sequence provide explicitly computable expression corresponding asymptotic covariance matrix',\n",
       " 'Title: Filtering of partially observed polynomial processes in discrete and\\n  continuous time\\nAuthors: Jan Kallsen, Ivo Richert\\nTopic: paper devote\\nKeywords: paper devote, polynomial, gaussian process share, observe, discrete continuous time, gaussian, devote filter, filter partially observe, observe polynomial, polynomial process discrete, partially observe, partially, partially observe polynomial, filter application polynomial process, gaussian state space model, process, prediction polynomial, gaussian equivalent, continuous time, state space model, gaussian state space, filter application polynomial, linear gaussian state space, application polynomial process, continuous time paper, partially observe polynomial process, observe polynomial process, smoothing, prediction polynomial process, polynomial process\\nIntro: \\nLemmatized Chunk: paper devote filter smoothing prediction polynomial process partially observe problem know allow explicit solution simple case linear gaussian state space model key insight underlie present piece research filter application polynomial process discrete counterpart indistinguishable gaussian process share first two moment describe construction gaussian equivalent polynomial process explicitly compute optimal linear filter predictor smoother polynomial process discrete continuous time consideration gaussian equivalent also open door parameter estimation linear quadratic optimal control context polynomial process',\n",
       " 'Title: Comparing regularisation paths of (conjugate) gradient estimators in\\n  ridge regression\\nAuthors: Laura Hucker, Markus Reiß, Thomas Stark\\nTopic: conjugate\\nKeywords: conjugate, penalize ridge criterion, gradient iterative algorithm, gradient descent, conjugate gradient iterate, conjugate gradient exhibit fast, algorithm minimize, minimize penalize ridge, conjugate gradient iterate share, conjugate gradient, regularisation path, minimize penalize, standard gradient descent, iterative algorithm, standard gradient flow, gradient flow ridge, oracle conjugate gradient iterate, gradient flow, gradient, iterative algorithm minimize, flow, compare regularisation path\\nIntro: \\nLemmatized Chunk: consider standard gradient descent gradient flow conjugate gradient iterative algorithm minimize penalize ridge criterion linear regression well know conjugate gradient exhibit fast numerical convergence statistical property iterate difficult assess due inherent nonlinearity dependency hand standard gradient flow linear method well know regularize property stop early explicit error decomposition able bind prediction error conjugate gradient iterate corresponding prediction error gradient flow transform iteration index way risk along entire regularisation path conjugate gradient iteration compare regularisation path standard linear method like gradient flow ridge regression particular oracle conjugate gradient iterate share optimality property gradient flow ridge regression oracle constant factor numerical example show similarity regularisation path practice',\n",
       " 'Title: Stochastic dominance of sums of risks under dependence conditions\\nAuthors: Jorge Navarro, José M. Zapata\\nTopic: dependent\\nKeywords: dependent, uncertainty due, risk dependence condition, positive dependence property, represent uncertainty due, stochastic dominance sum, comparison, dominance sum, represent, stochastic, environment, sum risk, stochastic dominance comparison, due, condition provide condition, due environment, stochastic dominance, comparison depend, dominance, uncertainty, dependence condition provide, dependence condition, provide condition, condition, provide, dominance comparison, risk\\nIntro: \\nLemmatized Chunk: provide condition stochastic dominance comparison risk associate risk represent uncertainty due environment dependent comparison depend copula distribution distribution provide two different condition represent new positive dependence property regard need symmetry asymmetry skew property illustrative example provide',\n",
       " 'Title: Graph Alignment via Birkhoff Relaxation\\nAuthors: Sushil Mahavir Varma, Irène Waldspurger, Laurent Massoulié\\nTopic: maximize edge\\nKeywords: maximize edge, graph alignment problem, graph, correspondence two graph, problem, relaxation, alignment, edge overlap, star, birkhoff relaxation, alignment problem, sigma, birkhoff, graph maximize, vertex correspondence, graph alignment via birkhoff, maximize edge overlap, graph alignment, quadratic assignment problem, find vertex, qap, find vertex correspondence, alignment via birkhoff relaxation\\nIntro: \\nLemmatized Chunk: consider graph alignment problem wherein objective find vertex correspondence two graph maximize edge overlap graph alignment problem instance quadratic assignment problem qap know np hard bad case even approximately solve paper analyze birkhoff relaxation tight convex relaxation qap present theoretical guarantee performance input follow gaussian wigner model specifically weight adjacency matrice correlate gaussian orthogonal ensemble correlation denote optimal solution qap birkhoff relaxation respectively show n n thus optimal solution transition small perturbation small well separate become large result allow guarantee simple rounding procedure align fraction vertice correctly whenever condition ensure success birkhoff relaxation state art',\n",
       " 'Title: Optimal and fast online change point estimation in linear regression\\nAuthors: Annika Hüselitz, Housen Li, Axel Munk\\nTopic: change\\nKeywords: change, online change point estimation, linear regression, problem sequential, point piecewise, linear regression model, single change point, regression model, single change, online change point, estimation single, fast online change, fast online change point, piecewise linear regression model, propose online change point, sequential estimation, problem sequential estimation, point estimation linear, piecewise linear regression, change point, gaussian setup, change point detector, piecewise linear, change point estimation, estimation linear regression, online change point detector\\nIntro: \\nLemmatized Chunk: consider problem sequential estimation single change point piecewise linear regression model gaussian setup demonstrate certain cusum type statistic attain minimax optimal rate localize change point minimax analysis unveil interesting phase transition jump discontinuity value kink change slope specifically jump minimax rate order n whereas kink scale n give sampling rate order far introduce algorithm propose online change point detector require constant computational step constant memory per incoming sample finally empirical performance method examine simulate real world datum set implementation available package floc github',\n",
       " 'Title: Kernel-based estimators for functional causal effects\\nAuthors: Yordan P. Raykov, Hengrui Luo, Justin D. Strait, Wasiur R. KhudaBukhsh\\nTopic: operator - value kernel\\nKeywords: operator - value kernel, causal effect estimator, estimator base empirical, propose causal effect estimator, kernel - base estimator, effect, causal, outcome, functional datum space, datum space, causal effect estimator base, estimator, propose causal, causal effect, kernel, functional causal, functional causal effect, functional, effect estimator, tailor, estimator base, effect estimator base, propose causal effect\\nIntro: \\nLemmatized Chunk: propose causal effect estimator base empirical means operator value kernel tailor functional datum space method address challenge high dimensionality sequential ordering model complexity preserve robustness treatment misspecification use structural assumption obtain compact representation potential outcome enable scalable estimation causal effect time across covariate provide theoretical regard consistency functional causal effect well empirical comparison range propose causal effect estimator application binary treatment setting functional outcome illustrate framework utility biomedical monitoring outcome exhibit complex temporal dynamic estimator accommodate scenario register covariate outcome align means well case require high order representation capture intricate covariate outcome interaction advancement extend causal inference dynamic domain offer new tool understand complex treatment effect functional datum setting',\n",
       " 'Title: Estimation of relative risk, odds ratio and their logarithms with\\n  guaranteed accuracy and controlled sample size ratio\\nAuthors: Luis Mendo\\nTopic: ratio logarithm\\nKeywords: ratio logarithm, control sample size ratio, guarantee accuracy, relative mean - square error, estimation relative risk, ratio average sample, ratio, relative, control sample, accuracy control, binary observation, logarithm guarantee accuracy, relative risk, relative mean - square, guarantee accuracy control, estimator guarantee, average sample size, independent binary, mean - square error, odd ratio, estimator, propose relative, batch fix size, error, sample size, target error, size ratio, accuracy control sample, independent binary observation, population, estimator propose, sample size ratio, control sample size\\nIntro: \\nLemmatized Chunk: give two population independent binary observation take parameter respectively estimator propose relative risk odd ratio logarithm estimator guarantee relative mean square error mean square error logarithmic version less target value ratio average sample size two population close prescribe value estimator also use group sampling whereby sample take batch fix size two population efficiency estimator respect rao bind good particular close small value target error',\n",
       " 'Title: Learning Causal Response Representations through Direct Effect Analysis\\nAuthors: Homer Durand, Gherardo Varando, Gustau Camps-Valls\\nTopic: conditional independence\\nKeywords: conditional independence, method aim extract, enable testable conditional independence, learn causal, bridg conditional independence, learn causal response representation, effect analysis propose, direct effect analysis, response representation, conditional independence testing, learn causal response, analysis propose, formulate optimisation problem, representation learning, causal, response representation direct, causal response representation, causal representation learning, causal response, response, learning, aim extract direction, variable . bridg conditional, causal representation, treatment variable . bridg, bridg conditional independence testing\\nIntro: \\nLemmatized Chunk: propose novel approach learn causal response representation method aim extract direction multidimensional outcome directly cause treatment variable bridg conditional independence testing causal representation learning formulate optimisation problem maximise evidence conditional independence treatment outcome give conditioning set formulation employ flexible regression model tailor specific application create versatile framework problem address generalise eigenvalue decomposition show mild assumption distribution large eigenvalue bound know distribution enable testable conditional independence also provide theoretical guarantee optimality learn representation term signal noise ratio fisher information maximisation finally demonstrate empirical effectiveness approach simulation real world experiment result underscore utility framework uncover direct causal effect within complex multivariate setting',\n",
       " 'Title: Provable Robust Overfitting Mitigation in Wasserstein Distributionally\\n  Robust Optimization\\nAuthors: Shuang Liu, Yihan Wang, Yifan Zhu, Yibo Miao, Xiao-Shan Gao\\nTopic: wasserstein distributionally robust optimization\\nKeywords: wasserstein distributionally robust optimization, optimize bad - case, distributionally robust optimization wasserstein, provable robust overfitting, pointwise adversarial perturbation, optimization wasserstein distributionally robust, distributional shift, robust, distributionally robust optimization, lead enhance generalization, enhance generalization unseen, compare standard, bad - case distributional shift, wasserstein distributionally, wasserstein distributionally robust, robust optimization wasserstein distributionally, uncertainty set, focus pointwise adversarial, bad - case distributional, distributionally robust, optimize bad - case distributional, robust overfitting mitigation, provable robust overfitting mitigation, compare standard adversarial, mitigation wasserstein distributionally, focus pointwise, lead enhance, standard adversarial training\\nIntro: \\nLemmatized Chunk: wasserstein distributionally robust optimization wdro optimize bad case distributional shift within specify uncertainty set lead enhance generalization unseen adversarial example compare standard adversarial training focus pointwise adversarial perturbation however wdro still suffer fundamentally robust overfitting problem consider statistical error address gap propose novel robust optimization framework new uncertainty set adversarial noise via wasserstein distance statistical error via kullback leibler divergence call statistically robust wdro establish robust generalization bound new optimization framework imply distribution adversarial performance least good statistically robust training loss high probability furthermore derive condition stackelberg nash equilibria exist learner adversary give optimal robust model certain sense finally extensive experiment demonstrate method significantly mitigate robust overfitting enhance robustness within framework wdro',\n",
       " 'Title: Generalizability of Neural Networks Minimizing Empirical Risk Based on\\n  Expressive Ability\\nAuthors: Lijia Yu, Yibo Miao, Yifan Zhu, Xiao-Shan Gao, Lijun Zhang\\nTopic: learn exhibit nice generalizability\\nKeywords: learn exhibit nice generalizability, generalizability, generalization bound, primary objective, learning method generalization, deep learning, risk base expressive, learning exhibit nice, minimize empirical risk base, neural network minimize, uniform generalization bound, neural network minimize empirical, base expressive ability, generalizability neural network, classic uniform generalization bound, classic uniform generalization, bound, minimize empirical, ability primary objective, learning, network, objective learning method, deep learning exhibit nice, expressive ability, generalization, primary objective learning, network minimize empirical risk, learning method, minimize empirical risk, ability primary, model deep learning, deep learning exhibit, expressive ability primary\\nIntro: \\nLemmatized Chunk: primary objective learning method generalization classic uniform generalization bound rely vc dimension rademacher complexity fail explain significant attribute parameterize model deep learning exhibit nice generalizability hand algorithm dependent generalization bound like stability bound often rely strict assumption establish generalizability less stringent assumption paper investigate generalizability neural network minimize approximately minimize empirical risk establish low bind population accuracy base expressiveness network indicate adequate large number training sample network size network include parameterize one generalize effectively additionally provide necessary condition generalization demonstrate certain datum distribution quantity training datum require ensure generalization exceed network size need represent corresponding datum distribution finally provide theoretical insight several phenomenon deep learning include robust generalization importance parameterization effect loss function generalization',\n",
       " 'Title: Improving discrepancy by moving a few points\\nAuthors: Gleb Smirnov, Roman Vershynin\\nTopic: point average reduce\\nKeywords: point average reduce, discrepancy move, iid sample move, discrepancy iid, specifically, point, discrepancy, sample point, sample, reduce kolmogorov - smirnov distance, modify, show, improve, improve discrepancy, move, show improve, iid sample, average reduce kolmogorov - smirnov, sample point average, improve discrepancy move, iid, sample move\\nIntro: \\nLemmatized Chunk: show improve discrepancy iid sample move point specifically modify sample point average reduce kolmogorov smirnov distance population distribution',\n",
       " 'Title: Visual tests using several safe confidence intervals\\nAuthors: Timothée Mathieu\\nTopic: hypothesis testing framework\\nKeywords: hypothesis testing framework, safe confidence interval, hypothesis testing, time visualize confidence, time visualize, statistical hypothesis testing, confidence interval, confidence region, interval construct use e-variable, confidence interval overlap, evidence hypothesis testing, statistical hypothesis, framework decide visually, confidence interval construct, sample equal, visualize confidence region, safe confidence, statistical hypothesis testing framework, testing framework decide, decide visually, testing framework, confidence, population significantly\\nIntro: \\nLemmatized Chunk: propose new statistical hypothesis testing framework decide visually use confidence interval whether means two sample equal one large method user time visualize confidence region means test decide means two population significantly different look whether two confidence interval overlap design test use confidence interval construct use provide measure evidence hypothesis testing propose sequential test test base overlap confidence interval test give finite time error bound probability error also illustrate practicality method apply comparison sequential learning algorithm',\n",
       " 'Title: Early-Stopped Mirror Descent for Linear Regression over Convex Bodies\\nAuthors: Tobias Wegel, Gil Kur, Patrick Rebeschini\\nTopic: optimization geometry strong\\nKeywords: optimization geometry strong, explicit regularization, widely use alternative, convex body early - stop, early - stop iterative optimization method, body early - stop iterative, early - stop mirror descent, unconstrained early - stop mirror descent, direct comparison early - stop, alternative explicit regularization, convex body, optimization method, optimization method widely, iterative optimization, efficient explicit regularization, direct comparison, method widely, explicit, optimization geometry, early - stop iterative optimization, early - stop iterative, convex body early - stop iterative, early - stop explicit regularization, regularization, body early - stop iterative optimization, iterative optimization method\\nIntro: \\nLemmatized Chunk: early stop iterative optimization method widely use alternative explicit regularization direct comparison early stop explicit regularization establish many optimization geometry however analysis depend heavily specific property optimization geometry strong convexity empirical objective remain unclear whether early stop could ever less statistically efficient explicit regularization particular shape constraint especially overparameterized regime address question study setting high dimensional linear regression additive gaussian noise ground truth assume lie know convex body task minimize sample mean square error main result show convex body design matrix absolute constant factor bad case risk unconstrained early stop mirror descent appropriate potential least square estimator constrain convex body achieve construct algorithmic regularizer base minkowski functional convex body',\n",
       " 'Title: Statistical Limits in Random Tensors with Multiple Correlated Spikes\\nAuthors: Yang Qi, Alexis Decurninge\\nTopic: symmetric random gaussian tensor\\nKeywords: symmetric random gaussian tensor, deformation symmetric, random gaussian, random matrix theory, random tensor multiple, tensor multiple correlate, study multi - spike, point likelihood, matrix theory study, statistical limit random, study phase transition, multiple correlate spike, random gaussian tensor, estimator rank, multi - spike tensor model, symmetric random gaussian, limit random tensor, random matrix, matrix theory, tool random matrix, theory study, symmetric random, find maximum likelihood, maximum likelihood estimator, study multi - spike tensor\\nIntro: \\nLemmatized Chunk: use tool random matrix theory study multi spike tensor model rank deformation symmetric random gaussian tensor particular thanks nature local optimization method use find maximum likelihood estimator model propose study phase transition phenomenon find critical point corresponding optimization problem point define karush kuhn tucker kkt condition moreover characterize limit alignment estimate signal corresponding critical point likelihood ground truth signal help result propose new estimator rank tensor weight solve system polynomial equation asymptotically unbiased contrary maximum likelihood estimator',\n",
       " 'Title: Drift estimation for rough processes under small noise asymptotic:\\n  trajectory fitting method\\nAuthors: Arnaud Gloter, Nakahiro Yoshida\\nTopic: parameter\\nKeywords: parameter, unknown, solution, small noise, solution stochastic, noise asymptotic, volterra, volterra kernel, process small, star, rough process, process, rough process small, volterra equation, stochastic, small noise asymptotic, unknown parameter, trajectory fitting estimator, stochastic volterra, drift estimation rough, estimation rough, drift function, drift estimation, equation, process small noise, volterra kernel singular, theta, function, trajectory fitting, estimation rough process, varepsilon, trajectory fitting method, stochastic volterra equation\\nIntro: \\nLemmatized Chunk: consider process solution stochastic volterra equation unknown parameter drift function volterra kernel singular give u assume diffusion coefficient proportional observation path construct trajectory fitting estimator show consistent asymptotically normal also specify identifiability condition insure convergence estimator',\n",
       " 'Title: Safety Verification of Nonlinear Stochastic Systems via Probabilistic\\n  Tube\\nAuthors: Zishun Liu, Saber Jafarpour, Yongxin Chen\\nTopic: set high probability\\nKeywords: set high probability, safety, verification problem, task certifying, deterministic safety verification, safe set, system trajectory remain, verification, probabilistic tube, stochastic, safe set high, deterministic safety verification problem, nonlinear stochastic system, stochastic safety verification, safety verification, stochastic safety verification problem, stochastic system, nonlinear stochastic, specifically task, safety verification problem, high probability\\nIntro: \\nLemmatized Chunk: address problem safety verification nonlinear stochastic system specifically task certify system trajectory remain within safe set high probability tackle challenge adopt set erosion strategy decouple effect stochastic disturbance deterministic dynamic approach convert stochastic safety verification problem safe set deterministic safety verification problem erode subset safe set success strategy hing depth erosion determine probabilistic tube bound deviation stochastic trajectory corresponding deterministic trajectory main contribution establishment tight bind probabilistic tube nonlinear stochastic system obtain probabilistic bind stochastic trajectory adopt martingale base approach core innovation lie design novel energy function associate average moment generate function form affine martingale generalization traditional use energy function derive precise bind probabilistic tube furthermore enhance bind incorporate union bind inequality strictly contractive dynamic integrate derive probabilistic tube set erosion strategy demonstrate safety verification problem nonlinear stochastic system reduce deterministic safety verification problem theoretical result validate application reachability base safety verification safe controller synthesis accompany several numerical example illustrate effectiveness',\n",
       " 'Title: Estimating weak Markov-switching AR(1) models\\nAuthors: Yacouba Boubacar Mainassara, Landy Rabehasaina, Armel Bra\\nTopic: model\\nKeywords: model, necessarily independent, estimate weak markov - switch, subject markovian, extend considerably range, innovation process extend, regime assumption, weak markov - switch, asymptotic property, model subject, process extend considerably, moment estimator autoregressive, model subject markovian, uncorrelated necessarily, present asymptotic property, independence assumption, standard independence assumption, present asymptotic, estimator autoregressive, error uncorrelate, markovian change regime, moment estimator, estimate weak, relax standard independence\\nIntro: \\nLemmatized Chunk: paper present asymptotic property moment estimator autoregressive short model subject markovian change regime assumption error uncorrelate necessarily independent relax standard independence assumption innovation process extend considerably range application markov switch model provide necessary condition prove consistency asymptotic normality moment estimator specific case particular attention pay estimation asymptotic covariance matrix finally simulation study application hourly meteorological datum present corroborate theoretical work',\n",
       " 'Title: A Near Complete Nonasymptotic Generalization Theory For Multilayer\\n  Neural Networks: Beyond the Bias-Variance Tradeoff\\nAuthors: Hao Yu, Xiangyang Ji\\nTopic: complete nonasymptotic generalization theory\\nKeywords: complete nonasymptotic generalization theory, arbitrary lipschitz activation, neural, loss, make explicit sense, complete nonasymptotic, lipschitz loss function, neural network, multilayer neural, multilayer neural network, boundness loss function, theory multilayer neural, general lipschitz loss function, lipschitz loss, complete nonasymptotic generalization, activation function, lipschitz, network, function, general lipschitz loss, nonasymptotic generalization theory, loss function, main text, mild condition, nonasymptotic generalization error bound, nonasymptotic generalization\\nIntro: \\nLemmatized Chunk: propose first near complete make explicit sense main text nonasymptotic generalization theory multilayer neural network arbitrary lipschitz activation general lipschitz loss function mild condition particular require boundness loss function commonly assume literature theory go beyond bias variance tradeoff align phenomenon typically encounter deep learning therefore sharp different exist nonasymptotic generalization error bound neural network explicitly propose explicit generalization error upper bind multilayer neural network arbitrary lipschitz activation broad enough lipschitz loss function without require either width depth hyperparameter neural network approach infinity specific neural network architect sparsity boundness norm particular activation function particular optimization algorithm boundness loss function take approximation error consideration general lipschitz activation also accommodate framework feature theory also consider approximation error furthermore show near minimax optimality theory multilayer relu network regression problem notably upper bind exhibit famous double descent phenomenon network distinguished characteristic compare exist result work emphasize view many classical result improve embrace unintuitive characteristic deep learning get good understanding',\n",
       " 'Title: Pseudo-Maximum Likelihood Theory for High-Dimensional Rank One Inference\\nAuthors: Curtis Grant, Aukosh Jagannath, Justin Ko\\nTopic: spike matrix model\\nKeywords: spike matrix model, spike matrix, develop pseudo - likelihood, pseudo - likelihood theory, high - dimensional rank one inference, variational principle, pseudo - maximum likelihood theory, develop pseudo - likelihood theory, matrix estimation, rank one matrix estimation, high dimensional, limit pseudo - maximum likelihood, high dimensional limit, pseudo - maximum likelihood estimator, pseudo - maximum likelihood, matrix model, likelihood theory high - dimensional, non-linear spike matrix model, matrix, matrix estimation problem, inference develop, theory high - dimensional, estimation problem, estimation, theory high - dimensional rank, dimensional limit, likelihood theory, pseudo - likelihood theory rank\\nIntro: \\nLemmatized Chunk: develop pseudo likelihood theory rank one matrix estimation problem high dimensional limit prove variational principle limit pseudo maximum likelihood also characterize performance corresponding pseudo maximum likelihood estimator show variational principle universal depend four parameter determine corresponding null model universality introduce notion equivalence estimation problem type particular show broad class estimation task include community detection sparse submatrix detection spike matrix model equivalent spike matrix model application obtain complete description performance least square good rank one estimator rank one matrix estimation problem',\n",
       " 'Title: Asymmetric Cross-Correlation in Multivariate Spatial Stochastic\\n  Processes: A Primer\\nAuthors: Xiaoqing Chen\\nTopic: climate\\nKeywords: climate, asymmetric cross-correlation, air, domain climate, cross-correlation, phenomenon, pandemics, social economy, multivariate spatial stochastic, spatial phenomenon ubiquitous, spatial stochastic process, ubiquitous, multivariate spatial stochastic process, primer multivariate spatial, multivariate spatial phenomenon, stochastic process, quality, air quality, span, multivariate spatial, asymmetric, spatial phenomenon, primer multivariate, primer multivariate spatial phenomenon, economy, span domain, spatial stochastic, phenomenon ubiquitous\\nIntro: \\nLemmatized Chunk: multivariate spatial phenomenon ubiquitous span domain climate pandemics air quality social economy cross correlation different quantity interest different location asymmetric general paper provide visualization structure property asymmetric well symmetric auto correlation review mainstream multivariate spatial model analyze capability accommodate asymmetric also illustrate difference model accuracy without asymmetric accommodation use simulate example',\n",
       " 'Title: Systemic Risk Management via Maximum Independent Set in Extremal\\n  Dependence Networks\\nAuthors: Qian Hui, Tiandong Wang\\nTopic: financial institution may accelerate\\nKeywords: financial institution may accelerate, risk management, systemic risk management, financial, accelerate risk contagion due, dependence, maximum independent set, strategy mitigate systemic, key financial, return key financial, dependence network failure, extremal dependence network, key financial institution, contagion due, risk management via maximum, institution may accelerate risk, management via maximum independent, failure key financial, extremal dependence, failure key, mitigate systemic risk, financial institution, risk contagion due, accelerate risk, stock return key, institution, risk contagion\\nIntro: \\nLemmatized Chunk: failure key financial institution may accelerate risk contagion due interconnection within system paper propose robust portfolio strategy mitigate systemic risk extreme event use stock return key financial institution indicator performance apply extreme value theory assess extremal dependence among stock financial institution construct network model base threshold approach capture extremal dependence analysis reveal different dependence structure Chinese financial system apply maximum independent set mis graph theory identify subset institution minimal extremal dependence facilitate construction diversify portfolios resilient risk contagion also compare performance propose portfolios market portfolios two economy',\n",
       " 'Title: Powerful rank verification for multivariate Gaussian data with any\\n  covariance structure\\nAuthors: Anav Sood\\nTopic: test compare\\nKeywords: test compare, powerful rank, dimensional multivariate gaussian datum, rank verification, large, small standardize difference, procedure, large observation reject, observation inside, gaussian datum, multivariate gaussian, test compare large, compare pair, verification multivariate gaussian, pair observation inside, powerful rank verification, covariance structure upon observe, test compare pair, infer large, large second large, rank verification multivariate, covariance structure, dimensional multivariate, datum covariance, small standardize difference reject, large observation, observation, multivariate gaussian datum\\nIntro: \\nLemmatized Chunk: upon observe dimensional multivariate gaussian datum infer large observation come large means covariance isotropic gutmann argue inference justify two sided difference means test compare large second large observation reject leverage tool selective inference provide generalization procedure apply covariance structure show procedure draw desire inference whenever two sided difference mean test compare pair observation inside outside top small standardize difference reject sometimes even test fail reject use insight argue procedure render exist simultaneous inference approach inadmissible observation independent possibly unequal variance equicorrelate procedure correspond exactly run two sided difference means test compare pair observation inside outside top small standardize difference',\n",
       " 'Title: Asymptotic Theory of Eigenvectors for Latent Embeddings with Generalized\\n  Laplacian Matrices\\nAuthors: Jianqing Fan, Yingying Fan, Jinchi Lv, Fan Yang, Diwen Yu\\nTopic: latent structural\\nKeywords: latent structural, structural information, graph manifold, underlying latent structural, information graph, latent embedding generalize, laplacian, encod underlying latent, generalize laplacian, laplacian matrice laplacian matrice, underlying latent, embedding generalize laplacian, matrice, laplacian matrice commonly, matrice laplacian matrice, real application, generalize laplacian matrice, generalize laplacian matrice laplacian, encod underlying, employ many real, laplacian matrice laplacian, matrice commonly employ, underlying latent structural information, laplacian matrice, commonly employ, latent structural information\\nIntro: \\nLemmatized Chunk: laplacian matrice commonly employ many real application encod underlying latent structural information graph manifold use normalization term naturally give rise random matrice dependency well know dependency major bottleneck new random matrix theory rmt development end paper formally introduce class generalize regularize laplacian matrice contain laplacian matrix random adjacency matrix specific case suggest new framework asymptotic theory eigenvector latent embedding generalize laplacian matrice eat gl new theory empower tool generalize quadratic vector equation deal rmt dependency delicate high order asymptotic expansion empirical spike eigenvector eigenvalue base local law asymptotic normality establish spike eigenvector eigenvalue enable conduct precise inference uncertainty quantification application involve generalize laplacian matrice flexibility discuss application suggest eat gl framework showcase validity numerical example',\n",
       " 'Title: Semi-Parametric Batched Global Multi-Armed Bandits with Covariates\\nAuthors: Sakshi Arya, Hyebin Song\\nTopic: semi - parametric batch global multi-armed\\nKeywords: semi - parametric batch global multi-armed, decision - making, goal maximizing, sequential decision - making, decision - maker select, batch global multi-armed, batch global multi-armed bandit, successive arm, batch, batch bandit, approach sequential decision - making, direction, covariate multi - armed bandit, batch successive arm elimination, multi - armed bandit covariate, maximize long - term reward, round goal, arm, global multi-armed, semi - parametric batch global, long - term reward, multi - armed bandit, decision - maker select arm, successive arm elimination, multi -armed, maximize long - term, global multi -armed bandit, goal maximize long - term, arm elimination\\nIntro: \\nLemmatized Chunk: multi armed bandit mab framework widely use approach sequential decision making decision maker select arm round goal maximize long term reward moreover many practical application personalize medicine recommendation system feedback provide batch contextual information available time decision making reward different arm related rather independent propose novel semi parametric framework batch bandit covariate share parameter across arm leverage single index regression sir model capture relationship arm reward balancing interpretability flexibility algorithm batch single index dynamic binning successive arm elimination bid employ batch successive arm elimination strategy dynamic binning mechanism guide single index direction consider two setting one pilot direction available another direction estimate datum derive theoretical regret bound case pilot direction available sufficient accuracy approach achieve minimax optimal rate nonparametric batch bandit circumvent curse dimensionality extensive experiment simulated real world dataset demonstrate effectiveness algorithm compare nonparametric batch bandit method introduce jiang batch',\n",
       " 'Title: Geometric Ergodicity of a Gibbs Algorithm for a Normal Model With a\\n  Horseshoe Prior\\nAuthors: Yasuyuki Hamura\\nTopic: sampler normal\\nKeywords: sampler normal, paper, linear regression, prove geometric ergodicity, prior paper, ergodic markov chain, horseshoe, normal linear, geometrically ergodic markov, linear regression model, regression model, normal linear regression, geometrically ergodic markov chain, normal model, normal linear regression model, gibbs sampler, produce geometrically ergodic, negative moment, three - parameter beta global prior, two - stage gibb sampler, geometric ergodicity, ergodicity gibbs, horseshoe prior, model horseshoe, beta global prior, two - stage gibbs, gibbs algorithm\\nIntro: \\nLemmatized Chunk: paper consider two stage gibb sampler normal linear regression model horseshoe prior assumption show produce geometrically ergodic markov chain particular prove geometric ergodicity three parameter beta global prior finite p negative moment number regression coefficient contrast case know general result applicable global parameter finite approximately p negative moment',\n",
       " 'Title: Uniform Limit Theory for Network Data\\nAuthors: Yuya Sasaki\\nTopic: variance estimator network - dependent\\nKeywords: variance estimator network - dependent, analysis focus pointwise, uniform limit theory, network - dependent, limit theory, law large number, uniform law large, suite limit theorem, network datum, estimator network - dependent process, robust variance estimator, gmm estimator, datum, network - dependent process, law large, theory network, network datum I present, limit theory network, provide comprehensive suite, network, estimator, uniform law, focus pointwise convergence, uniform limit, uniform, number, network - dependent datum, theory network datum, comprehensive suite limit, large number\\nIntro: \\nLemmatized Chunk: I present novel uniform law large number ulln network dependent datum kojevnikov marmer song km provide comprehensive suite limit theorem robust variance estimator network dependent process analysis focus pointwise convergence hand uniform convergence essential nonlinear estimator gmm estimator newey mcfadden section build km I establish ulln network dependence demonstrate utility prove consistency gmm estimator byproduct work novel maximal inequality network datum may prove useful future research beyond scope paper',\n",
       " 'Title: A Few Observations on Sample-Conditional Coverage in Conformal\\n  Prediction\\nAuthors: John C. Duchi\\nTopic: predictive confidence\\nKeywords: predictive confidence, revisit problem, prediction revisit, confidence set, type conditional validity, conformal prediction, observation sample - conditional coverage, problem construct, conformal prediction revisit, level high probability, set cover true, cover true outcome, guarantee conditional, split conformal method, conditional validity, predictive set, split conformal method achieve, obtain type, construct predictive, conditional, predictive set cover, observation sample - conditional, problem construct predictive, construct predictive confidence set, predictive confidence set, sample - conditional coverage conformal\\nIntro: \\nLemmatized Chunk: revisit problem construct predictive confidence set wish obtain type conditional validity provide new argument show split conformal method achieve near desire coverage level high probability guarantee conditional validation datum rather marginal addition directly consider approximate conditional coverage conditional covariate belong group interest would like guarantee predictive set cover true outcome show natural method perform quantile regression hold validation dataset yield minimax optimal guarantee coverage complement positive result also provide experimental evidence interesting work remain develop computationally efficient valid predictive inference method',\n",
       " \"Title: Aspects of a Generalized Theory of Sparsity based Inference in Linear\\n  Inverse Problems\\nAuthors: Ryan O'Dowd, Raghu G. Raj, Hrushikesh N. Mhaskar\\nTopic: inverse problem ubiquitous\\nKeywords: inverse problem ubiquitous, problem linear inverse problem, base inference linear, science engineering discipline ., signal processing, science engineering, past decade, sparsity base signal, sparsity base prior, sparsity base inference, engineering discipline ., flowering field, linear inverse, linear inverse problem linear, base signal processing, inverse problem linear, inverse problem linear inverse, field compressive sensing, inference linear inverse, inverse problem, problem linear inverse, compressive sensing, lead flowering, importance past, discipline . particular importance, field compressive, sparsity base signal processing, linear inverse problem\\nIntro: \\nLemmatized Chunk: linear inverse problem ubiquitous various science engineering discipline particular importance past decade incorporation sparsity base prior particular prior linear inverse problem lead flowering field compressive sensing cs sparsity base signal processing recently method base compound gaussian cg prior investigate demonstrate improve result cs practice paper first attempt identify elucidate fundamental structure underlie success cg method study cg context broad framework generalize sparsity base inference define notion generalize sparsity introduce weak null space property proceed generalize two well know method cs basis pursuit iteratively reweight least square irl show subset regularizer fit framework\",\n",
       " 'Title: Halfspace Representations of Path Polytopes of Trees\\nAuthors: Amer Goel, Aida Maraj, Alvaro Ribot\\nTopic: distinct leave\\nKeywords: distinct leave, distinct, edge indicator vector, vector, arise naturally polyhedral, minimal halfspace representation, representation path, path polytope, edge, naturally polyhedral geometry, convex hull, polytope, path, tree, path polytope tree, halfspace representation, leave, indicator, convex, representation path polytope, indicator vector, polytope arise naturally, halfspace, hull, edge indicator, halfspace representation path\\nIntro: \\nLemmatized Chunk: give tree path polytope convex hull edge indicator vector path two distinct leave polytope arise naturally polyhedral geometry application phylogenetics tropical geometry algebraic statistic provide minimal halfspace representation polytope construction make inductively use toric fiber product',\n",
       " 'Title: Modeling discrete common-shock risks through matrix distributions\\nAuthors: Martin Bladt, Eric C. K. Cheung, Oscar Peralta, Jae-Kyung Woo\\nTopic: evolve terminate markov chain\\nKeywords: evolve terminate markov chain, common - shock discrete phase - type, event influence share, common shock component, terminate markov chain, capture essential feature, bivariate common - shock discrete phase - type, bivariate common - shock discrete, discrete common - shock risk, share individual - specific factor, induce common shock ., describe dependency loss, share common evolution, common shock . construct, model discrete common - shock, markov chain share, model discrete common - shock risk, dependency loss modeling, risk event influence, jointly evolve terminate markov, jointly evolve terminating, evolve terminate markov, construct two jointly evolve\\nIntro: \\nLemmatized Chunk: introduce novel class bivariate common shock discrete phase type cdph distribution describe dependency loss modeling emphasis induce common shock construct two jointly evolve terminate markov chain share common evolution random time corresponding common shock component proceed independently capture essential feature risk event influence share individual specific factor derive explicit expression joint distribution termination time prove various class distributional property facilitate tractable analysis risk extend framework model random sum aggregate claim sum continuous phase type random variable count determine termination time show joint distribution belong multivariate phase type matrix exponential class develop estimation procedure cdph distribution use expectation maximization algorithm demonstrate applicability model simulation study application bivariate insurance claim frequency datum',\n",
       " 'Title: Location Characteristics of Conditional Selective Confidence Intervals\\n  via Polyhedral Methods\\nAuthors: Andreas Dzemski, Ryo Okui, Wenjie Wang\\nTopic: polyhedral method examine\\nKeywords: polyhedral method examine, selective confidence interval, parameter, selective confidence, base polyhedral, confidence interval, location characteristic conditional, interval, examine location characteristic, confidence interval via polyhedral, confidence interval base, conditional selective confidence interval, conditional selective confidence, characteristic conditional selective, selective confidence interval base, interval base, interval via polyhedral method, parameter highly significant, interval conditional, highly significant, test statistic conditional, polyhedral method, location characteristic, examine location, conditional selective\\nIntro: \\nLemmatized Chunk: examine location characteristic conditional selective confidence interval base polyhedral method interval construct distribution test statistic conditional upon event statistical significance case one sided test behavior interval vary depend whether parameter highly significant marginally significant parameter highly significant interval similar usual confidence interval derive without consider selection however parameter marginally significant interval fall extreme range deviate greatly estimate value parameter contrast interval conditional two sided significance yield extreme result although may exclude estimate parameter value',\n",
       " 'Title: Minimax Optimal Kernel Two-Sample Tests with Random Features\\nAuthors: Soumya Mukherjee, Bharath K. Sriperumbudur\\nTopic: hypothesis testing problem involve\\nKeywords: hypothesis testing problem involve, kernel hilbert space, distribution define general, random feature reproduce kernel, minimax optimal kernel two - sample, problem involve distribution define, optimal kernel two - sample test, hypothesis testing problem, distribution define, testing problem involve distribution, problem involve distribution, embed probability distribution, reproduce kernel hilbert space, testing problem involve, nonparametric hypothesis testing, involve distribution, probability distribution, involve distribution define, feature reproduce kernel hilbert, probability distribution prove, nonparametric hypothesis testing problem, minimax optimal two - sample test, test minimax, reproduce kernel hilbert, maximum mean discrepancy\\nIntro: \\nLemmatized Chunk: reproduce kernel hilbert space rkh embed probability distribution prove effective approach via mmd maximum mean discrepancy nonparametric hypothesis testing problem involve distribution define general domain substantial amount work topic recently minimax optimal two sample test construct incorporate unlike mmd mean element regularize version covariance operator however kernel algorithm computational complexity optimal test scale cubically sample size limit applicability paper propose spectral regularize two sample test base random fourier feature rff approximation investigate trade statistical optimality computational efficiency show propose test minimax optimal approximation order rff depend smoothness likelihood ratio decay rate eigenvalue integral operator sufficiently large develop practically implementable permutation base version propose test datum adaptive strategy select regularization parameter kernel finally numerical experiment simulate benchmark dataset demonstrate propose rff base test computationally efficient perform almost similar small drop power exact test',\n",
       " 'Title: Characterizing the Training-Conditional Coverage of Full Conformal\\n  Inference in High Dimensions\\nAuthors: Isaac Gibbs, Emmanuel J. Candès\\nTopic: full conformal quantile regression\\nKeywords: full conformal quantile regression, high dimension study, full conformal inference, property full conformal, characterize training - conditional coverage, show full conformal, study coverage property, coverage full conformal, size converge, coverage property full, full conformal variant, full conformal inference concentrate, sample size converge, full conformal, full, conformal inference, popular full conformal, full conformal quantile, conformal, inference high dimension, benefit full conformal, behaviour full conformal, high dimension, full conformal regression, ratio dimension\\nIntro: \\nLemmatized Chunk: study coverage property full conformal regression proportional asymptotic regime ratio dimension sample size converge constant setting exist theory tell full conformal inference unbiased sense average coverage lie desire level marginalize new test point training datum considerably less know behaviour method conditional training set result exact benefit full conformal inference much simple alternative method unclear paper investigate behaviour full conformal inference natural uncorrected alternative broad class regularize linear regression model show proportional asymptotic regime training conditional coverage full conformal inference concentrate target value hand simple alternative directly compare test training residual realize constant undercoverage bias result demonstrate necessity full conformal correct high dimensional overfitting also show methodology redundant related task tuning regularization level particular show full conformal inference still yield asymptotically valid coverage regularization level select use training set without consideration test point simulation show asymptotic approximation accurate finite sample readily extend popular full conformal variant full conformal quantile regression lasso directly meet assumption',\n",
       " 'Title: The Kolmogorov-Smirnov Statistic Revisited\\nAuthors: Elvis Han Cui, Yihao Li, Zhuang Liu\\nTopic: reference distribution\\nKeywords: reference distribution, statistic revisite kolmogorov - smirnov, classical nonparametric test widely, distribution, test widely, empirical process, statistic revisite, nonparametric test widely, process, empirical process theory, compare, kolmogorov - smirnov statistic, compare empirical distribution, empirical distribution function, classical nonparametric, statistic remain - explore, normalize empirical process, widely use compare, statistic remain, kolmogorov - smirnov statistic revisite, statistic, classical nonparametric test, nonparametric test, empirical, revisite kolmogorov - smirnov, kolmogorov - smirnov\\nIntro: \\nLemmatized Chunk: kolmogorov smirnov ks statistic classical nonparametric test widely use compare empirical distribution function reference distribution compare two empirical distribution despite broad applicability statistical hypothesis testing model validation certain aspect ks statistic remain explore among young generation particularly finite sample condition paper revisit ks statistic one sample two sample scenario consider one sided two sided variant derive exact probability supremum empirical process present unified treatment ks statistic diverse setting additionally explore discrete nature hit time normalize empirical process provide practical insight computation ks test study also discuss dvoretzky kiefer wolfowitz massart dkwm inequality highlight role construct confidence band distribution function use empirical process theory establish limit distribution ks statistic true distribution include unknown parameter finding extend exist result offer improve methodology statistical analysis hypothesis testing use ks statistic particularly finite sample scenario',\n",
       " 'Title: Robust statistical inference for accelerated life-tests with one-shot\\n  devices under log-logistic distributions\\nAuthors: María González-Calderón, María Jaenada, Leandro Pardo\\nTopic: operate\\nKeywords: operate, destroy, rebuild, statistical inference accelerate, robust, unit, one - shot, statistical inference, life - test one - shot device, inference accelerate life - test, device, device unit, failure occur, inference accelerate, determine whether failure occur, specific inspection time, specific inspection, determine whether failure, unit operate, subject left, alt, accelerate life - test one - shot, accelerate life - test, one - shot device log - logistic, statistical, one - shot device, inspection time, robust statistical, robust statistical inference, operational status, life - test one - shot\\nIntro: \\nLemmatized Chunk: one shot device unit operate either destroy need rebuild type device operational status assess specific inspection time determine whether failure occur consequently lifetime subject left right censoring one shot device usually highly reliable analyze reliability product accelerate life test alt plan typically employ subject device increase level stress factor thus allow life characteristic observe high stress condition extrapolate normal operating condition accelerate degradation process alt significantly reduce time require testing associate experimental cost recently robust inferential method gain considerable interest statistical analysis among weight minimum density power divergence estimator wmdpde widely recognize robust statistical property small loss efficiency work robust wmdpde associate statistical test develop log logistic lifetime distribution multiple stress explicit expression estimating equation asymptotic distribution estimator obtain far monte carlo simulation study present evaluate performance wmdpde practical application',\n",
       " 'Title: Minimax rate for learning kernels in operators\\nAuthors: Sichong Zhang, Xiong Wang, Fei Lu\\nTopic: offer powerful\\nKeywords: offer powerful, operator learning, operator learn kernel, problem statistical learning, operator datum lie, datum lie, intersection inverse problem, dependency function, space high - dimensional setting, reproduce kernel hilbert space, kernel operator learning, high - dimensional setting, learn kernel operator, function space high - dimensional, capture nonlocal dependency, nonlocal dependency function, statistical learning, dependency function space, offer powerful framework, capture nonlocal, learn kernel, nonlocal dependency, inverse problem statistical, powerful framework capturing, framework capture nonlocal\\nIntro: \\nLemmatized Chunk: learning kernel operator datum lie intersection inverse problem statistical learning offer powerful framework capture nonlocal dependency function space high dimensional setting contrast classical nonparametric regression inverse problem well pose kernel estimation involve compact normal operator ill pose deconvolution address challenge introduce adaptive spectral sobolev space unify sobolev space reproduce kernel hilbert space automatically discard component control term small eigenvalue within framework establish minimax convergence rate mean square error polynomial exponential spectral decay regime methodologically develop tame least square estimator achieve minimax upper rate via control left tail probability eigenvalue random normal matrix minimax low rate resolve challenge infinite dimensional measure projection',\n",
       " 'Title: Linear type conditional specifications for multivariate count variables\\nAuthors: Yang Lu, Wei Sun\\nTopic: model\\nKeywords: model, paper investigate conditional specification, multivariate count variable, linear type conditional specification, variable, exist spatial count model, spatial count datum literature, specification multivariate, count variable, multivariate, conditional specification, paper investigate conditional, count variable paper, conditional specification multivariate, spatial count model base, multivariate count, spatial count datum, count datum, specification multivariate count, spatial count, spatial count model, conditional model, count, conditional, paper investigate\\nIntro: \\nLemmatized Chunk: paper investigate conditional specification multivariate count variable recently spatial count datum literature propose several conditional model conditional expectation linear conditioning variable model much easy estimate exist spatial count model base gaussian random field however whether conditional specification compatible address investigate two large family conditional model compound autoregressive model random coefficient integer autoregressive model characterize solution two family model arbitrary dimension find handful admit solution show focus linearity condition conditional expectation considerable large family solution obtain suggest spatial count datum modeling semi parametric type specification impose conditional expectation structure preferable',\n",
       " 'Title: On the Glivenko-Cantelli theorem for real-valued empirical functions of\\n  stationary $α$-mixing and $β$-mixing sequences\\nAuthors: Ousmane Coulibaly, Harouna Sangaré\\nTopic: paper extend\\nKeywords: paper extend, dependence structure, empirical function, structure characterise, sufficient condition ensure, classical glivenko - cantelli theorem, theorem, mixing, beta, glivenko - cantelli theorem, dependence structure characterise, condition ensure family, extend classical, investigate sufficient condition ensure, empirical function stationary, function exhibit glivenko - cantelli, function dependence structure, extend classical glivenko - cantelli, glivenko - cantelli, real - value empirical, real - value function exhibit, classical glivenko - cantelli, alpha, investigate sufficient condition, theorem real - value empirical, glivenko - cantelli theorem real - value, real - value empirical function\\nIntro: \\nLemmatized Chunk: paper extend classical glivenko cantelli theorem real value empirical function dependence structure characterise mixing mixing condition investigate sufficient condition ensure family real value function exhibit glivenko cantelli gc property dependence setting analysis focus function class satisfy uniform entropy condition establish deviation bound mixing coefficient decay appropriate rate result refine exist literature relax independence assumption highlight role dependence empirical process convergence',\n",
       " \"Title: Stein's unbiased risk estimate and Hyvärinen's score matching\\nAuthors: Sulagna Ghosh, Nikolaos Ignatiadis, Frederic Koehler, Amber Lee\\nTopic: empirical bayesian shrinkage estimation\\nKeywords: empirical bayesian shrinkage estimation, observation corrupt normal, unbiased risk, minimize stein unbiased, choose signal distribution, corrupt normal, hyvärinen score matching, gram - modeling strategy estimating, signal distribution, estimate signal distribution, stein unbiased, risk estimate, well - specify signal distribution class, empirical bayesian prior, signal distribution minimize, corrupt normal noise, risk estimate hyvärinen, study two g-modeling strategy, signal, observation corrupt, study two g-modeling, stein unbiased risk, signal distribution class, well - specify signal distribution, optimal empirical bayesian shrinkage, minimize stein unbiased risk, support signal distribution, bayesian prior, unbiased risk estimate, stein unbiased risk estimate\\nIntro: \\nLemmatized Chunk: study two strategy estimate signal distribution empirical bayesian prior observation corrupt normal noise first choose signal distribution minimize stein unbiased risk estimate sure imply eddington tweedie bay denoiser approach motivate optimal empirical bayesian shrinkage estimation signal second select signal distribution minimize arinen score matching objective imply score derivative log marginal density target minimal fisher divergence estimate true marginal density strategy appear distinct know mathematically equivalent provide unified analysis sure score matching well specify signal distribution class misspecification classical well specify setting homoscedastic noise compactly support signal distribution establish nearly parametric rate convergence empirical bay regret fisher divergence commonly study misspecified model establish fast rate convergence oracle denoiser corresponding oracle inequality empirical result demonstrate competitiveness nonparametric maximum likelihood well specify setting show superior performance misspecification particularly setting involve heteroscedasticity side information\",\n",
       " 'Title: Modeling Extreme Events in the Presence of Inlier: A Mixture Approach\\nAuthors: Shivshankar Nila, Ishapathik Das, N. Balakrishna\\nTopic: electronics clinical trial\\nKeywords: electronics clinical trial, model, create modeling, model extreme event, model extreme, neglect inlier, experiment environmental datum, random phenomenon, life - testing experiment, phenomenon, extreme, life - testing experiment environmental, modeling, threshold, model challenge, defect poor quality, mixture approach, rainfall datum, tail approximation, challenge, inlier, datum, environmental datum, extreme event, create modeling challenge\\nIntro: \\nLemmatized Chunk: many random phenomenon life testing experiment environmental datum like rainfall datum often positive value excess zero create modeling challenge life testing immediate failure result zero lifetime often due defect poor quality especially electronics clinical trial failure call zero inlier difficult model use standard approach study extreme value scenario key issue select appropriate threshold accurate tail approximation population use asymptotic model extreme value mixture model address threshold estimation tail approximation conventional parametric bulk generalise pareto distribution gpd approach often neglect inlier lead suboptimal result paper introduce framework model extreme event inlier use gpd address threshold uncertainty effectively capture inlier zero model parameter estimate use maximum likelihood estimation mle method ensure optimal precision simulation study real world application demonstrate propose model significantly outperform traditional method typically neglect inlier origin',\n",
       " 'Title: Set and functional prediction: randomness, exchangeability, and\\n  conformal\\nAuthors: Vladimir Vovk\\nTopic: conformal prediction\\nKeywords: conformal prediction, efficiency conformal prediction, exchangeability prediction, case regression, conformal paper continue, efficiency conformal, prediction, conformal prediction compare, randomness prediction, set functional, continue study, prediction exchangeability prediction, paper continue study, study efficiency, case classification, randomness prediction exchangeability, compare general, paper continue, prediction exchangeability, general randomness, prediction compare, functional prediction, set functional prediction, general randomness prediction, conformal paper\\nIntro: \\nLemmatized Chunk: paper continue study efficiency conformal prediction compare general randomness prediction exchangeability prediction restrict case classification result also applicable case regression price pay efficiency attain average albeit respect wide range probability measure label space',\n",
       " 'Title: A Matsuoka-Based GARMA Model for Hydrological Forecasting: Theory,\\n  Estimation, and Applications\\nAuthors: Guilherme Pumi, Danilo Hiroshi Matsuoka, Taiane Schaedler Prass, Bruna Gregory Palm\\nTopic: continuous observation\\nKeywords: continuous observation, application time, require flexible approach, hydrology climatology, gaussian - base model fail, garma model hydrological, application time series, time series, matsuoka - base garma model, continuous observation constrain, traditional gaussian - base model fail, model fail capture, series natural, matsuoka autoregressive moving average, unit interval, time series natural, observation constrain, consist continuous observation, natural science, series natural science, matsuoka - distribute random component taking, constrain unit, environmental application, matsuoka - base garma, consist continuous\\nIntro: \\nLemmatized Chunk: time series natural science hydrology climatology environmental application often consist continuous observation constrain unit interval traditional gaussian base model fail capture bound require flexible approach paper introduce matsuoka autoregressive move average marma model extend garma framework assume matsuoka distribute random component take value arma like systematic structure allow random time dependent covariate parameter estimation perform via partial maximum likelihood pmle present asymptotic theory enable statistical inference include confidence interval model selection construct prediction interval propose novel bootstrap base method account dependence structure uncertainty comprehensive monte carlo simulation study assess finite sample performance propose methodology application forecasting useful water volume guarapiranga reservoir brazil showcase practical usefulness',\n",
       " 'Title: Kernel Estimation for Nonlinear Dynamics\\nAuthors: Marie-Christine Düker, Adam Waterbury\\nTopic: problem involve datum\\nKeywords: problem involve datum, nonlinear dependency remain, problem involve, cross-sectional dependency, temporal cross-sectional, scientific problem involve, involve datum exhibiting, scientific problem, involve datum, datum exhibiting, nonlinear dependency remain scarce, exhibit temporal, dependency remain scarce, problem involve datum exhibiting, nonlinear, nonlinear dynamic many scientific, scientific problem involve datum, datum exhibit temporal, temporal cross-sectional dependency, dynamic many scientific problem, nonlinear dynamic, linear dependency, dependency\\nIntro: \\nLemmatized Chunk: many scientific problem involve datum exhibit temporal dependency linear dependency extensively study theoretical analysis regression estimator nonlinear dependency remain scarce work study kernel base estimation procedure nonlinear dynamic within reproduce kernel hilbert space framework focus nonlinear vector autoregressive model derive nonasymptotic probabilistic bound deviation regularize kernel estimator nonlinear regression function key technical contribution concentration bound quadratic form stochastic matrice presence dependent datum independent interest additionally characterize condition multivariate kernel guarantee optimal convergence rate',\n",
       " 'Title: Learning sparse generalized linear models with binary outcomes via\\n  iterative hard thresholding\\nAuthors: Namiko Matsumoto, Arya Mazumdar\\nTopic: binary outcome\\nKeywords: binary outcome, learn sparse generalize linear, glm, sparse generalize linear model, logistic, capture potential nonlinear, sparse generalize linear, generalize linear model, generalize linear, iterative hard thresholding, nonlinear dependence, capture potential, regression, expressively capture potential nonlinear, linear model, potential nonlinear dependence, hard thresholding, binary iterative hard thresholding, iterative hard, dependence model, call binary iterative hard, expressively capture, potential nonlinear, capture potential nonlinear dependence, binary, logistic regression, expressively capture potential, model datum\\nIntro: \\nLemmatized Chunk: statistic generalize linear model glm widely use modeling datum expressively capture potential nonlinear dependence model outcome covariate within broad family glm binary outcome include logistic probit regression motivate common task binary classification possibly datum addition modern machine learning statistic datum often high dimensional yet low intrinsic dimension make sparsity constraint model another reasonable consideration work propose use analyze iterative hard thresholding project gradient descent relu loss algorithm call binary iterative hard thresholding biht parameter estimation sparse glm binary outcome establish biht statistically efficient converge correct solution parameter estimation general class sparse binary glm unlike many method learning glm include maximum likelihood estimation generalize approximate message passing glm tron kakade et bahmani et biht require knowledge glm link function offer flexibility generality allow algorithm learn arbitrary binary glm two application logistic probit regression additionally study regard show logistic regression algorithm fact statistically optimal sense order wise sample complexity match logarithmic factor low bind obtain previously good knowledge first work achieve statistical optimality logistic regression noise regime computationally efficient algorithm moreover probit regression sample complexity order obtain logistic regression',\n",
       " 'Title: Testing Thresholds and Spectral Properties of High-Dimensional Random\\n  Toroidal Graphs via Edgeworth-Style Expansions\\nAuthors: Samuel Baguley, Andreas Göbel, Marcus Pappik, Leon Schiller\\nTopic: high - dimensional random geometric graph\\nKeywords: high - dimensional random geometric graph, high - dimensional random geometric, study high - dimensional random geometric, close vertice, insert sufficiently, torus edge insert, dimensional torus edge, insert sufficiently close, vertice uniformly, study high - dimensional, random geometric graph, random toroidal graph, high - dimensional random toroidal graph, high - dimensional random, sufficiently close vertice, study high - dimensional random, edge insert sufficiently, close vertice respect, uniformly distribute, random geometric, vertice uniformly distribute, dimensional torus, high - dimensional random toroidal, property high - dimensional random\\nIntro: \\nLemmatized Chunk: study high dimensional random geometric graph rgg edge density vertice uniformly distribute dimensional torus edge insert sufficiently close vertice respect norm focus distinguish rgg er graph model edge probability far result consider either spherical rgg distance toroidal rgg distance however general distance many question remain open especially allow depend main reason rgg distance easily represent logical dimensional counterpart geometry overcome devise novel technique quantify dependence edge base modify edgeworth expansion technique yield first tight algorithmic upper bound distinguish toroidal rgg general norm graph fix achieve show sign triangle distinguish two model whole regime p additionally technique yield improve information theoretic low bound task show two distribution converge whenever strong currently well know low bind spherical rgg case general liu et stoc finally expansion allow tightly characterize spectral property toroidal rgg distance fix distance result partially resolve conjecture bangachev bresler colt prove distance metric rather underlying space responsible observe difference behavior spherical toroidal rgg',\n",
       " 'Title: A Unified Bayesian Perspective for Conventional and Robust Adaptive\\n  Filters\\nAuthors: Leszek Szczecinski, Jacob Benesty, Eduardo Vinicius Kuhn\\nTopic: recursive inference\\nKeywords: recursive inference, obtain solution well - know, structure solution, conventional robust adaptive, filter . apply, unified bayesian, bayesian principle, model observational, adaptive filter . apply, adaptive filter, robust adaptive filter, present new perspective, apply bayesian, unified bayesian perspective, principle recursive inference, bayesian perspective conventional, apply bayesian principle, interpretation adaptive filter ., filter work, origin interpretation, bayesian perspective, principle recursive, filter . apply bayesian, bayesian principle recursive, derive adaptive filter, series simplification, adaptive filter depend\\nIntro: \\nLemmatized Chunk: work present new perspective origin interpretation adaptive filter apply bayesian principle recursive inference state space model use series simplification regard structure solution present unified framework derivation many adaptive filter depend probabilistic model observational noise particular gaussian model obtain solution well know literature lm nlm kalman filter use noise obtain new family adaptive filter notably assumption laplacian noise obtain family robust filter sign error algorithm well know member algorithm derive effortlessly propose framework entirely new numerical example show illustrate property provide good insight performance derive adaptive filter',\n",
       " 'Title: Sequential Outlier Detection in Non-Stationary Time Series\\nAuthors: Florian Heinrichs, Patrick Bastian, Holger Dette\\nTopic: non-stationary time series\\nKeywords: non-stationary time series, detection non-stationary time, detection non-stationary, outlier detection, multiple testing problem, time series, non-stationary time, method, error probability successive, sequential outlier, sequential outlier detection, testing problem bounding, method test null, time series propose, time point, method sequential outlier, outlier detection non-stationary, time, non-stationary, null hypothesis, outlier, address multiple testing, bound error probability, series, probability successive test, sequential\\nIntro: \\nLemmatized Chunk: novel method sequential outlier detection time series propose method test null hypothesis outlier time point address multiple testing problem bound error probability successive test use extreme value theory asymptotic property test statistic study null hypothesis alternative finite sample property new detection scheme investigate means simulation study method compare alternative procedure recently propose statistic machine learning literature',\n",
       " 'Title: Generating Correlation Matrices with Graph Structures Using Convex\\n  Optimization\\nAuthors: Ali Fakhar, Kévin Polisano, Irène Gannaz, Sophie Achard\\nTopic: specific sparsity\\nKeywords: specific sparsity, work deal, compare exist technique, graph structure, theoretical correlation matrice, convex optimization, generate correlation, specific sparsity pattern, graph structure use convex, offer great flexibility compare, generate correlation matrice, correlation matrice specific, structure use convex optimization, base convex optimization, correlation matrice, sparsity pattern, matrice, matrice specific sparsity, convex optimization work, generation correlation matrice, correlation, generation theoretical correlation, offer great flexibility, matrice graph structure, optimization work deal, generation theoretical, theoretical correlation\\nIntro: \\nLemmatized Chunk: work deal generation theoretical correlation matrice specific sparsity pattern associate graph structure present novel approach base convex optimization offer great flexibility compare exist technique notably control mean entry distribution generate correlation matrice allow generation correlation matrice well represent realistic datum use benchmark statistical method graph inference',\n",
       " 'Title: Certified Decisions\\nAuthors: Isaiah Andrews, Jiafeng Chen\\nTopic: hypothesis test\\nKeywords: hypothesis test, certify, certify decision, loss, interval ubiquitous, research, test, confidence interval, hypothesis test confidence, decision - making often unclear, confidence interval ubiquitous, interval, test confidence interval, connection subsequent, hypothesis, decision hypothesis, subsequent decision - making, certify decision hypothesis test, ubiquitous empirical research, empirical research, connection subsequent decision - making, certify decision hypothesis, ubiquitous empirical, decision hypothesis test, decision, unclear\\nIntro: \\nLemmatized Chunk: hypothesis test confidence interval ubiquitous empirical research yet connection subsequent decision making often unclear develop theory certify decision pair recommend decision inferential guarantee specifically attach upper bound loss hold probability least recommend action show certificate allow safe risk control adoption decision ambiguity averse downstream decision maker far prove without loss limit attention arise minimax decision confidence set manski term decision set estimate parallel argument apply decision obtain setting unbounded loss',\n",
       " 'Title: Conformal Prediction Under Generalized Covariate Shift with Posterior\\n  Drift\\nAuthors: Baozhen Wang, Xingye Qiao\\nTopic: time - consume\\nKeywords: time - consume, transfer learning method, conformal prediction, transfer learning, collect sufficiently, target, training datum, shift posterior, generalize covariate, transfer learning approach, transfer, sufficiently many training datum, transfer learning method develop, real application, statistical learning, collect sufficiently many training, sufficiently many training, learning, shift posterior drift, application statistical, real application statistical, application statistical learning, covariate shift posterior, generalize covariate shift, target domain\\nIntro: \\nLemmatized Chunk: many real application statistical learning collect sufficiently many training datum often expensive time consume even unrealistic case transfer learning approach aim leverage knowledge related source domain improve learning performance target domain beneficial many transfer learning method develop various distributional assumption article study particular type classification problem call conformal prediction new distributional assumption transfer learning classifier conformal prediction framework predict set plausible label instead one single label datum instance afford cautious safe decision consider generalization covariate shift posterior drift setting transfer learning setting propose weight conformal classifier leverage source target sample coverage guarantee target domain theoretical study demonstrate favorable asymptotic property numerical study far illustrate usefulness propose method',\n",
       " 'Title: A Unified Framework for Semiparametrically Efficient Semi-Supervised\\n  Learning\\nAuthors: Zichun Xu, Daniela Witten, Ali Shojaie\\nTopic: unlabeled dataset\\nKeywords: unlabeled dataset, dataset, efficient semi-supervised, incorporate unlabeled datum, label dataset consisting, semi - supervise setting, dataset consist pair, semi - supervise learning, statistical inference, unlabel dataset improve, inference, efficient semi - supervise learning, efficiency, inference semi-supervised, incorporate unlabel dataset, semiparametrically efficient semi - supervise learning, consist pair, estimator, unlabeled, dataset improve, unified framework semiparametrically, semiparametrically efficient semi-supervised, semi -supervise, semi - supervise estimator, dataset improve upon inference, dataset consisting, semiparametric efficiency\\nIntro: \\nLemmatized Chunk: consider statistical inference semi supervise setting access label dataset consist pair unlabel dataset ask question circumstance much incorporate unlabeled dataset improve upon inference use label datum answer question investigate semi supervise learning lens semiparametric efficiency theory characterize efficiency low bound semi supervise setting arbitrary inferential problem show incorporate unlabeled datum potentially improve efficiency parameter well specify propose two type semi supervise estimator safe estimator impose minimal assumption simple compute guarantee least efficient initial supervise estimator efficient estimator strong assumption achieve semiparametric efficiency bind finding unify exist semiparametric efficiency result particular special case extend result much general class problem moreover show estimator flexibly incorporate predict outcome arise black box machine learning model thereby achieve goal prediction power inference ppi superior theoretical guarantee also provide complete understanding theoretical basis exist set ppi method finally apply theoretical framework develop derive analyze efficient semi estimator number setting include average treatment effect estimation demonstrate performance propose estimator via simulation',\n",
       " 'Title: Learning Density Evolution from Snapshot Data\\nAuthors: Rentian Yao, Atsushi Nitanda, Xiaohui Chen, Yun Yang\\nTopic: structure static\\nKeywords: structure static, temporal point cloud, static snapshot datum, density evolution, motivate learning dynamical, datum motivate learning, point cloud, evolution stochastic, entropy - regularize nonparametric maximum likelihood, noisy temporal point cloud, regression approach, temporal point, stochastic process, dynamical structure, paper present, learn dynamical, learning density evolution, regression approach estimating, learn dynamical structure, structure static snapshot, dynamical structure static, nonparametric maximum likelihood estimator, noisy temporal point, snapshot datum motivate, motivate learning\\nIntro: \\nLemmatized Chunk: motivate learn dynamical structure static snapshot datum paper present distribution scalar regression approach estimate density evolution stochastic process noisy temporal point cloud propose entropy regularize nonparametric maximum likelihood estimator npmle leverage entropic optimal transport smooth regularizer density flow show almost dimension free statistical rate convergence ground truth distribution exhibit strike phase transition phenomenon term number snapshot sample size efficiently compute design novel particle base grid free coordinate kl divergence gradient descent cklgd algorithm prove polynomial iteration complexity moreover provide numerical evidence synthetic datum support theoretical finding work contribute theoretical understanding practical computation estimate density evolution noisy observation arbitrary dimension',\n",
       " 'Title: Optimal Recovery Meets Minimax Estimation\\nAuthors: Ronald DeVore, Robert D. Nowak, Rahul Parhi, Guergana Petrova, Jonathan W. Siegel\\nTopic: meet minimax estimation\\nKeywords: meet minimax estimation, noisy observation, fundamental problem, estimate function, point sample, rate, minimax estimation fundamental, estimation fundamental problem, optimal recovery rate, recovery meet minimax, sigma, machine learning, minimax, estimation fundamental, recover optimal recovery, recovery rate, optimal recovery meet, optimal, optimal recovery, statistic machine learning, minimax rate, possibly noisy observation, optimal recovery meet minimax, possibly noisy, recovery meet minimax estimation, observation, recovery\\nIntro: \\nLemmatized Chunk: fundamental problem statistic machine learning estimate function possibly noisy observation point sample goal design numerical algorithm construct approximation prescribe norm asymptotically achieve good possible error function number observation variance noise problem receive considerable attention nonparametric statistic noisy observation optimal recovery noiseless observation quantitative bound require assumption know model class assumption classical result assume unit ball besov space nonparametric statistic good possible performance algorithm find know minimax rate study setting assumption noise gaussian optimal recovery good possible performance algorithm know optimal recovery rate also determine setting one would expect minimax rate recover optimal recovery rate noise level tend zero turn current result minimax rate carefully determine dependence limit take paper handle issue determine noise level aware nla minimax rate besov class error measure norm match upper low bound end result reconciliation minimax rate optimal recovery rate nla minimax rate continuously depend noise level recover optimal recovery rate tend zero',\n",
       " 'Title: Stronger Neyman Regret Guarantees for Adaptive Experimental Design\\nAuthors: Georgy Noarov, Riccardo Fogliato, Martin Bertran, Aaron Roth\\nTopic: design - base potential\\nKeywords: design - base potential, treatment effect, neyman, neyman regret, design - base potential outcome, strong neyman regret, unbiased average, neyman regret guarantee, potential outcome setting, unbiased average treatment effect, offer sublinear neyman regret, regret guarantee, average treatment, average treatment effect, experiment unbiased average, potential outcome, regret, sequential experiment unbiased, adaptive, unbiased average treatment, design - base potential outcome setting, strong neyman regret guarantee\\nIntro: \\nLemmatized Chunk: study design adaptive sequential experiment unbiased average treatment effect eat estimation design base potential outcome setting goal develop adaptive design offer sublinear neyman regret mean efficiency must approach hindsight optimal nonadaptive design recent work dai et al introduce clipogd first method achieve expect neyman regret mild condition work propose adaptive design substantially strong neyman regret guarantee particular modify clipogd obtain anytime neyman regret natural boundedness assumption far setting experimental unit covariate introduce study class contextual multigroup neyman regret guarantee give set possibly overlapp group base covariate adaptive design outperform group good design particular develop contextual adaptive design anytime multigroup neyman regret empirically validate propose design array experiment',\n",
       " 'Title: Invariance principle for the Gaussian Multiplicative Chaos via a high\\n  dimensional CLT with low rank increments\\nAuthors: Mriganka Basu Roy Chowdhury, Shirshendu Ganguly\\nTopic: exponentiate log - correlate gaussian process\\nKeywords: exponentiate log - correlate gaussian process, gaussian multiplicative chaos, random fractal measure, liouville quantum gravity, obtain exponentiate log - correlate, multiplicative chaos, rank increment gaussian multiplicative, fractal measure obtain, measure obtain exponentiating, seminal work kahane, seminal work, exponentiate log - correlate gaussian, theory liouville quantum, non-gaussian log - correlate process arise, exponentiate log - correlate, work kahane, canonical random fractal measure, log - correlate gaussian process, canonical random fractal, random fractal measure obtain, low rank increment gaussian, quantum field theory, log - correlate process arise, obtain exponentiate, increment gaussian multiplicative chaos, low rank increment\\nIntro: \\nLemmatized Chunk: gaussian multiplicative chaos gmc canonical random fractal measure obtain exponentiate log correlate gaussian process first construct seminal work kahane since serve important building block construction quantum field theory liouville quantum gravity however many natural setting log correlate process arise paper investigate universality gmc invariance principle consider model random fourier series process know log correlate gaussian fourier series classical object study recently counterpart investigate associate multiplicative chaos construct junnila show gaussian variable couple associate chaos measure almost surely mutually absolutely continuous throughout entire regime solve main open problem kim kriechbaum early establish result part regime main ingredient new high dimensional clt sum independent random vector belong rank one subspace error bound involve isotropic property covariance matrix sum expect find application proof rely path wise analysis skorokhod embedding well perturbative result square root positive semi definite matrice surprisingly appear new',\n",
       " 'Title: On a class of high dimensional linear regression methods with debiasing\\n  and thresholding\\nAuthors: Ying-Ao Wang, Yunyi Zhang, Ye Zhang\\nTopic: design analyze\\nKeywords: design analyze, linear regression, broad class linear, framework encompass traditional method, debiase thresholding, regression method, class high dimensional, regression, inspire classical, dimensional linear regression method, method, regression approach, introduce unified, high dimensional, unified framework, classical regularization, classical regularization theory, linear regression approach, analyze broad, high dimensional linear, broad class, regularization theory, linear regression method, introduce unified framework, inspire classical regularization, high dimensional linear regression, analyze broad class\\nIntro: \\nLemmatized Chunk: paper introduce unified framework inspire classical regularization theory design analyze broad class linear regression approach framework encompass traditional method like least square regression ridge regression well innovative technique include seven novel regression method landweber showalter regression within framework far propose class debiased thresholded regression method promote feature selection particularly term sparsity method may offer advantage conventional regression technique include lasso due ease computation via close form expression theoretically establish consistency result gaussian approximation theorem new class regularization method extensive numerical simulation far demonstrate debiased thresholded counterpart linear regression method exhibit favorable finite sample performance may preferable certain setting',\n",
       " 'Title: On the admissibility of bounds on the mean of discrete, scalar\\n  probability distributions from an iid sample\\nAuthors: Erik Learned-Miller\\nTopic: distribution\\nKeywords: distribution, scalar probability, sample distribution, discrete probability, finite set, admissible, set, finite set real, set real number, sample, probability distribution, admissible bound, low bound, set real, bound, iid sample address, iid sample, discrete probability distribution, multinomial distribution, iid, scalar probability distribution, produce low bind, produce low, real number\\nIntro: \\nLemmatized Chunk: address problem produce low bind mean discrete probability distribution know support finite set real number iid sample distribution constant equivalent bound mean multinomial distribution know support sample distribution main contribution characterize complete set admissible bind function sample space show certain previously publish bound admissible prove solution one set simple state optimization problem yield admissible bind single example bound trinomial bind miratrix stark previously publish without analysis admissibility without discussion full set alternative admissible bound addition variety result admissible bound prove optimal bound sample space support size great sample size great']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d323bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816fd0da25354bb6a0cf6d69db59e5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(texts_to_embed, show_progress_bar=True, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0332e4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00556959, -0.08677147,  0.01566073, ..., -0.00796295,\n",
       "         0.0161394 ,  0.0181052 ],\n",
       "       [-0.03148653,  0.02063416, -0.02636228, ...,  0.01823692,\n",
       "        -0.04266854,  0.02595046],\n",
       "       [-0.02209193, -0.10964639,  0.01226922, ...,  0.02770264,\n",
       "        -0.0562176 ,  0.03027316],\n",
       "       ...,\n",
       "       [-0.06320994, -0.10536822,  0.03407392, ...,  0.02706568,\n",
       "        -0.0054423 ,  0.02449497],\n",
       "       [-0.05030259, -0.04778542,  0.02321614, ...,  0.00786617,\n",
       "        -0.02373936,  0.0189565 ],\n",
       "       [-0.03310056, -0.02787063, -0.02968756, ...,  0.07887013,\n",
       "        -0.06026613,  0.01698399]], shape=(250, 384), dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3cc7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = embeddings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a9f0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7469879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "92462d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index size: 250\n"
     ]
    }
   ],
   "source": [
    "print(f\"FAISS Index size: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0de7f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"arxiv_index.faiss\")\n",
    "df_final_chunks.to_parquet(\"arxiv_metadata.parquet.gzip\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5741a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query, top_k=5):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return df_final_chunks.iloc[indices[0]]\n",
    "\n",
    "results = search_similar_chunks(\"Bayesian nonparametric regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d63d7d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>topic</th>\n",
       "      <th>keywords</th>\n",
       "      <th>chunk</th>\n",
       "      <th>chunk_lemmatized</th>\n",
       "      <th>enriched_text_rich</th>\n",
       "      <th>tfidf_top_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>Non-Bayesian Learning in Misspecified Models</td>\n",
       "      <td>Sebastian Bervoets, Mathieu Faure, Ludovic Renou</td>\n",
       "      <td>nuanced view</td>\n",
       "      <td>nuanced view, non-bayesian updating, misspecif...</td>\n",
       "      <td>Deviations from Bayesian updating are traditio...</td>\n",
       "      <td>deviation bayesian updating traditionally cate...</td>\n",
       "      <td>Title: Non-Bayesian Learning in Misspecified M...</td>\n",
       "      <td>bayesian, imply, sub, deviation, optimality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>On Robust Empirical Likelihood for Nonparametr...</td>\n",
       "      <td>Qin Fang, Shaojun Guo, Yang Hong, Xinghao Qiao</td>\n",
       "      <td>regression discontinuity design empirical</td>\n",
       "      <td>design empirical, discontinuity design empiric...</td>\n",
       "      <td>Empirical likelihood serves as a powerful tool...</td>\n",
       "      <td>empirical likelihood serve powerful tool const...</td>\n",
       "      <td>Title: On Robust Empirical Likelihood for Nonp...</td>\n",
       "      <td>rdd, empirical, likelihood, bias, regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>No-prior Bayesian inference reIMagined: probab...</td>\n",
       "      <td>Ryan Martin</td>\n",
       "      <td>go - strategy probabilistic</td>\n",
       "      <td>go - strategy, information lack, go - strategy...</td>\n",
       "      <td>When prior information is lacking, the go-to s...</td>\n",
       "      <td>prior information lack go strategy probabilist...</td>\n",
       "      <td>Title: No-prior Bayesian inference reIMagined:...</td>\n",
       "      <td>probabilistic, bay, approximation, quantificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Adaptive sparse variational approximations for...</td>\n",
       "      <td>Dennis Nieman, Botond Szabó</td>\n",
       "      <td>effectively</td>\n",
       "      <td>adaptive sparse variational approximation, gau...</td>\n",
       "      <td>Accurate tuning of hyperparameters is crucial ...</td>\n",
       "      <td>accurate tuning hyperparameter crucial ensure ...</td>\n",
       "      <td>Title: Adaptive sparse variational approximati...</td>\n",
       "      <td>variational, hyperparameter, bay, rate, theore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nonparametric local polynomial regression for ...</td>\n",
       "      <td>Moritz Jirak, Alois Kneip, Alexander Meister, ...</td>\n",
       "      <td>space</td>\n",
       "      <td>space, polynomial, nonparametric local, hilber...</td>\n",
       "      <td>We consider nonparametric regression with func...</td>\n",
       "      <td>consider nonparametric regression functional c...</td>\n",
       "      <td>Title: Nonparametric local polynomial regressi...</td>\n",
       "      <td>regression, polynomial, tuning, functional, no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                              title  \\\n",
       "106         106       Non-Bayesian Learning in Misspecified Models   \n",
       "40           40  On Robust Empirical Likelihood for Nonparametr...   \n",
       "94           94  No-prior Bayesian inference reIMagined: probab...   \n",
       "22           22  Adaptive sparse variational approximations for...   \n",
       "0             0  Nonparametric local polynomial regression for ...   \n",
       "\n",
       "                                               authors  \\\n",
       "106   Sebastian Bervoets, Mathieu Faure, Ludovic Renou   \n",
       "40      Qin Fang, Shaojun Guo, Yang Hong, Xinghao Qiao   \n",
       "94                                         Ryan Martin   \n",
       "22                         Dennis Nieman, Botond Szabó   \n",
       "0    Moritz Jirak, Alois Kneip, Alexander Meister, ...   \n",
       "\n",
       "                                         topic  \\\n",
       "106                               nuanced view   \n",
       "40   regression discontinuity design empirical   \n",
       "94                 go - strategy probabilistic   \n",
       "22                                 effectively   \n",
       "0                                        space   \n",
       "\n",
       "                                              keywords  \\\n",
       "106  nuanced view, non-bayesian updating, misspecif...   \n",
       "40   design empirical, discontinuity design empiric...   \n",
       "94   go - strategy, information lack, go - strategy...   \n",
       "22   adaptive sparse variational approximation, gau...   \n",
       "0    space, polynomial, nonparametric local, hilber...   \n",
       "\n",
       "                                                 chunk  \\\n",
       "106  Deviations from Bayesian updating are traditio...   \n",
       "40   Empirical likelihood serves as a powerful tool...   \n",
       "94   When prior information is lacking, the go-to s...   \n",
       "22   Accurate tuning of hyperparameters is crucial ...   \n",
       "0    We consider nonparametric regression with func...   \n",
       "\n",
       "                                      chunk_lemmatized  \\\n",
       "106  deviation bayesian updating traditionally cate...   \n",
       "40   empirical likelihood serve powerful tool const...   \n",
       "94   prior information lack go strategy probabilist...   \n",
       "22   accurate tuning hyperparameter crucial ensure ...   \n",
       "0    consider nonparametric regression functional c...   \n",
       "\n",
       "                                    enriched_text_rich  \\\n",
       "106  Title: Non-Bayesian Learning in Misspecified M...   \n",
       "40   Title: On Robust Empirical Likelihood for Nonp...   \n",
       "94   Title: No-prior Bayesian inference reIMagined:...   \n",
       "22   Title: Adaptive sparse variational approximati...   \n",
       "0    Title: Nonparametric local polynomial regressi...   \n",
       "\n",
       "                                       tfidf_top_terms  \n",
       "106        bayesian, imply, sub, deviation, optimality  \n",
       "40        rdd, empirical, likelihood, bias, regression  \n",
       "94   probabilistic, bay, approximation, quantificat...  \n",
       "22   variational, hyperparameter, bay, rate, theore...  \n",
       "0    regression, polynomial, tuning, functional, no...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef4d559c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Non-Bayesian Learning in Misspecified Models\n",
      "Authors: Sebastian Bervoets, Mathieu Faure, Ludovic Renou\n",
      "Topic: nuanced view\n",
      "TF-IDF: bayesian, imply, sub, deviation, optimality\n",
      "Chunk: Deviations from Bayesian updating are traditionally categorized as biases, errors, or fallacies, thus implying their inherent ``sub-optimality.'' We offer a more nuanced view. We demonstrate that, in learning problems with misspecified models, non-Bayesian updating can outperform Bayesian updating....\n",
      "\n",
      "Title: On Robust Empirical Likelihood for Nonparametric Regression with\n",
      "  Application to Regression Discontinuity Designs\n",
      "Authors: Qin Fang, Shaojun Guo, Yang Hong, Xinghao Qiao\n",
      "Topic: regression discontinuity design empirical\n",
      "TF-IDF: rdd, empirical, likelihood, bias, regression\n",
      "Chunk: Empirical likelihood serves as a powerful tool for constructing confidence intervals in nonparametric regression and regression discontinuity designs (RDD). The original empirical likelihood framework can be naturally extended to these settings using local linear smoothers, with Wilks' theorem holdi...\n",
      "\n",
      "Title: No-prior Bayesian inference reIMagined: probabilistic approximations of\n",
      "  inferential models\n",
      "Authors: Ryan Martin\n",
      "Topic: go - strategy probabilistic\n",
      "TF-IDF: probabilistic, bay, approximation, quantification, posterior\n",
      "Chunk: When prior information is lacking, the go-to strategy for probabilistic inference is to combine a \"default prior\" and the likelihood via Bayes's theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior di...\n",
      "\n",
      "Title: Adaptive sparse variational approximations for Gaussian process\n",
      "  regression\n",
      "Authors: Dennis Nieman, Botond Szabó\n",
      "Topic: effectively\n",
      "TF-IDF: variational, hyperparameter, bay, rate, theoretical\n",
      "Chunk: Accurate tuning of hyperparameters is crucial to ensure that models can generalise effectively across different settings. In this paper, we present theoretical guarantees for hyperparameter selection using variational Bayes in the nonparametric regression model. We construct a variational approximat...\n",
      "\n",
      "Title: Nonparametric local polynomial regression for functional covariates\n",
      "Authors: Moritz Jirak, Alois Kneip, Alexander Meister, Mario Pahl\n",
      "Topic: space\n",
      "TF-IDF: regression, polynomial, tuning, functional, nonparametric\n",
      "Chunk: We consider nonparametric regression with functional covariates, that is, they are elements of an infinite-dimensional Hilbert space. A locally polynomial estimator is constructed, where an orthonormal basis and various tuning parameters remain to be selected. We provide a general asymptotic upper b...\n"
     ]
    }
   ],
   "source": [
    "for _, row in results.iterrows():\n",
    "    print(f\"\\nTitle: {row.title}\\nAuthors: {row.authors}\\nTopic: {row.topic}\\nTF-IDF: {row.tfidf_top_terms}\")\n",
    "    print(f\"Chunk: {row.chunk[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a848e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: sentence-transformers in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (4.0.2)\n",
      "Requirement already satisfied: faiss-cpu in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (1.10.0)\n",
      "Requirement already satisfied: transformers in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (4.49.0)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.13.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: packaging<25,>=20 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (19.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (4.12.2)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: tqdm in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sentence-transformers) (0.29.0)\n",
      "Requirement already satisfied: filelock in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: jinja2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.34.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: setuptools in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.24.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/maksvell/Учеба/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp313-cp313-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading narwhals-1.34.1-py3-none-any.whl (325 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.24.0-cp313-cp313-macosx_11_0_arm64.whl (351 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: toml, tenacity, smmap, rpds-py, rapidfuzz, narwhals, cachetools, blinker, referencing, pydeck, gitdb, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 narwhals-1.34.1 pydeck-0.9.1 rapidfuzz-3.13.0 referencing-0.36.2 rpds-py-0.24.0 smmap-5.0.2 streamlit-1.44.1 tenacity-9.1.2 toml-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit sentence-transformers faiss-cpu transformers rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "91943969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a021e4d1977b41268d80b382a51c05ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93f7ce6d2a34781924ece5fc2eed4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0446a25f8a408da64fb76c6109d0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d32b03c12441709f66ffcb0d3d1f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa784ec9ba9e43528c11a537787dfa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d089b7a8b6ca41afbc6c5b115d79bf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec7946f5f214aaa8be31126fc35ce7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "95115ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_flan(prompt, max_tokens=256):\n",
    "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = flan_model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    return flan_tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a50528be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      👋 \u001b[1mWelcome to Streamlit!\u001b[0m\n",
      "\n",
      "      If you’d like to receive helpful onboarding emails, news, offers, promotions,\n",
      "      and the occasional swag, please enter your email address below. Otherwise,\n",
      "      leave this field blank.\n",
      "\n",
      "      \u001b[34mEmail: \u001b[0m ^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e00aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
