{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: more_itertools in ./.venv/lib/python3.11/site-packages (10.6.0)\n",
      "Requirement already satisfied: catboost in ./.venv/lib/python3.11/site-packages (1.2.7)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: implicit in ./.venv/lib/python3.11/site-packages (0.7.2)\n",
      "Requirement already satisfied: rectools in ./.venv/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: graphviz in ./.venv/lib/python3.11/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (from catboost) (3.10.1)\n",
      "Requirement already satisfied: plotly in ./.venv/lib/python3.11/site-packages (from catboost) (6.0.0)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.11/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from implicit) (4.67.1)\n",
      "Requirement already satisfied: attrs<24.0.0,>=19.1.0 in ./.venv/lib/python3.11/site-packages (from rectools) (23.2.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.8.2 in ./.venv/lib/python3.11/site-packages (from rectools) (2.10.6)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.20.1 in ./.venv/lib/python3.11/site-packages (from rectools) (2.27.2)\n",
      "Requirement already satisfied: typeguard<5.0.0,>=4.1.0 in ./.venv/lib/python3.11/site-packages (from rectools) (4.4.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in ./.venv/lib/python3.11/site-packages (from rectools) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.8.2->rectools) (0.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib->catboost) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->catboost) (3.2.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./.venv/lib/python3.11/site-packages (from plotly->catboost) (1.29.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksvell/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn more_itertools catboost scipy implicit rectools\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from itertools import islice, cycle\n",
    "from more_itertools import pairwise\n",
    "from catboost import CatBoostClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from rectools.dataset import Dataset\n",
    "from rectools.models import EASEModel, SASRecModel\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from implicit.nearest_neighbours import TFIDFRecommender, ItemItemRecommender, CosineRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users_df columns: ['user_id', 'age', 'income', 'sex', 'kids_flg', 'total_watch_time', 'average_session_duration', 'favorite_genres', 'favorite_countries', 'preferred_watch_time', 'preferred_days', 'genre_diversity', 'country_diversity', 'number_of_unique_items_watched', 'average_time_per_genre', 'watch_frequency', 'trending_score', 'seasonal_preference', 'average_completion_rate', 'binge_watching_score', 'weekend_watch_frequency', 'weekday_watch_frequency', 'preferred_duration', 'preferred_age_rating', 'average_time_between_sessions', 'max_session_duration', 'preferred_studios', 'preferred_directors', 'preferred_actors', 'genre_exploration_score', 'country_exploration_score', 'recent_activity_score', 'churn_risk_score', 'preferred_keywords', 'preferred_description_length', 'director_exploration_score', 'actor_exploration_score']\n",
      "items_df columns: ['item_id', 'content_type', 'title', 'title_orig', 'release_year', 'genres', 'countries', 'for_kids', 'age_rating', 'studios', 'directors', 'actors', 'description', 'keywords', 'release_year_cat', 'title_length', 'title_word_count', 'title_unique_chars', 'desc_word_count', 'desc_avg_word_length', 'desc_unique_words_ratio', 'num_keywords', 'keywords_unique_ratio', 'num_genres', 'num_countries', 'is_international', 'num_studios', 'num_directors', 'num_actors', 'release_decade', 'is_classic', 'is_recent', 'is_kids_content', 'is_series', 'is_adult_content', 'genre_diversity', 'country_diversity', 'release_month', 'release_day_of_week', 'keyword_diversity', 'studio_diversity', 'log_release_year', 'years_since_release', 'release_age_group', 'has_popular_genres', 'num_popular_genres', 'months_since_release', 'avg_desc_word_length', 'desc_unique_percentage', 'unique_genre_count', 'min_keywords_length', 'max_keywords_length', 'unique_actors_ratio', 'unique_directors_ratio', 'release_year_distance', 'log_years_since_release', 'unique_countries_count', 'genre_country_interaction', 'total_content_volume', 'age_rating_class', 'non_standard_genres_count', 'view_count', 'average_watch_duration', 'trending_score', 'seasonal_popularity', 'duration', 'recent_popularity', 'peak_view_period', 'rating_trend', 'positive_review_ratio', 'trending_velocity', 'seasonal_trend', 'view_count_last_month', 'view_count_last_week', 'average_rating_last_month', 'trending_acceleration', 'seasonal_trend_change', 'release_quarter', 'time_of_day_popularity']\n",
      "interactions_df columns: ['user_id', 'item_id', 'last_watch_dt', 'total_dur', 'watched_pct']\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "users_df = pd.read_csv('processed_users.csv')\n",
    "items_df = pd.read_csv('processed_items.csv')\n",
    "interactions_df = pd.read_csv('processed_interactions.csv', parse_dates=['last_watch_dt'])\n",
    "\n",
    "# Проверка колонок\n",
    "print(\"users_df columns:\", users_df.columns.tolist())\n",
    "print(\"items_df columns:\", items_df.columns.tolist())\n",
    "print(\"interactions_df columns:\", interactions_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coo_matrix(df, user_col='user_id', item_col='item_id', weight_col=None, users_mapping={}, items_mapping={}):\n",
    "    df = df.dropna(subset=[user_col, item_col])\n",
    "    if weight_col is None:\n",
    "        weights = np.ones(len(df), dtype=np.float32)\n",
    "    else:\n",
    "        weights = df[weight_col].astype(np.float32)\n",
    "    \n",
    "    user_indices = df[user_col].map(users_mapping.get).dropna().astype(int)\n",
    "    item_indices = df[item_col].map(items_mapping.get).dropna().astype(int)\n",
    "    valid_idx = user_indices.index.intersection(item_indices.index)\n",
    "    \n",
    "    return sp.coo_matrix(\n",
    "        (weights[valid_idx], (user_indices.loc[valid_idx], item_indices.loc[valid_idx])),\n",
    "        shape=(len(users_mapping), len(items_mapping))\n",
    "    )\n",
    "    \n",
    "def generate_implicit_recs_mapper(model, train_matrix, top_N, user_mapping, item_inv_mapping, filter_already_liked_items):\n",
    "    def _recs_mapper(user):\n",
    "        user_id = user_mapping.get(user, -1)\n",
    "        if user_id == -1:  \n",
    "            return most_pop_recommendations[:top_N] \n",
    "        recs = model.recommend(user_id, train_matrix, N=top_N, filter_already_liked_items=filter_already_liked_items)\n",
    "        recommended_items = []\n",
    "        for item in recs[0]: \n",
    "            try:\n",
    "                recommended_items.append(item_inv_mapping[item])\n",
    "            except KeyError:\n",
    "                recommended_items.append(most_pop_recommendations[len(recommended_items) % len(most_pop_recommendations)])\n",
    "        return recommended_items[:top_N]\n",
    "    return _recs_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRangeSplit:\n",
    "    def __init__(self, start_date, end_date=None, freq='W', periods=None, train_min_date=None, filter_cold_users=True, filter_cold_items=True, filter_already_seen=True):\n",
    "        self.start_date = pd.to_datetime(start_date)\n",
    "        self.end_date = end_date\n",
    "        self.freq = freq\n",
    "        self.periods = periods\n",
    "        self.train_min_date = pd.to_datetime(train_min_date)\n",
    "        self.filter_cold_users = filter_cold_users\n",
    "        self.filter_cold_items = filter_cold_items\n",
    "        self.filter_already_seen = filter_already_seen\n",
    "        self.date_range = pd.date_range(start=start_date, end=end_date, freq=freq, periods=periods)\n",
    "        self.max_n_splits = max(0, len(self.date_range) - 1)\n",
    "\n",
    "    def split(self, df, user_column='user_id', item_column='item_id', datetime_column='last_watch_dt', fold_stats=True):\n",
    "        df_datetime = df[datetime_column]\n",
    "        if self.train_min_date:\n",
    "            train_min_mask = df_datetime >= self.train_min_date\n",
    "        else:\n",
    "            train_min_mask = df_datetime.notnull()\n",
    "\n",
    "        date_range = self.date_range[(self.date_range >= df_datetime.min()) & (self.date_range <= df_datetime.max())]\n",
    "        for start, end in pairwise(date_range):\n",
    "            fold_info = {'Start date': start, 'End date': end}\n",
    "            train_mask = train_min_mask & (df_datetime < start)\n",
    "            train_idx = df.index[train_mask]\n",
    "            test_mask = (df_datetime >= start) & (df_datetime < end)\n",
    "            test_idx = df.index[test_mask]\n",
    "\n",
    "            if self.filter_cold_users:\n",
    "                new_users = np.setdiff1d(df.loc[test_idx, user_column].unique(), df.loc[train_idx, user_column].unique())\n",
    "                new_idx = df.index[test_mask & df[user_column].isin(new_users)]\n",
    "                test_idx = np.setdiff1d(test_idx, new_idx)\n",
    "                if fold_stats:\n",
    "                    fold_info['New users'] = len(new_users)\n",
    "\n",
    "            if self.filter_cold_items:\n",
    "                new_items = np.setdiff1d(df.loc[test_idx, item_column].unique(), df.loc[train_idx, item_column].unique())\n",
    "                new_idx = df.index[test_mask & df[item_column].isin(new_items)]\n",
    "                test_idx = np.setdiff1d(test_idx, new_idx)\n",
    "                if fold_stats:\n",
    "                    fold_info['New items'] = len(new_items)\n",
    "\n",
    "            if self.filter_already_seen:\n",
    "                train_pairs = df.loc[train_idx, [user_column, item_column]].set_index([user_column, item_column]).index\n",
    "                test_pairs = df.loc[test_idx, [user_column, item_column]].set_index([user_column, item_column]).index\n",
    "                intersection = train_pairs.intersection(test_pairs)\n",
    "                test_idx = test_idx[~test_pairs.isin(intersection)]\n",
    "                if fold_stats:\n",
    "                    fold_info['Known interactions'] = len(intersection)\n",
    "\n",
    "            if fold_stats:\n",
    "                fold_info['Train'] = len(train_idx)\n",
    "                fold_info['Test'] = len(test_idx)\n",
    "\n",
    "            yield train_idx, test_idx, fold_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMostPop:\n",
    "    def __init__(self, days=30, trend_weight=0.5, seasonal_weight=0.3):\n",
    "        self.days = days\n",
    "        self.trend_weight = trend_weight\n",
    "        self.seasonal_weight = seasonal_weight\n",
    "        self.popularity = None\n",
    "        \n",
    "    def fit(self, interactions_df, items_df):\n",
    "        recent = interactions_df[interactions_df['last_watch_dt'] > \n",
    "                                interactions_df['last_watch_dt'].max() - pd.Timedelta(days=self.days)]\n",
    "        base_pop = recent['item_id'].value_counts()\n",
    "        base_pop = (base_pop - base_pop.min()) / (base_pop.max() - base_pop.min() + 1e-10) \n",
    "        \n",
    "        trend_score = items_df.set_index('item_id')['trending_score'].fillna(0)\n",
    "        trend_score = (trend_score - trend_score.min()) / (trend_score.max() - trend_score.min() + 1e-10) \n",
    "        \n",
    "        seasonal_score = items_df.set_index('item_id')['seasonal_trend'].fillna(0)\n",
    "        seasonal_score = (seasonal_score - seasonal_score.min()) / (seasonal_score.max() - seasonal_score.min() + 1e-10)  \n",
    "        \n",
    "        final_score = (1 - self.trend_weight - self.seasonal_weight) * base_pop + \\\n",
    "                      self.trend_weight * trend_score.reindex(base_pop.index, fill_value=0) + \\\n",
    "                      self.seasonal_weight * seasonal_score.reindex(base_pop.index, fill_value=0)\n",
    "        \n",
    "        self.popularity = final_score.sort_values(ascending=False)\n",
    "        logger.info(f\"Top 10 items in popularity: {self.popularity.index[:10].tolist()}\")\n",
    "    \n",
    "    def recommend(self, users, N=10):\n",
    "        recs = self.popularity.index[:N]\n",
    "        return pd.DataFrame({'user_id': users, 'item_id': [list(recs)] * len(users)}).explode('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomUserKNN:\n",
    "    def __init__(self, K=5, weight_decay=0.9, similarity='cosine'):\n",
    "        self.K = min(K, 15)\n",
    "        self.weight_decay = weight_decay\n",
    "        self.similarity = similarity\n",
    "        self.interaction_matrix = None\n",
    "        self.user_norms = None\n",
    "        \n",
    "    def fit(self, interaction_matrix):\n",
    "        self.interaction_matrix = interaction_matrix.tocsr()\n",
    "        self.user_norms = sp.linalg.norm(self.interaction_matrix, axis=1)\n",
    "        self.user_norms[self.user_norms == 0] = 1e-10\n",
    "        \n",
    "    def recommend_batch(self, user_ids, users_mapping, items_inv_mapping, N=10):\n",
    "        user_indices = np.array([users_mapping.get(uid, -1) for uid in user_ids])\n",
    "        valid_mask = user_indices != -1  \n",
    "        \n",
    "        if not np.any(valid_mask):\n",
    "            return [list(range(min(N, self.interaction_matrix.shape[1]))) for _ in user_ids]\n",
    "            \n",
    "        valid_user_indices = user_indices[valid_mask]\n",
    "        valid_user_ids = user_ids[valid_mask]\n",
    "        \n",
    "        user_vectors = self.interaction_matrix[valid_user_indices]\n",
    "    \n",
    "        similarities = (user_vectors.dot(self.interaction_matrix.T) / \n",
    "                       (self.user_norms[valid_user_indices][:, None] * self.user_norms[None, :])).toarray()\n",
    "        \n",
    "        similarities[np.arange(len(valid_user_indices)), valid_user_indices] = -1\n",
    "        \n",
    "        top_users = np.argpartition(-similarities, self.K, axis=1)[:, :self.K]\n",
    "        top_similarities = np.take_along_axis(similarities, top_users, axis=1)\n",
    "        \n",
    "        valid_sim_mask = top_similarities > 0\n",
    "        \n",
    "        recommendations = []\n",
    "        for i, (user_idx, sims, top_u) in enumerate(zip(valid_user_indices, top_similarities, top_users)):\n",
    "            if not np.any(valid_sim_mask[i]):\n",
    "                recommendations.append(list(range(min(N, self.interaction_matrix.shape[1]))))\n",
    "                continue\n",
    "                \n",
    "            valid_top_users = top_u[valid_sim_mask[i]]\n",
    "            valid_sims = sims[valid_sim_mask[i]]\n",
    "            weights = valid_sims * self.weight_decay\n",
    "            \n",
    "            item_scores = Counter()\n",
    "            for u, w in zip(valid_top_users, weights):\n",
    "                items = self.interaction_matrix[u].indices\n",
    "                for item in items:\n",
    "                    item_scores[item] += w\n",
    "                    \n",
    "            if not item_scores:\n",
    "                recommendations.append(list(range(min(N, self.interaction_matrix.shape[1]))))\n",
    "            else:\n",
    "                recommendations.append([items_inv_mapping[item] for item, _ in item_scores.most_common(N)])\n",
    "        \n",
    "        result = []\n",
    "        j = 0\n",
    "        for i in range(len(user_ids)):\n",
    "            if valid_mask[i]:\n",
    "                result.append(recommendations[j])\n",
    "                j += 1\n",
    "            else:\n",
    "                result.append(list(range(min(N, self.interaction_matrix.shape[1]))))\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_catboost_features(train, users_df, items_df):\n",
    "    pos_samples = train[['user_id', 'item_id']].assign(label=1)\n",
    "    neg_samples = pd.DataFrame({\n",
    "        'user_id': np.random.choice(train['user_id'], len(train)),\n",
    "        'item_id': np.random.choice(items_df['item_id'], len(train))\n",
    "    }).assign(label=0)\n",
    "    neg_samples = neg_samples[~neg_samples.set_index(['user_id', 'item_id']).index.isin(\n",
    "        pos_samples.set_index(['user_id', 'item_id']).index)]\n",
    "    \n",
    "    data = pd.concat([pos_samples, neg_samples]).merge(users_df, on='user_id').merge(items_df, on='item_id')\n",
    "    \n",
    "    user_cols = ['age', 'total_watch_time', 'genre_diversity', 'country_diversity', 'watch_frequency']\n",
    "    item_cols = ['release_year', 'duration', 'trending_score', 'view_count', 'years_since_release']\n",
    "    cat_cols = ['favorite_genres', 'genres']\n",
    "    \n",
    "    X = data[user_cols + item_cols].fillna(0)\n",
    "    X[cat_cols] = data[cat_cols].fillna('unknown')\n",
    "    return X, data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(train, test, recs, top_N, items_df):\n",
    "    result = {}\n",
    "    test = test.reset_index()[['user_id', 'item_id']].dropna()\n",
    "    recs = recs[['user_id', 'item_id', 'rank']].dropna()\n",
    "\n",
    "    test_recs = test.set_index(['user_id', 'item_id']).join(\n",
    "        recs.set_index(['user_id', 'item_id']), how='left'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Логируем пересечение\n",
    "    intersection = len(set(test['item_id']) & set(recs[recs['rank'] <= top_N]['item_id']))\n",
    "    logger.info(f\"Intersection between test and recs: {intersection} items\")\n",
    "\n",
    "    # MAP@k\n",
    "    test_recs['cum_rank'] = test_recs.groupby('user_id')['rank'].cumcount() + 1\n",
    "    test_recs['cum_rank'] = test_recs['cum_rank'] / test_recs['rank'].fillna(test_recs['rank'].max() + 1)\n",
    "    result[f'MAP@{top_N}'] = (test_recs.groupby('user_id')['cum_rank'].sum() / \n",
    "                             test_recs.groupby('user_id')['rank'].transform('size').replace(0, 1)).mean()\n",
    "\n",
    "    # Novelty@k\n",
    "    n_users = train['user_id'].nunique()\n",
    "    item_pop = train.groupby('item_id')['user_id'].nunique()\n",
    "    recs_top = recs[recs['rank'] <= top_N].copy()\n",
    "    recs_top['novelty'] = recs_top['item_id'].map(lambda x: -np.log2(item_pop.get(x, 1) / n_users))\n",
    "    result[f'Novelty@{top_N}'] = recs_top.groupby('user_id')['novelty'].mean().mean()\n",
    "\n",
    "    # Diversity@k\n",
    "    item_features = sp.hstack([MultiLabelBinarizer().fit_transform(items_df['genres'].str.split(',')), \n",
    "                              sp.csr_matrix(items_df[['release_year', 'trending_score']].fillna(0))]).tocsr()\n",
    "    diversity_scores = []\n",
    "    for _, group in recs[recs['rank'] <= top_N].groupby('user_id'):\n",
    "        idx = np.where(np.isin(items_df['item_id'], group['item_id']))[0]\n",
    "        if len(idx) > 1:\n",
    "            sim = cosine_similarity(item_features[idx])\n",
    "            diversity_scores.append(1 - sim[np.triu_indices(len(idx), k=1)].mean())\n",
    "    result[f'Diversity@{top_N}'] = np.mean(diversity_scores) if diversity_scores else 0\n",
    "\n",
    "    # Serendipity@k\n",
    "    popular = train['item_id'].value_counts().head(100).index\n",
    "    result[f'Serendipity@{top_N}'] = recs[recs['rank'] <= top_N].groupby('user_id')['item_id'].apply(\n",
    "        lambda x: 1 - np.mean(np.isin(x, popular))\n",
    "    ).mean()\n",
    "\n",
    "    return pd.Series(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Start date   End date  New users  New items  Known interactions    Train  \\\n",
      "0 2021-08-01 2021-08-08      53408        174                   0  4203885   \n",
      "1 2021-08-08 2021-08-15      54662        152                   0  4587708   \n",
      "2 2021-08-15 2021-08-22      56014        114                   0  4985269   \n",
      "\n",
      "     Test  \n",
      "0  264039  \n",
      "1  276699  \n",
      "2  297228  \n"
     ]
    }
   ],
   "source": [
    "# Настройка фолдов\n",
    "last_date = interactions_df['last_watch_dt'].max().normalize()\n",
    "folds = 3\n",
    "start_date = last_date - pd.Timedelta(days=folds * 7)\n",
    "cv = TimeRangeSplit(start_date=start_date, periods=folds + 1, freq='W')\n",
    "folds_with_stats = list(cv.split(interactions_df, fold_stats=True))\n",
    "folds_info = pd.DataFrame([info for _, _, info in folds_with_stats])\n",
    "print(folds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_inv_mapping = dict(enumerate(interactions_df['user_id'].unique()))\n",
    "users_mapping = {v: k for k, v in users_inv_mapping.items()}\n",
    "items_inv_mapping = dict(enumerate(interactions_df['item_id'].unique()))\n",
    "items_mapping = {v: k for k, v in items_inv_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'MostPop': {'days': [7, 30], 'trend_weight': [0.3, 0.5], 'seasonal_weight': [0.2, 0.4]},\n",
    "    'UserKNN': {'K': [5, 15], 'weight_decay': [0.8, 0.9]},\n",
    "    'EASE': {'regularization': [100, 500]},\n",
    "    'SASRec': {\n",
    "        'n_factors': [64, 128],       \n",
    "        'n_blocks': [1, 2],          \n",
    "        'dropout_rate': [0.1, 0.3],   \n",
    "        'session_max_len': [50, 100], \n",
    "        'lr': [0.001, 0.0005]         \n",
    "    },\n",
    "    'CatBoost': {'depth': [4, 6], 'iterations': [100, 200]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, \"recommendation_training.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 10\n",
    "results = []\n",
    "last_date = interactions_df['last_watch_dt'].max()\n",
    "cv = TimeRangeSplit(start_date=last_date - pd.Timedelta(days=21), periods=4)\n",
    "folds = list(cv.split(interactions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 12:58:58,521 - INFO - Starting fold 1/3\n",
      "2025-03-05 12:59:00,844 - INFO - Fold 1: Dataset constructed with 4203885 training interactions\n",
      "2025-03-05 12:59:00,845 - INFO - Fold 1: Training MostPop with params {'days': 7, 'seasonal_weight': 0.2, 'trend_weight': 0.3}\n",
      "2025-03-05 12:59:00,882 - INFO - Top 10 items in popularity: [10440, 9728, 15297, 10813, 13070, 3734, 13865, 11863, 6743, 4631]\n",
      "2025-03-05 12:59:01,127 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 13:03:08,648 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.41348375592528547, 'Novelty@10': 7.304948853882878, 'Diversity@10': 0.31934903208890214, 'Serendipity@10': 0.4}\n",
      "2025-03-05 13:03:08,650 - INFO - Fold 1: Training MostPop with params {'days': 7, 'seasonal_weight': 0.2, 'trend_weight': 0.5}\n",
      "2025-03-05 13:03:08,671 - INFO - Top 10 items in popularity: [10813, 13070, 6743, 9052, 9728, 10440, 15297, 2099, 9354, 4631]\n",
      "2025-03-05 13:03:08,901 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 13:07:18,261 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.34234706196653064, 'Novelty@10': 10.397759583276924, 'Diversity@10': 0.23833526444560785, 'Serendipity@10': 0.7}\n",
      "2025-03-05 13:07:18,262 - INFO - Fold 1: Training MostPop with params {'days': 7, 'seasonal_weight': 0.4, 'trend_weight': 0.3}\n",
      "2025-03-05 13:07:18,357 - INFO - Top 10 items in popularity: [4631, 9728, 10440, 10813, 13070, 15297, 6743, 8636, 9052, 14763]\n",
      "2025-03-05 13:07:18,614 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 13:11:26,874 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.3679664866421491, 'Novelty@10': 9.477902082464498, 'Diversity@10': 0.23392650802720863, 'Serendipity@10': 0.5999999999999998}\n",
      "2025-03-05 13:11:26,876 - INFO - Fold 1: Training MostPop with params {'days': 7, 'seasonal_weight': 0.4, 'trend_weight': 0.5}\n",
      "2025-03-05 13:11:26,898 - INFO - Top 10 items in popularity: [10813, 13070, 4631, 6743, 9052, 2099, 9354, 8636, 14763, 14688]\n",
      "2025-03-05 13:11:27,154 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 13:15:35,051 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.33066717005825624, 'Novelty@10': 12.879074964289432, 'Diversity@10': 0.030603139059415585, 'Serendipity@10': 0.9000000000000002}\n",
      "2025-03-05 13:15:35,052 - INFO - Fold 1: Training MostPop with params {'days': 30, 'seasonal_weight': 0.2, 'trend_weight': 0.3}\n",
      "2025-03-05 13:15:35,083 - INFO - Top 10 items in popularity: [15297, 9728, 10440, 10813, 13070, 13865, 6743, 3734, 5601, 4631]\n",
      "2025-03-05 13:15:35,331 - INFO - Intersection between test and recs: 9 items\n",
      "2025-03-05 13:19:42,827 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.4078028880123428, 'Novelty@10': 8.36836189119221, 'Diversity@10': 0.33073652491633615, 'Serendipity@10': 0.5}\n",
      "2025-03-05 13:19:42,829 - INFO - Fold 1: Training MostPop with params {'days': 30, 'seasonal_weight': 0.2, 'trend_weight': 0.5}\n",
      "2025-03-05 13:19:42,868 - INFO - Top 10 items in popularity: [10813, 13070, 6743, 9052, 15297, 2099, 11308, 9354, 5601, 9728]\n",
      "2025-03-05 13:19:43,099 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 13:23:50,922 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.3355534438412356, 'Novelty@10': 12.077022053026637, 'Diversity@10': 0.10167571397042763, 'Serendipity@10': 0.8}\n",
      "2025-03-05 13:23:50,923 - INFO - Fold 1: Training MostPop with params {'days': 30, 'seasonal_weight': 0.4, 'trend_weight': 0.3}\n",
      "2025-03-05 13:23:50,966 - INFO - Top 10 items in popularity: [5601, 4631, 15297, 10813, 13070, 6743, 2045, 8636, 9728, 10440]\n",
      "2025-03-05 13:23:51,204 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 13:27:58,383 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.34215151083660206, 'Novelty@10': 9.845144616661646, 'Diversity@10': 0.22462887399491469, 'Serendipity@10': 0.5999999999999998}\n",
      "2025-03-05 13:27:58,384 - INFO - Fold 1: Training MostPop with params {'days': 30, 'seasonal_weight': 0.4, 'trend_weight': 0.5}\n",
      "2025-03-05 13:27:58,418 - INFO - Top 10 items in popularity: [10813, 13070, 5601, 4631, 6743, 9052, 2099, 11308, 9354, 2045]\n",
      "2025-03-05 13:27:58,655 - INFO - Intersection between test and recs: 7 items\n",
      "2025-03-05 13:50:16,882 - INFO - Fold 1: MostPop metrics computed - {'MAP@10': 0.36305409092855506, 'Novelty@10': 14.717605794014759, 'Diversity@10': 0.031745988877800335, 'Serendipity@10': 1.0}\n",
      "2025-03-05 13:50:16,884 - INFO - Starting fold 2/3\n",
      "2025-03-05 13:50:19,107 - INFO - Fold 2: Dataset constructed with 4587708 training interactions\n",
      "2025-03-05 13:50:19,107 - INFO - Fold 2: Training MostPop with params {'days': 7, 'seasonal_weight': 0.2, 'trend_weight': 0.3}\n",
      "2025-03-05 13:50:19,143 - INFO - Top 10 items in popularity: [10440, 15297, 9728, 10813, 13070, 6743, 4631, 7793, 13865, 9052]\n",
      "2025-03-05 13:50:19,396 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 13:54:36,509 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.3923959609550328, 'Novelty@10': 8.399547330637148, 'Diversity@10': 0.354181614316269, 'Serendipity@10': 0.5}\n",
      "2025-03-05 13:54:36,510 - INFO - Fold 2: Training MostPop with params {'days': 7, 'seasonal_weight': 0.2, 'trend_weight': 0.5}\n",
      "2025-03-05 13:54:36,533 - INFO - Top 10 items in popularity: [10813, 13070, 6743, 9052, 10440, 7055, 9728, 2099, 15297, 9354]\n",
      "2025-03-05 13:54:36,779 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 13:58:53,784 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.36536916389503044, 'Novelty@10': 10.634860661517822, 'Diversity@10': 0.24347042429734336, 'Serendipity@10': 0.7000000000000001}\n",
      "2025-03-05 13:58:53,785 - INFO - Fold 2: Training MostPop with params {'days': 7, 'seasonal_weight': 0.4, 'trend_weight': 0.3}\n",
      "2025-03-05 13:58:53,808 - INFO - Top 10 items in popularity: [4631, 10440, 10813, 13070, 9728, 15297, 6743, 8636, 1691, 9052]\n",
      "2025-03-05 13:58:54,051 - INFO - Intersection between test and recs: 9 items\n",
      "2025-03-05 14:03:10,986 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.3509454613159782, 'Novelty@10': 9.458844450540694, 'Diversity@10': 0.2347658029584072, 'Serendipity@10': 0.5999999999999998}\n",
      "2025-03-05 14:03:10,988 - INFO - Fold 2: Training MostPop with params {'days': 7, 'seasonal_weight': 0.4, 'trend_weight': 0.5}\n",
      "2025-03-05 14:03:11,010 - INFO - Top 10 items in popularity: [10813, 13070, 4631, 6743, 9052, 7055, 2099, 9354, 5730, 8636]\n",
      "2025-03-05 14:03:11,252 - INFO - Intersection between test and recs: 7 items\n",
      "2025-03-05 14:07:28,289 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.32634829012071176, 'Novelty@10': 13.421806261563402, 'Diversity@10': 0.03112467964952026, 'Serendipity@10': 0.9000000000000002}\n",
      "2025-03-05 14:07:28,291 - INFO - Fold 2: Training MostPop with params {'days': 30, 'seasonal_weight': 0.2, 'trend_weight': 0.3}\n",
      "2025-03-05 14:07:28,324 - INFO - Top 10 items in popularity: [15297, 10440, 9728, 10813, 13070, 13865, 3734, 6743, 12192, 4631]\n",
      "2025-03-05 14:07:28,573 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 14:11:46,000 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.3963292884132734, 'Novelty@10': 7.101661608006992, 'Diversity@10': 0.3782537799951661, 'Serendipity@10': 0.4}\n",
      "2025-03-05 14:11:46,001 - INFO - Fold 2: Training MostPop with params {'days': 30, 'seasonal_weight': 0.2, 'trend_weight': 0.5}\n",
      "2025-03-05 14:11:46,036 - INFO - Top 10 items in popularity: [10813, 13070, 6743, 9052, 15297, 10440, 9728, 7055, 2099, 11308]\n",
      "2025-03-05 14:11:46,296 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 14:16:03,212 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.36776837635173004, 'Novelty@10': 10.804904633331931, 'Diversity@10': 0.24348262391206624, 'Serendipity@10': 0.7000000000000001}\n",
      "2025-03-05 14:16:03,213 - INFO - Fold 2: Training MostPop with params {'days': 30, 'seasonal_weight': 0.4, 'trend_weight': 0.3}\n",
      "2025-03-05 14:16:03,259 - INFO - Top 10 items in popularity: [4631, 15297, 10813, 13070, 10440, 9728, 6743, 8636, 2045, 1691]\n",
      "2025-03-05 14:16:03,505 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 14:20:20,506 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.42089868710951234, 'Novelty@10': 9.630465153940634, 'Diversity@10': 0.21936985523652283, 'Serendipity@10': 0.5999999999999998}\n",
      "2025-03-05 14:20:20,507 - INFO - Fold 2: Training MostPop with params {'days': 30, 'seasonal_weight': 0.4, 'trend_weight': 0.5}\n",
      "2025-03-05 14:20:20,543 - INFO - Top 10 items in popularity: [10813, 13070, 4631, 6743, 9052, 7055, 2099, 11308, 9354, 2045]\n",
      "2025-03-05 14:20:20,788 - INFO - Intersection between test and recs: 6 items\n",
      "2025-03-05 14:24:38,211 - INFO - Fold 2: MostPop metrics computed - {'MAP@10': 0.4483671187516571, 'Novelty@10': 14.739597151194205, 'Diversity@10': 0.03240795525322282, 'Serendipity@10': 1.0}\n",
      "2025-03-05 14:24:38,212 - INFO - Starting fold 3/3\n",
      "2025-03-05 14:24:40,486 - INFO - Fold 3: Dataset constructed with 4985269 training interactions\n",
      "2025-03-05 14:24:40,487 - INFO - Fold 3: Training MostPop with params {'days': 7, 'seasonal_weight': 0.2, 'trend_weight': 0.3}\n",
      "2025-03-05 14:24:40,510 - INFO - Top 10 items in popularity: [10440, 9728, 15297, 10813, 13070, 6743, 13865, 4631, 7793, 12192]\n",
      "2025-03-05 14:24:40,783 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 14:29:16,707 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.39965895335309626, 'Novelty@10': 7.280771868805217, 'Diversity@10': 0.3922898979957585, 'Serendipity@10': 0.4000000000000001}\n",
      "2025-03-05 14:29:16,709 - INFO - Fold 3: Training MostPop with params {'days': 7, 'seasonal_weight': 0.2, 'trend_weight': 0.5}\n",
      "2025-03-05 14:29:16,733 - INFO - Top 10 items in popularity: [10813, 13070, 6743, 9052, 10440, 9728, 2099, 15297, 4631, 14688]\n",
      "2025-03-05 14:29:17,000 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 14:33:52,700 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.3404696964295351, 'Novelty@10': 10.067758158576893, 'Diversity@10': 0.23826524827521475, 'Serendipity@10': 0.7}\n",
      "2025-03-05 14:33:52,701 - INFO - Fold 3: Training MostPop with params {'days': 7, 'seasonal_weight': 0.4, 'trend_weight': 0.3}\n",
      "2025-03-05 14:33:52,724 - INFO - Top 10 items in popularity: [4631, 10440, 10813, 13070, 9728, 15297, 6743, 8636, 2045, 9052]\n",
      "2025-03-05 14:33:52,993 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 14:38:28,635 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.355263394559765, 'Novelty@10': 9.5585661332494, 'Diversity@10': 0.22836506135897847, 'Serendipity@10': 0.5999999999999998}\n",
      "2025-03-05 14:38:28,636 - INFO - Fold 3: Training MostPop with params {'days': 7, 'seasonal_weight': 0.4, 'trend_weight': 0.5}\n",
      "2025-03-05 14:38:28,660 - INFO - Top 10 items in popularity: [10813, 13070, 4631, 6743, 9052, 2099, 2045, 8636, 16290, 14763]\n",
      "2025-03-05 14:38:28,942 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 14:43:04,042 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.4041637925674575, 'Novelty@10': 13.526171865541246, 'Diversity@10': 0.048240065461802706, 'Serendipity@10': 0.9000000000000001}\n",
      "2025-03-05 14:43:04,043 - INFO - Fold 3: Training MostPop with params {'days': 30, 'seasonal_weight': 0.2, 'trend_weight': 0.3}\n",
      "2025-03-05 14:43:04,084 - INFO - Top 10 items in popularity: [10440, 15297, 9728, 10813, 13070, 13865, 6743, 3734, 4631, 4151]\n",
      "2025-03-05 14:43:04,386 - INFO - Intersection between test and recs: 10 items\n",
      "2025-03-05 14:47:40,052 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.4000449302951322, 'Novelty@10': 6.909237405220993, 'Diversity@10': 0.32054899659204145, 'Serendipity@10': 0.4000000000000001}\n",
      "2025-03-05 14:47:40,054 - INFO - Fold 3: Training MostPop with params {'days': 30, 'seasonal_weight': 0.2, 'trend_weight': 0.5}\n",
      "2025-03-05 14:47:40,090 - INFO - Top 10 items in popularity: [10813, 13070, 6743, 9052, 10440, 9728, 15297, 7055, 2099, 11308]\n",
      "2025-03-05 14:47:40,358 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 14:52:15,819 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.37304311006130564, 'Novelty@10': 10.778904332562663, 'Diversity@10': 0.24348262391206624, 'Serendipity@10': 0.7}\n",
      "2025-03-05 14:52:15,820 - INFO - Fold 3: Training MostPop with params {'days': 30, 'seasonal_weight': 0.4, 'trend_weight': 0.3}\n",
      "2025-03-05 14:52:15,856 - INFO - Top 10 items in popularity: [4631, 10440, 10813, 13070, 15297, 9728, 6743, 8636, 2045, 1691]\n",
      "2025-03-05 14:52:16,128 - INFO - Intersection between test and recs: 9 items\n",
      "2025-03-05 14:56:51,710 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.38673775775123365, 'Novelty@10': 9.603817353719155, 'Diversity@10': 0.2193698552365228, 'Serendipity@10': 0.5999999999999998}\n",
      "2025-03-05 14:56:51,711 - INFO - Fold 3: Training MostPop with params {'days': 30, 'seasonal_weight': 0.4, 'trend_weight': 0.5}\n",
      "2025-03-05 14:56:51,748 - INFO - Top 10 items in popularity: [10813, 13070, 4631, 6743, 9052, 7055, 2099, 11308, 9354, 2045]\n",
      "2025-03-05 14:56:52,020 - INFO - Intersection between test and recs: 8 items\n",
      "2025-03-05 15:01:27,802 - INFO - Fold 3: MostPop metrics computed - {'MAP@10': 0.3307565005916604, 'Novelty@10': 14.725651402673375, 'Diversity@10': 0.03240795525322282, 'Serendipity@10': 1.0}\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, test_idx, _) in enumerate(folds):\n",
    "    logger.info(f\"Starting fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    train = interactions_df.loc[train_idx]\n",
    "    test = interactions_df.loc[test_idx]\n",
    "    train_mat = get_coo_matrix(train, weight_col='watched_pct', users_mapping=users_mapping, items_mapping=items_mapping)\n",
    "\n",
    "    train_for_rectools = train.rename(columns={'last_watch_dt': 'datetime', 'watched_pct': 'weight'})\n",
    "    dataset = Dataset.construct(train_for_rectools)\n",
    "    logger.info(f\"Fold {fold_idx + 1}: Dataset constructed with {len(train)} training interactions\")\n",
    "\n",
    "    # MostPop\n",
    "    for params in ParameterGrid(param_grid['MostPop']):\n",
    "        logger.info(f\"Fold {fold_idx + 1}: Training MostPop with params {params}\")\n",
    "        model = CustomMostPop(**params)\n",
    "        model.fit(train, items_df)\n",
    "        recs = model.recommend(test['user_id'].unique(), N=top_N)\n",
    "        recs['rank'] = recs.groupby('user_id').cumcount() + 1\n",
    "        recs = recs[['user_id', 'item_id', 'rank']].dropna()\n",
    "        metrics = compute_metrics(train, test, recs, top_N, items_df)\n",
    "        logger.info(f\"Fold {fold_idx + 1}: MostPop metrics computed - {metrics.to_dict()}\")\n",
    "        results.append({'Model': 'MostPop', **params, **metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:15:12,794 - INFO - Starting fold 1/3\n",
      "2025-03-05 17:15:14,882 - INFO - Fold 1: Dataset constructed with 4203885 training interactions\n",
      "2025-03-05 17:15:14,882 - INFO - Fold 1: Training UserKNN with params {'K': 5, 'weight_decay': 0.8}\n",
      "Fold 1 recommendations: 100%|██████████| 99/99 [16:50<00:00, 10.21s/it]\n",
      "2025-03-05 17:32:05,953 - INFO - Intersection between test and recs: 4812 items\n",
      "2025-03-05 17:36:43,054 - INFO - Fold 1: UserKNN metrics computed - {'MAP@10': 0.33735213435741784, 'Novelty@10': 6.741157900714856, 'Diversity@10': 0.23717626901675748, 'Serendipity@10': 0.42635874121927647}\n",
      "2025-03-05 17:36:43,055 - INFO - Fold 1: Training UserKNN with params {'K': 5, 'weight_decay': 0.9}\n",
      "Fold 1 recommendations: 100%|██████████| 99/99 [16:46<00:00, 10.17s/it]\n",
      "2025-03-05 17:53:29,834 - INFO - Intersection between test and recs: 4812 items\n",
      "2025-03-05 17:58:07,012 - INFO - Fold 1: UserKNN metrics computed - {'MAP@10': 0.33735388874338273, 'Novelty@10': 6.741157096804505, 'Diversity@10': 0.23717585579158793, 'Serendipity@10': 0.4263577224847485}\n",
      "2025-03-05 17:58:07,012 - INFO - Fold 1: Training UserKNN with params {'K': 15, 'weight_decay': 0.8}\n",
      "Fold 1 recommendations: 100%|██████████| 99/99 [16:58<00:00, 10.29s/it]\n",
      "2025-03-05 18:15:05,705 - INFO - Intersection between test and recs: 4791 items\n",
      "2025-03-05 18:20:43,434 - INFO - Fold 1: UserKNN metrics computed - {'MAP@10': 0.34192443461110117, 'Novelty@10': 6.623919797995303, 'Diversity@10': 0.23921986726194525, 'Serendipity@10': 0.399923081500541}\n",
      "2025-03-05 18:20:43,435 - INFO - Fold 1: Training UserKNN with params {'K': 15, 'weight_decay': 0.9}\n",
      "Fold 1 recommendations:   0%|          | 0/99 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m     batch_users = test_users[i:i + batch_size]\n\u001b[32m     23\u001b[39m     batch_recs = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m: batch_users})\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     batch_recs[\u001b[33m'\u001b[39m\u001b[33mitem_id\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecommend_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_users\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems_inv_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_N\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     recs_list.append(batch_recs)\n\u001b[32m     27\u001b[39m recs = pd.concat(recs_list, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mCustomUserKNN.recommend_batch\u001b[39m\u001b[34m(self, user_ids, users_mapping, items_inv_mapping, N)\u001b[39m\n\u001b[32m     22\u001b[39m valid_user_ids = user_ids[valid_mask]\n\u001b[32m     24\u001b[39m user_vectors = \u001b[38;5;28mself\u001b[39m.interaction_matrix[valid_user_indices]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m similarities = (\u001b[43muser_vectors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minteraction_matrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m               \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser_norms\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_user_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43muser_norms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m).toarray()\n\u001b[32m     29\u001b[39m similarities[np.arange(\u001b[38;5;28mlen\u001b[39m(valid_user_indices)), valid_user_indices] = -\u001b[32m1\u001b[39m\n\u001b[32m     31\u001b[39m top_users = np.argpartition(-similarities, \u001b[38;5;28mself\u001b[39m.K, axis=\u001b[32m1\u001b[39m)[:, :\u001b[38;5;28mself\u001b[39m.K]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/scipy/sparse/_base.py:735\u001b[39m, in \u001b[36m_spbase.__truediv__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__truediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_divide\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_divide\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/scipy/sparse/_base.py:716\u001b[39m, in \u001b[36m_spbase._divide\u001b[39m\u001b[34m(self, other, true_divide, rdivide)\u001b[39m\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    715\u001b[39m         recip = np.divide(\u001b[32m1.\u001b[39m, other)\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    718\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m true_divide:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/scipy/sparse/_compressed.py:439\u001b[39m, in \u001b[36m_cs_matrix.multiply\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Matching shapes.\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shape == other.shape:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m[\u001b[49m\u001b[43mret\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mret\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;66;03m# Sparse row vector times...\u001b[39;00m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shape[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, test_idx, _) in enumerate(folds):\n",
    "    logger.info(f\"Starting fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    train = interactions_df.loc[train_idx]\n",
    "    test = interactions_df.loc[test_idx]\n",
    "    train_mat = get_coo_matrix(train, weight_col='watched_pct', users_mapping=users_mapping, items_mapping=items_mapping)\n",
    "\n",
    "    train_for_rectools = train.rename(columns={'last_watch_dt': 'datetime', 'watched_pct': 'weight'})\n",
    "    dataset = Dataset.construct(train_for_rectools)\n",
    "    logger.info(f\"Fold {fold_idx + 1}: Dataset constructed with {len(train)} training interactions\")\n",
    "\n",
    "    # UserKNN\n",
    "    for params in ParameterGrid(param_grid['UserKNN']):\n",
    "        logger.info(f\"Fold {fold_idx + 1}: Training UserKNN with params {params}\")\n",
    "        model = CustomUserKNN(**params)\n",
    "        model.fit(train_mat)\n",
    "        \n",
    "        batch_size = 1000\n",
    "        test_users = test['user_id'].unique()\n",
    "        recs_list = []\n",
    "        for i in tqdm(range(0, len(test_users), batch_size), desc=f\"Fold {fold_idx + 1} recommendations\"):\n",
    "            batch_users = test_users[i:i + batch_size]\n",
    "            batch_recs = pd.DataFrame({'user_id': batch_users})\n",
    "            batch_recs['item_id'] = model.recommend_batch(batch_users, users_mapping, items_inv_mapping, N=top_N)\n",
    "            recs_list.append(batch_recs)\n",
    "        \n",
    "        recs = pd.concat(recs_list, ignore_index=True)\n",
    "        \n",
    "        recs = recs.explode('item_id').reset_index(drop=True)\n",
    "        recs['rank'] = recs.groupby('user_id').cumcount() + 1\n",
    "        recs = recs[['user_id', 'item_id', 'rank']].dropna()\n",
    "        \n",
    "        metrics = compute_metrics(train, test, recs, top_N, items_df)\n",
    "        logger.info(f\"Fold {fold_idx + 1}: UserKNN metrics computed - {metrics.to_dict()}\")\n",
    "        results.append({'Model': 'UserKNN', **params, **metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 18:29:15,107 - INFO - Starting fold 1/3\n",
      "2025-03-05 18:29:17,397 - INFO - Fold 1: Dataset constructed with 4203885 training interactions\n",
      "2025-03-05 18:29:17,397 - INFO - Fold 1: Training EASEModel with params {'regularization': 100}\n",
      "2025-03-05 18:38:43,109 - INFO - Intersection between test and recs: 1398 items\n",
      "2025-03-05 18:39:18,090 - INFO - Fold 1: EASEModel metrics computed - {'MAP@10': 0.32944192999381217, 'Novelty@10': 17.880772904286495, 'Diversity@10': 0.25617116594833667, 'Serendipity@10': 0.9861706787828158}\n",
      "2025-03-05 18:39:18,091 - INFO - Fold 1: Training EASEModel with params {'regularization': 500}\n",
      "2025-03-05 18:48:43,818 - INFO - Intersection between test and recs: 1831 items\n",
      "2025-03-05 18:49:18,935 - INFO - Fold 1: EASEModel metrics computed - {'MAP@10': 0.32989102702892026, 'Novelty@10': 17.654285426355514, 'Diversity@10': 0.24108197145563234, 'Serendipity@10': 0.986169660048288}\n",
      "2025-03-05 18:49:18,936 - INFO - Starting fold 2/3\n",
      "2025-03-05 18:49:21,365 - INFO - Fold 2: Dataset constructed with 4587708 training interactions\n",
      "2025-03-05 18:49:21,366 - INFO - Fold 2: Training EASEModel with params {'regularization': 100}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m model = EASEModel(**params)\n\u001b[32m     16\u001b[39m model.fit(dataset)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m recs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_viewed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m recs[\u001b[33m'\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m'\u001b[39m] = recs.groupby(\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m).cumcount() + \u001b[32m1\u001b[39m\n\u001b[32m     19\u001b[39m recs = recs[[\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mitem_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m'\u001b[39m]].dropna()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/rectools/models/base.py:457\u001b[39m, in \u001b[36mModelBase.recommend\u001b[39m\u001b[34m(self, users, dataset, k, filter_viewed, items_to_recommend, add_rank_col, on_unsupported_targets)\u001b[39m\n\u001b[32m    454\u001b[39m reco_cold = \u001b[38;5;28mself\u001b[39m._init_semi_internal_reco_triplet()\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hot_user_ids.size > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     reco_hot = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recommend_u2i\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot_user_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_viewed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_item_ids_to_recommend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m warm_user_ids.size > \u001b[32m0\u001b[39m:\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recommends_for_warm:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/rectools/models/ease.py:154\u001b[39m, in \u001b[36mEASEModel._recommend_u2i\u001b[39m\u001b[34m(self, user_ids, dataset, k, filter_viewed, sorted_item_ids_to_recommend)\u001b[39m\n\u001b[32m    144\u001b[39m ranker = ImplicitRanker(\n\u001b[32m    145\u001b[39m     distance=Distance.DOT,\n\u001b[32m    146\u001b[39m     subjects_factors=user_items,\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m     num_threads=\u001b[38;5;28mself\u001b[39m.recommend_n_threads,\n\u001b[32m    150\u001b[39m )\n\u001b[32m    152\u001b[39m ui_csr_for_filter = user_items[user_ids] \u001b[38;5;28;01mif\u001b[39;00m filter_viewed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m all_user_ids, all_reco_ids, all_scores = \u001b[43mranker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubject_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_pairs_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mui_csr_for_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43msorted_object_whitelist\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorted_item_ids_to_recommend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_user_ids, all_reco_ids, all_scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/rectools/models/rank/rank_implicit.py:264\u001b[39m, in \u001b[36mImplicitRanker.rank\u001b[39m\u001b[34m(self, subject_ids, k, filter_pairs_csr, sorted_object_whitelist)\u001b[39m\n\u001b[32m    256\u001b[39m     ids, scores = \u001b[38;5;28mself\u001b[39m._rank_on_gpu(\n\u001b[32m    257\u001b[39m         object_factors=object_factors,\n\u001b[32m    258\u001b[39m         subject_factors=subject_factors,\n\u001b[32m   (...)\u001b[39m\u001b[32m    261\u001b[39m         filter_query_items=filter_query_items,\n\u001b[32m    262\u001b[39m     )\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     ids, scores = \u001b[43mimplicit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=c-extension-no-member\u001b[39;49;00m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_factors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubject_factors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreal_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mitem_norms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobject_norms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# query norms for COSINE distance are applied afterwards\u001b[39;49;00m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_query_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_query_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# queries x objects csr matrix for getting neginf scores\u001b[39;49;00m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_items\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# rectools doesn't support blacklist for now\u001b[39;49;00m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sorted_object_whitelist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    275\u001b[39m     ids = sorted_object_whitelist[ids]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtopk.pyx:41\u001b[39m, in \u001b[36mimplicit.cpu.topk.topk\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mtopk.pyx:47\u001b[39m, in \u001b[36mimplicit.cpu.topk._topk_batch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/scipy/sparse/_base.py:465\u001b[39m, in \u001b[36m_spbase.dot\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m * other\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/scipy/sparse/_base.py:678\u001b[39m, in \u001b[36m_spbase.__matmul__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    676\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScalar operands are not allowed, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    677\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33muse \u001b[39m\u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m\u001b[33m instead\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/scipy/sparse/_base.py:580\u001b[39m, in \u001b[36m_spbase._mul_dispatch\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    578\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mul_vector(other.ravel()).reshape(M, \u001b[32m1\u001b[39m)\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m other.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other.shape[\u001b[32m0\u001b[39m] == N:\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mul_multivector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mul_scalar(other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Учеба/data_analysis_and_information_retrieval/lab2/.venv/lib/python3.11/site-packages/scipy/sparse/_compressed.py:508\u001b[39m, in \u001b[36m_cs_matrix._mul_multivector\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[32m    506\u001b[39m fn = \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m.format + \u001b[33m'\u001b[39m\u001b[33m_matvecs\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    507\u001b[39m fn(M, N, n_vecs, \u001b[38;5;28mself\u001b[39m.indptr, \u001b[38;5;28mself\u001b[39m.indices, \u001b[38;5;28mself\u001b[39m.data,\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m    other.ravel(), result.ravel())\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_idx, test_idx, _) in enumerate(folds):\n",
    "    logger.info(f\"Starting fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    train = interactions_df.loc[train_idx]\n",
    "    test = interactions_df.loc[test_idx]\n",
    "    train_mat = get_coo_matrix(train, weight_col='watched_pct', users_mapping=users_mapping, items_mapping=items_mapping)\n",
    "\n",
    "    train_for_rectools = train.rename(columns={'last_watch_dt': 'datetime', 'watched_pct': 'weight'})\n",
    "    dataset = Dataset.construct(train_for_rectools)\n",
    "    logger.info(f\"Fold {fold_idx + 1}: Dataset constructed with {len(train)} training interactions\")\n",
    "\n",
    "    # EASEModel\n",
    "    for params in ParameterGrid(param_grid['EASE']):\n",
    "        logger.info(f\"Fold {fold_idx + 1}: Training EASEModel with params {params}\")\n",
    "        model = EASEModel(**params)\n",
    "        model.fit(dataset)\n",
    "        recs = model.recommend(test['user_id'].unique(), dataset, top_N, filter_viewed=True)\n",
    "        recs['rank'] = recs.groupby('user_id').cumcount() + 1\n",
    "        recs = recs[['user_id', 'item_id', 'rank']].dropna()\n",
    "        metrics = compute_metrics(train, test, recs, top_N, items_df)\n",
    "        logger.info(f\"Fold {fold_idx + 1}: EASEModel metrics computed - {metrics.to_dict()}\")\n",
    "        results.append({'Model': 'EASE', **params, **metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_idx, test_idx, _) in enumerate(folds):\n",
    "    logger.info(f\"Starting fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    train = interactions_df.loc[train_idx]\n",
    "    test = interactions_df.loc[test_idx]\n",
    "    train_mat = get_coo_matrix(train, weight_col='watched_pct', users_mapping=users_mapping, items_mapping=items_mapping)\n",
    "\n",
    "    train_for_rectools = train.rename(columns={'last_watch_dt': 'datetime', 'watched_pct': 'weight'})\n",
    "    dataset = Dataset.construct(train_for_rectools)\n",
    "    logger.info(f\"Fold {fold_idx + 1}: Dataset constructed with {len(train)} training interactions\")\n",
    "\n",
    "    # SASRecModel\n",
    "    for params in ParameterGrid(param_grid['SASRec']):\n",
    "        logger.info(f\"Fold {fold_idx + 1}: Training SASRecModel with params {params}\")\n",
    "        model = SASRecModel(\n",
    "            n_factors=params['n_factors'],        \n",
    "            n_blocks=params['n_blocks'],\n",
    "            session_max_len=params['session_max_len'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            lr=params['lr'],\n",
    "            n_heads=4,                           \n",
    "            batch_size=128,                      \n",
    "            epochs=3,                             \n",
    "            device='cpu'                         \n",
    "        )\n",
    "        model.fit(dataset)\n",
    "        recs = model.recommend(test['user_id'].unique(), dataset=dataset, k=top_N, filter_viewed=True)\n",
    "        recs['rank'] = recs.groupby('user_id').cumcount() + 1\n",
    "        recs = recs[['user_id', 'item_id', 'rank']].dropna()\n",
    "        metrics = compute_metrics(train, test, recs, top_N, items_df)\n",
    "        logger.info(f\"Fold {fold_idx + 1}: SASRecModel metrics computed - {metrics.to_dict()}\")\n",
    "        results.append({'Model': 'SASRec', **params, **metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_idx, test_idx, _) in enumerate(folds):\n",
    "    logger.info(f\"Starting fold {fold_idx + 1}/{len(folds)}\")\n",
    "    \n",
    "    train = interactions_df.loc[train_idx]\n",
    "    test = interactions_df.loc[test_idx]\n",
    "    train_mat = get_coo_matrix(train, weight_col='watched_pct', users_mapping=users_mapping, items_mapping=items_mapping)\n",
    "\n",
    "    train_for_rectools = train.rename(columns={'last_watch_dt': 'datetime', 'watched_pct': 'weight'})\n",
    "    dataset = Dataset.construct(train_for_rectools)\n",
    "    logger.info(f\"Fold {fold_idx + 1}: Dataset constructed with {len(train)} training interactions\")\n",
    "\n",
    "    # CatBoost\n",
    "    X_train, y_train = prepare_catboost_features(train, users_df, items_df)\n",
    "    for params in ParameterGrid(param_grid['CatBoost']):\n",
    "        logger.info(f\"Fold {fold_idx + 1}: Training CatBoost with params {params}\")\n",
    "        model = CatBoostClassifier(learning_rate=0.1, verbose=False, **params)\n",
    "        model.fit(X_train, y_train, cat_features=['favorite_genres', 'genres'])\n",
    "        test_pairs = pd.DataFrame([(u, i) for u in test['user_id'].unique() for i in items_df['item_id'].sample(100)], \n",
    "                                columns=['user_id', 'item_id'])\n",
    "        X_test = test_pairs.merge(users_df, on='user_id').merge(items_df, on='item_id')[X_train.columns]\n",
    "        test_pairs['score'] = model.predict_proba(X_test)[:, 1]\n",
    "        recs = test_pairs.sort_values('score', ascending=False).groupby('user_id').head(top_N)\n",
    "        recs['rank'] = recs.groupby('user_id').cumcount() + 1\n",
    "        recs = recs[['user_id', 'item_id', 'rank']].dropna()\n",
    "        metrics = compute_metrics(train, test, recs, top_N, items_df)\n",
    "        logger.info(f\"Fold {fold_idx + 1}: CatBoost metrics computed - {metrics.to_dict()}\")\n",
    "        results.append({'Model': 'CatBoost', **params, **metrics})\n",
    "\n",
    "    logger.info(f\"Completed fold {fold_idx + 1}/{len(folds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"Средние метрики по моделям:\")\n",
    "print(results_df.groupby('Model').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
